[{"id": "1", "question": {"enus": "A large mobile network operating company is building a machine learning model to predict customers who are likely to unsubscribe from the service. The company plans to offer an incentive for these customers as the cost of churn is far greater than the cost of the incentive. The model produces the following confusion matrix after evaluating on a test dataset of 100 customers: Based on the model evaluation results, why is this a viable model for production? ", "zhcn": "一家大型移动网络运营商正构建机器学习模型，以预测可能取消服务订阅的客户。鉴于客户流失的成本远高于激励措施的成本，该公司计划为这些客户提供激励。该模型在对100名客户的测试数据集进行评估后，生成了如下混淆矩阵：基于模型评估结果，为何该模型是适用于生产环境的可行方案？"}, "option": [{"option_text": {"zhcn": "模型准确率达86%，且公司因假阴性所承担的成本低于假阳性。", "enus": "The model is 86% accurate and the cost incurred by the company as a result of false negatives is less than the false positives."}, "option_flag": true}, {"option_text": {"zhcn": "该模型的precision为86%，低于其accuracy。", "enus": "The precision of the model is 86%, which is less than the accuracy of the model."}, "option_flag": false}, {"option_text": {"zhcn": "模型准确率达86%，且公司因假阳性产生的成本低于因假阴性产生的成本。", "enus": "The model is 86% accurate and the cost incurred by the company as a result of false positives is less than the false negatives."}, "option_flag": false}, {"option_text": {"zhcn": "该模型的precision为86%，高于其accuracy。", "enus": "The precision of the model is 86%, which is greater than the accuracy of the model."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question “A large mobile network operating company is building a machine learning model to predict customers who are likely to unsubscribe from the service... why is this a viable model for production?” is “The model is 86% accurate and the cost incurred by the company as a result of false negatives is less than the false positives.” \n\nThe goal of the model is to predict customers likely to churn so that an incentive can be offered, as the cost of churn (false negatives) is greater than the cost of the incentive. A model is viable when it can minimize the more costly errors. In this case, having fewer false negatives compared to false positives is beneficial because the cost of a customer actually churning (false negative) is higher than giving an incentive to a customer who wasn't going to churn (false positive).\n\nThe options about precision are incorrect because the question focuses on the overall viability of the model in terms of cost - accuracy and the comparison of false positive and false negative costs, not on the relationship between precision and accuracy. The option stating that the cost of false positives is less than false negatives is wrong because it goes against the fact that the cost of churn (false negatives) is far greater than the cost of the incentive (false positives). \n\nSo, the key factors distinguishing the real answer are its alignment with the business goal of minimizing the more costly false negatives, which is crucial for the model's viability in production, unlike the fake options that either focus on an irrelevant metric (precision) or misinterpret the cost relationship.", "zhcn": "针对问题\"某大型移动网络运营商正在构建一个机器学习模型，旨在预测可能流失的客户……为何该模型适合投入生产？\"的正确答案是：\"该模型准确率达到86%，且公司因漏判客户流失（假阴性）所产生的成本低于误判留存客户（假阳性）的成本。\"该模型的核心目标是通过预测潜在流失客户来提供挽留激励，因为客户流失的实际代价（即漏判成本）远高于向本不会流失的客户提供激励的成本。当模型能够有效减少代价更高的错误时，便具备生产可行性。在此案例中，降低漏判率优于降低误判率，因为真实客户流失带来的损失（假阴性）显然高于向非流失客户提供激励的支出（假阳性）。\n\n那些涉及精确率的选项并不适用，因为本题关注的是模型在成本效益层面的整体可行性——即准确率与假阳性/假阴性成本的比较，而非精确率与准确率的关系。而声称\"假阳性成本低于假阴性\"的选项则与事实相悖，毕竟客户流失的实际损失（假阴性）显著高于激励成本（假阳性）。因此，正确答案的关键在于契合企业最小化高代价错误（即假阴性）的商业目标，这正是模型具备生产可行性的决定性因素。其他干扰项要么聚焦无关指标（精确率），要么曲解了成本关系。"}, "answer": "A"}, {"id": "2", "question": {"enus": "A Machine Learning Specialist is designing a system for improving sales for a company. The objective is to use the large amount of information the company has on users' behavior and product preferences to predict which products users would like based on the users' similarity to other users. What should the Specialist do to meet this objective? ", "zhcn": "一位机器学习专家正为某家公司设计一套旨在提升销售业绩的系统。该目标在于，借助公司所掌握的海量用户行为数据与产品偏好信息，通过分析用户与其他用户的相似性，预测用户可能青睐的产品。那么，专家应如何实现这一目标呢？"}, "option": [{"option_text": {"zhcn": "使用Apache Spark ML在Amazon EMR上构建基于内容的过滤推荐引擎", "enus": "Build a content-based filtering recommendation engine with Apache Spark ML on Amazon EMR"}, "option_flag": false}, {"option_text": {"zhcn": "使用Apache Spark ML在Amazon EMR上构建协同过滤推荐引擎。", "enus": "Build a collaborative filtering recommendation engine with Apache Spark ML on Amazon EMR."}, "option_flag": true}, {"option_text": {"zhcn": "使用Apache Spark ML在Amazon EMR上构建基于模型的过滤推荐引擎", "enus": "Build a model-based filtering recommendation engine with Apache Spark ML on Amazon EMR"}, "option_flag": false}, {"option_text": {"zhcn": "\n基于Apache Spark ML在Amazon EMR上构建组合过滤推荐引擎", "enus": "Build a combinative filtering recommendation engine with Apache Spark ML on Amazon EMR"}, "option_flag": false}], "analysis": {"enus": "Many developers want to implement the famous Amazon model that was used to power the People who bought this also bought these items feature on Amazon.com. This model is based on a method called Collaborative Filtering. It takes items such as movies, books, and products that were rated highly by a set of users and recommending them to other users who also gave them high ratings. This method works well in domains where explicit ratings or implicit user actions can be gathered and analyzed. Reference: https://aws.amazon.com/blogs/big-data/building-a-recommendation-engine-with-spark-ml-on-amazon-emr-using-zeppelin/", "zhcn": "众多开发者都渴望实现亚马逊著名的推荐模型，即应用于亚马逊网站的\"购买此商品的顾客也同时购买\"功能。该模型基于名为\"协同过滤\"的方法，其核心机制是：通过收集用户群体高度评价的电影、书籍或商品等数据，再将它们推荐给给予相似高评价的其他用户。这种方法在能够系统收集并分析显性评分或隐性用户行为的领域表现尤为出色。参考链接：https://aws.amazon.com/blogs/big-data/building-a-recommendation-engine-with-spark-ml-on-amazon-emr-using-zeppelin/"}, "answer": "B"}, {"id": "3", "question": {"enus": "A Mobile Network Operator is building an analytics platform to analyze and optimize a company's operations using Amazon Athena and Amazon S3. The source systems send data in .CSV format in real time. The Data Engineering team wants to transform the data to the Apache Parquet format before storing it on Amazon S3. Which solution takes the LEAST effort to implement? ", "zhcn": "\n一家移动网络运营商正利用Amazon Athena和Amazon S3构建分析平台，以分析和优化公司运营。源系统实时以.CSV格式发送数据，数据工程团队希望在将数据存储到Amazon S3之前，将其转换为Apache Parquet格式。那么，哪种方案实现起来最省力？"}, "option": [{"option_text": {"zhcn": "在Amazon EC2实例上使用Apache Kafka Streams导入CSV数据，并借助Kafka Connect S3将数据序列化为Parquet格式。", "enus": "Ingest .CSV data using Apache Kafka Streams on Amazon EC2 instances and use Kafka Connect S3 to serialize data as Parquet"}, "option_flag": false}, {"option_text": {"zhcn": "\n从Amazon Kinesis Data Streams摄入CSV数据，并使用Amazon Glue将其转换为Parquet格式。", "enus": "Ingest .CSV data from Amazon Kinesis Data Streams and use Amazon Glue to convert data into Parquet."}, "option_flag": true}, {"option_text": {"zhcn": "在Amazon EMR集群中使用Apache Spark结构化流导入CSV数据，并借助Apache Spark将数据转换为Parquet格式。", "enus": "Ingest .CSV data using Apache Spark Structured Streaming in an Amazon EMR cluster and use Apache Spark to convert data into  Parquet."}, "option_flag": false}, {"option_text": {"zhcn": "\n从Amazon Kinesis Data Streams接入CSV数据，并借助Amazon Kinesis Data Firehose将其转换为Parquet格式。", "enus": "Ingest .CSV data from Amazon Kinesis Data Streams and use Amazon Kinesis Data Firehose to convert data into Parquet."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question ‘A Mobile Network Operator is building an analytics platform... Which solution takes the LEAST effort to implement?’ is ‘Ingest .CSV data from Amazon Kinesis Data Streams and use Amazon Glue to convert data into Parquet.’ \n\nAmazon Glue is a fully - managed service that simplifies data preparation and ETL (Extract, Transform, Load) processes. It can easily integrate with Amazon Kinesis Data Streams to ingest the real - time CSV data and convert it to Parquet format with minimal configuration and management effort.\n\nThe option of using Apache Kafka Streams on Amazon EC2 instances requires setting up and managing EC2 instances, installing and configuring Apache Kafka, and then using Kafka Connect S3. This involves significant infrastructure management and configuration work.\n\nUsing Apache Spark Structured Streaming in an Amazon EMR cluster also demands managing the EMR cluster, which includes tasks like cluster sizing, software installation, and resource allocation. Additionally, configuring Apache Spark for the data conversion adds to the complexity.\n\nThe option of using Amazon Kinesis Data Firehose to convert data into Parquet is incorrect because Kinesis Data Firehose is mainly designed for loading data into destinations like Amazon S3, Redshift, etc., and has limited data transformation capabilities compared to Amazon Glue. \n\nThe key factor that makes the real answer the least - effort solution is the fully - managed nature of Amazon Glue, which abstracts away the underlying infrastructure management and simplifies the data transformation process, distinguishing it from the more complex fake options.", "zhcn": "针对“某移动网络运营商正在构建分析平台...哪种方案实施难度最低？”这一问题，正确答案是“通过Amazon Kinesis Data Streams摄取CSV数据，并利用Amazon Glue将数据转换为Parquet格式”。Amazon Glue作为全托管服务，极大简化了数据准备和ETL流程。该服务可轻松对接Amazon Kinesis Data Streams，以最小配置管理成本实现实时CSV数据到Parquet格式的转换。\n\n相比之下，若采用Amazon EC2实例部署Apache Kafka Streams方案，则需完成EC2实例的配置管理、Apache Kafka的安装调试及Kafka Connect S3的集成，涉及大量基础设施运维工作。而使用Amazon EMR集群运行Apache Spark Structured Streaming的方案，不仅需要管理EMR集群（包括规模设定、软件安装和资源分配），还需额外配置Apache Spark完成数据转换，复杂度显著提升。\n\n至于采用Amazon Kinesis Data Firehose转换为Parquet的方案并不适用，因该服务主要设计用于将数据加载至S3、Redshift等目标存储，其数据转换能力远逊于Amazon Glue。正确答案的核心优势在于Amazon Glue的全托管特性——它彻底消除了底层设施管理负担，使数据转换流程极致简化，从而在实施效率上显著优于其他复杂方案。"}, "answer": "B"}, {"id": "4", "question": {"enus": "A city wants to monitor its air quality to address the consequences of air pollution. A Machine Learning Specialist needs to forecast the air quality in parts per million of contaminates for the next 2 days in the city. As this is a prototype, only daily data from the last year is available. Which model is MOST likely to provide the best results in Amazon SageMaker? ", "zhcn": "某市希望监测空气质量，以应对空气污染带来的后果。机器学习专家需要预测该市未来两天的空气质量，具体为污染物的百万分比浓度。由于这是一个原型项目，目前仅有过去一年的每日数据可用。在Amazon SageMaker中，哪种模型最有可能提供最佳结果？"}, "option": [{"option_text": {"zhcn": "在由全年数据构成的单个时间序列上，使用Amazon SageMaker的k-近邻（kNN）算法，并将预测器类型设置为回归器。", "enus": "Use the Amazon SageMaker k-Nearest-Neighbors (kNN) algorithm on the single time series consisting of the full year of data with a  predictor_type of regressor."}, "option_flag": false}, {"option_text": {"zhcn": "将Amazon SageMaker随机切割森林（RCF）应用于包含全年数据的单个时间序列。", "enus": "Use Amazon SageMaker Random Cut Forest (RCF) on the single time series consisting of the full year of data."}, "option_flag": false}, {"option_text": {"zhcn": "将Amazon SageMaker Linear Learner算法应用于由全年数据构成的单一时间序列，预测器类型为回归器。", "enus": "Use the Amazon SageMaker Linear Learner algorithm on the single time series consisting of the full year of data with a predictor_type  of regressor."}, "option_flag": true}, {"option_text": {"zhcn": "使用Amazon SageMaker Linear Learner算法，针对由全年数据构成的单一时间序列，并将预测器类型指定为分类器。", "enus": "Use the Amazon SageMaker Linear Learner algorithm on the single time series consisting of the full year of data with a predictor_type  of classifier."}, "option_flag": false}], "analysis": {"enus": "Reference: https://aws.amazon.com/blogs/machine-learning/build-a-model-to-predict-the-impact-of-weather-on-urban-air-quality-using-amazon- sagemaker/? ref=Welcome.AI", "zhcn": "参考来源：https://aws.amazon.com/blogs/machine-learning/build-a-model-to-predict-the-impact-of-weather-on-urban-air-quality-using-amazon- sagemaker/? ref=Welcome.AI"}, "answer": "C"}, {"id": "5", "question": {"enus": "A Data Engineer needs to build a model using a dataset containing customer credit card information How can the Data Engineer ensure the data remains encrypted and the credit card information is secure? ", "zhcn": "数据工程师需要使用包含客户信用卡信息的数据集构建模型，如何确保数据保持加密状态，且信用卡信息得到安全保障？"}, "option": [{"option_text": {"zhcn": "通过自定义加密算法对数据进行加密，并将数据存储在VPC中的Amazon SageMaker实例上。利用SageMaker DeepAR算法对信用卡号码进行随机化处理。", "enus": "Use a custom encryption algorithm to encrypt the data and store the data on an Amazon SageMaker instance in a VPC. Use the  SageMaker DeepAR algorithm to randomize the credit card numbers."}, "option_flag": false}, {"option_text": {"zhcn": "使用IAM策略对Amazon S3存储桶和Amazon Kinesis中的数据进行加密，自动丢弃信用卡号并插入虚假信用卡号。", "enus": "Use an IAM policy to encrypt the data on the Amazon S3 bucket and Amazon Kinesis to automatically discard credit card numbers and  insert fake credit card numbers."}, "option_flag": false}, {"option_text": {"zhcn": "利用Amazon SageMaker启动配置，在数据被复制到VPC中的SageMaker实例后对其进行加密；采用SageMaker主成分分析（PCA）算法，精简信用卡号码的长度。", "enus": "Use an Amazon SageMaker launch configuration to encrypt the data once it is copied to the SageMaker instance in a VPC. Use the  SageMaker principal component analysis (PCA) algorithm to reduce the length of the credit card numbers."}, "option_flag": false}, {"option_text": {"zhcn": "利用AWS KMS对Amazon S3和Amazon SageMaker上的数据进行加密，并借助AWS Glue对客户数据中的信用卡号进行脱敏处理。", "enus": "Use AWS KMS to encrypt the data on Amazon S3 and Amazon SageMaker, and redact the credit card numbers from the customer data  with AWS Glue."}, "option_flag": true}], "analysis": {"enus": "The correct answer to the question ‘A Data Engineer needs to build a model using a dataset containing customer credit card information. How can the Data Engineer ensure the data remains encrypted and the credit card information is secure?’ is ‘Use AWS KMS to encrypt the data on Amazon S3 and Amazon SageMaker, and redact the credit card numbers from the customer data with AWS Glue.’ \n\nAWS Key Management Service (KMS) is a managed service that makes it easy to create and control the encryption keys used to encrypt data. Encrypting data on Amazon S3 and Amazon SageMaker using KMS provides a high - level of security for the dataset. Additionally, redacting credit card numbers with AWS Glue ensures that sensitive information is removed from the data used in the model.\n\nThe first fake option suggests using a custom encryption algorithm. Custom algorithms may not have undergone the same level of security scrutiny as established services like AWS KMS, and randomizing credit card numbers with an algorithm is not a proper way to handle sensitive data. The second fake option uses an IAM policy for encryption, which is incorrect as IAM policies are for access control, not encryption, and inserting fake credit card numbers is not a valid security measure. The third fake option uses a launch configuration for encryption, which is not a standard way to encrypt data, and reducing the length of credit card numbers with PCA does not address the security of the data. These misconceptions lead to the incorrectness of the fake options, while the real answer uses well - established AWS services for encryption and data redaction.", "zhcn": "针对“数据工程师需使用含客户信用卡信息的数据集构建模型，应如何确保数据加密及信用卡信息安全？”这一问题，正确答案是：“通过AWS KMS对Amazon S3和Amazon SageMaker中的数据进行加密，并利用AWS Glue对客户数据中的信用卡号执行脱敏处理。”AWS密钥管理服务（KMS）作为一项托管服务，可便捷地创建并控制用于数据加密的密钥。通过KMS对Amazon S3和Amazon SageMaker中的数据进行加密，能为数据集提供高等级安全防护。此外，借助AWS Glue对信用卡号进行脱敏，可确保模型训练所用的数据中不保留敏感信息。\n\n首个干扰项建议采用自定义加密算法。然而自定义算法未必经过如AWS KMS等成熟服务同等强度的安全验证，且通过算法随机化信用卡号并非处理敏感数据的合规方式。第二个干扰项误用IAM策略进行加密——IAM策略实用于访问控制而非数据加密，而插入虚假信用卡信息亦非有效的安全措施。第三个干扰项试图通过启动配置实现加密，这并非标准的数据加密方法，且使用主成分分析（PCA）缩短信用卡号长度无法从根本上保障数据安全。这些认知偏差导致干扰项均存在谬误，而正确答案则采用了业界成熟的AWS服务实现加密与数据脱敏。"}, "answer": "D"}, {"id": "6", "question": {"enus": "A Machine Learning Specialist is using an Amazon SageMaker notebook instance in a private subnet of a corporate VPC. The ML Specialist has important data stored on the Amazon SageMaker notebook instance's Amazon EBS volume, and needs to take a snapshot of that EBS volume. However, the ML Specialist cannot find the Amazon SageMaker notebook instance's EBS volume or Amazon EC2 instance within the VPC. Why is the ML Specialist not seeing the instance visible in the VPC? ", "zhcn": "\n一位机器学习专家正在企业VPC的私有子网中使用一个Amazon SageMaker笔记本实例。该机器学习专家的重要数据存储在Amazon SageMaker笔记本实例的Amazon EBS卷上，故需要为该EBS卷创建快照。然而，该机器学习专家在VPC内既找不到Amazon SageMaker笔记本实例的EBS卷，也找不到其对应的Amazon EC2实例。为何该机器学习专家在VPC中看不到该实例？"}, "option": [{"option_text": {"zhcn": "\nAmazon SageMaker notebook instances基于客户账户内的EC2实例，但它们运行在VPC之外。", "enus": "Amazon SageMaker notebook instances are based on the EC2 instances within the customer account, but they run outside of VPCs."}, "option_flag": false}, {"option_text": {"zhcn": "Amazon SageMaker notebook 实例构建于客户账户内的 Amazon ECS 服务之上。", "enus": "Amazon SageMaker notebook instances are based on the Amazon ECS service within customer accounts."}, "option_flag": false}, {"option_text": {"zhcn": "Amazon SageMaker 笔记本实例基于运行在 AWS 服务账户内的 EC2 实例构建而成。", "enus": "Amazon SageMaker notebook instances are based on EC2 instances running within AWS service accounts."}, "option_flag": true}, {"option_text": {"zhcn": "Amazon SageMaker notebook 实例基于运行在AWS服务账户内的ECS实例构建而成。", "enus": "Amazon SageMaker notebook instances are based on AWS ECS instances running within AWS service accounts."}, "option_flag": false}], "analysis": {"enus": "Reference: https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html", "zhcn": "参考链接：https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html"}, "answer": "C"}, {"id": "7", "question": {"enus": "A Machine Learning Specialist is building a model that will perform time series forecasting using Amazon SageMaker. The Specialist has finished training the model and is now planning to perform load testing on the endpoint so they can configure Auto Scaling for the model variant. Which approach will allow the Specialist to review the latency, memory utilization, and CPU utilization during the load test? ", "zhcn": "一位机器学习专家正在构建一个利用Amazon SageMaker进行时间序列预测的模型。模型训练完成后，该专家计划对终端节点进行负载测试，以便为模型变体配置自动扩缩容功能。若要在此次负载测试中同步监测延迟、内存利用率及CPU利用率指标，应采用以下哪种方案？"}, "option": [{"option_text": {"zhcn": "借助Amazon Athena与Amazon QuickSight，可实时分析写入Amazon S3的SageMaker日志，并在日志生成过程中实现可视化呈现。", "enus": "Review SageMaker logs that have been written to Amazon S3 by leveraging Amazon Athena and Amazon QuickSight to visualize logs as  they are being produced."}, "option_flag": false}, {"option_text": {"zhcn": "为集中展示亚马逊SageMaker输出的延迟、内存利用率和CPU利用率指标，请生成亚马逊CloudWatch监控看板。", "enus": "Generate an Amazon CloudWatch dashboard to create a single view for the latency, memory utilization, and CPU utilization metrics that  are outputted by Amazon SageMaker."}, "option_flag": true}, {"option_text": {"zhcn": "构建自定义的Amazon CloudWatch日志组，随后运用Amazon ES与Kibana平台，在Amazon SageMaker生成日志数据的同时即可进行实时查询与可视化呈现。", "enus": "Build custom Amazon CloudWatch Logs and then leverage Amazon ES and Kibana to query and visualize the log data as it is generated  by Amazon SageMaker."}, "option_flag": false}, {"option_text": {"zhcn": "将亚马逊SageMaker生成的亚马逊云监控日志发送至亚马逊ES服务，并借助Kibana对日志数据进行查询与可视化分析。", "enus": "Send Amazon CloudWatch Logs that were generated by Amazon SageMaker to Amazon ES and use Kibana to query and visualize the  log data."}, "option_flag": false}], "analysis": {"enus": "Reference: https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html", "zhcn": "参考文档：亚马逊SageMaker监控功能与CloudWatch集成指南  \n（原文链接：https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html）"}, "answer": "B"}, {"id": "8", "question": {"enus": "A manufacturing company has structured and unstructured data stored in an Amazon S3 bucket. A Machine Learning Specialist wants to use SQL to run queries on this data. Which solution requires the LEAST effort to be able to query this data? ", "zhcn": "一家制造公司将其结构化与非结构化数据存储于亚马逊S3存储桶中。机器学习专家需使用SQL语言对此数据进行查询。若要实现数据查询，何种解决方案所需投入精力最少？"}, "option": [{"option_text": {"zhcn": "借助AWS Data Pipeline对数据进行转换处理，并运用Amazon RDS执行查询操作。", "enus": "Use AWS Data Pipeline to transform the data and Amazon RDS to run queries."}, "option_flag": false}, {"option_text": {"zhcn": "利用AWS Glue进行数据编目，再通过Amazon Athena执行查询。", "enus": "Use AWS Glue to catalogue the data and Amazon Athena to run queries."}, "option_flag": true}, {"option_text": {"zhcn": "利用AWS Batch对数据进行ETL处理，并通过Amazon Aurora执行查询操作。", "enus": "Use AWS Batch to run ETL on the data and Amazon Aurora to run the queries."}, "option_flag": false}, {"option_text": {"zhcn": "利用AWS Lambda进行数据转换，并通过Amazon Kinesis Data Analytics执行查询分析。", "enus": "Use AWS Lambda to transform the data and Amazon Kinesis Data Analytics to run queries."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question ‘A manufacturing company has structured and unstructured data stored in an Amazon S3 bucket. A Machine Learning Specialist wants to use SQL to run queries on this data. Which solution requires the LEAST effort to be able to query this data?’ is ‘Use AWS Glue to catalogue the data and Amazon Athena to run queries.’ \n\nAWS Glue can automatically discover and catalog data in S3, and Amazon Athena allows users to run SQL queries directly on S3 data without the need for complex data loading or transformation. This combination provides a server - less and straightforward way to query data.\n\nThe fake options involve more complex processes. Using AWS Data Pipeline to transform data and Amazon RDS to run queries requires setting up and managing an RDS instance and data transformation pipelines, which is time - consuming. AWS Batch for ETL and Amazon Aurora for queries also involves setting up and managing an Aurora database and running batch jobs for ETL. Using AWS Lambda to transform data and Amazon Kinesis Data Analytics to run queries is mainly designed for real - time data streams, not ideal for querying static data in S3, and also requires more development and management effort. \n\nThe simplicity and server - less nature of the AWS Glue and Athena combination are the key factors that make it the least - effort solution, distinguishing it from the more complex fake options.", "zhcn": "针对“某制造公司在Amazon S3存储桶中存有结构化和非结构化数据，一位机器学习专家希望使用SQL查询该数据。哪种方案能以最小工作量实现查询？”这一问题，正确答案是“使用AWS Glue编录数据，并通过Amazon Athena执行查询”。AWS Glue能自动发现并编录S3中的数据，而Amazon Athena可直接对S3数据运行SQL查询，无需复杂的数据加载或转换流程。这种组合提供了无需管理服务器、操作简便的查询方案。\n\n其他干扰选项均涉及更复杂的流程：使用AWS Data Pipeline转换数据并通过Amazon RDS查询，需要配置管理RDS实例及数据转换管道，耗时耗力；采用AWS Batch进行ETL并用Amazon Aurora查询，则需部署Aurora数据库并运行批量ETL任务；而利用AWS Lambda转换数据并结合Amazon Kinesis Data Analytics查询，主要适用于实时数据流场景，对静态S3数据的查询并非其设计初衷，且需更多开发管理投入。正是AWS Glue与Athena组合的简洁性与无服务器特性，使其成为最小工作量的解决方案，与其余复杂选项形成鲜明对比。"}, "answer": "B"}, {"id": "9", "question": {"enus": "A Machine Learning Specialist is developing a custom video recommendation model for an application. The dataset used to train this model is very large with millions of data points and is hosted in an Amazon S3 bucket. The Specialist wants to avoid loading all of this data onto an Amazon SageMaker notebook instance because it would take hours to move and will exceed the attached 5 GB Amazon EBS volume on the notebook instance. Which approach allows the Specialist to use all the data to train the model? ", "zhcn": "一位机器学习专家正在为某应用程序开发定制化视频推荐模型。训练模型所用的数据集包含数百万个数据点，规模极为庞大，目前存储于亚马逊S3存储桶中。由于将所有数据加载到亚马逊SageMaker笔记本实例需耗时数小时，且会超出该实例附加的5GB亚马逊EBS存储容量，专家希望避免此操作。请问采用何种方法可确保专家能够使用全部数据完成模型训练？"}, "option": [{"option_text": {"zhcn": "将数据集的较小子集载入SageMaker笔记本并在本地进行训练。验证训练代码能否正常执行，并确认模型参数设置合理。随后使用S3存储桶中的完整数据集，通过Pipe输入模式启动SageMaker训练任务。", "enus": "Load a smaller subset of the data into the SageMaker notebook and train locally. Confirm that the training code is executing and the  model parameters seem reasonable. Initiate a SageMaker training job using the full dataset from the S3 bucket using Pipe input mode."}, "option_flag": true}, {"option_text": {"zhcn": "在AWS深度学习AMI上启动一台Amazon EC2实例，并将S3存储桶挂载至该实例。先使用少量数据进行训练，以验证训练代码与超参数配置是否恰当。随后返回Amazon SageMaker平台，利用完整数据集完成模型训练。", "enus": "Launch an Amazon EC2 instance with an AWS Deep Learning AMI and attach the S3 bucket to the instance. Train on a small amount of  the data to verify the training code and hyperparameters. Go back to Amazon SageMaker and train using the full dataset"}, "option_flag": false}, {"option_text": {"zhcn": "利用AWS Glue对数据的小规模样本进行模型训练，以验证数据与Amazon SageMaker的兼容性。随后通过Pipe输入模式，调用S3存储桶中的完整数据集启动SageMaker训练任务。", "enus": "Use AWS Glue to train a model using a small subset of the data to confirm that the data will be compatible with Amazon SageMaker.  Initiate a SageMaker training job using the full dataset from the S3 bucket using Pipe input mode."}, "option_flag": false}, {"option_text": {"zhcn": "将数据子集载入SageMaker笔记本进行本地训练，确保代码正常运行且模型参数设置合理。随后启动搭载AWS深度学习镜像的Amazon EC2实例，并挂载S3存储桶以完成全量数据集训练。", "enus": "Load a smaller subset of the data into the SageMaker notebook and train locally. Confirm that the training code is executing and the  model parameters seem reasonable. Launch an Amazon EC2 instance with an AWS Deep Learning AMI and attach the S3 bucket to train  the full dataset."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to load a smaller subset of the data into the SageMaker notebook, train locally, confirm the training code and model parameters, and then initiate a SageMaker training job using the full dataset from the S3 bucket in Pipe input mode. This approach addresses the problem of the large dataset and limited EBS volume on the notebook instance. By training on a small subset locally first, the Specialist can quickly verify the code and parameters. Then, Pipe input mode in SageMaker allows the model to stream data directly from S3 during training, eliminating the need to load the entire dataset onto the instance.\n\nThe first fake option of using an EC2 instance with an AWS Deep Learning AMI to train on a small amount of data and then going back to SageMaker for full - dataset training is inefficient. It involves additional steps of setting up and managing an EC2 instance, which is not necessary when SageMaker can handle the full - dataset training directly.\n\nThe second fake option of using AWS Glue to train on a small subset to check compatibility is unnecessary. AWS Glue is mainly for data integration and ETL, not for model training verification. The same goal can be achieved more simply by training locally on a subset in the SageMaker notebook.\n\nThe third fake option of launching an EC2 instance to train the full dataset after local verification is also over - complicated. It adds the complexity of managing an EC2 instance, while SageMaker can handle the large - scale training more effectively with Pipe input mode. These fake options are likely chosen based on a misunderstanding of the capabilities of SageMaker and the appropriate tools for each step of model development.", "zhcn": "针对该问题的正确解决方案是：先将数据的小规模子集载入SageMaker笔记本进行本地训练，以验证训练代码与模型参数；随后通过Sipe输入模式启动SageMaker训练任务，直接流式读取S3存储桶中的完整数据集。这种方法巧妙化解了笔记本实例因EBS存储容量限制而难以处理大型数据集的问题。通过本地小规模试训练，专家可快速完成代码逻辑与参数调优的验证；而SageMaker的管道输入模式则能在正式训练时直接从S3实时流式传输数据，避免将海量数据加载至本地实例。\n\n首个干扰方案提出采用搭载AWS深度学习AMI的EC2实例进行小规模训练后再回归SageMaker，此方案效率低下。它不仅需要额外配置管理EC2实例，且未能发挥SageMaker直接处理全量数据训练的优势。\n\n第二个干扰方案建议通过AWS Glue验证小规模数据兼容性实属多此一举。AWS Glue核心功能在于数据集成与ETL处理，而非模型训练验证，其目标完全可通过在SageMaker笔记本中进行本地子集训练更简洁地实现。\n\n第三个干扰方案主张在本地验证后启动EC2实例进行全量数据训练，同样使流程复杂化。该方案不仅增加了EC2实例的管理负担，且未能善用SageMaker管道输入模式高效处理大规模训练任务的特性。这些干扰方案的选择，往往源于对SageMaker功能边界及模型开发各阶段适用工具的误解。"}, "answer": "A"}, {"id": "10", "question": {"enus": "A Machine Learning Specialist has completed a proof of concept for a company using a small data sample, and now the Specialist is ready to implement an end- to-end solution in AWS using Amazon SageMaker. The historical training data is stored in Amazon RDS. Which approach should the Specialist use for training a model using that data? ", "zhcn": "一位机器学习专家已利用小样本数据为公司完成了概念验证，现准备基于亚马逊SageMaker在AWS平台上部署端到端解决方案。历史训练数据存储于Amazon RDS数据库中，此时专家应采用何种方案利用该数据训练模型？"}, "option": [{"option_text": {"zhcn": "在笔记本中直接连接SQL数据库并导入数据。", "enus": "Write a direct connection to the SQL database within the notebook and pull data in"}, "option_flag": false}, {"option_text": {"zhcn": "利用AWS数据管道将微软SQL Server中的数据推送至Amazon S3，并在笔记本中提供S3存储路径。", "enus": "Push the data from Microsoft SQL Server to Amazon S3 using an AWS Data Pipeline and provide the S3 location within the notebook."}, "option_flag": true}, {"option_text": {"zhcn": "将数据迁移至Amazon DynamoDB，并在笔记本中建立与DynamoDB的连接以获取数据。", "enus": "Move the data to Amazon DynamoDB and set up a connection to DynamoDB within the notebook to pull data in."}, "option_flag": false}, {"option_text": {"zhcn": "利用AWS DMS服务将数据迁移至Amazon ElastiCache，并在笔记本环境中配置连接以快速获取数据。", "enus": "Move the data to Amazon ElastiCache using AWS DMS and set up a connection within the notebook to pull data in for fast access."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question ‘A Machine Learning Specialist has completed a proof of concept for a company using a small data sample, and now the Specialist is ready to implement an end - to - end solution in AWS using Amazon SageMaker. The historical training data is stored in Amazon RDS. Which approach should the Specialist use for training a model using that data?’ is ‘Push the data from Microsoft SQL Server to Amazon S3 using an AWS Data Pipeline and provide the S3 location within the notebook.’\n\nAmazon SageMaker is well - integrated with Amazon S3, and it natively supports reading data from S3 for model training. Using AWS Data Pipeline to transfer data from Amazon RDS to S3 ensures a smooth and scalable data transfer process. Once the data is in S3, SageMaker can easily access it for training.\n\nThe option of writing a direct connection to the SQL database within the notebook and pulling data in is not ideal because it can lead to performance issues and potential security risks. RDS databases are not optimized for high - volume data retrieval for machine learning training.\n\nMoving the data to Amazon DynamoDB is not a suitable choice as DynamoDB is a NoSQL database designed for different use - cases such as high - performance key - value and document - based data storage. It is not the best fit for storing and retrieving large - scale tabular data used in machine learning.\n\nUsing Amazon ElastiCache, which is an in - memory caching service, is also inappropriate. ElastiCache is mainly used to reduce the load on databases by caching frequently accessed data, not for storing large - scale training data for machine learning models.\n\nIn summary, the integration capabilities and suitability of Amazon S3 for SageMaker training make the option of pushing data to S3 the correct approach, distinguishing it from the other fake options.", "zhcn": "针对“机器学习专家已使用小样本数据完成企业验证性项目，现需基于历史训练数据（存储于Amazon RDS）在AWS平台通过Amazon SageMaker部署端到端解决方案。应如何利用该数据训练模型？”的正确解决方案是：通过AWS数据管道将微软SQL Server中的数据推送至Amazon S3，并在笔记本中指定S3存储路径。\n\nAmazon SageMaker与Amazon S3具有深度集成优势，可直接读取S3中的数据用于模型训练。通过AWS数据管道实现从Amazon RDS到S3的数据传输，既能保障流程顺畅性，又具备弹性扩展能力。数据存入S3后，SageMaker即可高效调用训练资源。\n\n若在笔记本中直接配置SQL数据库连接并提取数据，不仅易引发性能瓶颈，还存在安全隐患。况且RDS数据库本身并未针对机器学习训练所需的大规模数据检索场景进行优化。\n\n将数据迁移至Amazon DynamoDB并非合适之选。作为NoSQL数据库，DynamoDB专为高性能键值对及文档型数据存储场景设计，并不适用于机器学习常用的结构化表格数据的存储与调用。\n\n采用内存缓存服务Amazon ElastiCache亦不恰当。该服务主要通过缓存高频访问数据来减轻数据库压力，而非用于存储机器学习模型所需的大规模训练数据集。\n\n综上，凭借Amazon S3与SageMaker训练功能的无缝协作特性，将数据推送至S3的方案展现出明显的适配优势，使其从其他干扰项中脱颖而出成为正确选择。"}, "answer": "B"}, {"id": "11", "question": {"enus": "A Machine Learning Specialist receives customer data for an online shopping website. The data includes demographics, past visits, and locality information. The Specialist must develop a machine learning approach to identify the customer shopping patterns, preferences, and trends to enhance the website for better service and smart recommendations. Which solution should the Specialist recommend? ", "zhcn": "一位机器学习专家收到了某购物网站提供的客户数据，其中包含用户画像、历史访问记录及地域信息。该专家需要构建一套机器学习方案，用以精准捕捉消费者的购物习惯、偏好倾向与流行趋势，从而优化网站功能，实现智能推荐服务。在此情境下，专家应当提出何种解决方案？"}, "option": [{"option_text": {"zhcn": "针对给定的离散数据集，运用隐含狄利克雷分布模型对客户数据库进行模式识别。", "enus": "Latent Dirichlet Allocation (LDA) for the given collection of discrete data to identify patterns in the customer database."}, "option_flag": false}, {"option_text": {"zhcn": "一个至少包含三层结构、初始权重随机设定的神经网络，用于识别客户数据库中的规律模式。", "enus": "A neural network with a minimum of three layers and random initial weights to identify patterns in the customer database."}, "option_flag": false}, {"option_text": {"zhcn": "基于用户互动与关联性的协同过滤技术，用于识别客户数据库中的行为模式。", "enus": "Collaborative filtering based on user interactions and correlations to identify patterns in the customer database."}, "option_flag": true}, {"option_text": {"zhcn": "对随机子样本应用随机切割森林（RCF）算法，以识别客户数据库中的潜在规律。", "enus": "Random Cut Forest (RCF) over random subsamples to identify patterns in the customer database."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is ‘Collaborative filtering based on user interactions and correlations to identify patterns in the customer database.’ This is because collaborative filtering is well - suited for analyzing customer shopping patterns, preferences, and trends in an online shopping context. It works by finding similarities between users based on their past interactions (such as purchases, clicks) and then making recommendations based on what similar users have liked.\n\n‘Latent Dirichlet Allocation (LDA)’ is mainly used for topic modeling in text data, not for analyzing shopping - related customer data. A common misconception might be thinking that it can handle any discrete data, but it is not designed for this type of customer behavior analysis.\n\nA neural network with three layers and random initial weights is a very general approach. While neural networks can be used for various tasks, they require careful design, large amounts of data, and proper training. Without specific features tailored to shopping data, it may not be the most efficient way to identify shopping patterns. The pitfall here is assuming that a neural network is a one - size - fits - all solution.\n\n‘Random Cut Forest (RCF)’ is typically used for anomaly detection, not for identifying normal shopping patterns, preferences, and trends. One might wrongly choose it thinking it can find patterns in general, but its main application is to detect outliers in data.\n\nIn summary, collaborative filtering's ability to leverage user - to - user and item - to - item correlations makes it the most appropriate choice for this online shopping customer data analysis, distinguishing it from the fake answer options.", "zhcn": "对于本题而言，正确答案应为\"基于用户交互与关联性的协同过滤技术，用于识别客户数据库中的行为模式\"。原因在于，协同过滤特别适用于分析网络购物场景中的客户消费习惯、偏好倾向及消费趋势。该技术通过挖掘用户历史交互数据（如购买记录、点击行为）建立用户相似度模型，进而依据相似用户群体的偏好进行个性化推荐。\n\n潜在狄利克雷分布（LDA）主要应用于文本数据的主题建模，而非购物相关的客户数据分析。常见的误解是认为它能处理任意离散数据，但该算法本身并非为消费者行为分析场景所设计。\n\n具有三层随机初始权重的神经网络属于通用型解决方案。尽管神经网络可胜任多种任务，但其需要精细的结构设计、海量训练数据及严格的参数调优。若缺乏针对购物数据的特征工程优化，此类网络并非识别消费模式的最优方案。此选项的误区在于将神经网络视为万能解决方案。\n\n随机切割森林（RCF）通常用于异常值检测，而非分析常规的购物模式与消费趋势。若误选此方法，可能是因其\"模式发现\"的泛化特性被误解，但该算法的核心功能实为数据异常点识别。\n\n综上所述，协同过滤技术通过有效利用用户间与商品间的关联关系，使其成为在线购物客户数据分析的最适方案，这一特性使其在本题中显著区别于其他干扰选项。"}, "answer": "C"}, {"id": "12", "question": {"enus": "A Machine Learning Specialist is working with a large company to leverage machine learning within its products. The company wants to group its customers into categories based on which customers will and will not churn within the next 6 months. The company has labeled the data available to the Specialist. Which machine learning model type should the Specialist use to accomplish this task? ", "zhcn": "某大型企业正与一位机器学习专家合作，旨在将机器学习技术融入其产品体系。企业希望根据客户在未来六个月内的流失可能性对其进行分类，并已为专家提供了标注好的数据集。为达成此目标，该专家应采用何种机器学习模型类型？"}, "option": [{"option_text": {"zhcn": "线性回归", "enus": "Linear regression"}, "option_flag": false}, {"option_text": {"zhcn": "分类", "enus": "Classification"}, "option_flag": true}, {"option_text": {"zhcn": "聚类分析", "enus": "Clustering"}, "option_flag": false}, {"option_text": {"zhcn": "强化学习", "enus": "Reinforcement learning"}, "option_flag": false}], "analysis": {"enus": "The goal of classification is to determine to which class or category a data point (customer in our case) belongs to. For classification problems, data scientists would use historical data with predefined target variables AKA labels (churner/non-churner) \" answers that need to be predicted \" to train an algorithm. With classification, businesses can answer the following questions: ✑ Will this customer churn or not? ✑ Will a customer renew their subscription? ✑ Will a user downgrade a pricing plan? ✑ Are there any signs of unusual customer behavior? Reference: https://www.kdnuggets.com/2019/05/churn-prediction-machine-learning.html", "zhcn": "分类任务的核心在于判定数据点（此处指客户）所属的类别或范畴。在处理分类问题时，数据科学家会采用带有预定义目标变量（即标签，如流失客户/非流失客户）的历史数据——这些标签即待预测的答案——来训练算法模型。通过分类技术，企业能够解答以下关键问题：  \n✧ 该客户是否会流失？  \n✧ 客户是否将续订服务？  \n✧ 用户是否会降级定价方案？  \n✧ 是否存在异常客户行为的征兆？  \n参考文献：https://www.kdnuggets.com/2019/05/churn-prediction-machine-learning.html"}, "answer": "B"}, {"id": "13", "question": {"enus": "The displayed graph is from a forecasting model for testing a time series. Considering the graph only, which conclusion should a Machine Learning Specialist make about the behavior of the model? ", "zhcn": "根据图表所示，该图像源自用于时间序列测试的预测模型。仅从图像表现判断，机器学习专家应如何评价该模型的行为特征？"}, "option": [{"option_text": {"zhcn": "该模型对趋势性和季节性变化的预测都颇为精准。", "enus": "The model predicts both the trend and the seasonality well"}, "option_flag": true}, {"option_text": {"zhcn": "模型对趋势的预测相当准确，但在季节性波动方面则有所欠缺。", "enus": "The model predicts the trend well, but not the seasonality."}, "option_flag": false}, {"option_text": {"zhcn": "模型对季节性的预测颇为精准，却未能捕捉到整体趋势。", "enus": "The model predicts the seasonality well, but not the trend."}, "option_flag": false}, {"option_text": {"zhcn": "该模型未能准确捕捉趋势性与季节性变化。", "enus": "The model does not predict the trend or the seasonality well."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question ‘The displayed graph is from a forecasting model for testing a time series. Considering the graph only, which conclusion should a Machine Learning Specialist make about the behavior of the model?’ is ‘The model predicts both the trend and the seasonality well’. \n\nIn time - series forecasting, trend refers to the long - term movement of the data, and seasonality refers to the repeating patterns over fixed intervals. If the graph shows that the model's predictions closely follow the long - term direction of the actual data (indicating a good prediction of the trend) and also accurately capture the repeating patterns within the data (indicating a good prediction of seasonality), then it can be concluded that the model predicts both well.\n\nThe fake answer ‘The model predicts the trend well, but not the seasonality’ would be chosen if the graph showed that the long - term movement was captured but the repeating patterns were not. ‘The model predicts the seasonality well, but not the trend’ would be selected if the repeating patterns were predicted accurately but the long - term direction was off. And ‘The model does not predict the trend or the seasonality well’ would be picked when the model's predictions deviated significantly from both the long - term movement and the repeating patterns. Since the real answer is correct, it means the graph indicates that the model is successful in capturing both aspects of the time - series data.", "zhcn": "针对问题“该图表展示的是用于时间序列测试的预测模型。仅从图表来看，机器学习专家应如何评价该模型的表现？”的正确答案是：“该模型对趋势性和季节性特征的预测均表现良好”。在时间序列预测中，趋势性指数据的长期变化方向，季节性则指数据在固定周期内重复出现的规律性波动。若图表显示模型的预测曲线既紧跟实际数据的长期走向（表明趋势预测准确），又能精确捕捉数据中的周期性波动（表明季节性预测准确），则可判定模型对这两个维度都实现了有效预测。\n\n而错误选项“模型能预测趋势但未能捕捉季节性”适用于图表显示模型虽把握长期趋势却遗漏周期性规律的场景；“模型能预测季节性但未能捕捉趋势”对应模型虽还原周期波动但偏离长期方向的情况；“模型对趋势和季节性的预测均不理想”则适用于预测值与实际数据的长期走向和周期性特征皆存在显著偏差的情形。由于本题正确答案成立，说明图表证明模型成功捕捉了时间序列数据的双重特征。"}, "answer": "A"}, {"id": "14", "question": {"enus": "A company wants to classify user behavior as either fraudulent or normal. Based on internal research, a Machine Learning Specialist would like to build a binary classifier based on two features: age of account and transaction month. The class distribution for these features is illustrated in the figure provided. Based on this information, which model would have the HIGHEST accuracy? ", "zhcn": "某公司需对用户行为进行欺诈与正常的分类。根据内部研究，一位机器学习专家计划基于账户存续时长和交易月份这两个特征构建二元分类器。附图展示了这些特征对应的类别分布情况。基于现有信息，哪种模型的预测准确率会最高？"}, "option": [{"option_text": {"zhcn": "采用缩放指数线性单元（SELU）激活函数的长短期记忆（LSTM）模型。", "enus": "Long short-term memory (LSTM) model with scaled exponential linear unit (SELU)"}, "option_flag": false}, {"option_text": {"zhcn": "逻辑回归", "enus": "Logistic regression"}, "option_flag": false}, {"option_text": {"zhcn": "采用非线性核函数的支持向量机（SVM）", "enus": "Support vector machine (SVM) with non-linear kernel"}, "option_flag": true}, {"option_text": {"zhcn": "采用双曲正切激活函数的单层感知机", "enus": "Single perceptron with tanh activation function"}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is ‘Support vector machine (SVM) with non - linear kernel’. The key factor here is the nature of the data distribution. From the problem, we know that the classification is based on two features (age of account and transaction month), and the class distribution in the figure is likely to be non - linearly separable.\n\nAn SVM with a non - linear kernel is well - suited for such non - linearly separable data. It can map the input data into a higher - dimensional space where a hyperplane can be found to separate the two classes (fraudulent and normal).\n\nThe ‘Long short - term memory (LSTM) model with scaled exponential linear unit (SELU)’ is mainly used for sequential data, like time - series data, and is overkill for a simple binary classification problem based on just two features.\n\n‘Logistic regression’ and ‘Single perceptron with tanh activation function’ are linear models. They assume that the data can be separated by a straight line (or hyperplane in higher dimensions), which is likely not the case given the non - linear class distribution, so they will not achieve the highest accuracy.\n\nCommon misconceptions might lead one to choose the fake options. For example, one might choose LSTM thinking that any machine - learning problem requires a complex model. And for the linear models, a lack of understanding of the non - linear nature of the data could lead to their selection.", "zhcn": "对于本题，正确答案应为“采用非线性核函数的支持向量机（SVM）”。其核心考量在于数据分布的特性。根据题目描述，分类任务基于两个特征（账户存续时长与交易月份），而图示中的类别分布呈现明显的非线性可分态势。\n\n采用非线性核函数的SVM特别适用于此类非线性可分数据。它能够将原始数据映射到高维特征空间，从而找到有效区分两类（欺诈交易与正常交易）的超平面。\n\n而“带有缩放指数线性单元（SELU）的长短期记忆网络（LSTM）”主要适用于时序数据等序列问题，对于仅有两个特征的简单二分类场景显得过于复杂。“逻辑回归”与“采用tanh激活函数的单层感知机”属于线性模型，它们假设数据可通过直线（或高维空间中的超平面）划分，但当前数据的非线性分布特性将使这类模型难以达到最高分类精度。\n\n常见误解可能导致选择错误选项。例如误认为机器学习问题必须采用复杂模型而选择LSTM，或由于未能理解数据的非线性特性而选择线性模型。"}, "answer": "C"}, {"id": "15", "question": {"enus": "A Machine Learning Specialist at a company sensitive to security is preparing a dataset for model training. The dataset is stored in Amazon S3 and contains Personally Identifiable Information (PII). The dataset: ✑ Must be accessible from a VPC only. ✑ Must not traverse the public internet. How can these requirements be satisfied? ", "zhcn": "某涉密企业的机器学习专家正在为模型训练准备数据集。该数据集存放于Amazon S3存储服务中，且包含个人身份识别信息。现有安全要求如下：  \n✧ 数据集仅允许通过虚拟私有云访问  \n✧ 数据传输不得经过公共互联网  \n\n请问如何满足这些技术要求？"}, "option": [{"option_text": {"zhcn": "创建VPC终端节点，并配置存储桶访问策略，限定仅允许指定VPC终端节点及其对应VPC进行访问。", "enus": "Create a VPC endpoint and apply a bucket access policy that restricts access to the given VPC endpoint and the VPC."}, "option_flag": true}, {"option_text": {"zhcn": "创建VPC终端节点，并配置存储桶访问策略，允许来自指定VPC终端节点及Amazon EC2实例的访问权限。", "enus": "Create a VPC endpoint and apply a bucket access policy that allows access from the given VPC endpoint and an Amazon EC2 instance."}, "option_flag": false}, {"option_text": {"zhcn": "创建VPC终端节点，并配置网络访问控制列表（NACLs），确保仅允许指定VPC终端节点与亚马逊EC2实例之间的流量互通。", "enus": "Create a VPC endpoint and use Network Access Control Lists (NACLs) to allow trafic between only the given VPC endpoint and an  Amazon EC2 instance."}, "option_flag": false}, {"option_text": {"zhcn": "创建VPC终端节点，并通过安全组限制对指定VPC终端节点及亚马逊EC2实例的访问权限。", "enus": "Create a VPC endpoint and use security groups to restrict access to the given VPC endpoint and an Amazon EC2 instance"}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Create a VPC endpoint and apply a bucket access policy that restricts access to the given VPC endpoint and the VPC.” This meets the requirements as it ensures that the dataset stored in Amazon S3, which contains PII, is accessible only from the VPC and does not traverse the public internet. By using a bucket access policy to restrict access to the specific VPC endpoint and the VPC, the data remains within the secure VPC environment.\n\nThe fake answer options involve allowing access from an Amazon EC2 instance. These options are incorrect because the requirement is to restrict access to the VPC only, not to allow access from an individual EC2 instance. Using NACLs or security groups to control access between the VPC endpoint and an EC2 instance does not ensure that the access is limited to the VPC as required. A common misconception might be thinking that allowing an EC2 instance access is equivalent to VPC - wide access, but an EC2 instance is just a single resource within the VPC and allowing it access does not meet the strict VPC - only access requirement.", "zhcn": "针对该问题的正确答案是\"创建VPC端点，并配置存储桶访问策略，将访问权限限制于指定VPC端点及对应VPC\"。此方案符合要求，能确保存储于Amazon S3中包含个人身份信息的数据集仅可通过VPC内部访问，且流量不会经过公共互联网。通过配置存储桶策略将访问权限限定于特定VPC端点及所属VPC，可确保数据始终处于安全的VPC环境中。\n\n错误答案选项涉及允许亚马逊EC2实例访问的方案。这些选项不符合要求，因为核心诉求是仅限VPC内部访问，而非允许单个EC2实例访问。使用网络ACL或安全组控制VPC端点与EC2实例之间的访问权限，并不能确保实现要求的VPC全域访问限制。常见误解在于认为允许EC2实例访问即等同于VPC全域访问，但EC2实例仅是VPC中的单个资源，允许其访问并不能满足严格的VPC专属访问要求。"}, "answer": "A"}, {"id": "16", "question": {"enus": "During mini-batch training of a neural network for a classification problem, a Data Scientist notices that training accuracy oscillates. What is the MOST likely cause of this issue? ", "zhcn": "在针对分类问题的小批量训练神经网络过程中，一位数据科学家发现训练准确率出现波动。导致该现象最可能的原因是什么？"}, "option": [{"option_text": {"zhcn": "该数据集中的类别分布并不均衡。", "enus": "The class distribution in the dataset is imbalanced."}, "option_flag": false}, {"option_text": {"zhcn": "数据集随机打乱功能已停用。", "enus": "Dataset shufiing is disabled."}, "option_flag": false}, {"option_text": {"zhcn": "批次规模过大。", "enus": "The batch size is too big."}, "option_flag": false}, {"option_text": {"zhcn": "学习速率相当之快。", "enus": "The learning rate is very high."}, "option_flag": true}], "analysis": {"enus": "Reference: https://towardsdatascience.com/deep-learning-personal-notes-part-1-lesson-2-8946fe970b95", "zhcn": "参考来源：https://towardsdatascience.com/deep-learning-personal-notes-part-1-lesson-2-8946fe970b95"}, "answer": "D"}, {"id": "17", "question": {"enus": "An employee found a video clip with audio on a company's social media feed. The language used in the video is Spanish. English is the employee's first language, and they do not understand Spanish. The employee wants to do a sentiment analysis. What combination of services is the MOST eficient to accomplish the task? ", "zhcn": "公司一名员工在社交媒体推送中发现了一段带音频的视频片段。该视频使用西班牙语录制，而该员工的母语为英语且不通晓西班牙语。该员工希望进行情感倾向分析，要最高效地完成此任务，下列哪种服务组合最为适宜？"}, "option": [{"option_text": {"zhcn": "Amazon Transcribe、Amazon Translate 与 Amazon Comprehend", "enus": "Amazon Transcribe, Amazon Translate, and Amazon Comprehend"}, "option_flag": true}, {"option_text": {"zhcn": "Amazon Transcribe、Amazon Comprehend 与 Amazon SageMaker 序列到序列模型", "enus": "Amazon Transcribe, Amazon Comprehend, and Amazon SageMaker seq2seq"}, "option_flag": false}, {"option_text": {"zhcn": "亚马逊语音转文本服务、亚马逊语言翻译服务，以及亚马逊SageMaker神经主题模型（NTM）。", "enus": "Amazon Transcribe, Amazon Translate, and Amazon SageMaker Neural Topic Model (NTM)"}, "option_flag": false}, {"option_text": {"zhcn": "亚马逊语音转文本、亚马逊语言翻译与亚马逊SageMaker极速文本分析", "enus": "Amazon Transcribe, Amazon Translate and Amazon SageMaker BlazingText"}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is “Amazon Transcribe, Amazon Translate, and Amazon Comprehend”. This combination is most efficient for sentiment analysis of the Spanish - language video with audio. Amazon Transcribe is used to convert the audio in the video into text. Amazon Translate then translates the Spanish text into English, which the employee can understand. Amazon Comprehend is a natural - language processing service specifically designed for sentiment analysis, making it ideal for this task.\n\nThe fake answer options involve Amazon SageMaker services. Amazon SageMaker seq2seq is more for sequence - to - sequence tasks like machine translation, but Amazon Translate is a dedicated translation service and more straightforward here. Amazon SageMaker Neural Topic Model (NTM) is for topic modeling, not sentiment analysis. Amazon SageMaker BlazingText is for text classification and word embeddings, not directly for sentiment analysis. A common pitfall might be choosing these SageMaker services thinking they can handle all aspects of the task, but they are not as focused on sentiment analysis as Amazon Comprehend. This is why the real answer option is the best choice for the given task.", "zhcn": "对于这道题目，正确答案是“Amazon Transcribe、Amazon Translate 和 Amazon Comprehend”这一组合。该方案能最高效地完成西班牙语视频音频的情感分析任务：首先通过Amazon Transcribe将视频中的语音转换为文本，再由Amazon Translate把西语文本翻译成员工可理解的英语，最后借助专精自然语言情感分析的Amazon Comprehend完成核心分析。\n\n干扰选项中涉及的Amazon SageMaker系列服务并不适用——其seq2seq组件侧重于机器翻译等序列转换任务，而本题已存在专用翻译服务Amazon Translate；Neural Topic Model（NTM）适用于主题建模而非情感分析；BlazingText虽能处理文本分类与词嵌入，但并非直接针对情感分析场景。常见误区在于试图用SageMaker服务包办所有环节，但相较于专门的情感分析工具Amazon Comprehend，这些服务的功能定位不够聚焦。因此原答案才是本任务的最佳选择。"}, "answer": "A"}, {"id": "18", "question": {"enus": "A Machine Learning Specialist is packaging a custom ResNet model into a Docker container so the company can leverage Amazon SageMaker for training. The Specialist is using Amazon EC2 P3 instances to train the model and needs to properly configure the Docker container to leverage the NVIDIA GPUs. What does the Specialist need to do? ", "zhcn": "一位机器学习专家正在将定制开发的ResNet模型封装至Docker容器中，以便企业能够借助亚马逊SageMaker平台进行模型训练。该专家采用亚马逊EC2 P3实例开展模型训练工作，需对Docker容器进行正确配置以充分发挥NVIDIA GPU的运算效能。请问专家应当如何完成相关配置？"}, "option": [{"option_text": {"zhcn": "将NVIDIA驱动程序与Docker镜像捆绑打包。", "enus": "Bundle the NVIDIA drivers with the Docker image."}, "option_flag": true}, {"option_text": {"zhcn": "构建兼容NVIDIA-Docker的Docker容器。", "enus": "Build the Docker container to be NVIDIA-Docker compatible."}, "option_flag": false}, {"option_text": {"zhcn": "调整Docker容器的文件结构，以便在GPU实例上运行。", "enus": "Organize the Docker container's file structure to execute on GPU instances."}, "option_flag": false}, {"option_text": {"zhcn": "在Amazon SageMaker的CreateTrainingJob请求体中配置GPU参数。", "enus": "Set the GPU fiag in the Amazon SageMaker CreateTrainingJob request body."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is ‘Bundle the NVIDIA drivers with the Docker image.’ This is because for the Docker container to use the NVIDIA GPUs on EC2 P3 instances, the necessary NVIDIA drivers must be present within the container. Without these drivers, the container won't be able to interact with the GPUs.\n\nThe option ‘Build the Docker container to be NVIDIA - Docker compatible’ is incorrect. While NVIDIA - Docker is related to GPU support, it mainly simplifies the process of using GPUs in Docker, but alone it does not ensure the presence of the required drivers. ‘Organize the Docker container's file structure to execute on GPU instances’ is wrong as file structure organization does not directly address the need for GPU drivers. ‘Set the GPU flag in the Amazon SageMaker CreateTrainingJob request body’ is also incorrect because this flag only indicates the intention to use GPUs, but without the drivers in the container, the GPUs cannot be utilized. So, bundling the NVIDIA drivers is the key step, which is why it is the real answer option.", "zhcn": "问题的正确答案是\"将NVIDIA驱动程序与Docker镜像捆绑打包\"。这是因为要让Docker容器能够调用EC2 P3实例上的NVIDIA GPU，容器内部必须预装相应的NVIDIA驱动程序。若缺少这些驱动，容器将无法与GPU进行交互。\n\n而\"构建兼容NVIDIA-Docker的Docker容器\"这一选项并不正确。虽然NVIDIA-Docker确实与GPU支持相关，但其主要作用是简化Docker使用GPU的流程，本身并不能确保驱动程序的部署。\"调整Docker容器的文件结构以适应GPU实例\"同样有误，因为文件结构的调整无法直接解决GPU驱动缺失的问题。至于\"在Amazon SageMaker的CreateTrainingJob请求体中设置GPU标志\"也不正确，该标志仅表明使用GPU的意向，若容器内未安装驱动，GPU仍然无法被调用。\n\n因此，捆绑NVIDIA驱动程序才是关键步骤，这也是该选项成为正确答案的根本原因。"}, "answer": "A"}, {"id": "19", "question": {"enus": "A Machine Learning Specialist is building a logistic regression model that will predict whether or not a person will order a pizza. The Specialist is trying to build the optimal model with an ideal classification threshold. What model evaluation technique should the Specialist use to understand how different classification thresholds will impact the model's performance? ", "zhcn": "一位机器学习专家正在构建逻辑回归模型，用于预测顾客是否会订购披萨。该专家试图通过最佳分类阈值来构建最优模型。请问应采用何种模型评估方法，才能帮助专家理解不同分类阈值对模型性能的影响？"}, "option": [{"option_text": {"zhcn": "受试者工作特征曲线", "enus": "Receiver operating characteristic (ROC) curve"}, "option_flag": true}, {"option_text": {"zhcn": "误判率", "enus": "Misclassification rate"}, "option_flag": false}, {"option_text": {"zhcn": "均方根误差", "enus": "Root Mean Square Error (RMSE)"}, "option_flag": false}, {"option_text": {"zhcn": "L1 范数", "enus": "L1 norm"}, "option_flag": false}], "analysis": {"enus": "Reference: https://docs.aws.amazon.com/machine-learning/latest/dg/binary-model-insights.html", "zhcn": "参考来源：https://docs.aws.amazon.com/machine-learning/latest/dg/binary-model-insights.html"}, "answer": "A"}, {"id": "20", "question": {"enus": "An interactive online dictionary wants to add a widget that displays words used in similar contexts. A Machine Learning Specialist is asked to provide word features for the downstream nearest neighbor model powering the widget. What should the Specialist do to meet these requirements? ", "zhcn": "一款在线互动词典计划增设显示近义语境词汇的小组件，现需机器学习专家为驱动该组件的近邻模型提供词汇特征向量。专家应当采取何种方案以满足需求？"}, "option": [{"option_text": {"zhcn": "生成独热词编码向量。", "enus": "Create one-hot word encoding vectors."}, "option_flag": true}, {"option_text": {"zhcn": "为每个词汇生成一组同义词，可借助亚马逊土耳其机器人平台实现。", "enus": "Produce a set of synonyms for every word using Amazon Mechanical Turk."}, "option_flag": false}, {"option_text": {"zhcn": "生成能够存储与所有其他词汇间编辑距离的词嵌入向量。", "enus": "Create word embedding vectors that store edit distance with every other word."}, "option_flag": false}, {"option_text": {"zhcn": "下载基于大型语料库预训练的词嵌入模型。", "enus": "Download word embeddings pre-trained on a large corpus."}, "option_flag": false}], "analysis": {"enus": "Reference: https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-object2vec-adds-new-features-that-support-automatic-negative- sampling-and- speed-up-training/", "zhcn": "参考来源：https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-object2vec-adds-new-features-that-support-automatic-negative- sampling-and- speed-up-training/"}, "answer": "A"}, {"id": "21", "question": {"enus": "A Machine Learning Specialist is configuring Amazon SageMaker so multiple Data Scientists can access notebooks, train models, and deploy endpoints. To ensure the best operational performance, the Specialist needs to be able to track how often the Scientists are deploying models, GPU and CPU utilization on the deployed SageMaker endpoints, and all errors that are generated when an endpoint is invoked. Which services are integrated with Amazon SageMaker to track this information? (Choose two.) ", "zhcn": "亚马逊机器学习专家正在配置Amazon SageMaker平台，以便多位数据科学家能够访问笔记本书写环境、训练模型并部署服务终端。为保障系统的最佳运行效能，该专家需持续追踪科学家们部署模型的频率、已部署SageMaker终端上的GPU与CPU资源利用率，以及终端调用时产生的所有错误信息。下列哪两项服务与Amazon SageMaker原生集成，可协助实现上述监控目标？（请选择两项正确答案）"}, "option": [{"option_text": {"zhcn": "AWS CloudTrail", "enus": "AWS CloudTrail"}, "option_flag": true}, {"option_text": {"zhcn": "AWS健康服务", "enus": "AWS Health"}, "option_flag": false}, {"option_text": {"zhcn": "AWS Trusted Advisor", "enus": "AWS Trusted Advisor"}, "option_flag": false}, {"option_text": {"zhcn": "亚马逊云监控", "enus": "Amazon CloudWatch"}, "option_flag": true}, {"option_text": {"zhcn": "AWS Config", "enus": "AWS Config"}, "option_flag": false}], "analysis": {"enus": "Reference: https://aws.amazon.com/sagemaker/faqs/", "zhcn": "参考来源：https://aws.amazon.com/sagemaker/faqs/"}, "answer": "AD"}, {"id": "22", "question": {"enus": "A retail chain has been ingesting purchasing records from its network of 20,000 stores to Amazon S3 using Amazon Kinesis Data Firehose. To support training an improved machine learning model, training records will require new but simple transformations, and some attributes will be combined. The model needs to be retrained daily. Given the large number of stores and the legacy data ingestion, which change will require the LEAST amount of development effort? ", "zhcn": "一家零售连锁企业一直通过亚马逊Kinesis数据消防带服务，将其两万家门店的采购记录实时汇入亚马逊S3存储平台。为提升机器学习模型的训练效果，训练数据需进行几项简单的新型转换处理，并将部分属性字段加以整合。该模型需实现每日自动重训练。考虑到门店规模庞大且存在传统数据接入方式，下列哪种改造方案所需开发投入最为精简？"}, "option": [{"option_text": {"zhcn": "要求各门店将数据采集方式切换为通过AWS存储网关在本地捕获，随后导入Amazon S3存储服务，再运用AWS Glue进行数据转换处理。", "enus": "Require that the stores to switch to capturing their data locally on AWS Storage Gateway for loading into Amazon S3, then use AWS Glue  to do the transformation."}, "option_flag": false}, {"option_text": {"zhcn": "部署一个运行Apache Spark的亚马逊EMR集群，并配置相应的数据转换逻辑。该集群需每日处理亚马逊S3中持续累积的数据记录，将处理后的新数据及转换结果输出至亚马逊S3存储空间。", "enus": "Deploy an Amazon EMR cluster running Apache Spark with the transformation logic, and have the cluster run each day on the  accumulating records in Amazon S3, outputting new/transformed records to Amazon S3."}, "option_flag": false}, {"option_text": {"zhcn": "部署一套搭载转换逻辑的亚马逊EC2实例集群，对积存在亚马逊S3的数据记录进行转换处理，并将转换后的记录输出至亚马逊S3存储空间。", "enus": "Spin up a fieet of Amazon EC2 instances with the transformation logic, have them transform the data records accumulating on Amazon  S3, and output the transformed records to Amazon S3."}, "option_flag": false}, {"option_text": {"zhcn": "在Kinesis Data Firehose数据流的下游接入一条Amazon Kinesis数据分析流，通过SQL语句将原始记录属性转化为简洁的转换值。", "enus": "Insert an Amazon Kinesis Data Analytics stream downstream of the Kinesis Data Firehose stream that transforms raw record attributes  into simple transformed values using SQL."}, "option_flag": true}], "analysis": {"enus": "The correct answer to the question is “Insert an Amazon Kinesis Data Analytics stream downstream of the Kinesis Data Firehose stream that transforms raw record attributes into simple transformed values using SQL.” This option requires the least development effort because it can leverage the existing Kinesis Data Firehose setup. Kinesis Data Analytics allows for data transformation using SQL, which is a well - known and relatively simple language, and it can be easily integrated downstream of the existing ingestion stream.\n\nThe option “Require that the stores to switch to capturing their data locally on AWS Storage Gateway for loading into Amazon S3, then use AWS Glue to do the transformation” would require significant changes at the store level, as they need to switch their data - capturing method, which involves a lot of development and operational effort.\n\n“Deploy an Amazon EMR cluster running Apache Spark with the transformation logic, and have the cluster run each day on the accumulating records in Amazon S3, outputting new/transformed records to Amazon S3” requires setting up and managing an EMR cluster, which is complex and involves dealing with Spark, a more advanced framework, increasing the development and management overhead.\n\n“Spin up a fleet of Amazon EC2 instances with the transformation logic, have them transform the data records accumulating on Amazon S3, and output the transformed records to Amazon S3” involves managing a fleet of EC2 instances, including instance provisioning, scaling, and maintenance, which is a high - effort task.\n\nThe simplicity of using SQL in Kinesis Data Analytics and the ability to integrate with the existing Kinesis Data Firehose stream are the key factors that make this the real answer, distinguishing it from the more complex and high - effort fake options.", "zhcn": "针对该问题的正确答案是：\"在Kinesis Data Firehose数据流下游接入Amazon Kinesis Data Analytics流，通过SQL将原始记录属性转换为精简的转换值\"。此方案开发成本最低，因其可直接利用现有Kinesis Data Firehose配置。Kinesis Data Analytics支持使用SQL进行数据转换，这种语言广为人知且操作简便，能轻松与现有数据摄入流实现下游集成。\n\n若采用\"要求各门店改用AWS Storage Gateway本地捕获数据并加载至Amazon S3，再通过AWS Glue进行转换\"方案，需对门店系统进行重大改造——不仅需要变更数据捕获方式，还会产生大量开发运维工作。\n\n而\"部署运行Apache Spark的Amazon EMR集群，每日对Amazon S3中累积记录执行转换逻辑，并将转换后的新记录输出至Amazon S3\"方案，需配置管理EMR集群，涉及Spark等高级框架的复杂操作，将显著增加开发管理负担。\n\n至于\"启动搭载转换逻辑的Amazon EC2实例集群，对Amazon S3累积数据执行转换后输出至Amazon S3\"方案，则需要管理EC2实例集群的配置、扩展及维护等高强度工作。\n\n正是基于Kinesis Data Analytics的SQL操作简便性及其与现有Kinesis Data Firehose流的无缝集成能力，使该方案从众多高复杂度备选方案中脱颖而出，成为最优解。"}, "answer": "D"}, {"id": "23", "question": {"enus": "A Machine Learning Specialist is building a convolutional neural network (CNN) that will classify 10 types of animals. The Specialist has built a series of layers in a neural network that will take an input image of an animal, pass it through a series of convolutional and pooling layers, and then finally pass it through a dense and fully connected layer with 10 nodes. The Specialist would like to get an output from the neural network that is a probability distribution of how likely it is that the input image belongs to each of the 10 classes. Which function will produce the desired output? ", "zhcn": "一位机器学习专家正在构建一个用于识别10种动物的卷积神经网络（CNN）。该专家设计了一系列网络层结构：输入动物图像后，数据会依次经过若干卷积层和池化层，最终进入包含10个节点的全连接层。为使神经网络输出能呈现该图像分别属于10个类别概率分布，应采用哪种激活函数？"}, "option": [{"option_text": {"zhcn": "辍学", "enus": "Dropout"}, "option_flag": false}, {"option_text": {"zhcn": "平滑L1损失函数", "enus": "Smooth L1 loss"}, "option_flag": false}, {"option_text": {"zhcn": "“Softmax”", "enus": "Softmax"}, "option_flag": true}, {"option_text": {"zhcn": "修正线性单元（ReLU）", "enus": "Rectified linear units (ReLU)"}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is ‘Softmax’. The Softmax function is specifically designed to convert the raw scores (logits) from the output layer of a neural network into a probability distribution. In this case, with 10 nodes in the fully - connected layer representing 10 animal classes, the Softmax function will transform the values of these nodes so that they sum up to 1, indicating the probability that the input image belongs to each of the 10 classes.\n\n‘Dropout’ is a regularization technique used to prevent overfitting by randomly ‘dropping out’ some neurons during training, not to generate a probability distribution. ‘Smooth L1 loss’ is a loss function used to measure the difference between the predicted and actual values during training, not for outputting probabilities. ‘Rectified linear units (ReLU)’ are activation functions that introduce non - linearity to the neural network by converting negative values to zero, but they do not produce a probability distribution. These misconceptions might lead one to choose the fake options, while the nature of the Softmax function makes it the appropriate choice for generating the desired probability output.", "zhcn": "对于该问题的正确答案是“Softmax”。Softmax函数专为将神经网络输出层产生的原始分数（logits）转化为概率分布而设计。在当前场景中，全连接层的十个节点对应十种动物类别，Softmax函数会将这些节点的数值转换为总和为1的概率形式，从而直观呈现输入图像属于各个类别的可能性。  \n“Dropout”作为一种正则化技术，其作用是通过随机“丢弃”部分神经元来防止过拟合，而非生成概率分布；“Smooth L1 loss”是训练过程中用于衡量预测值与真实值差异的损失函数，并不输出概率；而“修正线性单元（ReLU）”作为激活函数，通过将负值归零来引入非线性特性，同样不具备生成概率分布的功能。这些常见误解可能导致选择错误选项，而Softmax函数的本质特性使其成为生成目标概率输出的恰当选择。"}, "answer": "C"}, {"id": "24", "question": {"enus": "A Machine Learning Specialist trained a regression model, but the first iteration needs optimizing. The Specialist needs to understand whether the model is more frequently overestimating or underestimating the target. What option can the Specialist use to determine whether it is overestimating or underestimating the target value? ", "zhcn": "一位机器学习专家训练了一个回归模型，但初始版本还需优化。该专家需要判断模型更倾向于高估还是低估预测目标。下列哪种方法能帮助专家确认预测值偏离的方向？"}, "option": [{"option_text": {"zhcn": "均方根误差", "enus": "Root Mean Square Error (RMSE)"}, "option_flag": false}, {"option_text": {"zhcn": "残差散点图", "enus": "Residual plots"}, "option_flag": true}, {"option_text": {"zhcn": "曲线下面积", "enus": "Area under the curve"}, "option_flag": false}, {"option_text": {"zhcn": "混淆矩阵", "enus": "Confusion matrix"}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is ‘Residual plots’. Residuals are the differences between the observed values and the predicted values in a regression model. A residual plot shows these residuals on the y - axis and the independent variable or the predicted values on the x - axis. By examining the residual plot, the Machine Learning Specialist can clearly see if the residuals are mostly positive (indicating underestimation) or mostly negative (indicating overestimation).\n\n‘Root Mean Square Error (RMSE)’ is a measure of the average magnitude of the residuals, but it does not tell whether the model is overestimating or underestimating. It only gives an overall sense of how far the predictions are from the actual values on average. \n\n‘Area under the curve’ is commonly used in classification problems, especially for evaluating the performance of binary classifiers, and has no direct relevance to understanding over - or under - estimation in a regression model. \n\n‘Confusion matrix’ is also a tool for classification problems. It shows the number of true positives, false positives, true negatives, and false negatives, and is not applicable for analyzing over - or under - estimation in regression. \n\nThe key factor that makes ‘Residual plots’ the real answer is their ability to directly display the nature of the differences between predicted and actual values in a way that reveals over - or under - estimation, which the other fake options cannot do.", "zhcn": "该问题的正确答案是“残差图”。残差指的是回归模型中观测值与预测值之间的差异。残差图以y轴表示这些残差，x轴则对应自变量或预测值。通过审视残差图，机器学习专家能够清晰判断残差主要呈正值（表明模型存在低估倾向）还是负值（表明存在高估倾向）。\n\n“均方根误差（RMSE）”衡量的是残差的平均幅度，但无法揭示模型究竟存在高估还是低估倾向，仅能反映预测值相对于实际值的平均偏离程度。“曲线下面积”通常用于分类问题，特别是在评估二分类器性能时使用，与理解回归模型中的高估或低估现象无直接关联。“混淆矩阵”同样是分类问题的评估工具，通过展示真正例、假正例、真反例和假反例的数量来评估模型性能，不适用于回归模型中的高低估分析。\n\n“残差图”之所以成为正确答案，关键在于其能够直观呈现预测值与实际值差异的性质，从而揭示模型的高估或低估倾向——这是其他干扰选项所不具备的核心功能。"}, "answer": "B"}, {"id": "25", "question": {"enus": "A company wants to classify user behavior as either fraudulent or normal. Based on internal research, a Machine Learning Specialist would like to build a binary classifier based on two features: age of account and transaction month. The class distribution for these features is illustrated in the figure provided. Based on this information, which model would have the HIGHEST recall with respect to the fraudulent class? ", "zhcn": "某公司需对用户行为进行欺诈与非欺诈分类。根据内部研究，一位机器学习专家计划基于账户存续时长和交易月份这两个特征构建二元分类器。各特征对应的类别分布已通过图示呈现。基于上述信息，哪种模型能对欺诈类别实现最高的召回率？"}, "option": [{"option_text": {"zhcn": "决策树", "enus": "Decision tree"}, "option_flag": false}, {"option_text": {"zhcn": "线性支持向量机（SVM）", "enus": "Linear support vector machine (SVM)"}, "option_flag": false}, {"option_text": {"zhcn": "朴素贝叶斯分类器", "enus": "Naive Bayesian classifier"}, "option_flag": true}, {"option_text": {"zhcn": "采用S形激活函数的单层感知机", "enus": "Single Perceptron with sigmoidal activation function"}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is ‘Naive Bayesian classifier’. Recall for the fraudulent class measures the proportion of actual fraudulent cases that are correctly predicted as such. A Naive Bayesian classifier is well - suited for this problem because it can handle probabilistic relationships between features and classes effectively. It can estimate the probability of a transaction being fraudulent given the age of the account and the transaction month.\n\nThe ‘Decision tree’ might not perform as well in terms of recall for the fraudulent class. Decision trees can be sensitive to small variations in the data and may overfit, leading to poor generalization and potentially missing some fraudulent cases. \n\nA ‘Linear support vector machine (SVM)’ assumes a linear boundary between classes. The class distribution in the figure may not have a simple linear relationship, so an SVM might misclassify many fraudulent cases, resulting in lower recall. \n\nA ‘Single Perceptron with sigmoidal activation function’ is a simple neural network model. It has limited complexity and may not be able to capture the complex patterns in the data related to fraud detection, thus having lower recall compared to the Naive Bayesian classifier. \n\nCommon misconceptions might lead one to choose the fake options. For example, decision trees are popular and easy to understand, so one might assume they can handle any classification problem well. SVMs are known for their good performance in many classification tasks, but the linear assumption can be a pitfall here. And the simplicity of a single perceptron might be wrongly thought to be sufficient for this task. The ability of the Naive Bayesian classifier to work with probabilistic relationships and handle the given features makes it the best choice for achieving high recall for the fraudulent class.", "zhcn": "对于本题，正确答案应为“朴素贝叶斯分类器”。欺诈类别的召回率衡量的是实际欺诈案例中被正确预测的比例。朴素贝叶斯分类器能有效处理特征与类别之间的概率关系，因此特别适合解决此类问题。它能够根据账户存续时长和交易月份，估算交易涉嫌欺诈的概率。\n\n决策树在欺诈类别的召回率方面表现可能欠佳。该模型对数据的细微波动较为敏感，容易产生过拟合，导致泛化能力下降，从而漏判部分欺诈案例。线性支持向量机假设类别间存在线性分界，但图示的类别分布可能并不符合简单线性关系，因此可能导致大量欺诈案例误判，拉低召回率。采用S型激活函数的单层感知机作为简单神经网络模型，其复杂度有限，难以捕捉欺诈检测所需的复杂数据模式，故其召回率会低于朴素贝叶斯分类器。\n\n常见误解可能导致选择错误选项。例如决策树因通俗易懂而备受青睐，但人们可能误以为其能胜任所有分类任务；支持向量机虽在多类任务中表现优异，但其线性假设在此处反而成为陷阱；单层感知机的简易性也可能让人误以为足以应对当前场景。而朴素贝叶斯分类器处理概率关系的特性及其对给定特征的适配性，使其成为实现欺诈高召回率的最佳选择。"}, "answer": "C"}, {"id": "26", "question": {"enus": "A Machine Learning Specialist kicks off a hyperparameter tuning job for a tree-based ensemble model using Amazon SageMaker with Area Under the ROC Curve (AUC) as the objective metric. This workfiow will eventually be deployed in a pipeline that retrains and tunes hyperparameters each night to model click-through on data that goes stale every 24 hours. With the goal of decreasing the amount of time it takes to train these models, and ultimately to decrease costs, the Specialist wants to reconfigure the input hyperparameter range(s). Which visualization will accomplish this? ", "zhcn": "一位机器学习专家利用亚马逊SageMaker服务平台，以ROC曲线下面积（AUC）作为目标指标，启动了基于树模型的集成学习超参数调优任务。该工作流最终将部署于每晚重新训练模型的流水线系统中——由于数据每24小时便会失效，需通过持续调参来预测点击率。为缩短模型训练时间并降低计算成本，专家计划重新配置输入超参数的范围。下列哪种可视化方案可实现这一目标？"}, "option": [{"option_text": {"zhcn": "一幅直方图，用于显示最重要的输入特征是否符合高斯分布。", "enus": "A histogram showing whether the most important input feature is Gaussian."}, "option_flag": false}, {"option_text": {"zhcn": "一幅散点图，通过目标变量对数据点进行色彩区分，运用t分布随机邻域嵌入技术（t-SNE）将众多输入变量转化为更易解读的维度呈现。", "enus": "A scatter plot with points colored by target variable that uses t-Distributed Stochastic Neighbor Embedding (t-SNE) to visualize the  large number of input variables in an easier-to-read dimension."}, "option_flag": true}, {"option_text": {"zhcn": "散点图展示了目标指标在每次训练迭代中的表现变化。", "enus": "A scatter plot showing the performance of the objective metric over each training iteration."}, "option_flag": false}, {"option_text": {"zhcn": "散点图呈现了最大树深度与目标指标之间的关联性。", "enus": "A scatter plot showing the correlation between maximum tree depth and the objective metric."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is “A scatter plot with points colored by target variable that uses t - Distributed Stochastic Neighbor Embedding (t - SNE) to visualize the large number of input variables in an easier - to - read dimension.” The goal is to decrease training time and costs by reconfiguring input hyperparameter ranges. t - SNE is useful for visualizing high - dimensional data in a lower dimension. By using this visualization, the Specialist can understand the relationships between input variables and the target variable more clearly. This understanding can help in identifying which input variables are most relevant and can be used to narrow down the hyperparameter search space, thus reducing training time and costs.\n\nA “histogram showing whether the most important input feature is Gaussian” is not relevant to reconfiguring hyperparameter ranges for reducing training time. The Gaussian nature of a single feature does not directly inform about which hyperparameters to adjust.\n\nA “scatter plot showing the performance of the objective metric over each training iteration” mainly focuses on the training progress but does not help in understanding the input variables' relationships to reconfigure hyperparameter ranges.\n\nA “scatter plot showing the correlation between maximum tree depth and the objective metric” only considers one hyperparameter (maximum tree depth). It does not provide a comprehensive view of all input variables to effectively reconfigure the overall hyperparameter ranges.\n\nThe key factor distinguishing the real answer is its ability to handle and visualize multiple input variables in a way that can guide hyperparameter range reconfiguration, which is crucial for the goal of reducing training time and costs.", "zhcn": "该问题的正确答案是“采用t-SNE（t分布随机邻域嵌入）技术、按目标变量着色的散点图，该方法通过降维将大量输入变量转化为更易解读的可视化形态”。其核心目标在于通过重构超参数范围来缩短训练时间并降低成本。t-SNE技术能有效将高维数据映射到低维空间进行可视化，使专家能够更清晰地洞察输入变量与目标变量之间的关联。这种认知有助于识别关键输入变量，从而缩小超参数搜索空间，实现训练效率的提升。\n\n而“展示最重要输入特征是否服从高斯分布的直方图”与重构超参数范围以提升训练效率的目标无关。单一特征的高斯分布特性并不能直接指导超参数调整策略。\n\n“呈现目标指标随训练迭代次数变化趋势的散点图”主要反映训练进程，但无助于理解输入变量间关系以重构超参数范围。\n\n“显示最大树深度与目标指标相关性的散点图”仅考虑单一超参数（最大树深度），无法提供全面视角来有效重构整体超参数范围。\n\n真正答案的关键优势在于：它能通过对多输入变量的可视化处理，为超参数范围重构提供明确指导，这正是实现训练成本优化目标的核心所在。"}, "answer": "B"}, {"id": "27", "question": {"enus": "A Machine Learning Specialist is creating a new natural language processing application that processes a dataset comprised of 1 million sentences. The aim is to then run Word2Vec to generate embeddings of the sentences and enable different types of predictions. Here is an example from the dataset: \"The quck BROWN FOX jumps over the lazy dog.` Which of the following are the operations the Specialist needs to perform to correctly sanitize and prepare the data in a repeatable manner? (Choose three.) ", "zhcn": "一位机器学习专家正在开发一款新型自然语言处理应用，需处理包含百万句量的数据集。该项目旨在通过Word2Vec技术生成语句的嵌入向量，以支持多种预测功能。现有一则数据示例：\"The quck BROWN FOX jumps over the lazy dog.\" 请选出专家需采用哪三项操作，方能以可复现的方式正确完成数据清洗与预处理？"}, "option": [{"option_text": {"zhcn": "进行词性标注，仅保留动作动词与名词。", "enus": "Perform part-of-speech tagging and keep the action verb and the nouns only."}, "option_flag": false}, {"option_text": {"zhcn": "将所有单词转为小写，使句子规范化。", "enus": "Normalize all words by making the sentence lowercase."}, "option_flag": true}, {"option_text": {"zhcn": "使用英文停用词词典移除停用词。", "enus": "Remove stop words using an English stopword dictionary."}, "option_flag": true}, {"option_text": {"zhcn": "将\"quck\"的排版错误修正为\"quick\"。", "enus": "Correct the typography on \"quck\" to \"quick."}, "option_flag": false}, {"option_text": {"zhcn": "将句子中的所有词语进行独热编码。", "enus": "One-hot encode all words in the sentence."}, "option_flag": false}, {"option_text": {"zhcn": "将句子切分为单词。", "enus": "Tokenize the sentence into words."}, "option_flag": true}], "analysis": {"enus": "The correct operations for sanitizing and preparing the data for Word2Vec are normalizing words to lowercase, removing stop words, and tokenizing sentences into words. \n\nNormalizing words to lowercase ensures that words with different capitalizations are treated as the same, which is important for consistent embeddings. Removing stop words, which are common words like \"the\" and \"and\" that don't carry much semantic meaning, helps to focus on the more significant words in the sentences. Tokenizing the sentences into words is a fundamental step as Word2Vec operates on individual words.\n\nThe fake options are not necessary or appropriate. Part - of - speech tagging and keeping only action verbs and nouns can lead to loss of important context. Correcting typography is not essential for Word2Vec, as it can learn from the context of the misspelled words. One - hot encoding is not suitable for Word2Vec, which aims to generate dense vector embeddings rather than the sparse vectors produced by one - hot encoding.", "zhcn": "为Word2Vec模型进行数据清洗与准备的正确操作包括：将单词统一转为小写、去除停用词，以及将语句切分为独立词汇。单词小写规范化能确保不同大小写形式的词汇被视作同一单元，这对保持词向量的一致性至关重要。去除\"the\"、\"and\"这类语义承载量较少的常见停用词，有助于聚焦句中含有实质意义的词汇。而将语句进行词汇切分则是基础步骤，因为Word2Vec本质是对独立词汇进行建模。\n\n至于其他干扰选项，既非必要亦不适用。例如词性标注后仅保留动作动词和名词的做法，可能导致重要语境信息丢失；校正排版错误对Word2Vec并非必需，因为模型本身能够从误拼词汇的上下文中进行学习；而独热编码则与Word2Vec的特性相悖——该模型旨在生成稠密向量嵌入，而非独热编码产生的稀疏向量。"}, "answer": "BCF"}, {"id": "28", "question": {"enus": "A company is using Amazon Polly to translate plaintext documents to speech for automated company announcements. However, company acronyms are being mispronounced in the current documents. How should a Machine Learning Specialist address this issue for future documents? ", "zhcn": "某公司正采用Amazon Polly将纯文本文档转换为语音，用于自动播放企业公告。然而当前文档中的公司缩写词存在发音错误。机器学习专家应当如何调整，以确保后续文档的发音准确性？"}, "option": [{"option_text": {"zhcn": "将现有文档转换为带有发音标记的SSML格式。", "enus": "Convert current documents to SSML with pronunciation tags."}, "option_flag": true}, {"option_text": {"zhcn": "创建一份得体的发音词典。", "enus": "Create an appropriate pronunciation lexicon."}, "option_flag": false}, {"option_text": {"zhcn": "输出语音标记以辅助发音。", "enus": "Output speech marks to guide in pronunciation."}, "option_flag": false}, {"option_text": {"zhcn": "利用Amazon Lex对文本文件进行发音预处理。", "enus": "Use Amazon Lex to preprocess the text files for pronunciation"}, "option_flag": false}], "analysis": {"enus": "Reference: https://docs.aws.amazon.com/polly/latest/dg/ssml.html", "zhcn": "参考文档：亚马逊Polly服务SSML指南（AWS官方文档链接：https://docs.aws.amazon.com/polly/latest/dg/ssml.html）"}, "answer": "A"}, {"id": "29", "question": {"enus": "An insurance company is developing a new device for vehicles that uses a camera to observe drivers' behavior and alert them when they appear distracted. The company created approximately 10,000 training images in a controlled environment that a Machine Learning Specialist will use to train and evaluate machine learning models. During the model evaluation, the Specialist notices that the training error rate diminishes faster as the number of epochs increases and the model is not accurately inferring on the unseen test images. Which of the following should be used to resolve this issue? (Choose two.) ", "zhcn": "一家保险公司正在研发一款车载新型装置，该装置通过摄像头监测驾驶员行为，并在察觉其分心时发出警示。公司已在受控环境中创建了约一万张训练图像，供机器学习专家用于训练和评估模型。专家在模型评估过程中发现，随着训练周期增加，训练误差率下降速度过快，且模型对未见过测试图像的推断结果欠佳。应采取以下哪两项措施解决此问题？（请选择两项答案）"}, "option": [{"option_text": {"zhcn": "为模型引入梯度消失机制。", "enus": "Add vanishing gradient to the model."}, "option_flag": false}, {"option_text": {"zhcn": "对训练数据进行增强处理。", "enus": "Perform data augmentation on the training data."}, "option_flag": true}, {"option_text": {"zhcn": "使神经网络架构更为精妙。", "enus": "Make the neural network architecture complex."}, "option_flag": false}, {"option_text": {"zhcn": "在模型中运用梯度检验。", "enus": "Use gradient checking in the model."}, "option_flag": false}, {"option_text": {"zhcn": "为模型加入L2正则化。", "enus": "Add L2 regularization to the model."}, "option_flag": true}], "analysis": {"enus": "The correct answers to address the issue of the model not accurately inferring on unseen test images while having a fast - diminishing training error rate are “Perform data augmentation on the training data.” and “Add L2 regularization to the model.”\n\nData augmentation is crucial as the training data was created in a controlled environment, which may lead to overfitting. By performing data augmentation, we can increase the diversity of the training data, making the model more robust and better able to generalize to unseen test images.\n\nAdding L2 regularization to the model helps prevent overfitting. It adds a penalty term to the loss function, which discourages the model from having extremely large weights. This makes the model simpler and less likely to fit the noise in the training data, thus improving its performance on unseen data.\n\nThe option “Add vanishing gradient to the model” is incorrect because the vanishing gradient is a problem in neural networks, not a solution. It causes the gradients to become extremely small during backpropagation, making it difficult for the model to learn.\n\nMaking the neural network architecture complex is also wrong. A more complex architecture is likely to overfit the training data even more, exacerbating the problem of poor performance on unseen test images.\n\n“Use gradient checking in the model” is mainly used for debugging the implementation of backpropagation to ensure that the gradients are calculated correctly. It does not directly address the issue of overfitting and poor generalization on unseen data.\n\nIn summary, the key factors that distinguish the real answer options are their ability to combat overfitting and improve the model's generalization, while the fake options either introduce problems or do not address the root cause of the issue.", "zhcn": "针对模型在训练误差迅速下降后却难以准确推断未见测试图像的问题，正确答案应为“对训练数据进行数据增强”与“在模型中添加L2正则化”。由于训练数据采集自受控环境，模型易出现过拟合现象。通过数据增强，我们能有效提升训练数据的多样性，使模型更具鲁棒性，从而更好地适应未知测试图像。\n\n引入L2正则化机制可抑制过拟合趋势。该技术通过在损失函数中增加惩罚项，约束模型权重不过度膨胀。这种处理既简化了模型结构，又降低了其对训练数据噪声的敏感度，进而提升模型在新数据上的表现。\n\n至于“为模型添加梯度消失”这一选项显然错误，因为梯度消失是神经网络中的常见问题而非解决手段——它会导致反向传播过程中梯度值急剧衰减，阻碍模型的有效学习。\n\n单纯增加神经网络架构复杂度的方案亦不可取。更复杂的结构往往会加剧对训练数据的过拟合，使模型在未知测试图像上的性能进一步恶化。\n\n而“采用梯度检验”主要用于验证反向传播实现的正确性，确保梯度计算无误。该方法并未直接针对过拟合及泛化能力不足的核心问题。\n\n综上，有效解决方案的关键在于能够抑制过拟合并提升泛化能力。错误选项或会引入新问题，或未触及问题本质，均不可取。"}, "answer": "BE"}, {"id": "30", "question": {"enus": "When submitting Amazon SageMaker training jobs using one of the built-in algorithms, which common parameters MUST be specified? (Choose three.) ", "zhcn": "在使用亚马逊SageMaker内置算法提交训练任务时，必须指定以下哪三个通用参数？（请选择三项。）"}, "option": [{"option_text": {"zhcn": "用于识别训练数据在Amazon S3存储桶中位置的训练通道。", "enus": "The training channel identifying the location of training data on an Amazon S3 bucket."}, "option_flag": true}, {"option_text": {"zhcn": "验证通道用于标识亚马逊S3存储桶中验证数据所在的位置。", "enus": "The validation channel identifying the location of validation data on an Amazon S3 bucket."}, "option_flag": false}, {"option_text": {"zhcn": "亚马逊SageMaker可代用户执行任务时所承担的IAM角色。", "enus": "The IAM role that Amazon SageMaker can assume to perform tasks on behalf of the users."}, "option_flag": false}, {"option_text": {"zhcn": "算法所用超参数以JSON数组形式呈现，具体格式参照对应文档说明。", "enus": "Hyperparameters in a JSON array as documented for the algorithm used."}, "option_flag": false}, {"option_text": {"zhcn": "Amazon EC2 实例类型决定了训练任务将采用 CPU 还是 GPU 进行运算。", "enus": "The Amazon EC2 instance class specifying whether training will be run using CPU or GPU."}, "option_flag": true}, {"option_text": {"zhcn": "指定输出路径，用于确定训练完成的模型在Amazon S3存储桶中的保存位置。", "enus": "The output path specifying where on an Amazon S3 bucket the trained model will persist."}, "option_flag": true}], "analysis": {"enus": "The correct answers to the question ‘When submitting Amazon SageMaker training jobs using one of the built - in algorithms, which common parameters MUST be specified? (Choose three.)’ are ‘The training channel identifying the location of training data on an Amazon S3 bucket.’, ‘The Amazon EC2 instance class specifying whether training will be run using CPU or GPU.’, and ‘The output path specifying where on an Amazon S3 bucket the trained model will persist.’\n\nThe training data location is essential as the algorithm needs data to train on. The EC2 instance class must be specified so that SageMaker knows the computing resources to use for the training process. And the output path is necessary to save the trained model.\n\nThe ‘validation channel identifying the location of validation data on an Amazon S3 bucket’ is not mandatory. While validation data can help evaluate the model, training can proceed without it. The IAM role, although important for granting permissions, is a default configuration in many cases and not always a parameter that must be explicitly specified during job submission. Hyperparameters can often use default values provided by the algorithm, so they are not always required to be specified. These key differences explain why the real answer options are chosen over the fake ones.", "zhcn": "关于“使用内置算法提交亚马逊SageMaker训练任务时，必须指定哪些通用参数？（请选择三项）”的正确答案为：  \n\n1. **训练数据通道**（指明训练数据在Amazon S3存储桶中的位置）；  \n2. **亚马逊EC2实例类型**（指定使用CPU或GPU进行训练）；  \n3. **模型输出路径**（指定训练好的模型在Amazon S3存储桶中的保存位置）。  \n\n训练数据位置是核心参数，因为算法需依赖数据进行模型训练；EC2实例类型决定了训练任务所需的计算资源；而输出路径则确保训练后的模型得以持久化存储。  \n\n至于“验证数据通道”（用于指定验证数据在S3桶中的位置）并非必选项。虽然验证数据有助于评估模型性能，但训练任务无需验证数据亦可执行。IAM角色虽在权限管理中至关重要，但在多数场景下已作为默认配置存在，无需在提交任务时显式指定。超参数通常可直接采用算法提供的默认值，因此也非强制填写项。正是这些关键差异，区分了必选参数与备选参数。"}, "answer": "AEF"}, {"id": "31", "question": {"enus": "A monitoring service generates 1 TB of scale metrics record data every minute. A Research team performs queries on this data using Amazon Athena. The queries run slowly due to the large volume of data, and the team requires better performance. How should the records be stored in Amazon S3 to improve query performance? ", "zhcn": "监控服务每分钟产生1TB规模指标记录数据。研究团队使用Amazon Athena对此数据进行查询，由于数据量庞大，查询运行缓慢，团队需要提升查询性能。请问应如何将记录存储在Amazon S3中才能优化查询性能？"}, "option": [{"option_text": {"zhcn": "CSV文件", "enus": "CSV files"}, "option_flag": false}, {"option_text": {"zhcn": "拼花地板文件", "enus": "Parquet files"}, "option_flag": true}, {"option_text": {"zhcn": "压缩版JSON", "enus": "Compressed JSON"}, "option_flag": false}, {"option_text": {"zhcn": "“RecordIO”", "enus": "RecordIO"}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question ‘A monitoring service generates 1 TB of scale metrics record data every minute. A Research team performs queries on this data using Amazon Athena. The queries run slowly due to the large volume of data, and the team requires better performance. How should the records be stored in Amazon S3 to improve query performance?’ is ‘Parquet files’. Parquet is a columnar storage format that is highly optimized for analytics workloads. When querying large datasets with Athena, columnar formats like Parquet allow Athena to read only the columns that are relevant to the query, significantly reducing the amount of data scanned and thus improving query performance.\n\n‘CSV files’ are a simple text - based format. They are row - based, which means Athena has to read entire rows even if only a few columns are needed for the query, leading to slower performance. ‘Compressed JSON’ is also row - based and although compression can save storage space, it doesn't offer the same query - performance benefits as a columnar format. ‘RecordIO’ is mainly used for storing and transporting data in a more efficient way in machine - learning scenarios and is not optimized for Athena's querying needs. The columnar nature of Parquet and its suitability for analytics queries are the key factors that make it the real answer option over the fake options.", "zhcn": "对于\"监控服务每分钟产生1TB指标记录数据，研究团队使用Amazon Athena查询时因数据量过大导致速度缓慢，应如何优化S3存储结构以提升查询性能\"这一问题，正确答案应为\"采用Parquet文件格式\"。Parquet作为一种列式存储格式，专为分析型工作负载深度优化。当使用Athena查询海量数据时，该格式能确保系统仅读取查询涉及的列，从而大幅减少数据扫描量，显著提升查询效率。\n\n相比之下：  \n• **CSV文件**采用基于文本的行式存储，即使查询仅需少数列，仍须读取整行数据，导致性能低下  \n• **压缩JSON**虽节省存储空间，但同样基于行式存储，无法提供列式格式的查询性能优势  \n• **RecordIO**主要应用于机器学习场景的高效数据传输，并未针对Athena查询需求进行优化  \n\nParquet的列式存储特性及其对分析型查询的适配性，使其从备选方案中脱颖而出成为最优解。"}, "answer": "B"}, {"id": "32", "question": {"enus": "Machine Learning Specialist is working with a media company to perform classification on popular articles from the company's website. The company is using random forests to classify how popular an article will be before it is published. A sample of the data being used is below. Given the dataset, the Specialist wants to convert the Day_Of_Week column to binary values. What technique should be used to convert this column to binary values? ", "zhcn": "机器学习专家正与一家传媒公司合作，对其网站热门文章进行自动分类。该公司采用随机森林算法，在文章发布前预测其受欢迎程度。现有数据样本如下所示。针对当前数据集，专家需将\"星期几\"列转换为二进制数值。请问应采用何种技术完成该列数据的二值化转换？"}, "option": [{"option_text": {"zhcn": "二值化", "enus": "Binarization"}, "option_flag": false}, {"option_text": {"zhcn": "独热编码", "enus": "One-hot encoding"}, "option_flag": true}, {"option_text": {"zhcn": "分词", "enus": "Tokenization"}, "option_flag": false}, {"option_text": {"zhcn": "标准化变换", "enus": "Normalization transformation"}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question ‘Given the dataset, the Specialist wants to convert the Day_Of_Week column to binary values. What technique should be used to convert this column to binary values?’ is ‘One - hot encoding’. One - hot encoding is a technique used to convert categorical variables into a binary vector representation. In the case of the Day_Of_Week column, which is a categorical variable with multiple categories (e.g., Monday, Tuesday, etc.), one - hot encoding will create a separate binary column for each category. For a particular row, only the column corresponding to the actual day of the week will have a value of 1, and the rest will be 0.\n\n‘Binarization’ is generally used to convert numerical values into binary values based on a threshold, not for categorical variables like Day_Of_Week. ‘Tokenization’ is mainly used in natural language processing to break text into tokens and is not relevant for converting a categorical column to binary. ‘Normalization transformation’ is used to scale numerical data to a specific range and is not applicable for this categorical conversion task. The nature of the Day_Of_Week as a categorical variable makes one - hot encoding the appropriate technique, distinguishing it from the fake options.", "zhcn": "针对“给定数据集后，专家需将‘星期几’列转换为二进制数值，应采用何种技术实现该转换？”这一问题，正确答案是“独热编码”。独热编码是一种将分类变量转化为二进制向量表示的技术。以“星期几”这一列为例，作为包含多个类别（如周一、周二等）的分类变量，独热编码会为每个类别单独创建一个二值列。对于特定数据行，只有对应实际星期几的列会被标记为1，其余列则标记为0。\n\n“二值化”通常用于基于阈值将数值型数据转换为二进制形式，不适用于“星期几”这类分类变量；“分词处理”主要应用于自然语言处理领域，旨在将文本拆分为标记单元，与分类列的二进制转换无关；“归一化变换”用于将数值数据缩放至特定范围，对此类分类变量转换任务并不适用。正是由于“星期几”作为分类变量的特性，使得独热编码成为恰当选择，从而与其他干扰选项区分开来。"}, "answer": "B"}, {"id": "33", "question": {"enus": "A gaming company has launched an online game where people can start playing for free, but they need to pay if they choose to use certain features. The company needs to build an automated system to predict whether or not a new user will become a paid user within 1 year. The company has gathered a labeled dataset from 1 million users. The training dataset consists of 1,000 positive samples (from users who ended up paying within 1 year) and 999,000 negative samples (from users who did not use any paid features). Each data sample consists of 200 features including user age, device, location, and play patterns. Using this dataset for training, the Data Science team trained a random forest model that converged with over 99% accuracy on the training set. However, the prediction results on a test dataset were not satisfactory Which of the following approaches should the Data Science team take to mitigate this issue? (Choose two.) ", "zhcn": "一家游戏公司推出了一款在线游戏，玩家可免费进入体验，但若想使用特定功能则需付费。该公司需构建一套自动化系统，用于预测新用户是否会在一年内转化为付费用户。目前公司已收集了来自100万名用户的标注数据集，其中训练集包含1000个正样本（即一年内最终付费的用户）和999,000个负样本（未使用任何付费功能的用户）。每个数据样本涵盖200项特征，包括用户年龄、设备、地理位置及游戏行为模式。数据科学团队利用该数据集训练随机森林模型，在训练集上收敛后准确率超过99%，但在测试集上的预测效果却不理想。为改善此问题，数据科学团队应采取以下哪两种措施？（请选择两项）"}, "option": [{"option_text": {"zhcn": "在随机森林中增加更多深层决策树，使模型能够学习更丰富的特征。", "enus": "Add more deep trees to the random forest to enable the model to learn more features."}, "option_flag": false}, {"option_text": {"zhcn": "在训练数据集中加入测试数据集中的样本副本。", "enus": "Include a copy of the samples in the test dataset in the training dataset."}, "option_flag": false}, {"option_text": {"zhcn": "通过复制正样本并对复制数据添加微量噪声，以生成更多正样本。", "enus": "Generate more positive samples by duplicating the positive samples and adding a small amount of noise to the duplicated data."}, "option_flag": true}, {"option_text": {"zhcn": "调整成本函数，使误判情形对成本值的影响大于误报情形。", "enus": "Change the cost function so that false negatives have a higher impact on the cost value than false positives."}, "option_flag": true}, {"option_text": {"zhcn": "调整成本函数，使误判情形对成本值的影响大于漏判情形。", "enus": "Change the cost function so that false positives have a higher impact on the cost value than false negatives."}, "option_flag": false}], "analysis": {"enus": "The question is about mitigating the poor test - set performance of a random forest model trained on a highly imbalanced dataset (1,000 positive and 999,000 negative samples).\n\nThe correct approaches are:\n1. “Generate more positive samples by duplicating the positive samples and adding a small amount of noise to the duplicated data.”: The dataset has a severe class imbalance. By generating more positive samples, we can balance the class distribution, allowing the model to learn better patterns from the under - represented positive class. This can improve the model's ability to predict paid users accurately on the test set.\n2. “Change the cost function so that false negatives have a higher impact on the cost value than false positives.”: In this case, a false negative (predicting a paid user as a non - paid user) is more costly for the company. By increasing the cost of false negatives, the model will be more motivated to correctly identify positive samples, thus improving its performance on the test set.\n\nThe fake answer options are incorrect for the following reasons:\n- “Add more deep trees to the random forest to enable the model to learn more features.”: The problem is class imbalance, not the model's ability to learn features. Adding more deep trees may lead to overfitting on the dominant negative class, further worsening the test - set performance.\n- “Include a copy of the samples in the test dataset in the training dataset.”: This is a fundamental violation of the data splitting principle. Using test data in training will lead to over - optimistic performance estimates and inaccurate evaluation of the model's generalization ability.\n- “Change the cost function so that false positives have a higher impact on the cost value than false negatives.”: Since the company wants to predict paid users, false negatives are more harmful. Penalizing false positives more will not help in accurately identifying paid users and is counter - productive.\n\nCommon misconceptions that might lead to choosing the fake options include not fully understanding the problem of class imbalance, misunderstanding the purpose of cost functions, and not following proper data splitting rules.", "zhcn": "问题涉及如何改善基于高度不平衡数据集（1,000个正样本与999,000个负样本）训练的随机森林模型在测试集上的表现不佳。正确的解决思路如下：\n\n1. **\"通过复制正样本并添加微量噪声以生成更多正样本\"**：数据集存在严重的类别不平衡。通过增加正样本数量，可使类别分布趋于均衡，从而让模型更好地从样本稀少的正类中学习规律。此举能提升模型在测试集上准确预测付费用户的能力。\n\n2. **\"调整损失函数，使假阴性样本对损失值的影响高于假阳性样本\"**：在本场景中，将付费用户误判为非付费用户（假阴性）对企业造成的损失更大。通过提高假阴性的惩罚权重，模型会更专注于正确识别正样本，进而提升测试集表现。\n\n错误选项的不可行原因如下：\n\n- **\"为随机森林添加更多深度树以增强模型特征学习能力\"**：问题核心在于类别不平衡而非特征学习能力不足。增加深度树可能导致模型对占主导地位的负类过拟合，反而加剧测试集性能恶化。\n\n- **\"将测试数据集中的样本复制到训练数据集\"**：此举违背数据划分基本原则。训练阶段使用测试数据会导致性能评估过于乐观，无法准确衡量模型泛化能力。\n\n- **\"调整损失函数使假阳性比假阴性产生更大损失值\"**：由于企业目标是精准预测付费用户，假阴性的危害性更大。若加重假阳性的惩罚，反而会阻碍模型准确识别付费用户，与目标背道而驰。\n\n选择错误选项的常见认知误区包括：未能充分理解类别不平衡问题的本质，对损失函数的作用存在误解，以及忽视规范的数据划分原则。"}, "answer": "CD"}, {"id": "34", "question": {"enus": "A Data Scientist is developing a machine learning model to predict future patient outcomes based on information collected about each patient and their treatment plans. The model should output a continuous value as its prediction. The data available includes labeled outcomes for a set of 4,000 patients. The study was conducted on a group of individuals over the age of 65 who have a particular disease that is known to worsen with age. Initial models have performed poorly. While reviewing the underlying data, the Data Scientist notices that, out of 4,000 patient observations, there are 450 where the patient age has been input as 0. The other features for these observations appear normal compared to the rest of the sample population How should the Data Scientist correct this issue? ", "zhcn": "一位数据科学家正在开发机器学习模型，旨在根据收集到的患者信息及治疗方案预测其未来健康状况。该模型需输出连续数值作为预测结果。现有数据集包含4000名患者的标注结果。此项研究针对65岁以上患有特定疾病的群体展开，该疾病已知会随年龄增长而恶化。初步模型表现不佳。数据科学家在核查底层数据时发现，在4000条患者记录中，有450条记录的年龄输入值为0，而这些观测记录的其他特征与样本总体相比均呈现正常状态。数据科学家应如何解决此问题？"}, "option": [{"option_text": {"zhcn": "从数据集中删除所有年龄标记为0的记录。", "enus": "Drop all records from the dataset where age has been set to 0."}, "option_flag": false}, {"option_text": {"zhcn": "将数据集中年龄字段值为0的记录，替换为该字段的均值或中位数。", "enus": "Replace the age field value for records with a value of 0 with the mean or median value from the dataset"}, "option_flag": true}, {"option_text": {"zhcn": "从数据集中移除年龄特征，并利用其余特征训练模型。", "enus": "Drop the age feature from the dataset and train the model using the rest of the features."}, "option_flag": false}, {"option_text": {"zhcn": "运用k-means聚类算法处理缺失特征。", "enus": "Use k-means clustering to handle missing features"}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Replace the age field value for records with a value of 0 with the mean or median value from the dataset”. This approach is appropriate because the age of 0 is clearly an input error as the study is on individuals over 65. Replacing these values with the mean or median retains the useful information from the other features of these records, which can contribute to the model's learning.\n\nThe option “Drop all records from the dataset where age has been set to 0” is not ideal as it discards a significant number (450 out of 4000) of data points. This can lead to loss of valuable information from the other features of these records, potentially reducing the model's performance.\n\n“Drop the age feature from the dataset and train the model using the rest of the features” is also a poor choice. Age is an important factor as the disease is known to worsen with age, so removing this feature can lead to a less accurate model.\n\n“Use k - means clustering to handle missing features” is irrelevant here because the age values are not missing but incorrectly entered as 0. K - means clustering is used for grouping data based on similarity, not for correcting input errors. Thus, replacing the incorrect age values with mean or median is the best approach, distinguishing it from the fake options.", "zhcn": "对于题目中提及的年龄字段值为0的记录，正确的处理方式是「将其替换为数据集的均值或中位数」。由于本研究对象为65岁以上人群，年龄值为0显然属于录入错误，采用该替换方法既能修正异常值，又能保留这些记录中其他特征的有效信息，从而助力模型学习。  \n  \n若选择「直接删除所有年龄值为0的记录」则欠妥——这将导致4000条数据中损失450个样本，其他特征的宝贵信息也会随之湮灭，可能削弱模型性能。  \n  \n「删除年龄特征并仅使用其余特征训练模型」亦非良策。已知该疾病随年龄增长而恶化，年龄是重要影响因素，剔除这一特征会降低模型预测精度。  \n  \n至于「采用k均值聚类处理缺失特征」的方法在此并不适用，因为问题并非数据缺失，而是数值被误输为0。k均值聚类本用于基于相似性的数据分组，而非纠正输入错误。  \n  \n综上，通过均值或中位数替换修正异常年龄值是最佳方案，其余选项均存在明显缺陷。"}, "answer": "B"}, {"id": "35", "question": {"enus": "A Data Science team is designing a dataset repository where it will store a large amount of training data commonly used in its machine learning models. As Data Scientists may create an arbitrary number of new datasets every day, the solution has to scale automatically and be cost-effective. Also, it must be possible to explore the data using SQL. Which storage scheme is MOST adapted to this scenario? ", "zhcn": "一个数据科学团队正在设计数据集存储库，用于集中存储其机器学习模型常用的大规模训练数据。由于数据科学家可能每日创建任意数量的新数据集，该解决方案需具备自动扩展能力且符合成本效益。同时，必须支持通过SQL进行数据探索。下列存储方案中哪种最符合此场景需求？"}, "option": [{"option_text": {"zhcn": "将数据集以文件形式存储于Amazon S3中。", "enus": "Store datasets as files in Amazon S3."}, "option_flag": true}, {"option_text": {"zhcn": "将数据集以文件形式存储于挂载在Amazon EC2实例的Amazon EBS卷中。", "enus": "Store datasets as files in an Amazon EBS volume attached to an Amazon EC2 instance."}, "option_flag": false}, {"option_text": {"zhcn": "将数据集以表格形式存储于多节点亚马逊Redshift集群中。", "enus": "Store datasets as tables in a multi-node Amazon Redshift cluster."}, "option_flag": false}, {"option_text": {"zhcn": "将数据集存储为 Amazon DynamoDB 中的全局表。", "enus": "Store datasets as global tables in Amazon DynamoDB."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is ‘Store datasets as files in Amazon S3.’ Amazon S3 is highly scalable, allowing it to handle an arbitrary number of new datasets created daily with ease. It is also cost - effective as you only pay for the storage you use. Moreover, S3 can be integrated with services like Amazon Athena, which enables data exploration using SQL.\n\nThe option ‘Store datasets as files in an Amazon EBS volume attached to an Amazon EC2 instance’ is not suitable because EBS volumes are mainly designed for use with EC2 instances and have limitations in terms of scalability and cost - efficiency for large - scale data storage.\n\n‘Store datasets as tables in a multi - node Amazon Redshift cluster’ is a powerful data warehousing solution, but it is more expensive and may not scale as automatically as S3 for an unpredictable number of new datasets.\n\n‘Store datasets as global tables in Amazon DynamoDB’ is a NoSQL database. While it is scalable, it does not support SQL natively, so it cannot meet the requirement of data exploration using SQL. This is why ‘Store datasets as files in Amazon S3’ is the most appropriate option, distinguishing it from the fake options.", "zhcn": "对于该问题的正确答案是\"将数据集以文件形式存储于Amazon S3\"。Amazon S3具有高度可扩展性，能够轻松应对每日新增任意数量数据集的需求。其成本效益亦十分突出，您只需为实际使用的存储空间付费。此外，S3可与Amazon Athena等服务无缝集成，支持通过SQL进行数据探索。  \n\n而\"将数据集存储于挂载至Amazon EC2实例的Amazon EBS卷\"这一方案并不适用，因为EBS卷主要设计用于EC2实例，在大规模数据存储的扩展性和成本效益方面存在局限。  \n\n\"将数据集以表形式存储于多节点Amazon Redshift集群\"虽是强大的数据仓库解决方案，但成本较高，且面对不可预测的新增数据集时，其自动扩展能力不如S3灵活。  \n\n\"将数据集以全局表形式存储于Amazon DynamoDB\"作为NoSQL数据库，虽具备扩展性，但无法原生支持SQL查询，故不能满足基于SQL进行数据探索的要求。  \n\n正因如此，\"将数据集以文件形式存储于Amazon S3\"成为最合适的选择，这也使其在诸多选项中脱颖而出。"}, "answer": "A"}, {"id": "36", "question": {"enus": "A Machine Learning Specialist deployed a model that provides product recommendations on a company's website. Initially, the model was performing very well and resulted in customers buying more products on average. However, within the past few months, the Specialist has noticed that the effect of product recommendations has diminished and customers are starting to return to their original habits of spending less. The Specialist is unsure of what happened, as the model has not changed from its initial deployment over a year ago. Which method should the Specialist try to improve model performance? ", "zhcn": "一位机器学习专家部署了一套为某公司网站提供商品推荐服务的模型。该模型初期表现卓越，有效提升了顾客的平均购买金额。然而近几个月来，专家发现推荐效果逐渐减弱，顾客消费习惯似乎正回归到以往较低的水平。尽管该模型自一年前部署以来从未经过改动，专家仍无法确定症结所在。此时，应采取何种方法提升模型效能？"}, "option": [{"option_text": {"zhcn": "该模型必须彻底重新设计，因其无法有效处理产品库存的变动。", "enus": "The model needs to be completely re-engineered because it is unable to handle product inventory changes."}, "option_flag": false}, {"option_text": {"zhcn": "模型超参数需定期更新，以防出现偏移。", "enus": "The model's hyperparameters should be periodically updated to prevent drift."}, "option_flag": false}, {"option_text": {"zhcn": "模型需定期基于原始数据重新训练，同时加入正则化项以应对产品库存变动。", "enus": "The model should be periodically retrained from scratch using the original data while adding a regularization term to handle product  inventory changes"}, "option_flag": false}, {"option_text": {"zhcn": "随着产品库存的变化，应定期使用原始训练数据结合新增数据对模型进行重新训练。", "enus": "The model should be periodically retrained using the original training data plus new data as product inventory changes."}, "option_flag": true}], "analysis": {"enus": "The correct answer to the question is “The model should be periodically retrained using the original training data plus new data as product inventory changes.” This is because over time, the product inventory on the website changes, and the model's performance has declined. By incorporating new data along with the original training data, the model can adapt to these changes and better predict customer preferences.\n\nThe option “The model needs to be completely re - engineered because it is unable to handle product inventory changes” is an overreaction. There's no indication that the model's fundamental design is flawed; it just needs to be updated with new data.\n\n“Updating the model's hyperparameters to prevent drift” might not address the root cause. The issue here is related to changes in product inventory, not hyperparameter settings.\n\n“Retraining the model from scratch using only the original data with a regularization term” doesn't account for the new product inventory. The model needs new data to adapt to the current state of the product catalog.\n\nThe key factor in choosing the real answer is that it directly addresses the problem of changing product inventory by including new data in the retraining process, which is why it stands out from the fake options.", "zhcn": "对于该问题的正确答案是：\"模型应随着产品库存的变化，定期使用原始训练数据结合新增数据进行重新训练。\"这是因为网站上的产品库存会随时间推移而变动，导致模型性能逐渐下降。通过将新增数据与原始训练数据相结合进行再训练，模型能够适应这些变化，从而更精准地预测客户偏好。\n\n而\"模型需要彻底重构，因其无法应对产品库存变化\"这一选项实属过度反应。目前并无迹象表明模型的基础设计存在缺陷，仅需通过新增数据更新即可。\n\n\"调整模型超参数以防止漂移\"可能无法解决根本问题。当前困境源于产品库存变动，与超参数设置无关。\n\n\"仅采用原始数据添加正则化项重新训练模型\"则未考虑新产品库存的影响。模型必须借助新数据才能适应当前产品目录的实际情况。\n\n选择正确答案的关键在于：该方案通过将新增数据纳入再训练过程，直指产品库存变动的核心问题，这使其从其他干扰项中脱颖而出。"}, "answer": "D"}, {"id": "37", "question": {"enus": "A Machine Learning Specialist working for an online fashion company wants to build a data ingestion solution for the company's Amazon S3- based data lake. The Specialist wants to create a set of ingestion mechanisms that will enable future capabilities comprised of: ✑ Real-time analytics ✑ Interactive analytics of historical data ✑ Clickstream analytics ✑ Product recommendations Which services should the Specialist use? ", "zhcn": "某在线时尚公司的机器学习专家计划为公司基于亚马逊S3的数据湖构建一套数据摄取方案。该专家需要设计一组数据接入机制，以支撑未来实现以下功能：  \n✧ 实时数据分析  \n✧ 历史数据交互式分析  \n✧ 点击流分析  \n✧ 商品推荐系统  \n请问专家应当采用哪些服务？"}, "option": [{"option_text": {"zhcn": "以AWS Glue作为数据目录；通过Amazon Kinesis数据流及数据分析服务实现实时数据洞察；借助Amazon Kinesis数据火线将数据输送至Amazon ES进行点击流分析；运用Amazon EMR生成个性化产品推荐方案。", "enus": "AWS Glue as the data catalog; Amazon Kinesis Data Streams and Amazon Kinesis Data Analytics for real-time data insights; Amazon  Kinesis Data Firehose for delivery to Amazon ES for clickstream analytics; Amazon EMR to generate personalized product  recommendations"}, "option_flag": true}, {"option_text": {"zhcn": "以Amazon Athena作为数据目录：通过Amazon Kinesis数据流与Amazon Kinesis数据分析服务实现近实时数据洞察；运用Amazon Kinesis数据火线进行点击流分析；借助AWS Glue生成个性化产品推荐方案。", "enus": "Amazon Athena as the data catalog: Amazon Kinesis Data Streams and Amazon Kinesis Data Analytics for near-real-time data insights;  Amazon Kinesis Data Firehose for clickstream analytics; AWS Glue to generate personalized product recommendations"}, "option_flag": false}, {"option_text": {"zhcn": "以AWS Glue作为元数据目录；通过Amazon Kinesis数据流及数据分析服务实现历史数据洞察；借助Amazon Kinesis数据火线将数据实时输送至Amazon ES进行点击流分析；采用Amazon EMR框架生成个性化商品推荐方案。", "enus": "AWS Glue as the data catalog; Amazon Kinesis Data Streams and Amazon Kinesis Data Analytics for historical data insights; Amazon  Kinesis Data Firehose for delivery to Amazon ES for clickstream analytics; Amazon EMR to generate personalized product  recommendations"}, "option_flag": false}, {"option_text": {"zhcn": "以Amazon Athena作为数据目录核心；通过Amazon Kinesis数据流与数据分析服务挖掘历史数据价值；借助Amazon DynamoDB流处理技术实现用户点击行为分析；运用AWS Glue构建个性化商品推荐引擎。", "enus": "Amazon Athena as the data catalog; Amazon Kinesis Data Streams and Amazon Kinesis Data Analytics for historical data insights;  Amazon DynamoDB streams for clickstream analytics; AWS Glue to generate personalized product recommendations"}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to use “AWS Glue as the data catalog; Amazon Kinesis Data Streams and Amazon Kinesis Data Analytics for real - time data insights; Amazon Kinesis Data Firehose for delivery to Amazon ES for clickstream analytics; Amazon EMR to generate personalized product recommendations”. \n\nAWS Glue is a suitable data catalog as it can manage metadata for the data lake, making it easier to organize and access data. Amazon Kinesis Data Streams and Analytics are used for real - time data insights, which is crucial for real - time analytics. Kinesis Data Firehose can efficiently deliver clickstream data to Amazon ES for analytics. Amazon EMR is well - equipped to handle the complex processing required for generating personalized product recommendations.\n\nThe first fake option is incorrect because Amazon Athena is mainly for ad - hoc querying of data in S3 and not a dedicated data catalog like AWS Glue. Also, AWS Glue is not the right tool for generating personalized product recommendations; Amazon EMR is better for such complex processing.\n\nThe second fake option misuses Amazon Kinesis Data Streams and Analytics for historical data insights. These services are designed for real - time data, not historical data analysis.\n\nThe third fake option has multiple issues. Amazon Athena is not a proper data catalog. Using Kinesis for historical data insights is wrong as it's for real - time. Amazon DynamoDB streams are not the best choice for clickstream analytics compared to Kinesis Data Firehose. And again, AWS Glue is not suitable for generating product recommendations. \n\nThese common misconceptions might arise from a lack of understanding of the specific use - cases and capabilities of each AWS service.", "zhcn": "该问题的正确答案是采用“AWS Glue作为数据目录；通过Amazon Kinesis Data Streams和Amazon Kinesis Data Analytics实现实时数据洞察；利用Amazon Kinesis Data Firehose将数据传送至Amazon ES进行点击流分析；借助Amazon EMR生成个性化产品推荐”。AWS Glue适合作为数据目录，因其能有效管理数据湖的元数据，使数据组织与访问更为高效。Amazon Kinesis Data Streams与Analytics专攻实时数据洞察，这对实时分析至关重要。Kinesis Data Firehose可高效将点击流数据输送至Amazon ES进行分析，而Amazon EMR则具备处理复杂计算的能力，适用于生成个性化产品推荐。\n\n第一个错误选项的问题在于：Amazon Athena主要用于对S3中的数据进行即席查询，并非像AWS Glue那样专用于数据目录。此外，AWS Glue并不适合生成个性化产品推荐，此类复杂处理任务由Amazon EMR承担更为合适。\n\n第二个错误选项误将Amazon Kinesis Data Streams和Analytics用于历史数据分析。这两项服务本为实时数据设计，而非针对历史数据追溯。\n\n第三个错误选项存在多处谬误：Amazon Athena并非合适的数据目录工具；将适用于实时处理的Kinesis用于历史数据洞察显然不当；相较于Kinesis Data Firehose，Amazon DynamoDB流并非点击流分析的最佳选择；而AWS Glue仍不适用于产品推荐生成。这些常见误解往往源于对各项AWS服务特定应用场景及功能边界的模糊认知。"}, "answer": "A"}, {"id": "38", "question": {"enus": "A company is observing low accuracy while training on the default built-in image classification algorithm in Amazon SageMaker. The Data Science team wants to use an Inception neural network architecture instead of a ResNet architecture. Which of the following will accomplish this? (Choose two.) ", "zhcn": "某公司在使用亚马逊SageMaker内置默认图像分类算法进行训练时发现准确率偏低。数据科学团队希望采用Inception神经网络架构替代原有的ResNet架构。下列哪两项措施能够实现这一目标？（请选择两个正确答案。）"}, "option": [{"option_text": {"zhcn": "对内置图像分类算法进行定制，采用Inception架构并应用于模型训练。", "enus": "Customize the built-in image classification algorithm to use Inception and use this for model training."}, "option_flag": false}, {"option_text": {"zhcn": "请向 SageMaker 团队提交技术支持请求，将默认的图像分类算法更改为 Inception。", "enus": "Create a support case with the SageMaker team to change the default image classification algorithm to Inception."}, "option_flag": false}, {"option_text": {"zhcn": "将搭载Inception网络的TensorFlow Estimator封装至Docker容器，并用于模型训练。", "enus": "Bundle a Docker container with TensorFlow Estimator loaded with an Inception network and use this for model training."}, "option_flag": true}, {"option_text": {"zhcn": "在Amazon SageMaker中结合TensorFlow Estimator运用自定义代码，通过Inception网络架构加载模型，并将其应用于模型训练过程。", "enus": "Use custom code in Amazon SageMaker with TensorFlow Estimator to load the model with an Inception network, and use this for model  training."}, "option_flag": true}, {"option_text": {"zhcn": "将初始网络代码下载并利用apt-get安装至亚马逊EC2实例，随后将该实例配置为亚马逊SageMaker平台中的Jupyter笔记本运行环境。", "enus": "Download and apt-get install the inception network code into an Amazon EC2 instance and use this instance as a Jupyter notebook in  Amazon SageMaker."}, "option_flag": false}], "analysis": {"enus": "The correct answers to the question are “Bundle a Docker container with TensorFlow Estimator loaded with an Inception network and use this for model training” and “Use custom code in Amazon SageMaker with TensorFlow Estimator to load the model with an Inception network, and use this for model  training”. \n\nFor the first real answer, bundling a Docker container with a TensorFlow Estimator loaded with an Inception network allows for a self - contained environment to train the model using the desired architecture. The second real answer enables the use of custom code in Amazon SageMaker with a TensorFlow Estimator to load the Inception model, providing flexibility in the training process.\n\nThe first fake answer is incorrect because the built - in image classification algorithm in Amazon SageMaker is not easily customizable to switch from ResNet to Inception. The second fake answer is wrong as creating a support case to change the default algorithm is not a valid approach; users are expected to implement custom solutions. The third fake answer is also incorrect as simply downloading and installing the Inception code on an EC2 instance and using it as a Jupyter notebook in SageMaker does not address integrating the model properly for training within the SageMaker environment. These fake options represent common misconceptions about the flexibility and usage of Amazon SageMaker's built - in algorithms and custom model integration.", "zhcn": "该问题的正确答案是：“将搭载Inception网络的TensorFlow Estimator与Docker容器整合，用于模型训练”以及“在Amazon SageMaker中结合TensorFlow Estimator使用自定义代码加载Inception网络模型，并应用于模型训练”。针对第一个有效方案，通过整合搭载Inception网络的TensorFlow Estimator与Docker容器，可构建独立环境以基于指定架构进行模型训练。第二个有效方案则允许在Amazon SageMaker中灵活运用自定义代码配合TensorFlow Estimator加载Inception模型，为训练过程提供定制化空间。\n\n第一个错误选项不可取，因为Amazon SageMaker内置图像分类算法无法轻松实现从ResNet到Inception架构的切换。第二个错误方案中通过创建技术支持案例来更改默认算法的做法并不成立，用户应当自行实现定制解决方案。第三个错误方案仅涉及在EC2实例上下载安装Inception代码并作为SageMaker的Jupyter笔记本使用，未能解决在SageMaker环境中正确集成模型进行训练的核心问题。这些错误选项反映了对Amazon SageMaker内置算法灵活性及自定义模型集成方式的常见误解。"}, "answer": "CD"}, {"id": "39", "question": {"enus": "A Machine Learning Specialist built an image classification deep learning model. However, the Specialist ran into an overfitting problem in which the training and testing accuracies were 99% and 75%, respectively. How should the Specialist address this issue and what is the reason behind it? ", "zhcn": "一位机器学习专家构建了一个图像分类深度学习模型，但遇到了过拟合问题——训练集准确率高达99%，而测试集准确率仅为75%。请问这位专家应当如何解决此问题？其背后的成因又是什么？"}, "option": [{"option_text": {"zhcn": "学习速率应适当提高，因为优化过程目前陷入了局部极小值的困境。", "enus": "The learning rate should be increased because the optimization process was trapped at a local minimum."}, "option_flag": false}, {"option_text": {"zhcn": "鉴于模型泛化能力尚有不足，建议适当提高全连接层的丢弃率。", "enus": "The dropout rate at the fiatten layer should be increased because the model is not generalized enough."}, "option_flag": false}, {"option_text": {"zhcn": "与展平层相邻的全连接层维度应适当增加，因为当前模型的复杂度尚有不足。", "enus": "The dimensionality of dense layer next to the fiatten layer should be increased because the model is not complex enough."}, "option_flag": false}, {"option_text": {"zhcn": "由于优化过程在达到全局最小值前便已终止，应适当增加训练轮次。", "enus": "The epoch number should be increased because the optimization process was terminated before it reached the global minimum."}, "option_flag": true}], "analysis": {"enus": "Reference: https://www.tensorfiow.org/tutorials/keras/overfit_and_underfit", "zhcn": "参考来源：https://www.tensorfiow.org/tutorials/keras/overfit_and_underfit"}, "answer": "D"}, {"id": "40", "question": {"enus": "A Machine Learning team uses Amazon SageMaker to train an Apache MXNet handwritten digit classifier model using a research dataset. The team wants to receive a notification when the model is overfitting. Auditors want to view the Amazon SageMaker log activity report to ensure there are no unauthorized API calls. What should the Machine Learning team do to address the requirements with the least amount of code and fewest steps? ", "zhcn": "一个机器学习团队正在运用Amazon SageMaker平台，基于研究数据集训练Apache MXNet手写数字分类模型。该团队希望在模型出现过拟合时能接收到通知。审计人员则需要查看Amazon SageMaker的日志活动报告，以确认不存在未经授权的API调用。机器学习团队应当采取何种方案，才能以最简代码和最少步骤满足这些需求？\n\n（注：专有名词如Amazon SageMaker、Apache MXNet、API均保留原表达，符合技术文档惯例；中文表达采用\"运用\"\"基于\"\"出现过拟合\"\"未经授权\"等专业术语，并通过\"应当采取何种方案\"\"以...满足这些需求\"等句式保持逻辑严谨性，同时避免直译的生硬感。）"}, "option": [{"option_text": {"zhcn": "实现一项AWS Lambda功能，用于将Amazon SageMaker的API调用记录同步至Amazon S3存储服务。同时编写代码向Amazon CloudWatch推送自定义指标，并在CloudWatch中创建告警机制，通过Amazon SNS服务在模型出现过拟合时接收通知。", "enus": "Implement an AWS Lambda function to log Amazon SageMaker API calls to Amazon S3. Add code to push a custom metric to Amazon  CloudWatch. Create an alarm in CloudWatch with Amazon SNS to receive a notification when the model is overfitting."}, "option_flag": false}, {"option_text": {"zhcn": "利用AWS CloudTrail将Amazon SageMaker的API调用日志记录至Amazon S3存储桶，并编写代码向Amazon CloudWatch推送自定义指标。随后在CloudWatch中设置警报机制，通过Amazon SNS接收模型过拟合时的实时通知。", "enus": "Use AWS CloudTrail to log Amazon SageMaker API calls to Amazon S3. Add code to push a custom metric to Amazon CloudWatch.  Create an alarm in CloudWatch with Amazon SNS to receive a notification when the model is overfitting."}, "option_flag": true}, {"option_text": {"zhcn": "实现一个AWS Lambda函数，用于将Amazon SageMaker的API调用记录至AWS CloudTrail。添加代码以向Amazon CloudWatch推送自定义指标。在CloudWatch中创建告警机制，并通过Amazon SNS接收模型过拟合时的通知。", "enus": "Implement an AWS Lambda function to log Amazon SageMaker API calls to AWS CloudTrail. Add code to push a custom metric to  Amazon CloudWatch. Create an alarm in CloudWatch with Amazon SNS to receive a notification when the model is overfitting."}, "option_flag": false}, {"option_text": {"zhcn": "利用AWS CloudTrail将Amazon SageMaker的API调用记录存储至Amazon S3，并配置Amazon SNS服务，以便在模型出现过拟合时接收实时通知。", "enus": "Use AWS CloudTrail to log Amazon SageMaker API calls to Amazon S3. Set up Amazon SNS to receive a notification when the model is  overfitting"}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Use AWS CloudTrail to log Amazon SageMaker API calls to Amazon S3. Add code to push a custom metric to Amazon CloudWatch.  Create an alarm in CloudWatch with Amazon SNS to receive a notification when the model is overfitting.” \n\nAWS CloudTrail is a native service that automatically logs API calls, which requires no additional code to set up the logging of SageMaker API calls to S3, making it a simple and efficient choice for auditors to view the log activity. To detect model over - fitting, pushing a custom metric to CloudWatch and creating an alarm with SNS is a standard and straightforward way to get notifications.\n\nThe first fake option suggests using an AWS Lambda function to log API calls. Lambda functions require writing and managing code, which adds complexity and more steps compared to using CloudTrail. The second fake option is also about using a Lambda function for logging API calls to CloudTrail, which is redundant as CloudTrail already captures API calls natively. The third fake option fails to mention pushing a custom metric to CloudWatch. Without a custom metric, there is no way to accurately determine if the model is overfitting, so it cannot meet the requirement of getting a notification when overfitting occurs.\n\nIn summary, the real answer option provides the simplest and most effective way to meet both the auditing and over - fitting notification requirements with the least amount of code and fewest steps.", "zhcn": "针对该问题的正确答案是\"使用AWS CloudTrail将Amazon SageMaker的API调用记录至Amazon S3，添加代码将自定义指标推送至Amazon CloudWatch，并在CloudWatch中创建警报配合Amazon SNS接收模型过拟合通知\"。AWS CloudTrail作为原生服务可自动记录API调用，无需额外代码即可实现SageMaker API调用至S3的日志记录，使审计人员能够便捷查看日志活动。为检测模型过拟合，向CloudWatch推送自定义指标并创建SNS警报是实现通知的标准且直接的方式。\n\n第一个干扰方案建议使用AWS Lambda函数记录API调用。但Lambda需编写和维护代码，相较于直接使用CloudTrail更为复杂。第二个干扰方案虽提及通过Lambda将API日志传至CloudTrail，然CloudTrail本身已具备原生记录功能，此方案实属冗余。第三个干扰方案未包含向CloudWatch推送自定义指标的关键步骤，而缺乏自定义指标将无法准确判断模型是否过拟合，故无法满足过拟合时发送通知的核心需求。\n\n综上所述，正确答案方案以最简代码和最少步骤同时满足了审计需求与过拟合通知要求，是实现目标的最优路径。"}, "answer": "B"}, {"id": "41", "question": {"enus": "A Machine Learning Specialist is building a prediction model for a large number of features using linear models, such as linear regression and logistic regression. During exploratory data analysis, the Specialist observes that many features are highly correlated with each other. This may make the model unstable. What should be done to reduce the impact of having such a large number of features? ", "zhcn": "一位机器学习专家正在运用线性回归与逻辑回归等线性模型，为海量特征构建预测模型。在探索性数据分析阶段，该专家发现多个特征之间存在高度相关性，这种情况可能导致模型稳定性下降。面对如此庞大的特征数量，应采取何种措施来降低其对模型的影响？"}, "option": [{"option_text": {"zhcn": "对高度相关的特征进行独热编码处理。", "enus": "Perform one-hot encoding on highly correlated features."}, "option_flag": false}, {"option_text": {"zhcn": "对高度相关的特征采用矩阵乘法进行处理。", "enus": "Use matrix multiplication on highly correlated features."}, "option_flag": false}, {"option_text": {"zhcn": "运用主成分分析（PCA）构建新的特征空间。", "enus": "Create a new feature space using principal component analysis (PCA)"}, "option_flag": true}, {"option_text": {"zhcn": "运用皮尔逊相关系数。", "enus": "Apply the Pearson correlation coeficient."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is ‘Create a new feature space using principal component analysis (PCA)’. PCA is a dimensionality - reduction technique that transforms a set of correlated variables into a set of uncorrelated variables called principal components. By doing so, it can reduce the number of features while retaining most of the important information, which helps to make the linear model more stable.\n\n‘Perform one - hot encoding on highly correlated features’ is incorrect because one - hot encoding is used to convert categorical variables into a binary matrix, and it does not address the issue of correlated features. ‘Use matrix multiplication on highly correlated features’ has no direct relation to reducing the impact of correlated features. Matrix multiplication is a general linear algebra operation and does not serve the purpose of dimensionality reduction. ‘Apply the Pearson correlation coefficient’ only measures the linear relationship between two variables. It does not reduce the number of features or solve the problem of model instability caused by highly correlated features.\n\nA common misconception might be thinking that one - hot encoding can somehow handle correlated features, or that matrix multiplication can transform the features to reduce correlation. Also, some may misunderstand that measuring correlation (using Pearson coefficient) is the same as reducing its impact on the model.", "zhcn": "对于题目中\"如何通过特征工程降低线性模型因特征共线性而产生的不稳定性\"这一问题，正确答案是\"采用主成分分析法构建新特征空间\"。主成分分析法作为一种降维技术，能够将存在相关性的变量集转化为一组互不相关的主成分变量。这种方法在保留关键信息的同时有效削减特征数量，从而增强线性模型的稳定性。\n\n而\"对高相关特征进行独热编码\"这一处理方式并不恰当，因为独热编码适用于将分类变量转换为二元矩阵，并不能解决特征间的共线性问题。\"对高相关特征实施矩阵乘法\"同样与缓解共线性无关，该运算属于通用线性代数操作，并不具备降维功能。\"应用皮尔逊相关系数\"仅能衡量变量间的线性关联程度，既无法减少特征维度，也不能解决高相关特征导致的模型失稳问题。\n\n常见的理解偏差包括：误认为独热编码可以处理特征相关性，或期待矩阵乘法能通过变换特征来降低关联度。此外，部分人可能将衡量相关性的皮尔逊系数与消除相关性影响的方法混为一谈。"}, "answer": "C"}, {"id": "42", "question": {"enus": "A Machine Learning Specialist is implementing a full Bayesian network on a dataset that describes public transit in New York City. One of the random variables is discrete, and represents the number of minutes New Yorkers wait for a bus given that the buses cycle every 10 minutes, with a mean of 3 minutes. Which prior probability distribution should the ML Specialist use for this variable? ", "zhcn": "一位机器学习专家正在基于描述纽约市公共交通的数据集构建完整的贝叶斯网络。其中一个随机变量为离散型，代表在公交车每10分钟一班的情况下纽约民众的候车时间（已知平均等候时间为3分钟）。针对该变量，机器学习专家应采用何种先验概率分布？"}, "option": [{"option_text": {"zhcn": "泊松分布", "enus": "Poisson distribution"}, "option_flag": false}, {"option_text": {"zhcn": "均匀分布", "enus": "Uniform distribution"}, "option_flag": false}, {"option_text": {"zhcn": "正态分布", "enus": "Normal distribution"}, "option_flag": false}, {"option_text": {"zhcn": "二项分布", "enus": "Binomial distribution"}, "option_flag": true}], "analysis": {"enus": "The correct answer to the question is the ‘Binomial distribution’. The binomial distribution is appropriate here because the situation can be modeled as a series of independent yes - no trials. In the context of waiting for a bus that cycles every 10 minutes, we can think of each short time interval within those 10 minutes as a trial where the ‘success’ could be the bus arriving. The number of minutes waited is a discrete variable, and binomial distribution is used for discrete random variables representing the number of successes in a fixed number of trials.\n\nThe ‘Poisson distribution’ is typically used to model the number of events occurring in a fixed interval of time or space when these events happen randomly and independently at a constant average rate, which doesn't fit this bus - waiting scenario. The ‘Uniform distribution’ implies that all possible values within a range are equally likely, but when waiting for a bus, the probability is not evenly spread over the 10 - minute interval. The ‘Normal distribution’ is used for continuous variables, while the number of minutes waited is a discrete variable. These differences in the nature of the distributions are why the binomial distribution is the real answer, distinguishing it from the fake options.", "zhcn": "问题的正确答案是“二项分布”。此处适用二项分布，是因为该场景可被建模为一系列独立的二元试验。在等候每10分钟一班公交车的情境中，我们可以将10分钟内的每个微小时间区间视作一次试验，而“成功”则对应公交车到站的事件。等待的分钟数为离散变量，二项分布正是用于描述固定次数试验中成功次数的离散随机变量。\n\n“泊松分布”通常用于模拟固定时间或空间内随机独立发生且平均速率恒定的事件数量，这与候车场景的特征不符。“均匀分布”意味着取值范围内所有结果概率均等，但公交车到站概率在10分钟区间内并非均匀分布。“正态分布”适用于连续变量，而等待时间分钟数是离散变量。正是这些分布特性上的差异，使二项分布成为真正答案，从而与其他干扰选项区分开来。"}, "answer": "D"}, {"id": "43", "question": {"enus": "A Data Science team within a large company uses Amazon SageMaker notebooks to access data stored in Amazon S3 buckets. The IT Security team is concerned that internet-enabled notebook instances create a security vulnerability where malicious code running on the instances could compromise data privacy. The company mandates that all instances stay within a secured VPC with no internet access, and data communication trafic must stay within the AWS network. How should the Data Science team configure the notebook instance placement to meet these requirements? ", "zhcn": "某大型公司的数据科学团队采用Amazon SageMaker笔记本来访问存储于Amazon S3桶中的数据。鉴于可连接互联网的笔记本实例可能引发恶意代码窃取数据隐私的安全隐患，IT安全部门要求所有实例必须部署在无互联网访问的受保护VPC内，且数据通信流量必须限制在AWS网络内部。为满足这些要求，数据科学团队应如何配置笔记本实例的部署方案？"}, "option": [{"option_text": {"zhcn": "将Amazon SageMaker笔记本实例关联至VPC内的私有子网，并将Amazon SageMaker终端节点与S3存储桶部署在同一VPC中。", "enus": "Associate the Amazon SageMaker notebook with a private subnet in a VPC. Place the Amazon SageMaker endpoint and S3 buckets  within the same VPC."}, "option_flag": false}, {"option_text": {"zhcn": "将Amazon SageMaker笔记本关联至VPC内的私有子网。", "enus": "Associate the Amazon SageMaker notebook with a private subnet in a VP"}, "option_flag": false}, {"option_text": {"zhcn": "运用IAM策略授予对Amazon S3与Amazon SageMaker的访问权限。将Amazon SageMaker笔记本实例关联至VPC的私有子网中，并确保该VPC已配置S3 VPC终端节点及Amazon SageMaker VPC终端节点。", "enus": "Use IAM policies to grant access to Amazon S3 and Amazon  SageMaker.  C. Associate the Amazon SageMaker notebook with a private subnet in a VPC. Ensure the VPC has S3 VPC endpoints and Amazon  SageMaker VPC endpoints attached to it."}, "option_flag": true}, {"option_text": {"zhcn": "将Amazon SageMaker笔记本实例关联至VPC环境中的私有子网。需确保该VPC已配置NAT网关，并设置仅允许出站连接访问Amazon S3及Amazon SageMaker的安全组。", "enus": "Associate the Amazon SageMaker notebook with a private subnet in a VPC. Ensure the VPC has a NAT gateway and an associated  security group allowing only outbound connections to Amazon S3 and Amazon SageMaker."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to use IAM policies to grant access to Amazon S3 and Amazon SageMaker and associate the Amazon SageMaker notebook with a private subnet in a VPC, ensuring the VPC has S3 VPC endpoints and Amazon SageMaker VPC endpoints attached to it. This setup meets the requirements as VPC endpoints allow the notebook instances to communicate with S3 and SageMaker services within the AWS network, without the need for internet access, thus keeping the data traffic within the AWS network and enhancing security. IAM policies further control access to these services.\n\nThe first fake option of placing the Amazon SageMaker endpoint and S3 buckets within the same VPC is incorrect because endpoints are not placed within the VPC in the same way as resources; instead, VPC endpoints are used to enable private connectivity. The second fake option, which just mentions associating the notebook with a private subnet without specifying VPC endpoints, does not ensure that data communication will stay within the AWS network and may lead to security risks. The third fake option of using a NAT gateway allows outbound internet access, which violates the requirement of having no internet access for the instances. These common pitfalls often stem from misunderstandings of how VPC endpoints work and the security implications of using NAT gateways.", "zhcn": "针对该问题的正确答案是：通过IAM策略授予Amazon S3与Amazon SageMaker的访问权限，并将Amazon SageMaker笔记本实例关联至VPC的私有子网中，同时确保该VPC已配置S3 VPC终端节点与Amazon SageMaker VPC终端节点。此方案完全符合要求——VPC终端节点可使笔记本实例在AWS网络内部与S3及SageMaker服务通信，无需依赖互联网连接，既确保数据流量始终处于AWS骨干网络内，又提升了安全性。而IAM策略则进一步细化了对这些服务的访问控制。\n\n第一个干扰选项提出将Amazon SageMaker终端节点和S3存储桶置于同一VPC内，其错误在于混淆了终端节点与资源部署逻辑：终端节点并非以资源形式部署在VPC中，而是用于建立私有连接通道。第二个干扰选项仅提及将笔记本关联至私有子网却未明确配置VPC终端节点，这无法保证数据通信始终局限于AWS网络，可能存在安全隐患。第三个干扰选项采用NAT网关的方案，虽然允许实例访问外网，但违背了实例禁止连接互联网的核心要求。这些常见误区往往源于对VPC终端节点运作机制的误解，以及未能充分认知NAT网关在安全层面的潜在风险。"}, "answer": "C"}, {"id": "44", "question": {"enus": "A Machine Learning Specialist has created a deep learning neural network model that performs well on the training data but performs poorly on the test data. Which of the following methods should the Specialist consider using to correct this? (Choose three.) ", "zhcn": "一位机器学习专家构建了一个深度学习神经网络模型，该模型在训练数据上表现优异，但在测试数据上表现欠佳。请问该专家应考虑采用以下哪些方法来解决此问题？（选择三项。）"}, "option": [{"option_text": {"zhcn": "降低正则化强度。", "enus": "Decrease regularization."}, "option_flag": false}, {"option_text": {"zhcn": "增强正则化强度。", "enus": "Increase regularization."}, "option_flag": true}, {"option_text": {"zhcn": "提高退学率。", "enus": "Increase dropout."}, "option_flag": true}, {"option_text": {"zhcn": "降低辍学率。", "enus": "Decrease dropout."}, "option_flag": false}, {"option_text": {"zhcn": "增加特征组合。", "enus": "Increase feature combinations."}, "option_flag": false}, {"option_text": {"zhcn": "减少特征组合。", "enus": "Decrease feature combinations."}, "option_flag": true}], "analysis": {"enus": "The question describes a situation where a deep - learning neural network performs well on training data but poorly on test data, which indicates overfitting.\n\nThe correct answers are “Increase regularization.”, “Increase dropout.”, and “Decrease feature combinations.” Regularization adds a penalty term to the loss function, discouraging the model from learning overly complex patterns in the training data, thus reducing overfitting. Increasing dropout randomly “drops out” (ignores) a certain percentage of neurons during training, which helps the model to learn more robust features and avoid relying too much on specific neurons, also combating overfitting. Decreasing feature combinations reduces the complexity of the model by removing potentially redundant or noisy features, making the model more generalizable.\n\nThe fake answer options are counter - productive. “Decrease regularization” would allow the model to fit the training data even more closely, exacerbating overfitting. “Decrease dropout” would make the model more likely to rely on specific neurons and patterns in the training data, increasing overfitting. “Increase feature combinations” would add more complexity to the model, which is likely to cause it to overfit further. The common misconception might be thinking that increasing complexity (more features, less regularization, less dropout) always leads to better performance, without considering the problem of overfitting.", "zhcn": "题目描述了一种情况：深度学习神经网络在训练数据上表现良好，但在测试数据上表现不佳，这表明出现了过拟合。正确答案是\"增加正则化\"\"增加丢弃法\"和\"减少特征组合\"。正则化通过在损失函数中添加惩罚项，抑制模型学习训练数据中过于复杂的模式，从而减轻过拟合。增加丢弃法会在训练过程中随机\"丢弃\"（即忽略）一定比例的神经元，这有助于模型学习更具鲁棒性的特征，避免过度依赖特定神经元，也能抑制过拟合。减少特征组合通过剔除可能冗余或带有噪声的特征来降低模型复杂度，从而提升模型的泛化能力。\n\n错误选项则会适得其反。\"减少正则化\"会让模型更紧密地拟合训练数据，加剧过拟合；\"减少丢弃法\"会增加模型对训练数据中特定神经元和模式的依赖，加重过拟合；\"增加特征组合\"会提升模型复杂度，很可能导致过拟合进一步恶化。常见的误解可能是认为增加复杂度（更多特征、更弱正则化、更少丢弃）总能提升模型性能，而忽略了过拟合问题。"}, "answer": "BCF"}, {"id": "45", "question": {"enus": "A Data Scientist needs to create a serverless ingestion and analytics solution for high-velocity, real-time streaming data. The ingestion process must buffer and convert incoming records from JSON to a query-optimized, columnar format without data loss. The output datastore must be highly available, and Analysts must be able to run SQL queries against the data and connect to existing business intelligence dashboards. Which solution should the Data Scientist build to satisfy the requirements? ", "zhcn": "数据科学家需要构建一套无服务器架构的数据摄取与分析方案，用以处理高速实时流数据。数据摄取过程需实现缓冲功能，并将输入的JSON格式记录无损转换为查询优化的列式存储格式。输出数据存储须具备高可用性，且分析师能够对数据执行SQL查询，并连接现有商业智能仪表板。请问数据科学家应如何设计该解决方案以满足上述需求？"}, "option": [{"option_text": {"zhcn": "在AWS Glue数据目录中为传入数据格式创建元数据结构。通过Amazon Kinesis Data Firehose传输流实时推送数据，并借助AWS Glue数据目录将数据转换为Apache Parquet或ORC格式后存入Amazon S3。数据分析师可使用Amazon Athena直接查询S3中的数据，并通过Athena的JDBC连接器将商业智能工具与数据平台对接。", "enus": "Create a schema in the AWS Glue Data Catalog of the incoming data format. Use an Amazon Kinesis Data Firehose delivery stream to  stream the data and transform the data to Apache Parquet or ORC format using the AWS Glue Data Catalog before delivering to Amazon  S3. Have the Analysts query the data directly from Amazon S3 using Amazon Athena, and connect to BI tools using the Athena Java  Database Connectivity (JDBC) connector."}, "option_flag": true}, {"option_text": {"zhcn": "将每条JSON记录写入Amazon S3的临时中转区。利用S3上传事件触发AWS Lambda函数，将数据转换为Apache Parquet或ORC格式后写入S3的处理数据存储区。数据分析师可通过Amazon Athena直接查询S3中的数据，并通过Athena的JDBC连接器接入各类商业智能工具。", "enus": "Write each JSON record to a staging location in Amazon S3. Use the S3 Put event to trigger an AWS Lambda function that transforms  the data into Apache Parquet or ORC format and writes the data to a processed data location in Amazon S3. Have the Analysts query the  data directly from Amazon S3 using Amazon Athena, and connect to BI tools using the Athena Java Database Connectivity (JDBC)  connector."}, "option_flag": false}, {"option_text": {"zhcn": "将每条JSON记录写入Amazon S3的暂存区，利用S3上传事件触发AWS Lambda函数，将数据转换为Apache Parquet或ORC格式后载入Amazon RDS PostgreSQL数据库。最终由分析师通过该RDS数据库进行查询并生成数据看板。", "enus": "Write each JSON record to a staging location in Amazon S3. Use the S3 Put event to trigger an AWS Lambda function that transforms  the data into Apache Parquet or ORC format and inserts it into an Amazon RDS PostgreSQL database. Have the Analysts query and run  dashboards from the RDS database."}, "option_flag": false}, {"option_text": {"zhcn": "利用Amazon Kinesis Data Analytics接入流式数据，通过实时SQL查询将记录转换为Apache Parquet格式后传输至Amazon S3。随后，分析师可借助Amazon Athena直接查询Amazon S3中的数据，并通过Athena的JDBC连接器与商业智能工具实现无缝对接。", "enus": "Use Amazon Kinesis Data Analytics to ingest the streaming data and perform real-time SQL queries to convert the records to Apache  Parquet before delivering to Amazon S3. Have the Analysts query the data directly from Amazon S3 using Amazon Athena and connect to  BI tools using the Athena Java Database Connectivity (JDBC) connector."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to create a schema in the AWS Glue Data Catalog of the incoming data format, use an Amazon Kinesis Data Firehose delivery stream to stream and transform the data to Apache Parquet or ORC format using the AWS Glue Data Catalog before delivering to Amazon S3, and have analysts query the data via Amazon Athena and connect to BI tools using the Athena JDBC connector. This option is ideal as Kinesis Data Firehose is designed for high - velocity, real - time streaming data. It can buffer the incoming data and convert it to a query - optimized, columnar format (Parquet or ORC) with the help of the AWS Glue Data Catalog, meeting the ingestion requirements. Amazon S3 is highly available, and Athena allows analysts to run SQL queries directly on S3 data and connect to BI tools.\n\nThe first fake option using S3 Put events to trigger Lambda functions for data transformation lacks the buffering capability required for high - velocity data, which may lead to data loss during peak traffic. \n\nThe second fake option inserting transformed data into an Amazon RDS PostgreSQL database is not suitable as RDS is a relational database and may not handle high - velocity streaming data as efficiently as a serverless solution. Also, it adds complexity in terms of database management and may not scale as well as the chosen solution. \n\nThe third fake option using Amazon Kinesis Data Analytics for real - time SQL queries to convert records to Parquet is incorrect because Kinesis Data Analytics is mainly for real - time analytics on streaming data, not for the specific task of buffering and converting incoming JSON records to a columnar format as required in the question. These factors distinguish the real answer from the fake options.", "zhcn": "针对该问题的正确答案是：在AWS Glue数据目录中创建对应数据格式的元数据方案，通过Amazon Kinesis Data Firehose传输流实时接收数据，并借助AWS Glue数据目录将数据转换为Apache Parquet或ORC列式格式后存入Amazon S3，最终由分析人员使用Amazon Athena查询数据，并通过Athena JDBC连接器对接商业智能工具。此方案最具优势，因为Kinesis Data Firehose专为处理高速实时流数据而设计，既能缓冲数据流，又能利用Glue数据目录将数据转换为查询优化的列式格式（Parquet或ORC），完美契合数据摄取需求。Amazon S3具备高可用性，Athena则支持直接对S3数据执行SQL查询并连接BI工具。\n\n而第一个干扰选项采用S3上传事件触发Lambda函数进行数据转换，缺乏处理高速数据所需的缓冲能力，在流量峰值时可能导致数据丢失；第二个干扰选项将转换后数据存入Amazon RDS PostgreSQL数据库，由于RDS是关系型数据库，处理高速流数据的效率不如无服务器方案，且会增加数据库管理复杂度及可扩展性限制；第三个干扰选项使用Amazon Kinesis Data Analytics通过实时SQL查询转换Parquet格式，该服务主要面向流数据实时分析，并不具备题目要求的缓冲JSON数据并转换为列式格式的特化功能。这些关键差异使得正确答案从干扰选项中脱颖而出。"}, "answer": "A"}, {"id": "46", "question": {"enus": "An online reseller has a large, multi-column dataset with one column missing 30% of its data. A Machine Learning Specialist believes that certain columns in the dataset could be used to reconstruct the missing data. Which reconstruction approach should the Specialist use to preserve the integrity of the dataset? ", "zhcn": "某线上经销商持有一份包含多列数据的大型数据集，其中某列数据存在30%的缺失。一位机器学习专家认为，可以利用数据集中的某些列来重建缺失数据。请问该专家应采用何种重建方法，才能最大限度保证数据集的完整性？"}, "option": [{"option_text": {"zhcn": "列删", "enus": "Listwise deletion"}, "option_flag": false}, {"option_text": {"zhcn": "末次观测值结转法", "enus": "Last observation carried forward"}, "option_flag": false}, {"option_text": {"zhcn": "多重填补", "enus": "Multiple imputation"}, "option_flag": true}, {"option_text": {"zhcn": "均值填补", "enus": "Mean substitution"}, "option_flag": false}], "analysis": {"enus": "Reference: https://worldwidescience.org/topicpages/i/imputing+missing+values.html", "zhcn": "参考来源：https://worldwidescience.org/topicpages/i/imputing+missing+values.html"}, "answer": "C"}, {"id": "47", "question": {"enus": "A company is setting up an Amazon SageMaker environment. The corporate data security policy does not allow communication over the internet. How can the company enable the Amazon SageMaker service without enabling direct internet access to Amazon SageMaker notebook instances? ", "zhcn": "某公司正在部署亚马逊SageMaker环境。根据企业数据安全政策，严禁通过互联网进行数据传输。在不对亚马逊SageMaker笔记本实例开放直接互联网访问的前提下，该公司应如何启用此项服务？"}, "option": [{"option_text": {"zhcn": "在企业虚拟私有云中创建NAT网关。", "enus": "Create a NAT gateway within the corporate VPC."}, "option_flag": true}, {"option_text": {"zhcn": "将亚马逊SageMaker流量经由本地网络进行路由传输。", "enus": "Route Amazon SageMaker trafic through an on-premises network."}, "option_flag": false}, {"option_text": {"zhcn": "在企业虚拟私有云中创建Amazon SageMaker VPC接口端点。", "enus": "Create Amazon SageMaker VPC interface endpoints within the corporate VPC."}, "option_flag": false}, {"option_text": {"zhcn": "与托管Amazon SageMaker的Amazon VPC建立VPC对等连接。", "enus": "Create VPC peering with Amazon VPC hosting Amazon SageMaker."}, "option_flag": false}], "analysis": {"enus": "Reference: https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-dg.pdf (46)", "zhcn": "参考来源：https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-dg.pdf (46)"}, "answer": "A"}, {"id": "48", "question": {"enus": "A Machine Learning Specialist is training a model to identify the make and model of vehicles in images. The Specialist wants to use transfer learning and an existing model trained on images of general objects. The Specialist collated a large custom dataset of pictures containing different vehicle makes and models. What should the Specialist do to initialize the model to re-train it with the custom data? ", "zhcn": "机器学习专家正在训练一个模型，用于识别图像中车辆的品牌与型号。该专家计划采用迁移学习方法，借助一个已针对通用物体图像完成预训练的现有模型。专家已整理完成包含各类车辆品牌和型号的大型定制数据集。为使用该定制数据重新训练模型，专家应如何对模型进行初始化？"}, "option": [{"option_text": {"zhcn": "在所有层级（包括最后的全连接层）中采用随机权重初始化模型。", "enus": "Initialize the model with random weights in all layers including the last fully connected layer."}, "option_flag": false}, {"option_text": {"zhcn": "在所有层级加载预训练权重，并将最后的全连接层进行替换。", "enus": "Initialize the model with pre-trained weights in all layers and replace the last fully connected layer."}, "option_flag": true}, {"option_text": {"zhcn": "在所有层中以随机权重初始化模型，并替换末端的全连接层。", "enus": "Initialize the model with random weights in all layers and replace the last fully connected layer."}, "option_flag": false}, {"option_text": {"zhcn": "在所有层（包括最后的全连接层）均采用预训练权重进行模型初始化。", "enus": "Initialize the model with pre-trained weights in all layers including the last fully connected layer."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question “A Machine Learning Specialist is training a model to identify the make and model of vehicles in images...” is “Initialize the model with pre - trained weights in all layers and replace the last fully connected layer.” Transfer learning leverages knowledge from a pre - trained model on a large general dataset. By initializing the model with pre - trained weights in all layers, the model can utilize the general feature extraction capabilities learned from the previous task.\n\nThe last fully connected layer is responsible for making predictions based on the features extracted by the earlier layers. Since the original model was trained on general objects and now we want to classify vehicle makes and models, replacing this layer allows the model to learn the specific patterns and relationships relevant to vehicles.\n\nThe option “Initialize the model with random weights in all layers including the last fully connected layer” and “Initialize the model with random weights in all layers and replace the last fully connected layer” discard the valuable pre - trained knowledge, which would require much more data and time for the model to learn basic features from scratch. The option “Initialize the model with pre - trained weights in all layers including the last fully connected layer” is incorrect because the last layer of the pre - trained model is optimized for the original task of general object classification, not for vehicle make and model classification. So, it needs to be replaced. This is why the chosen real answer is correct and stands out from the fake options.", "zhcn": "针对问题“一位机器学习专家正在训练模型以识别图像中车辆的品牌和型号……”的正确答案是：“为所有层加载预训练权重，并替换最后的全连接层。”迁移学习能够利用在大型通用数据集上预训练模型所获得的知识。通过为所有层加载预训练权重，模型可以沿用从前一任务中学到的通用特征提取能力。\n\n最后的全连接层负责根据前面层提取的特征进行预测。由于原始模型是针对通用物体分类任务训练的，而现在我们需要对车辆品牌型号进行分类，替换该层可使模型学习与车辆相关的特定模式及关联。\n\n而“为所有层（包括最后全连接层）加载随机权重”及“为所有层加载随机权重并替换最后全连接层”这两种方案会舍弃宝贵的预训练知识，导致模型需要更多数据和时间从头学习基础特征。至于“为所有层（包括最后全连接层）加载预训练权重”的方案亦不可取，因为预训练模型的最后一层是针对原始通用物体分类任务优化的，不适用于车辆品牌型号分类场景，必须进行替换。\n\n因此，所选正确答案具有合理性，并能有效区别于其他干扰选项。"}, "answer": "B"}, {"id": "49", "question": {"enus": "An ofice security agency conducted a successful pilot using 100 cameras installed at key locations within the main ofice. Images from the cameras were uploaded to Amazon S3 and tagged using Amazon Rekognition, and the results were stored in Amazon ES. The agency is now looking to expand the pilot into a full production system using thousands of video cameras in its ofice locations globally. The goal is to identify activities performed by non-employees in real time Which solution should the agency consider? ", "zhcn": "某办公安全机构在总部关键区域部署了百个监控摄像头，成功完成试点项目。摄像头采集的画面实时上传至亚马逊S3存储系统，并借助亚马逊Rekognition技术进行智能标记，最终分析结果存储于亚马逊ES数据库。目前该机构计划将试点升级为全球办公点的全面部署，拟在全球各办公场所铺设数千个摄像设备，旨在实时识别非内部人员的行为动态。针对这一需求，该机构应如何规划系统解决方案？"}, "option": [{"option_text": {"zhcn": "在各分支机构及每台摄像头处配置代理服务器，将RTSP视频流传输至独立的亚马逊Kinesis视频流。针对每条视频流，运用亚马逊Rekognition视频服务创建流处理器，通过预设员工人脸库进行面部识别，并在检测到非授权人员时触发告警机制。", "enus": "Use a proxy server at each local ofice and for each camera, and stream the RTSP feed to a unique Amazon Kinesis Video Streams video  stream. On each stream, use Amazon Rekognition Video and create a stream processor to detect faces from a collection of known  employees, and alert when non-employees are detected."}, "option_flag": false}, {"option_text": {"zhcn": "在各分支机构及每台摄像头处配置代理服务器，将RTSP视频流实时传输至独立的亚马逊Kinesis视频流通道。通过亚马逊Rekognition图像识别技术，针对每条视频流从已知员工人脸库中进行面部识别，一旦发现非授权人员即刻触发告警机制。", "enus": "Use a proxy server at each local ofice and for each camera, and stream the RTSP feed to a unique Amazon Kinesis Video Streams video  stream. On each stream, use Amazon Rekognition Image to detect faces from a collection of known employees and alert when non-  employees are detected."}, "option_flag": false}, {"option_text": {"zhcn": "部署AWS DeepLens摄像头，并通过DeepLens_Kinesis_Video模块将各摄像头的视频流实时传输至Amazon Kinesis Video Streams。针对每条视频流，运用Amazon Rekognition Video服务创建流处理器，基于预设人脸库进行实时面部检测，并在识别到非雇员时触发告警机制。", "enus": "Install AWS DeepLens cameras and use the DeepLens_Kinesis_Video module to stream video to Amazon Kinesis Video Streams for  each camera. On each stream, use Amazon Rekognition Video and create a stream processor to detect faces from a collection on each  stream, and alert when non-employees are detected."}, "option_flag": false}, {"option_text": {"zhcn": "部署AWS DeepLens摄像头，并通过DeepLens_Kinesis_Video模块将各摄像头的视频流实时传输至Amazon Kinesis Video Streams。针对每条视频流，启动AWS Lambda函数截取图像片段，随后调用Amazon Rekognition Image服务，从预设的员工人脸库中进行比对识别。当系统检测到非授权人员时，将自动触发告警机制。", "enus": "Install AWS DeepLens cameras and use the DeepLens_Kinesis_Video module to stream video to Amazon Kinesis Video Streams for each  camera. On each stream, run an AWS Lambda function to capture image fragments and then call Amazon Rekognition Image to detect  faces from a collection of known employees, and alert when non-employees are detected."}, "option_flag": true}], "analysis": {"enus": "Reference: https://aws.amazon.com/blogs/machine-learning/video-analytics-in-the-cloud-and-at-the-edge-with-aws-deeplens-and-kinesis-video- streams/", "zhcn": "参考链接：https://aws.amazon.com/blogs/machine-learning/video-analytics-in-the-cloud-and-at-the-edge-with-aws-deeplens-and-kinesis-video-streams/"}, "answer": "D"}, {"id": "50", "question": {"enus": "A Marketing Manager at a pet insurance company plans to launch a targeted marketing campaign on social media to acquire new customers. Currently, the company has the following data in Amazon Aurora: ✑ Profiles for all past and existing customers ✑ Profiles for all past and existing insured pets ✑ Policy-level information ✑ Premiums received ✑ Claims paid What steps should be taken to implement a machine learning model to identify potential new customers on social media? ", "zhcn": "某宠物保险公司市场经理计划在社交媒体上启动精准营销活动以拓展新客源。目前公司亚马逊云关系型数据库中存在以下数据：  \n✑ 历史及现有客户档案  \n✑ 历史及现有投保宠物档案  \n✑ 保单层级信息  \n✑ 已收保费记录  \n✑ 已赔付理赔数据  \n请问应如何部署机器学习模型，从而在社交媒体平台上精准识别潜在新客户？"}, "option": [{"option_text": {"zhcn": "对客户画像数据进行回归分析，以洞悉不同消费群体的核心特征。随后在社交媒体上寻找具有相似特征的用户画像。", "enus": "Use regression on customer profile data to understand key characteristics of consumer segments. Find similar profiles on social media"}, "option_flag": false}, {"option_text": {"zhcn": "对客户画像数据进行聚类分析，以洞悉不同消费群体的核心特征。随后在社交媒体上寻找具有相似特征的用户画像。", "enus": "Use clustering on customer profile data to understand key characteristics of consumer segments. Find similar profiles on social media"}, "option_flag": false}, {"option_text": {"zhcn": "利用客户画像数据构建推荐引擎，深入洞悉不同消费群体的核心特征。随后在社交媒体上精准匹配具有相似特征的用户画像。", "enus": "Use a recommendation engine on customer profile data to understand key characteristics of consumer segments. Find similar profiles  on social media."}, "option_flag": true}, {"option_text": {"zhcn": "对客户画像数据运用决策树分类器，以洞悉不同消费群体的核心特征。随后在社交媒体上寻找具有相似特征的用户画像。", "enus": "Use a decision tree classifier engine on customer profile data to understand key characteristics of consumer segments. Find similar  profiles on social media."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is “Use a recommendation engine on customer profile data to understand key characteristics of consumer segments. Find similar profiles on social media.” A recommendation engine is well - suited for this task as it can analyze the existing customer profiles in Amazon Aurora and identify patterns and preferences. It can then recommend potential new customers on social media who share similar characteristics, which aligns with the goal of acquiring new customers through targeted marketing.\n\nThe fake answer “Use regression on customer profile data to understand key characteristics of consumer segments. Find similar profiles on social media” is incorrect because regression is mainly used to predict a continuous numerical value, such as predicting premiums or claims amounts. It is not the best fit for identifying consumer segments and potential new customers.\n\n“Use clustering on customer profile data to understand key characteristics of consumer segments. Find similar profiles on social media” is wrong. While clustering can group similar customers, it doesn't directly recommend potential new customers. It just creates groups within the existing data and doesn't have the forward - looking recommendation aspect like a recommendation engine.\n\n“Use a decision tree classifier engine on customer profile data to understand key characteristics of consumer segments. Find similar profiles on social media” is off - target. A decision tree classifier is typically used for classification problems, like predicting whether a customer will file a claim or not. It's not optimized for the task of finding potential new customers on social media based on existing customer profiles.\n\nThe key factor distinguishing the real answer is the ability of the recommendation engine to recommend new potential customers, making it the most appropriate choice for this marketing campaign scenario.", "zhcn": "针对该问题的正确答案是：“利用推荐引擎分析客户画像数据，以理解消费群体的关键特征，并在社交媒体上寻找相似画像。”推荐引擎非常适合此项任务，因为它能分析Amazon Aurora中现有的客户画像，识别其行为模式与偏好，进而推荐社交媒体上具有相似特征的新潜在客户——这与通过精准营销获取新客户的目标高度契合。\n\n而错误答案“使用回归分析客户画像数据来理解消费群体关键特征，并在社交媒体寻找相似画像”并不恰当。因为回归分析主要用于预测连续数值（如保费或理赔金额），并不适用于识别消费群体和潜在新客户。\n\n“采用聚类分析处理客户画像数据以理解消费群体特征，并在社交媒体定位相似画像”同样存在问题。虽然聚类能将相似客户分组，但它无法直接推荐潜在新客户。该方法仅能在现有数据内部构建分组，缺乏推荐引擎所具有的前瞻性推荐能力。\n\n“通过决策树分类器分析客户画像数据来理解消费群体特征，并在社交媒体匹配相似画像”也偏离目标。决策树分类器通常用于解决分类问题（例如预测客户是否会申请理赔），并不适用于基于现有客户画像在社交媒体寻找潜在新客户的场景。\n\n真正的关键区别在于：推荐引擎具有推荐新潜在客户的能力，这使其成为本营销场景中最合适的选择。"}, "answer": "C"}, {"id": "51", "question": {"enus": "A manufacturing company has a large set of labeled historical sales data. The manufacturer would like to predict how many units of a particular part should be produced each quarter. Which machine learning approach should be used to solve this problem? ", "zhcn": "某制造企业拥有一批标注完备的历史销售数据。该企业希望预测特定零部件每季度应生产的数量。针对这一需求，应采用何种机器学习方法予以解决？"}, "option": [{"option_text": {"zhcn": "逻辑回归", "enus": "Logistic regression"}, "option_flag": false}, {"option_text": {"zhcn": "随机切割森林（RCF）", "enus": "Random Cut Forest (RCF)"}, "option_flag": true}, {"option_text": {"zhcn": "主成分分析（PCA）", "enus": "Principal component analysis (PCA)"}, "option_flag": false}, {"option_text": {"zhcn": "线性回归", "enus": "Linear regression"}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question ‘A manufacturing company has a large set of labeled historical sales data. The manufacturer would like to predict how many units of a particular part should be produced each quarter. Which machine learning approach should be used to solve this problem?’ is ‘Random Cut Forest (RCF)’. RCF is suitable for anomaly detection and forecasting in time - series data. Since the company has historical sales data and wants to predict future production quantities, RCF can analyze patterns in the data and make accurate forecasts.\n\n‘Logistic regression’ is mainly used for classification problems where the output is a discrete class label, not for predicting continuous values like the number of units to produce, so it is not appropriate. ‘Principal component analysis (PCA)’ is a dimensionality - reduction technique, which aims to reduce the number of features in the data rather than making predictions about production quantities. ‘Linear regression’ assumes a linear relationship between variables, and sales data may have complex, non - linear patterns that linear regression might not capture effectively. These limitations of the fake answer options make RCF the better choice for this production - prediction problem.", "zhcn": "针对“某制造企业拥有大量带标签的历史销售数据，希望预测特定零部件每季度产量应如何规划”这一问题，正确答案应为“随机切割森林（RCF）”算法。该算法擅长处理时间序列数据的异常检测与趋势预测，恰好能利用企业积累的历史销售数据，通过分析数据内在模式实现对生产数量的精准预估。\n\n而“逻辑回归”主要用于离散型分类任务，不适用于连续数值的产量预测；“主成分分析（PCA）”是特征降维技术，其核心目标在于简化数据集特征维度，而非直接生成产量预测值；“线性回归”虽可处理连续值预测，但假设变量间存在线性关系，而销售数据往往包含复杂非线性特征，线性模型难以有效捕捉。正因上述选项存在这些局限性，RCF在此类生产预测场景中更具优势。"}, "answer": "B"}, {"id": "52", "question": {"enus": "A financial services company is building a robust serverless data lake on Amazon S3. The data lake should be fiexible and meet the following requirements: ✑ Support querying old and new data on Amazon S3 through Amazon Athena and Amazon Redshift Spectrum. ✑ Support event-driven ETL pipelines ✑ Provide a quick and easy way to understand metadata Which approach meets these requirements? ", "zhcn": "一家金融服务公司正在Amazon S3上构建一个强健的无服务器数据湖。该数据湖需具备灵活性，并满足以下要求：  \n✑ 支持通过Amazon Athena和Amazon Redshift Spectrum查询Amazon S3上的历史数据与新增数据  \n✑ 支持事件驱动的ETL流程  \n✑ 提供便捷直观的元数据理解方式  \n何种方案符合这些需求？"}, "option": [{"option_text": {"zhcn": "利用AWS Glue爬虫采集S3数据，通过AWS Lambda函数触发Glue ETL任务处理流程，并借助AWS Glue数据目录实现元数据的检索与发现。", "enus": "Use an AWS Glue crawler to crawl S3 data, an AWS Lambda function to trigger an AWS Glue ETL job, and an AWS Glue Data catalog to  search and discover metadata."}, "option_flag": true}, {"option_text": {"zhcn": "利用AWS Glue爬虫采集S3数据，通过AWS Lambda函数触发AWS Batch任务，并借助外部Apache Hive元数据存储库进行元数据的检索与发现。", "enus": "Use an AWS Glue crawler to crawl S3 data, an AWS Lambda function to trigger an AWS Batch job, and an external Apache Hive  metastore to search and discover metadata."}, "option_flag": false}, {"option_text": {"zhcn": "利用AWS Glue爬虫程序采集S3数据，通过Amazon CloudWatch警报触发AWS Batch任务，并借助AWS Glue数据目录实现元数据的检索与发现。", "enus": "Use an AWS Glue crawler to crawl S3 data, an Amazon CloudWatch alarm to trigger an AWS Batch job, and an AWS Glue Data Catalog to  search and discover metadata."}, "option_flag": false}, {"option_text": {"zhcn": "利用AWS Glue爬虫采集S3数据，通过Amazon CloudWatch警报触发AWS Glue ETL任务，并借助外部Apache Hive元存储进行元数据的检索与发现。", "enus": "Use an AWS Glue crawler to crawl S3 data, an Amazon CloudWatch alarm to trigger an AWS Glue ETL job, and an external Apache Hive  metastore to search and discover metadata."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is ‘Use an AWS Glue crawler to crawl S3 data, an AWS Lambda function to trigger an AWS Glue ETL job, and an AWS Glue Data catalog to search and discover metadata.’ \n\nAWS Glue crawlers can scan S3 data and infer its schema, which is essential for querying data in Athena and Redshift Spectrum. AWS Lambda is well - suited for event - driven architectures, allowing it to trigger AWS Glue ETL jobs in response to events, thus meeting the event - driven ETL pipeline requirement. The AWS Glue Data Catalog offers an easy way to search and understand metadata, fulfilling the metadata requirement.\n\nThe fake options have issues. Using AWS Batch in the first and third fake options is not the best fit for event - driven ETL pipelines as Batch is more for running batch computing workloads. An external Apache Hive metastore in the first and fourth fake options adds complexity; the AWS Glue Data Catalog is a native and more integrated solution for metadata management in an AWS serverless data lake. Also, using Amazon CloudWatch alarms in the third and fourth fake options is not as efficient as AWS Lambda for triggering ETL jobs in an event - driven manner. These key differentiators make the real answer the correct choice for meeting all the requirements.", "zhcn": "该问题的正确答案是“使用AWS Glue爬虫程序抓取S3数据，通过AWS Lambda函数触发AWS Glue ETL任务，并利用AWS Glue数据目录搜索和发现元数据”。AWS Glue爬虫能够扫描S3数据并推断其结构模式，这对通过Athena和Redshift Spectrum查询数据至关重要。AWS Lambda尤其适合事件驱动型架构，可基于事件触发AWS Glue ETL任务，从而满足事件驱动型ETL流程的需求。AWS Glue数据目录提供了便捷的元数据搜索与理解途径，完美契合元数据管理要求。\n\n错误选项存在明显缺陷：第一和第三选项使用AWS Batch并非事件驱动型ETL流程的最佳方案，因为Batch更适用于批处理计算场景。第一和第四选项采用外部Apache Hive元存储会增加系统复杂性，而AWS Glue数据目录作为原生服务，能为无服务器数据湖提供更集成的元数据管理方案。此外，第三和第四选项依赖Amazon CloudWatch警报触发ETL任务，在事件驱动响应效率上远不如AWS Lambda。这些关键差异使得原答案成为满足所有需求的正解。"}, "answer": "A"}, {"id": "53", "question": {"enus": "A company's Machine Learning Specialist needs to improve the training speed of a time-series forecasting model using TensorFlow. The training is currently implemented on a single-GPU machine and takes approximately 23 hours to complete. The training needs to be run daily. The model accuracy is acceptable, but the company anticipates a continuous increase in the size of the training data and a need to update the model on an hourly, rather than a daily, basis. The company also wants to minimize coding effort and infrastructure changes. What should the Machine Learning Specialist do to the training solution to allow it to scale for future demand? ", "zhcn": "某公司的机器学习专家需要提升基于TensorFlow的时间序列预测模型的训练速度。当前模型在单GPU机器上完成训练需耗时约23小时，且需每日执行训练任务。虽然模型精度已达要求，但公司预计训练数据量将持续增长，且模型更新频率需从每日一次提升至每小时一次。在此过程中，公司希望尽量控制代码修改量及基础设施变动。机器学习专家应如何调整训练方案，以确保其具备应对未来需求的可扩展性？"}, "option": [{"option_text": {"zhcn": "请勿改动TensorFlow代码。将机器更换为配备更强性能GPU的设备，以加速训练进程。", "enus": "Do not change the TensorFlow code. Change the machine to one with a more powerful GPU to speed up the training."}, "option_flag": false}, {"option_text": {"zhcn": "将TensorFlow代码改写为基于Amazon SageMaker的Horovod分布式框架实现。根据业务目标需求，将训练任务并行扩展至任意数量的机器集群。", "enus": "Change the TensorFlow code to implement a Horovod distributed framework supported by Amazon SageMaker. Parallelize the training  to as many machines as needed to achieve the business goals."}, "option_flag": true}, {"option_text": {"zhcn": "改用内置的AWS SageMaker DeepAR模型。根据业务目标需求，将训练任务并行扩展至相应规模的机器集群。", "enus": "Switch to using a built-in AWS SageMaker DeepAR model. Parallelize the training to as many machines as needed to achieve the  business goals."}, "option_flag": false}, {"option_text": {"zhcn": "将训练任务迁移至Amazon EMR平台，根据业务需求动态调配计算资源，实现分布式并行处理。", "enus": "Move the training to Amazon EMR and distribute the workload to as many machines as needed to achieve the business goals."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Change the TensorFlow code to implement a Horovod distributed framework supported by Amazon SageMaker. Parallelize the training to as many machines as needed to achieve the business goals.” This approach addresses the future scalability requirements as the company anticipates an increase in training - data size and a more frequent model update schedule. Horovod is a framework that simplifies distributed training in TensorFlow, which means it can parallelize the training across multiple machines with relatively minimal changes to the existing TensorFlow code, meeting the company's goal of minimizing coding effort.\n\nThe option “Do not change the TensorFlow code. Change the machine to one with a more powerful GPU to speed up the training” is not scalable. As the data size grows, simply upgrading the GPU won't be sufficient in the long - run, and it doesn't address the need for more frequent model updates.\n\n“Switch to using a built - in AWS SageMaker DeepAR model. Parallelize the training to as many machines as needed to achieve the business goals” requires changing the model, which goes against the requirement of minimizing coding effort since the current model already has acceptable accuracy.\n\n“Move the training to Amazon EMR and distribute the workload to as many machines as needed to achieve the business goals” is not ideal as EMR is more focused on big - data processing frameworks like Hadoop and Spark, and it may require significant changes to the existing TensorFlow - based training code, which the company wants to avoid.\n\nIn summary, the Horovod - based solution is the best choice as it balances scalability, coding effort, and infrastructure changes to meet the company's future demands.", "zhcn": "针对该问题，正确答案是\"修改TensorFlow代码，实现在Amazon SageMaker上运行的Horovod分布式框架。根据业务需求将训练任务并行扩展至任意数量的机器\"。这一方案能够满足公司未来可扩展性需求——随着训练数据量增长和模型更新频率加快，Horovod作为简化TensorFlow分布式训练的框架，只需对现有代码进行最小改动即可实现多机并行训练，符合公司控制编码工作量的核心目标。\n\n而\"不修改TensorFlow代码，通过升级更强性能GPU加速训练\"的方案缺乏可扩展性。当数据规模持续扩大时，单纯升级GPU无法满足长期需求，也无法应对更高频的模型更新要求。\n\n若\"改用AWS SageMaker内置DeepAR模型并行扩展训练机器\"则需更换现有模型，这与最小化编码工作的要求相悖——毕竟当前模型已具备可接受的准确度。\n\n至于\"将训练迁移至Amazon EMR并通过多机分布式处理实现目标\"的方案，由于EMR主要面向Hadoop/Spark等大数据处理框架，对现有基于TensorFlow的训练代码可能需要重大调整，这恰恰是公司希望避免的情况。\n\n综上所述，基于Horovod的解决方案在可扩展性、编码工作量和技术架构调整之间取得了最佳平衡，最能满足公司未来的发展需求。"}, "answer": "B"}, {"id": "54", "question": {"enus": "Which of the following metrics should a Machine Learning Specialist generally use to compare/evaluate machine learning classification models against each other? ", "zhcn": "机器学习专家通常应采用以下哪种指标来比较或评估不同机器学习分类模型的性能？"}, "option": [{"option_text": {"zhcn": "忆起", "enus": "Recall"}, "option_flag": false}, {"option_text": {"zhcn": "误判率", "enus": "Misclassification rate"}, "option_flag": false}, {"option_text": {"zhcn": "平均绝对百分比误差（MAPE）", "enus": "Mean absolute percentage error (MAPE)"}, "option_flag": false}, {"option_text": {"zhcn": "ROC曲线下面积（AUC）", "enus": "Area Under the ROC Curve (AUC)"}, "option_flag": true}], "analysis": {"enus": "The correct answer to the question ‘Which of the following metrics should a Machine Learning Specialist generally use to compare/evaluate machine learning classification models against each other?’ is ‘Area Under the ROC Curve (AUC)’. AUC is a widely - used metric for classification models as it measures the model's ability to distinguish between positive and negative classes across all possible classification thresholds. It provides a single scalar value that summarizes the overall performance of the model, making it easy to compare different classification models.\n\n‘Recall’ only focuses on the proportion of actual positive cases that are correctly predicted, ignoring the false positives. So, it gives a one - sided view of the model's performance and is not ideal for overall model comparison. ‘Misclassification rate’ simply counts the proportion of incorrect predictions, but it doesn't account for the different costs associated with false positives and false negatives. ‘Mean absolute percentage error (MAPE)’ is mainly used for regression problems, not for classification. Since it measures the error in predicting continuous values, it is completely inappropriate for evaluating classification models. These limitations are the key reasons why the fake options are not suitable for comparing classification models, and AUC is the real answer.", "zhcn": "针对“机器学习专家应使用以下哪种指标来比较和评估不同分类模型”这一问题，正确答案是“受试者工作特征曲线下面积（AUC）”。AUC是评估分类模型的常用指标，它能衡量模型在所有可能分类阈值下区分正负样本的能力。该指标通过单一标量值概括模型的整体性能，便于直接对比不同分类模型的优劣。\n\n而“召回率”仅关注被正确预测的真实正例比例，忽略了假正例的影响，因此对模型性能的评估存在片面性，不适用于整体模型比较。“误分类率”虽能计算错误预测的比例，但未区分假正例与假反例带来的不同代价。“平均绝对百分比误差（MAPE）”主要适用于回归问题，其连续值误差的测量特性完全不适合评估分类模型。正是这些局限性使得干扰项无法有效用于分类模型比较，从而凸显AUC作为正确答案的合理性。"}, "answer": "D"}, {"id": "55", "question": {"enus": "A company is running a machine learning prediction service that generates 100 TB of predictions every day. A Machine Learning Specialist must generate a visualization of the daily precision-recall curve from the predictions, and forward a read-only version to the Business team. Which solution requires the LEAST coding effort? ", "zhcn": "一家公司正在运行一项机器学习预测服务，每日生成高达100 TB的预测数据。机器学习专家需根据这些预测结果绘制每日精确率-召回率曲线图，并将只读版本发送给业务团队。在以下方案中，哪种方案所需的编码工作量最少？"}, "option": [{"option_text": {"zhcn": "每日运行Amazon EMR工作流以生成精确率-召回率数据，并将结果存储于Amazon S3中。授予业务团队对S3存储内容的只读访问权限。", "enus": "Run a daily Amazon EMR workfiow to generate precision-recall data, and save the results in Amazon S3. Give the Business team read-  only access to S3."}, "option_flag": false}, {"option_text": {"zhcn": "在Amazon QuickSight中生成每日精确率-召回率数据，并将分析结果发布至与业务团队共享的仪表盘。", "enus": "Generate daily precision-recall data in Amazon QuickSight, and publish the results in a dashboard shared with the Business team."}, "option_flag": false}, {"option_text": {"zhcn": "每日运行Amazon EMR工作流以生成精确率-召回率数据，并将结果存储于Amazon S3中。通过Amazon QuickSight对数据阵列进行可视化分析，最终将分析图表发布至与业务团队共享的监控看板。", "enus": "Run a daily Amazon EMR workfiow to generate precision-recall data, and save the results in Amazon S3. Visualize the arrays in Amazon  QuickSight, and publish them in a dashboard shared with the Business team."}, "option_flag": true}, {"option_text": {"zhcn": "在亚马逊ES中生成每日精确率-召回率数据，并将分析结果发布至与业务团队共享的仪表盘。", "enus": "Generate daily precision-recall data in Amazon ES, and publish the results in a dashboard shared with the Business team."}, "option_flag": false}], "analysis": {"enus": "The correct answer is “Run a daily Amazon EMR workflow to generate precision - recall data, and save the results in Amazon S3. Visualize the arrays in Amazon QuickSight, and publish them in a dashboard shared with the Business team.” This option requires the least coding effort because Amazon QuickSight is a fully - managed business intelligence service that allows for easy visualization of data. It has a user - friendly interface for creating visualizations such as precision - recall curves, eliminating the need for extensive coding.\n\nThe option “Run a daily Amazon EMR workflow to generate precision - recall data, and save the results in Amazon S3. Give the Business team read - only access to S3” only provides data access but no visualization. The business team would need to write code to visualize the data, so it is not the best choice.\n\nThe option “Generate daily precision - recall data in Amazon QuickSight” is incorrect because QuickSight is mainly for visualization, not for generating machine - learning related precision - recall data. It lacks the necessary machine - learning capabilities to generate such data.\n\nThe option “Generate daily precision - recall data in Amazon ES” is wrong as Amazon ES (Elasticsearch) is a search and analytics engine, not designed for generating precision - recall data for machine - learning predictions. It would require significant coding to adapt it for this purpose.\n\nIn summary, the real answer combines the data - generation power of Amazon EMR with the easy - to - use visualization capabilities of Amazon QuickSight, minimizing the coding effort compared to the fake options.", "zhcn": "正确答案是：\"每日运行Amazon EMR工作流生成精确率-召回率数据，并将结果保存至Amazon S3。通过Amazon QuickSight将数据阵列可视化，最终在共享给业务团队的仪表板上发布。\" 该方案编码工作量最小，因为Amazon QuickSight作为全托管的商业智能服务，可轻松实现数据可视化。其用户友好的界面能直接生成精确率-召回率曲线等可视化图表，无需大量编程工作。\n\n而\"每日运行Amazon EMR工作流生成精确率-召回率数据，将结果保存至Amazon S3，并授予业务团队S3只读权限\"这一方案仅提供数据访问权限，未实现可视化功能。业务团队仍需编写代码才能完成数据可视化，故非最优选择。\n\n至于\"在Amazon QuickSight中生成每日精确率-召回率数据\"的方案并不成立，因为QuickSight主要功能是数据可视化，而非生成机器学习相关的精确率-召回率数据，其本身不具备生成此类数据所需的机器学习能力。\n\n同样地，\"通过Amazon ES生成每日精确率-召回率数据\"也不可行。Amazon ES（Elasticsearch）作为搜索分析引擎，并非为生成机器学习预测的精确率-召回率数据而设计，若强行适配此用途则需要大量编码工作。\n\n综上，正确答案巧妙结合了Amazon EMR的数据生成能力与Amazon QuickSight的易用可视化特性，相比其他选项最大程度降低了编码复杂度。"}, "answer": "C"}, {"id": "56", "question": {"enus": "A Machine Learning Specialist is preparing data for training on Amazon SageMaker. The Specialist is using one of the SageMaker built-in algorithms for the training. The dataset is stored in .CSV format and is transformed into a numpy.array, which appears to be negatively affecting the speed of the training. What should the Specialist do to optimize the data for training on SageMaker? ", "zhcn": "一位机器学习专家正在为亚马逊SageMaker平台上的模型训练准备数据。该专家计划采用SageMaker内置算法进行训练，当前数据集以CSV格式存储，且被转换为numpy.array格式，但这一转换操作似乎拖慢了训练速度。为优化SageMaker平台上的训练数据，该专家应当采取何种改进措施？"}, "option": [{"option_text": {"zhcn": "利用SageMaker的批量转换功能，将训练数据转化为DataFrame格式。", "enus": "Use the SageMaker batch transform feature to transform the training data into a DataFrame."}, "option_flag": false}, {"option_text": {"zhcn": "使用AWS Glue将数据压缩为Apache Parquet格式。", "enus": "Use AWS Glue to compress the data into the Apache Parquet format."}, "option_flag": false}, {"option_text": {"zhcn": "将数据集转换为RecordIO协议缓冲区格式。", "enus": "Transform the dataset into the RecordIO protobuf format."}, "option_flag": true}, {"option_text": {"zhcn": "利用SageMaker超参数优化功能，自动实现数据调优。", "enus": "Use the SageMaker hyperparameter optimization feature to automatically optimize the data."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is ‘Transform the dataset into the RecordIO protobuf format.’ Amazon SageMaker's built - in algorithms are optimized to work efficiently with the RecordIO protobuf format. This format allows for faster data transfer and processing during training, which can significantly improve the training speed, especially when dealing with large datasets.\n\nThe option ‘Use the SageMaker batch transform feature to transform the training data into a DataFrame’ is incorrect. Batch transform is used for making predictions on large datasets, not for optimizing data for training. Also, working with a DataFrame may not be as efficient as the RecordIO protobuf format for SageMaker training.\n\n‘Use AWS Glue to compress the data into the Apache Parquet format’ is wrong because while Parquet is a columnar storage format useful for analytics, SageMaker's built - in algorithms are not specifically optimized for it. It may not lead to the best training performance.\n\n‘Use the SageMaker hyperparameter optimization feature to automatically optimize the data’ is a misapplication. Hyperparameter optimization is used to find the best set of hyperparameters for a model, not to optimize the data itself.\n\nThe key factor that makes the real answer correct is the compatibility and optimization of the RecordIO protobuf format with SageMaker's built - in algorithms, which is crucial for efficient training, thus distinguishing it from the fake options.", "zhcn": "问题的正确答案是\"将数据集转换为RecordIO protobuf格式\"。亚马逊SageMaker内置算法经过专门优化，能够高效处理RecordIO protobuf格式。该格式可显著提升训练期间的数据传输与处理速度，尤其在处理大规模数据集时，能带来显著的训练效率提升。\n\n而\"使用SageMaker批量转换功能将训练数据转为DataFrame格式\"这一选项并不正确。批量转换功能适用于对大规模数据集进行预测推断，而非优化训练数据。此外，DataFrame格式在SageMaker训练中的效率往往不及RecordIO protobuf格式。\n\n\"使用AWS Glue将数据压缩为Apache Parquet格式\"同样不妥。尽管Parquet列式存储格式适用于数据分析场景，但SageMaker内置算法并未针对此格式进行专门优化，因此无法获得最佳训练效果。\n\n\"使用SageMaker超参数优化功能自动优化数据\"属于功能误用。超参数优化旨在寻找模型的最佳超参数组合，而非对数据本身进行优化。\n\n正确答案的核心优势在于RecordIO protobuf格式与SageMaker内置算法的高度兼容性和优化适配，这种协同作用确保了训练过程的高效性，从而使其在众多选项中脱颖而出。"}, "answer": "C"}, {"id": "57", "question": {"enus": "A Machine Learning Specialist is required to build a supervised image-recognition model to identify a cat. The ML Specialist performs some tests and records the following results for a neural network-based image classifier: Total number of images available = 1,000 Test set images = 100 (constant test set) The ML Specialist notices that, in over 75% of the misclassified images, the cats were held upside down by their owners. Which techniques can be used by the ML Specialist to improve this specific test error? ", "zhcn": "本公司现需聘请一位机器学习专家，负责构建监督式图像识别模型以实现猫咪识别功能。该专家通过测试记录了基于神经网络图像分类器的以下数据：可用图像总量为1000张，测试集图像数量为100张（采用固定测试集）。专家发现，在超过75%的误判图像中，猫咪均被主人倒置托举。针对这一特定测试误差，可采取哪些优化技术予以改进？"}, "option": [{"option_text": {"zhcn": "通过为训练图像增加旋转变化来扩充训练数据。", "enus": "Increase the training data by adding variation in rotation for training images."}, "option_flag": false}, {"option_text": {"zhcn": "增加模型训练的迭代次数。", "enus": "Increase the number of epochs for model training"}, "option_flag": true}, {"option_text": {"zhcn": "增加神经网络的层数。", "enus": "Increase the number of layers for the neural network."}, "option_flag": false}, {"option_text": {"zhcn": "提高倒数第二层的丢弃率。", "enus": "Increase the dropout rate for the second-to-last layer."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is ‘Increase the number of epochs for model training’. When the ML Specialist notices that most misclassified images are of cats held upside - down, increasing the number of epochs allows the model to train for a longer time on the existing data. This gives the model more opportunities to learn complex patterns, including those related to upside - down cats, potentially reducing the test error.\n\nThe option ‘Increase the training data by adding variation in rotation for training images’ is a good general approach for improving image - recognition models, but the question doesn't ask for a general improvement. It focuses on the specific situation where the data is already given and the issue is with the current model's performance on upside - down cats.\n\n‘Increase the number of layers for the neural network’ might make the model more complex, but there's no indication that the current model lacks complexity. It could lead to overfitting without addressing the specific problem of upside - down cats.\n\n‘Increase the dropout rate for the second - to - last layer’ is mainly used to prevent overfitting. Since the problem isn't about overfitting but about the model's inability to recognize upside - down cats, this option won't solve the specific test error. So, the key factor is that increasing epochs directly targets the model's learning ability to handle the misclassified cases, distinguishing it from the other fake options.", "zhcn": "问题的正确答案是\"增加模型训练的迭代轮次\"。当机器学习专家发现被错误分类的图像大多是倒置的猫咪图片时，增加训练轮次可使模型在现有数据上获得更充分的训练。这将为模型提供更多学习复杂模式的机会——包括倒置猫咪的特征识别，从而有望降低测试误差。\n\n至于\"通过增加训练图像旋转变化来扩充训练数据\"这一选项，虽然是提升图像识别模型的通用良策，但本题所求并非普适性改进方案。该情境明确限定在已给定数据集的前提下，着重解决当前模型对倒置猫咪图像的识别缺陷。\n\n\"增加神经网络层数\"或许能提升模型复杂度，但现有模型并未表现出复杂度不足的迹象。此举反而可能引发过拟合，却无法针对性解决倒置猫咪的识别难题。\n\n\"提升倒数第二层的丢弃率\"主要用于防止过拟合。由于当前问题症结在于模型无法识别倒置猫咪图像而非过拟合，该选项自然无法化解这一特定测试误差。\n\n因此关键之处在于：增加训练轮次能直接增强模型处理错误分类案例的学习能力，这一特性使其从其他干扰项中脱颖而出。"}, "answer": "B"}, {"id": "58", "question": {"enus": "A Machine Learning Specialist needs to be able to ingest streaming data and store it in Apache Parquet files for exploration and analysis. Which of the following services would both ingest and store this data in the correct format? ", "zhcn": "机器学习专家需要能够实时处理数据流，并将其存储为Apache Parquet格式文件以供探索分析。下列哪项服务可同时完成数据摄取并以正确格式存储？"}, "option": [{"option_text": {"zhcn": "AWS数据迁移服务", "enus": "AWS DMS"}, "option_flag": false}, {"option_text": {"zhcn": "Amazon Kinesis Data Streams", "enus": "Amazon Kinesis Data Streams"}, "option_flag": false}, {"option_text": {"zhcn": "亚马逊实时数据流服务", "enus": "Amazon Kinesis Data Firehose"}, "option_flag": true}, {"option_text": {"zhcn": "Amazon Kinesis Data Analytics", "enus": "Amazon Kinesis Data Analytics"}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question 'A Machine Learning Specialist needs to be able to ingest streaming data and store it in Apache Parquet files for exploration and analysis. Which of the following services would both ingest and store this data in the correct format?' is 'Amazon Kinesis Data Firehose'. Amazon Kinesis Data Firehose is designed to ingest streaming data and can directly store it in various destinations, including Amazon S3 in Apache Parquet format, which meets the requirements of the question.\n\n'AWS DMS' is mainly used for migrating databases, not for ingesting and storing streaming data in the required format. 'Amazon Kinesis Data Streams' is focused on capturing, buffering, and processing real - time streaming data but does not handle the storage in the desired format on its own. 'Amazon Kinesis Data Analytics' is used for analyzing streaming data in real - time, not for ingesting and storing it as Parquet files. The key factor that makes 'Amazon Kinesis Data Firehose' the real answer is its ability to perform both ingestion and storage in the correct format, setting it apart from the fake options.", "zhcn": "对于问题\"机器学习专家需要能够摄取流式数据并将其存储为Apache Parquet格式文件以供探索分析，下列哪项服务可同时以正确格式完成数据摄取与存储？\"的正确答案是\"Amazon Kinesis Data Firehose\"。该服务专为流式数据摄取设计，并能直接将数据存储至多种目标位置（包括以Apache Parquet格式存入Amazon S3），完美契合题目要求。\n\n\"AWS DMS\"主要用于数据库迁移，而非以指定格式处理流式数据的摄取与存储；\"Amazon Kinesis Data Streams\"侧重于实时流数据的捕获、缓冲与处理，但本身不涉及指定格式的存储操作；\"Amazon Kinesis Data Analytics\"用于实时分析流数据，而非以Parquet文件形式进行数据摄取和存储。真正使\"Amazon Kinesis Data Firehose\"成为正确答案的关键在于其兼具数据摄取与合规格式存储的双重能力，这一特性使其从其他干扰项中脱颖而出。"}, "answer": "C"}, {"id": "59", "question": {"enus": "A data scientist has explored and sanitized a dataset in preparation for the modeling phase of a supervised learning task. The statistical dispersion can vary widely between features, sometimes by several orders of magnitude. Before moving on to the modeling phase, the data scientist wants to ensure that the prediction performance on the production data is as accurate as possible. Which sequence of steps should the data scientist take to meet these requirements? ", "zhcn": "一位数据科学家已完成对数据集的探索与清理工作，为监督学习任务的建模阶段做好准备。不同特征之间的统计离散程度可能差异显著，有时甚至达到数个数量级。在进入建模阶段之前，该数据科学家希望确保生产环境中的预测性能达到最优。为实现这一目标，其应当遵循怎样的操作流程？"}, "option": [{"option_text": {"zhcn": "对数据集进行随机抽样，随后将其划分为训练集、验证集和测试集。", "enus": "Apply random sampling to the dataset. Then split the dataset into training, validation, and test sets."}, "option_flag": false}, {"option_text": {"zhcn": "将数据集划分为训练集、验证集和测试集。随后对训练集进行归一化处理，并将相同的缩放参数同步应用于验证集与测试集。", "enus": "Split the dataset into training, validation, and test sets. Then rescale the training set and apply the same scaling to the validation and  test sets."}, "option_flag": false}, {"option_text": {"zhcn": "对数据集进行归一化处理，随后将其划分为训练集、验证集和测试集。", "enus": "Rescale the dataset. Then split the dataset into training, validation, and test sets."}, "option_flag": false}, {"option_text": {"zhcn": "将数据集划分为训练集、验证集和测试集，随后分别对训练集、验证集与测试集进行归一化处理。", "enus": "Split the dataset into training, validation, and test sets. Then rescale the training set, the validation set, and the test set independently."}, "option_flag": true}], "analysis": {"enus": "Reference: https://www.kdnuggets.com/2018/12/six-steps-master-machine-learning-data-preparation.html", "zhcn": "参考来源：https://www.kdnuggets.com/2018/12/six-steps-master-machine-learning-data-preparation.html"}, "answer": "D"}, {"id": "60", "question": {"enus": "A Machine Learning Specialist is assigned a TensorFlow project using Amazon SageMaker for training, and needs to continue working for an extended period with no Wi-Fi access. Which approach should the Specialist use to continue working? ", "zhcn": "一位机器学习专家负责利用Amazon SageMaker平台开展基于TensorFlow的模型训练项目，但需要在无法连接Wi-Fi的环境下长期工作。请问该专家应采用何种方案以确保工作持续进行？"}, "option": [{"option_text": {"zhcn": "在他们的笔记本电脑上安装Python 3和boto3，并在此环境下继续推进代码开发工作。", "enus": "Install Python 3 and boto3 on their laptop and continue the code development using that environment."}, "option_flag": false}, {"option_text": {"zhcn": "从GitHub获取亚马逊SageMaker平台所使用的TensorFlow Docker容器至本地环境，并运用亚马逊SageMaker Python SDK对代码进行测试。", "enus": "Download the TensorFlow Docker container used in Amazon SageMaker from GitHub to their local environment, and use the Amazon  SageMaker Python SDK to test the code."}, "option_flag": true}, {"option_text": {"zhcn": "请前往tensorfiow.org下载TensorFlow，以便在SageMaker环境中模拟TensorFlow内核运行环境。", "enus": "Download TensorFlow from tensorfiow.org to emulate the TensorFlow kernel in the SageMaker environment."}, "option_flag": false}, {"option_text": {"zhcn": "将SageMaker笔记本下载至本地环境后，用户需在个人电脑上安装Jupyter Notebooks，即可在本地笔记本中继续开发工作。", "enus": "Download the SageMaker notebook to their local environment, then install Jupyter Notebooks on their laptop and continue the  development in a local notebook."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question ‘A Machine Learning Specialist is assigned a TensorFlow project using Amazon SageMaker for training, and needs to continue working for an extended period with no Wi-Fi access. Which approach should the Specialist use to continue working?’ is ‘Download the TensorFlow Docker container used in Amazon SageMaker from GitHub to their local environment, and use the Amazon SageMaker Python SDK to test the code.’ This is because the Docker container encapsulates all the necessary dependencies and configurations of the SageMaker TensorFlow environment. By using the SageMaker Python SDK, the Specialist can closely mimic the SageMaker training process locally.\n\nThe option ‘Install Python 3 and boto3 on their laptop and continue the code development using that environment’ is incorrect. While Python 3 and boto3 are useful, they do not provide the full - fledged SageMaker TensorFlow environment, lacking important components for seamless continuation of the project.\n\n‘Download TensorFlow from tensorfiow.org to emulate the TensorFlow kernel in the SageMaker environment’ is wrong because simply downloading TensorFlow won't replicate the specific SageMaker - tailored environment, as SageMaker has its own optimizations and configurations.\n\n‘Download the SageMaker notebook to their local environment, then install Jupyter Notebooks on their laptop and continue the development in a local notebook’ is also incorrect. A local Jupyter Notebook lacks the SageMaker - specific runtime and dependencies, so the code may not run as expected.\n\nIn summary, the real answer provides a way to replicate the SageMaker TensorFlow environment locally, which is crucial for continuing work without Wi - Fi, distinguishing it from the fake options.", "zhcn": "针对问题“一位机器学习专家负责一个使用Amazon SageMaker进行训练的TensorFlow项目，需要在无法连接Wi-Fi的情况下长期持续工作。该专家应采用哪种方案？”，正确答案是：“从GitHub将Amazon SageMaker使用的TensorFlow Docker容器下载至本地环境，并利用Amazon SageMaker Python SDK测试代码。”这是因为Docker容器封装了SageMaker TensorFlow环境的所有必要依赖项和配置。通过使用SageMaker Python SDK，专家可以在本地高度模拟SageMaker的训练流程。\n\n而“在笔记本电脑上安装Python 3和boto3，并基于该环境继续开发代码”这一选项并不正确。虽然Python 3和boto3是常用工具，但它们无法提供完整的SageMaker TensorFlow环境，缺少确保项目无缝衔接的关键组件。\n\n“从tensorflow.org下载TensorFlow以模拟SageMaker环境中的TensorFlow内核”也是错误的。仅下载TensorFlow无法复现SageMaker特有的定制环境，因为SageMaker自身包含专属优化配置。\n\n“将SageMaker笔记本下载至本地环境，然后在笔记本电脑上安装Jupyter Notebook并在本地笔记本中继续开发”同样不可行。本地Jupyter Notebook缺乏SageMaker特定的运行时环境与依赖项，代码可能无法按预期运行。\n\n综上，正确答案提供了在本地复现SageMaker TensorFlow环境的方法，这对于无网络连接时的持续工作至关重要，这也是其与错误选项的根本区别所在。"}, "answer": "B"}, {"id": "61", "question": {"enus": "A Machine Learning Specialist is working with a large cybersecurity company that manages security events in real time for companies around the world. The cybersecurity company wants to design a solution that will allow it to use machine learning to score malicious events as anomalies on the data as it is being ingested. The company also wants be able to save the results in its data lake for later processing and analysis. What is the MOST eficient way to accomplish these tasks? ", "zhcn": "一位机器学习专家正与一家大型网络安全公司合作，该公司为全球企业提供实时安全事件监控服务。该网络安全公司希望设计一套解决方案，能够在数据录入时运用机器学习技术，将恶意事件作为异常数据进行风险评分，同时还需能将分析结果存储至数据湖中，以便后续处理与深度挖掘。如何以最高效的方式实现这些目标？"}, "option": [{"option_text": {"zhcn": "通过亚马逊Kinesis数据火线流进行数据摄取，并借助亚马逊Kinesis数据随机切割森林分析算法实现异常检测。随后通过Kinesis数据火线流将处理结果实时传输至亚马逊S3存储服务。", "enus": "Ingest the data using Amazon Kinesis Data Firehose, and use Amazon Kinesis Data Analytics Random Cut Forest (RCF) for anomaly  detection. Then use Kinesis Data Firehose to stream the results to Amazon S3."}, "option_flag": false}, {"option_text": {"zhcn": "利用Amazon EMR将数据实时接入Apache Spark Streaming流处理平台，结合Spark MLlib机器学习库中的k-means算法实现异常检测。随后通过Amazon EMR将处理结果存入Apache Hadoop分布式文件系统（HDFS），设置副本数为三，构建数据湖存储体系。", "enus": "Ingest the data into Apache Spark Streaming using Amazon EMR, and use Spark MLlib with k-means to perform anomaly detection.  Then store the results in an Apache Hadoop Distributed File System (HDFS) using Amazon EMR with a replication factor of three as the  data lake."}, "option_flag": true}, {"option_text": {"zhcn": "将数据导入并存储于Amazon S3中，随后借助AWS Batch服务与AWS深度学习AMI，基于TensorFlow框架对Amazon S3内的数据实施k-means模型训练。", "enus": "Ingest the data and store it in Amazon S3. Use AWS Batch along with the AWS Deep Learning AMIs to train a k-means model using  TensorFlow on the data in Amazon S3."}, "option_flag": false}, {"option_text": {"zhcn": "将数据导入并存储于Amazon S3中，通过按需触发的AWS Glue任务对新增数据进行转换处理。随后调用Amazon SageMaker内置的随机切割森林（RCF）模型，对数据中的异常情况进行检测。", "enus": "Ingest the data and store it in Amazon S3. Have an AWS Glue job that is triggered on demand transform the new data. Then use the  built-in Random Cut Forest (RCF) model within Amazon SageMaker to detect anomalies in the data."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question ‘A Machine Learning Specialist is working with a large cybersecurity company that manages security events in real - time... What is the MOST efficient way to accomplish these tasks?’ is ‘Ingest the data into Apache Spark Streaming using Amazon EMR, and use Spark MLlib with k - means to perform anomaly detection. Then store the results in an Apache Hadoop Distributed File System (HDFS) using Amazon EMR with a replication factor of three as the data lake.’\n\nThe key requirement is to perform real - time anomaly scoring while ingesting data and save results for later analysis. Apache Spark Streaming on Amazon EMR is well - suited for real - time data processing. Spark MLlib provides a k - means implementation that can quickly detect anomalies in the streaming data. Storing the results in HDFS on Amazon EMR with a replication factor of three ensures data durability and is suitable for later processing.\n\nThe first fake option using Amazon Kinesis Data Firehose and Kinesis Data Analytics RCF might be less efficient as it involves multiple Kinesis services, which could add complexity and potentially higher costs. The second fake option of storing data in S3 and using AWS Batch with TensorFlow for k - means training is not suitable for real - time anomaly detection as it first stores the data and then trains the model. The third fake option with AWS Glue and Amazon SageMaker RCF also has a delay as it requires an on - demand Glue job to transform data, which does not meet the real - time processing requirement. These factors make the real answer the most efficient choice for the given task.", "zhcn": "针对“机器学习专家与一家实时处理安全事件的网络安全公司合作……完成这些任务最高效的方式是什么？”这一问题，正确答案为：“通过Amazon EMR将数据接入Apache Spark Streaming，运用Spark MLlib中的k-means算法实现异常检测，随后将结果存入由Amazon EMR托管、副本因子设为3的Apache Hadoop分布式文件系统（HDFS）作为数据湖。”\n\n该方案的核心要求是在数据接入时同步完成实时异常评分，并存储结果供后续分析。基于Amazon EMR的Apache Spark Streaming能够高效处理实时数据流，而Spark MLlib内建的k-means算法可快速识别流数据中的异常模式。将检测结果存入副本因子为3的HDFS，既通过数据冗余机制保障了存储可靠性，又为后续分析任务提供了理想的数据基础。\n\n相比之下，第一个干扰项采用Amazon Kinesis Data Firehose与Kinesis Data Analytics RCF的方案可能效率较低——多层级Kinesis服务的串联会增加系统复杂性及潜在成本。第二个干扰项先将数据存储于S3，再通过AWS Batch结合TensorFlow进行k-means训练的方式无法满足实时检测需求，因其需先完成存储再进行模型训练。第三个干扰项使用AWS Glue与Amazon SageMaker RCF的方案存在处理延迟：按需启动的Glue数据转换作业难以满足实时处理要求。综合考量，原答案所述方案是实现该任务最高效的选择。"}, "answer": "B"}, {"id": "62", "question": {"enus": "A Data Scientist wants to gain real-time insights into a data stream of GZIP files. Which solution would allow the use of SQL to query the stream with the LEAST latency? ", "zhcn": "一位数据科学家希望实时解析GZIP压缩文件的数据流。若要使用SQL查询数据流并实现最低延迟，下列哪种解决方案最为适宜？"}, "option": [{"option_text": {"zhcn": "借助AWS Lambda函数对数据进行转换的Amazon Kinesis数据分析服务。", "enus": "Amazon Kinesis Data Analytics with an AWS Lambda function to transform the data."}, "option_flag": true}, {"option_text": {"zhcn": "使用AWS Glue并搭配自定义ETL脚本来实现数据转换。", "enus": "AWS Glue with a custom ETL script to transform the data."}, "option_flag": false}, {"option_text": {"zhcn": "利用亚马逊Kinesis客户端库对数据进行转换，并将其存储至亚马逊ES集群。", "enus": "An Amazon Kinesis Client Library to transform the data and save it to an Amazon ES cluster."}, "option_flag": false}, {"option_text": {"zhcn": "借助Amazon Kinesis Data Firehose对数据进行转换后，将其存入Amazon S3存储桶。", "enus": "Amazon Kinesis Data Firehose to transform the data and put it into an Amazon S3 bucket."}, "option_flag": false}], "analysis": {"enus": "Reference: https://aws.amazon.com/big-data/real-time-analytics-featured-partners/", "zhcn": "参考来源：https://aws.amazon.com/big-data/real-time-analytics-featured-partners/"}, "answer": "A"}, {"id": "63", "question": {"enus": "A retail company intends to use machine learning to categorize new products. A labeled dataset of current products was provided to the Data Science team. The dataset includes 1,200 products. The labeled dataset has 15 features for each product such as title dimensions, weight, and price. Each product is labeled as belonging to one of six categories such as books, games, electronics, and movies. Which model should be used for categorizing new products using the provided dataset for training? ", "zhcn": "一家零售企业计划采用机器学习技术对新上市商品进行自动分类。数据科学团队已获得现有产品的标注数据集，该数据集涵盖1200种商品，每条记录包含标题、尺寸、重量及价格等15项特征。所有商品均已被标注为六大类别之一，包括图书、游戏、电子设备和影音制品等。基于现有标注数据集进行模型训练时，应采用何种分类模型来实现新商品的智能分类？"}, "option": [{"option_text": {"zhcn": "一个采用multi:softmax目标参数的XGBoost模型。", "enus": "AnXGBoost model where the objective parameter is set to multi:softmax"}, "option_flag": false}, {"option_text": {"zhcn": "一种采用深度卷积神经网络（CNN）架构的模型，其末层激活函数为柔性最大值函数。", "enus": "A deep convolutional neural network (CNN) with a softmax activation function for the last layer"}, "option_flag": true}, {"option_text": {"zhcn": "回归森林中树木数量与产品类别数目相等。", "enus": "A regression forest where the number of trees is set equal to the number of product categories"}, "option_flag": false}, {"option_text": {"zhcn": "基于循环神经网络（RNN）的DeepAR预测模型", "enus": "A DeepAR forecasting model based on a recurrent neural network (RNN)"}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is ‘A deep convolutional neural network (CNN) with a softmax activation function for the last layer’. A CNN is well - suited for this product categorization task. CNNs can automatically learn complex patterns and features from the input data. The softmax activation function in the last layer is ideal for multi - class classification problems, like classifying products into one of six categories.\n\nThe ‘An XGBoost model where the objective parameter is set to multi:softmax’ option might seem relevant as XGBoost is a powerful algorithm for classification. However, XGBoost is more suitable for tabular data where the relationships between features are relatively straightforward. In this case, with potentially complex patterns in product features, a CNN can better capture the nuances.\n\nThe ‘A regression forest where the number of trees is set equal to the number of product categories’ is incorrect because a regression forest is mainly used for regression tasks to predict continuous values, not for multi - class classification.\n\nThe ‘A DeepAR forecasting model based on a recurrent neural network (RNN)’ is wrong as DeepAR is designed for time - series forecasting. This problem is about product categorization, not predicting future values based on time - series data.\n\nThe key factor is that the CNN with softmax is specialized for multi - class classification and can handle the complex feature patterns in the product data, which differentiates it from the fake answer options.", "zhcn": "对于产品分类问题，正确答案应为“采用Softmax激活函数作为末层的深度卷积神经网络（CNN）”。CNN架构在此类任务中具有独特优势，能够从输入数据中自动学习复杂的特征模式。末层的Softmax函数尤其适合多类别分类场景，例如将产品精准划分至六大类别之一。\n\n尽管“设定objective参数为multi:softmax的XGBoost模型”看似可行——毕竟XGBoost确实是优秀的分类算法——但它更适用于特征关系相对规整的表格型数据。而产品特征可能存在的复杂非线性关系，恰恰是CNN更能捕捉的细微之处。\n\n至于“树的数量等于产品类别数的回归森林”这一选项，其根本谬误在于：回归森林专用于预测连续值的回归任务，而非多类别分类场景。而“基于循环神经网络（RNN）的DeepAR预测模型”显然误用了时序预测工具，本案的核心是产品分类而非时间序列预测。\n\n究其本质，带Softmax的CNN方案既具备多类别分类的专业性，又能有效解析产品数据中的复杂特征模式，这正是其他干扰选项所欠缺的核心竞争力。"}, "answer": "B"}, {"id": "64", "question": {"enus": "A Data Scientist is working on an application that performs sentiment analysis. The validation accuracy is poor, and the Data Scientist thinks that the cause may be a rich vocabulary and a low average frequency of words in the dataset. Which tool should be used to improve the validation accuracy? ", "zhcn": "一位数据科学家正在开发一款用于情感分析的应用程序。目前验证准确率不甚理想，他认为问题可能源于数据集词汇量丰富但单词平均出现频率较低。此时应采用何种工具来提升验证准确率？"}, "option": [{"option_text": {"zhcn": "亚马逊Comprehend语法分析与实体识别", "enus": "Amazon Comprehend syntax analysis and entity detection"}, "option_flag": false}, {"option_text": {"zhcn": "Amazon SageMaker BlazingText 连续词袋模式", "enus": "Amazon SageMaker BlazingText cbow mode"}, "option_flag": false}, {"option_text": {"zhcn": "自然语言工具包（NLTK）词干提取与停用词过滤", "enus": "Natural Language Toolkit (NLTK) stemming and stop word removal"}, "option_flag": false}, {"option_text": {"zhcn": "Scikit-learn术语频率-逆文档频率（TF-IDF）向量生成器", "enus": "Scikit-leam term frequency-inverse document frequency (TF-IDF) vectorizer"}, "option_flag": true}], "analysis": {"enus": "Reference: https://monkeylearn.com/sentiment-analysis/", "zhcn": "参考来源：https://monkeylearn.com/sentiment-analysis/"}, "answer": "D"}, {"id": "65", "question": {"enus": "Machine Learning Specialist is building a model to predict future employment rates based on a wide range of economic factors. While exploring the data, the Specialist notices that the magnitude of the input features vary greatly. The Specialist does not want variables with a larger magnitude to dominate the model. What should the Specialist do to prepare the data for model training? ", "zhcn": "机器学习专家正在构建一个模型，旨在通过多元经济指标预测未来就业率。在数据探索过程中，专家发现各输入特征的数值量级差异显著。为避免较大数值范围的变量主导模型训练，专家应当如何对数据进行预处理？"}, "option": [{"option_text": {"zhcn": "对数据进行分位数分箱处理，将其划分为分类区间，通过以分布特征替代数值量级的方式，完整保留数据内在的关联性。", "enus": "Apply quantile binning to group the data into categorical bins to keep any relationships in the data by replacing the magnitude with  distribution."}, "option_flag": false}, {"option_text": {"zhcn": "对字段进行笛卡尔积变换，以生成不受数量级影响的全新组合。", "enus": "Apply the Cartesian product transformation to create new combinations of fields that are independent of the magnitude."}, "option_flag": false}, {"option_text": {"zhcn": "对数据进行归一化处理，确保每个字段的均值为0、方差为1，从而消除量纲差异带来的影响。", "enus": "Apply normalization to ensure each field will have a mean of 0 and a variance of 1 to remove any significant magnitude."}, "option_flag": true}, {"option_text": {"zhcn": "对原始特征施加正交稀疏二元组合（OSB）变换，通过固定尺寸的滑动窗口生成数量级相近的新特征。", "enus": "Apply the orthogonal sparse bigram (OSB) transformation to apply a fixed-size sliding window to generate new features of a similar  magnitude."}, "option_flag": false}], "analysis": {"enus": "Reference: https://docs.aws.amazon.com/machine-learning/latest/dg/data-transformations-reference.html", "zhcn": "参考来源：https://docs.aws.amazon.com/machine-learning/latest/dg/data-transformations-reference.html"}, "answer": "C"}, {"id": "66", "question": {"enus": "A Machine Learning Specialist must build out a process to query a dataset on Amazon S3 using Amazon Athena. The dataset contains more than 800,000 records stored as plaintext CSV files. Each record contains 200 columns and is approximately 1.5 MB in size. Most queries will span 5 to 10 columns only. How should the Machine Learning Specialist transform the dataset to minimize query runtime? ", "zhcn": "机器学习专家需要构建一套流程，通过亚马逊雅典娜服务查询存储在亚马逊S3数据集。该数据集包含逾80万条记录，以纯文本CSV格式存储，每条记录涵盖200个数据列，单条记录大小约为1.5MB。多数查询仅涉及其中5至10个数据列。为最大限度缩短查询耗时，机器学习专家应当如何优化该数据集结构？"}, "option": [{"option_text": {"zhcn": "将记录转换为Apache Parquet格式。", "enus": "Convert the records to Apache Parquet format."}, "option_flag": true}, {"option_text": {"zhcn": "将记录转换为JSON格式。", "enus": "Convert the records to JSON format."}, "option_flag": false}, {"option_text": {"zhcn": "将记录转换为GZIP格式的CSV文件。", "enus": "Convert the records to GZIP CSV format."}, "option_flag": false}, {"option_text": {"zhcn": "将记录转换为XML格式。", "enus": "Convert the records to XML format."}, "option_flag": false}], "analysis": {"enus": "Using compressions will reduce the amount of data scanned by Amazon Athena, and also reduce your S3 bucket storage. It's a Win-Win for your AWS bill. Supported formats: GZIP, LZO, SNAPPY (Parquet) and ZLIB. Reference: https://www.cloudforecast.io/blog/using-parquet-on-athena-to-save-money-on-aws/", "zhcn": "采用压缩技术可显著减少Amazon Athena扫描的数据量，同时降低S3存储桶的存储空间占用。这对您的AWS费用支出来说实属双赢之策。支持的压缩格式包括：GZIP、LZO、SNAPPY（Parquet格式）以及ZLIB。参考链接：https://www.cloudforecast.io/blog/using-parquet-on-athena-to-save-money-on-aws/"}, "answer": "A"}, {"id": "67", "question": {"enus": "A Machine Learning Specialist is developing a daily ETL workfiow containing multiple ETL jobs. The workfiow consists of the following processes: * Start the workfiow as soon as data is uploaded to Amazon S3. * When all the datasets are available in Amazon S3, start an ETL job to join the uploaded datasets with multiple terabyte-sized datasets already stored in Amazon S3. * Store the results of joining datasets in Amazon S3. * If one of the jobs fails, send a notification to the Administrator. Which configuration will meet these requirements? ", "zhcn": "一位机器学习专家正在设计包含多项ETL任务的日常数据处理流程。该流程包含以下环节：  \n* 一旦数据上传至亚马逊S3服务，立即启动流程；  \n* 当所有数据集在亚马逊S3中就绪后，启动ETL任务，将新上传的数据集与已存储于亚马逊S3的多个TB级数据集进行关联整合；  \n* 将关联后的结果数据集存回亚马逊S3；  \n* 若任一任务执行失败，需向管理员发送通知。  \n请问何种配置方案可满足上述需求？"}, "option": [{"option_text": {"zhcn": "利用AWS Lambda触发AWS Step Functions工作流，以监测Amazon S3中数据集上传完成状态。通过AWS Glue对数据集进行关联整合。若流程出现异常，借助Amazon CloudWatch警报机制向管理员发送SNS通知。", "enus": "Use AWS Lambda to trigger an AWS Step Functions workfiow to wait for dataset uploads to complete in Amazon S3. Use AWS Glue to  join the datasets. Use an Amazon CloudWatch alarm to send an SNS notification to the Administrator in the case of a failure."}, "option_flag": true}, {"option_text": {"zhcn": "运用AWS Lambda构建ETL工作流，以启动Amazon SageMaker笔记本实例。通过生命周期配置脚本整合数据集，并将处理结果持久化存储至Amazon S3。若运行异常，则借助Amazon CloudWatch警报向管理员发送SNS通知。", "enus": "Develop the ETL workfiow using AWS Lambda to start an Amazon SageMaker notebook instance. Use a lifecycle configuration script to  join the datasets and persist the results in Amazon S3. Use an Amazon CloudWatch alarm to send an SNS notification to the Administrator  in the case of a failure."}, "option_flag": false}, {"option_text": {"zhcn": "采用AWS Batch构建ETL工作流，当数据上传至Amazon S3时自动触发作业启动。通过AWS Glue对Amazon S3中的数据集进行关联整合。若运行异常，则借助Amazon CloudWatch警报机制向管理员发送SNS通知。", "enus": "Develop the ETL workfiow using AWS Batch to trigger the start of ETL jobs when data is uploaded to Amazon S3. Use AWS Glue to join  the datasets in Amazon S3. Use an Amazon CloudWatch alarm to send an SNS notification to the Administrator in the case of a failure."}, "option_flag": false}, {"option_text": {"zhcn": "利用AWS Lambda实现函数级联调用，一旦数据上传至Amazon S3，即可自动触发后续Lambda函数读取并关联存储于S3中的数据集。若出现运行故障，系统将通过Amazon CloudWatch警报向管理员发送SNS通知。", "enus": "Use AWS Lambda to chain other Lambda functions to read and join the datasets in Amazon S3 as soon as the data is uploaded to  Amazon S3. Use an Amazon CloudWatch alarm to send an SNS notification to the Administrator in the case of a failure."}, "option_flag": false}], "analysis": {"enus": "Reference: https://aws.amazon.com/step-functions/use-cases/", "zhcn": "参考来源：https://aws.amazon.com/step-functions/use-cases/"}, "answer": "A"}, {"id": "68", "question": {"enus": "An agency collects census information within a country to determine healthcare and social program needs by province and city. The census form collects responses for approximately 500 questions from each citizen. Which combination of algorithms would provide the appropriate insights? (Choose two.) ", "zhcn": "某国普查机构为掌握各省市医疗与社会福利需求，定期开展人口普查。普查问卷涵盖近500项居民信息采集项。下列哪两种算法组合最适用于此类数据分析？（请选择两项）"}, "option": [{"option_text": {"zhcn": "因子分解机（FM）算法", "enus": "The factorization machines (FM) algorithm"}, "option_flag": false}, {"option_text": {"zhcn": "隐含狄利克雷分布（LDA）算法", "enus": "The Latent Dirichlet Allocation (LDA) algorithm"}, "option_flag": false}, {"option_text": {"zhcn": "主成分分析（PCA）算法", "enus": "The principal component analysis (PCA) algorithm"}, "option_flag": true}, {"option_text": {"zhcn": "k-means聚类算法", "enus": "The k-means algorithm"}, "option_flag": true}, {"option_text": {"zhcn": "随机切割森林（RCF）算法", "enus": "The Random Cut Forest (RCF) algorithm"}, "option_flag": false}], "analysis": {"enus": "The PCA and K-means algorithms are useful in collection of data using census form.", "zhcn": "主成分分析与K均值聚类算法在人口普查表格的数据采集中具有重要应用价值。"}, "answer": "CD"}, {"id": "69", "question": {"enus": "A large consumer goods manufacturer has the following products on sale: * 34 different toothpaste variants * 48 different toothbrush variants * 43 different mouthwash variants The entire sales history of all these products is available in Amazon S3. Currently, the company is using custom-built autoregressive integrated moving average (ARIMA) models to forecast demand for these products. The company wants to predict the demand for a new product that will soon be launched. Which solution should a Machine Learning Specialist apply? ", "zhcn": "一家大型消费品制造商现正销售以下产品：  \n* 34种不同配方的牙膏  \n* 48款不同类型的牙刷  \n* 43种不同功效的漱口水  \n\n所有产品的完整销售数据均存储于Amazon S3中。目前，该公司采用自定义的自回归综合移动平均（ARIMA）模型对这些产品进行需求预测。随着新品即将上市，制造商希望提前预估其市场需求。机器学习专家应当采用何种解决方案？"}, "option": [{"option_text": {"zhcn": "为新产品定制ARIMA模型以预测其需求量。", "enus": "Train a custom ARIMA model to forecast demand for the new product."}, "option_flag": false}, {"option_text": {"zhcn": "训练Amazon SageMaker DeepAR算法以预测新产品的需求量。", "enus": "Train an Amazon SageMaker DeepAR algorithm to forecast demand for the new product."}, "option_flag": true}, {"option_text": {"zhcn": "训练亚马逊SageMaker平台的k-means聚类算法，以预测新产品的市场需求。", "enus": "Train an Amazon SageMaker k-means clustering algorithm to forecast demand for the new product."}, "option_flag": false}, {"option_text": {"zhcn": "训练定制化的XGBoost模型，以精准预测新产品的市场需求。", "enus": "Train a custom XGBoost model to forecast demand for the new product."}, "option_flag": false}], "analysis": {"enus": "The Amazon SageMaker DeepAR forecasting algorithm is a supervised learning algorithm for forecasting scalar (one-dimensional) time series using recurrent neural networks (RNN). Classical forecasting methods, such as autoregressive integrated moving average (ARIMA) or exponential smoothing (ETS), fit a single model to each individual time series. They then use that model to extrapolate the time series into the future. Reference: https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html", "zhcn": "Amazon SageMaker DeepAR预测算法是一种基于递归神经网络（RNN）的监督学习算法，专用于标量（一维）时间序列的预测。与传统预测方法（如自回归积分滑动平均模型ARIMA或指数平滑法ETS）针对每个单独时间序列拟合单一模型不同，该算法通过RNN整合相关序列信息进行联合训练。传统方法仅依赖各时间序列自身历史数据进行外推预测，而DeepAR能够捕捉多个相关序列间的潜在模式，从而提升预测精度。参考链接：https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html"}, "answer": "B"}, {"id": "70", "question": {"enus": "A Machine Learning Specialist uploads a dataset to an Amazon S3 bucket protected with server-side encryption using AWS KMS. How should the ML Specialist define the Amazon SageMaker notebook instance so it can read the same dataset from Amazon S3? ", "zhcn": "一位机器学习专家将数据集上传至采用AWS KMS服务端加密保护的Amazon S3存储桶。为确保该专家能通过Amazon SageMaker笔记本实例读取同一数据集，应如何配置此笔记本实例？"}, "option": [{"option_text": {"zhcn": "请配置安全组规则，允许所有HTTP入站与出站流量，并将该安全组关联至Amazon SageMaker笔记本实例。", "enus": "Define security group(s) to allow all HTTP inbound/outbound trafic and assign those security group(s) to the Amazon SageMaker  notebook instance."}, "option_flag": false}, {"option_text": {"zhcn": "请将亚马逊SageMaker笔记本实例配置为可访问该虚拟私有云。", "enus": "׀¡onfigure the Amazon SageMaker notebook instance to have access to the VP"}, "option_flag": false}, {"option_text": {"zhcn": "请在KMS密钥策略中授予笔记本KMS角色相应权限。  \nC. 为Amazon SageMaker笔记本分配一个具有S3数据集读取权限的IAM角色，并在KMS密钥策略中向该角色授予权限。", "enus": "Grant permission in the KMS key policy to the  notebook's KMS role.  C. Assign an IAM role to the Amazon SageMaker notebook with S3 read access to the dataset. Grant permission in the KMS key policy to  that role."}, "option_flag": false}, {"option_text": {"zhcn": "将用于加密 Amazon S3 数据的 KMS 密钥同样配置到 Amazon SageMaker 笔记本实例中。", "enus": "Assign the same KMS key used to encrypt data in Amazon S3 to the Amazon SageMaker notebook instance."}, "option_flag": true}], "analysis": {"enus": "Reference: https://docs.aws.amazon.com/sagemaker/latest/dg/encryption-at-rest.html", "zhcn": "参考文档：亚马逊AWS官方指南 - Amazon SageMaker 数据静态加密  \n（原文链接：https://docs.aws.amazon.com/sagemaker/latest/dg/encryption-at-rest.html）"}, "answer": "D"}, {"id": "71", "question": {"enus": "A Data Scientist needs to migrate an existing on-premises ETL process to the cloud. The current process runs at regular time intervals and uses PySpark to combine and format multiple large data sources into a single consolidated output for downstream processing. The Data Scientist has been given the following requirements to the cloud solution: ✑ Combine multiple data sources. ✑ Reuse existing PySpark logic. ✑ Run the solution on the existing schedule. ✑ Minimize the number of servers that will need to be managed. Which architecture should the Data Scientist use to build this solution? ", "zhcn": "一位数据科学家需要将现有的本地ETL流程迁移至云端。当前流程按固定时间间隔运行，使用PySpark整合多个大型数据源并格式化，最终生成统一输出供下游处理。该数据科学家已获知云端解决方案需满足以下要求：  \n✑ 融合多数据源  \n✑ 复用现有PySpark逻辑  \n✑ 按原定计划执行任务  \n✑ 最大限度减少待维护服务器数量  \n请问该数据科学家应采用何种架构来构建此解决方案？"}, "option": [{"option_text": {"zhcn": "将原始数据写入Amazon S3存储服务。根据现有调度计划，配置AWS Lambda函数以向常驻的Amazon EMR集群提交Spark作业步骤。运用现有的PySpark逻辑在EMR集群上运行ETL数据处理任务，并将处理结果输出至Amazon S3的指定存储区域，便于下游环节调用使用。", "enus": "Write the raw data to Amazon S3. Schedule an AWS Lambda function to submit a Spark step to a persistent Amazon EMR cluster based  on the existing schedule. Use the existing PySpark logic to run the ETL job on the EMR cluster. Output the results to a processed  location in Amazon S3 that is accessible for downstream use."}, "option_flag": false}, {"option_text": {"zhcn": "将原始数据写入Amazon S3存储服务。创建AWS Glue ETL作业对输入数据进行抽取、转换和加载处理。该ETL作业采用PySpark编写，以复用现有逻辑。基于现有调度计划新建AWS Glue触发器，用于自动触发ETL作业执行。配置ETL作业的输出目标至Amazon S3中可供下游使用的处理结果存储位置。", "enus": "Write the raw data to Amazon S3. Create an AWS Glue ETL job to perform the ETL processing against the input data. Write the ETL job  in PySpark to leverage the existing logic. Create a new AWS Glue trigger to trigger the ETL job based on the existing schedule. Configure  the output target of the ETL job to write to a processed location in Amazon S3 that is accessible for downstream use."}, "option_flag": false}, {"option_text": {"zhcn": "将原始数据写入Amazon S3存储服务。依照现有调度计划配置AWS Lambda函数，用于处理来自Amazon S3的输入数据。使用Python编写Lambda函数逻辑，并整合现有PySpark代码以实现ETL流程。最终将处理结果输出至Amazon S3的指定存储区域，便于下游环节调用使用。", "enus": "Write the raw data to Amazon S3. Schedule an AWS Lambda function to run on the existing schedule and process the input data from  Amazon S3. Write the Lambda logic in Python and implement the existing PySpark logic to perform the ETL process. Have the Lambda  function output the results to a processed location in Amazon S3 that is accessible for downstream use."}, "option_flag": false}, {"option_text": {"zhcn": "利用亚马逊Kinesis数据流分析服务，可对输入数据进行实时流处理，并通过流式SQL查询实现所需的流内数据转换。最终将处理结果输出至亚马逊S3存储服务中指定区域，便于下游环节调用使用。", "enus": "Use Amazon Kinesis Data Analytics to stream the input data and perform real-time SQL queries against the stream to carry out the  required transformations within the stream. Deliver the output results to a processed location in Amazon S3 that is accessible for  downstream use."}, "option_flag": true}], "analysis": {"enus": "The correct answer to the question is to use Amazon Kinesis Data Analytics. This is because it allows for real - time processing of data streams, which can combine multiple data sources. It also supports SQL queries that can achieve similar transformations as the existing PySpark logic. Moreover, it is a fully - managed service, which means it minimizes the number of servers that need to be managed.\n\nThe option of using an AWS Lambda function to submit a Spark step to a persistent Amazon EMR cluster requires managing the EMR cluster, which goes against the requirement of minimizing server management. \n\nThe AWS Glue ETL job option, while it can use PySpark and be scheduled, is more focused on batch processing rather than potentially handling data in a real - time or near - real - time manner that might be needed for a seamless migration. \n\nThe option of using an AWS Lambda function to run the ETL process has limitations. Lambda functions have resource constraints such as memory and execution time, and implementing complex PySpark logic in a Lambda function may not be feasible or efficient. \n\nThese factors make Amazon Kinesis Data Analytics the best choice, distinguishing it from the other fake answer options.", "zhcn": "对于该问题的正确答案是采用Amazon Kinesis Data Analytics。原因在于该服务支持实时处理数据流，并能整合多路数据源。同时其内置的SQL查询功能可实现与现有PySpark逻辑相近的数据转换效果。作为全托管服务，它还能最大限度减少需要维护的服务器数量。\n\n若采用AWS Lambda函数向持久化Amazon EMR集群提交Spark任务，则需要管理EMR集群，这与减少服务器管理的需求相悖。而AWS Glue ETL作业虽支持PySpark与定时调度，但其侧重批处理模式，难以满足无缝迁移可能需要的实时或近实时数据处理要求。至于通过AWS Lambda函数运行ETL流程的方案存在明显局限——Lambda函数存在内存和执行时间等资源约束，在其中实现复杂PySpark逻辑既不可行也缺乏效率。\n\n综上可知，Amazon Kinesis Data Analytics凭借其独特优势成为最佳选择，与其他干扰项形成鲜明对比。"}, "answer": "D"}, {"id": "72", "question": {"enus": "A Data Scientist is building a model to predict customer churn using a dataset of 100 continuous numerical features. The Marketing team has not provided any insight about which features are relevant for churn prediction. The Marketing team wants to interpret the model and see the direct impact of relevant features on the model outcome. While training a logistic regression model, the Data Scientist observes that there is a wide gap between the training and validation set accuracy. Which methods can the Data Scientist use to improve the model performance and satisfy the Marketing team's needs? (Choose two.) ", "zhcn": "一位数据科学家正在利用包含100个连续数值特征的数据集构建客户流失预测模型。市场营销团队未提供任何关于哪些特征与流失预测相关的指导。该团队希望解读模型，并观察相关特征对模型结果的直接影响。在训练逻辑回归模型时，数据科学家发现训练集与验证集的准确率存在显著差异。此时，数据科学家可采用哪两种方法来提升模型性能并满足市场营销团队的需求？"}, "option": [{"option_text": {"zhcn": "为分类器加入L1正则化", "enus": "Add L1 regularization to the classifier"}, "option_flag": false}, {"option_text": {"zhcn": "为数据集增添功能", "enus": "Add features to the dataset"}, "option_flag": true}, {"option_text": {"zhcn": "执行递归特征消除", "enus": "Perform recursive feature elimination"}, "option_flag": false}, {"option_text": {"zhcn": "执行t分布随机邻域嵌入（t-SNE）", "enus": "Perform t-distributed stochastic neighbor embedding (t-SNE)"}, "option_flag": false}, {"option_text": {"zhcn": "进行线性判别分析", "enus": "Perform linear discriminant analysis"}, "option_flag": true}], "analysis": {"enus": "The correct answers to improve the model performance and meet the Marketing team's needs are “Add features to the dataset” and “Perform linear discriminant analysis”. Adding features can enrich the model with more information, potentially capturing patterns related to customer churn that were previously missed, especially when the Marketing team hasn't specified relevant features. Linear discriminant analysis helps in finding a linear combination of features that separates classes, which can enhance the model's predictive power and is interpretable, satisfying the Marketing team's need to understand the impact of features.\n\nThe fake option “Add L1 regularization to the classifier” is incorrect because L1 regularization shrinks some coefficients to zero, making the model sparser. While it can help with overfitting, it might eliminate potentially useful features, and the Marketing team hasn't provided guidance on which features are relevant. “Perform recursive feature elimination” is also wrong as it removes features step - by - step based on their importance, but without Marketing's input, we don't know which features are truly relevant and could end up discarding useful ones. “Perform t - distributed stochastic neighbor embedding (t - SNE)” is mainly used for data visualization in high - dimensional data and is not a method for directly improving the performance of a logistic regression model for prediction and interpretability.", "zhcn": "为提升模型性能并满足营销团队需求，正确答案应为“增加数据集特征”与“执行线性判别分析”。增加特征能为模型注入更丰富的信息，有助于捕捉以往遗漏的客户流失规律，尤其在营销团队未明确相关特征时更具价值。线性判别分析通过寻找区分用户类别的特征线性组合，既能增强模型预测能力，又具备可解释性，契合营销团队理解特征影响的需求。\n\n而错误选项“对分类器添加L1正则化”并不适用——虽然L1正则化能通过压缩部分系数至零来简化模型，但可能剔除潜在有用特征，且营销团队并未提供特征优先级指引。“执行递归特征消除”同样不妥，该方法逐步剔除次要特征，但在缺乏业务指导时易误删关键信息。“执行t分布随机邻域嵌入（t-SNE）”主要用于高维数据可视化，无法直接提升逻辑回归模型的预测性能与可解释性。"}, "answer": "BE"}, {"id": "73", "question": {"enus": "An aircraft engine manufacturing company is measuring 200 performance metrics in a time-series. Engineers want to detect critical manufacturing defects in near- real time during testing. All of the data needs to be stored for ofiine analysis. What approach would be the MOST effective to perform near-real time defect detection? ", "zhcn": "一家航空发动机制造企业正在对200项性能指标进行时间序列监测。工程师们需要在测试过程中近乎实时地发现关键制造缺陷，同时所有数据都需存档供离线分析。要实施近实时缺陷检测，何种方法最具实效性？"}, "option": [{"option_text": {"zhcn": "运用AWS IoT Analytics实现数据采集、存储与深度分析。通过其内置的Jupyter Notebook功能，可对数据进行异常检测分析。", "enus": "Use AWS IoT Analytics for ingestion, storage, and further analysis. Use Jupyter notebooks from within AWS IoT Analytics to carry out  analysis for anomalies."}, "option_flag": false}, {"option_text": {"zhcn": "利用Amazon S3进行数据接入、存储与深度分析，并通过Amazon EMR集群运行Apache Spark ML中的k-means聚类算法，以精准识别异常模式。", "enus": "Use Amazon S3 for ingestion, storage, and further analysis. Use an Amazon EMR cluster to carry out Apache Spark ML k-means  clustering to determine anomalies."}, "option_flag": true}, {"option_text": {"zhcn": "采用Amazon S3进行数据接入、存储与深度分析，并运用Amazon SageMaker随机切割森林（RCF）算法精准识别异常模式。", "enus": "Use Amazon S3 for ingestion, storage, and further analysis. Use the Amazon SageMaker Random Cut Forest (RCF) algorithm to  determine anomalies."}, "option_flag": false}, {"option_text": {"zhcn": "采用Amazon Kinesis Data Firehose进行数据摄取，并借助Amazon Kinesis Data Analytics随机切割森林（RCF）算法实现异常检测。通过Kinesis Data Firehose将数据存储至Amazon S3中，以便开展深度分析。", "enus": "Use Amazon Kinesis Data Firehose for ingestion and Amazon Kinesis Data Analytics Random Cut Forest (RCF) to perform anomaly  detection. Use Kinesis Data Firehose to store data in Amazon S3 for further analysis."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Use Amazon S3 for ingestion, storage, and further analysis. Use an Amazon EMR cluster to carry out Apache Spark ML k - means clustering to determine anomalies.” \n\nFor this aircraft engine testing scenario with 200 time - series performance metrics, k - means clustering in Apache Spark on an EMR cluster is well - suited for detecting critical manufacturing defects in near - real time. K - means clustering can group similar data points together, and any data points that fall outside these groups can be identified as anomalies, which could indicate manufacturing defects. Amazon S3 provides reliable and cost - effective storage for all the data, facilitating both near - real - time analysis and offline analysis.\n\nThe option of using AWS IoT Analytics is less effective as it is more tailored for IoT device data processing and may not be optimized for the large - scale time - series performance data from aircraft engine testing. \n\nUsing Amazon SageMaker Random Cut Forest (RCF) algorithm might be a viable option for anomaly detection, but it is more focused on single - dimensional or low - dimensional data streams. With 200 performance metrics, the k - means clustering in a multi - dimensional space using Apache Spark on EMR can provide a more comprehensive view of the data. \n\nThe option involving Amazon Kinesis services is mainly designed for real - time streaming data processing. While it can handle ingestion, storage, and anomaly detection, the additional complexity of using multiple Kinesis services may not be necessary for this scenario, and the k - means clustering approach on EMR can offer a more straightforward and effective solution for detecting manufacturing defects. \n\nThese are the reasons why the selected real answer option is the most appropriate for near - real - time defect detection in this aircraft engine testing case.", "zhcn": "针对飞机发动机测试场景中涉及200项时间序列性能指标的数据，采用\"借助Amazon S3实现数据接入、存储与深度分析，并通过Amazon EMR集群运行Apache Spark ML的k-means聚类算法来识别异常\"的方案最为适宜。该方案中，基于EMR集群的Apache Spark k-means聚类技术能够有效实现近实时关键制造缺陷检测。该算法通过将相似数据点归集，从而甄别出偏离群体的异常数据——这些异常往往指向潜在制造缺陷。Amazon S3则为海量数据提供了高可靠性、低成本的存储方案，同时支持近实时分析与离线研究。\n\n相较而言，AWS IoT Analytics方案适用性较弱，因其更专注于物联网设备数据处理，未必能优化处理飞机发动机测试产生的大规模时序数据。虽然Amazon SageMaker随机砍伐森林（RCF）算法可用于异常检测，但其更擅长处理单维或低维数据流。面对200维性能指标，采用EMR集群上Apache Spark实现的多维空间k-means聚类能提供更全面的数据洞察。而涉及Amazon Kinesis服务的方案主要面向实时流数据处理，虽具备数据接入、存储和异常检测能力，但在此场景中使用多项Kinesis服务会引入不必要的复杂性。相比之下，EMR平台的k-means聚类方案为制造缺陷检测提供了更直接有效的解决路径。\n\n综上，在当前飞机发动机测试案例中，所选方案确实是最适合实现近实时缺陷检测的优选策略。"}, "answer": "B"}, {"id": "74", "question": {"enus": "A Machine Learning team runs its own training algorithm on Amazon SageMaker. The training algorithm requires external assets. The team needs to submit both its own algorithm code and algorithm-specific parameters to Amazon SageMaker. What combination of services should the team use to build a custom algorithm in Amazon SageMaker? (Choose two.) ", "zhcn": "某机器学习团队在Amazon SageMaker平台上运行自研的训练算法。该训练过程需调用外部资源，因此团队既要提交自有算法代码，又需配置算法专属参数。若要在Amazon SageMaker中构建定制化算法，应选择哪两项服务组合？（请选出两个正确答案）"}, "option": [{"option_text": {"zhcn": "AWS Secrets Manager", "enus": "AWS Secrets Manager"}, "option_flag": false}, {"option_text": {"zhcn": "AWS CodeStar", "enus": "AWS CodeStar"}, "option_flag": false}, {"option_text": {"zhcn": "Amazon ECR", "enus": "Amazon ECR"}, "option_flag": true}, {"option_text": {"zhcn": "Amazon ECS", "enus": "Amazon ECS"}, "option_flag": false}, {"option_text": {"zhcn": "亚马逊S3", "enus": "Amazon S3"}, "option_flag": true}], "analysis": {"enus": "The correct answers to the question are ‘Amazon ECR’ and ‘Amazon S3’. Amazon ECR (Elastic Container Registry) is used to store Docker container images. Since the team has its own training algorithm, they can package it into a Docker image and store it in Amazon ECR, which Amazon SageMaker can then access to run the algorithm. Amazon S3 (Simple Storage Service) is a highly scalable object storage service. The team can use it to store external assets required by the training algorithm as well as algorithm - specific parameters. \n\n‘AWS Secrets Manager’ is mainly used for managing, retrieving, and rotating secrets like database credentials, API keys, etc., and is not directly related to building and running a custom algorithm in SageMaker. ‘AWS CodeStar’ is a service that provides a unified user interface to manage software development projects, which is not relevant to the task of storing algorithm code and assets for SageMaker. ‘Amazon ECS’ (Elastic Container Service) is a container orchestration service, but it is not the primary service for storing and providing access to the custom algorithm for Amazon SageMaker. These are the reasons why ‘Amazon ECR’ and ‘Amazon S3’ are the real answer options, differentiating them from the fake options.", "zhcn": "该问题的正确答案是\"Amazon ECR\"与\"Amazon S3\"。Amazon ECR（弹性容器注册表）专用于存储Docker容器镜像。由于该团队拥有专属训练算法，可将其封装至Docker镜像并存放于Amazon ECR中，Amazon SageMaker便能直接调用该镜像运行算法。Amazon S3（简单存储服务）作为高扩展性对象存储服务，可供团队存放训练算法所需的外部资源及算法特定参数。\n\n\"AWS Secrets Manager\"主要用于管理、获取及轮转数据库凭证、API密钥等敏感信息，与在SageMaker中构建运行自定义算法的场景无直接关联。\"AWS CodeStar\"作为统一管理软件开发项目的集成界面，与为SageMaker存储算法代码及资源的任务无关。\"Amazon ECS\"（弹性容器服务）虽是容器编排工具，但并非为Amazon SageMaker提供自定义算法存储及访问的核心服务。\n\n正是基于上述特性，\"Amazon ECR\"和\"Amazon S3\"成为本质选项，从而与其他干扰项形成显著区别。"}, "answer": "CE"}, {"id": "75", "question": {"enus": "A Machine Learning Specialist wants to determine the appropriate SageMakerVariantInvocationsPerInstance setting for an endpoint automatic scaling configuration. The Specialist has performed a load test on a single instance and determined that peak requests per second (RPS) without service degradation is about 20 RPS. As this is the first deployment, the Specialist intends to set the invocation safety factor to 0.5. Based on the stated parameters and given that the invocations per instance setting is measured on a per-minute basis, what should the Specialist set as the SageMakerVariantInvocationsPerInstance setting? ", "zhcn": "一位机器学习专家需要为端点自动伸缩配置确定合适的SageMakerVariantInvocationsPerInstance参数值。通过对单实例进行负载测试，该专家已确认在保持服务不降级的前提下，每秒最高请求处理量约为20RPS。由于属于首次部署，专家计划将调用安全系数设定为0.5。基于上述参数，且已知单实例调用量以分钟为计量单位，请问应如何设定SageMakerVariantInvocationsPerInstance的数值？"}, "option": [{"option_text": {"zhcn": "十", "enus": "10"}, "option_flag": false}, {"option_text": {"zhcn": "三十", "enus": "30"}, "option_flag": false}, {"option_text": {"zhcn": "六百", "enus": "600"}, "option_flag": true}, {"option_text": {"zhcn": "两千四百", "enus": "2,400"}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is ‘600’. First, the peak requests per second (RPS) without service degradation is 20 RPS. The invocation safety factor is 0.5, so the safe RPS is 20 * 0.5 = 10 RPS. Since the SageMakerVariantInvocationsPerInstance setting is measured on a per - minute basis, we convert the safe RPS to per - minute by multiplying by 60 (as there are 60 seconds in a minute). So, 10 * 60 = 600.\n\nThe option ‘10’ is incorrect because it is the safe RPS and not the invocations per minute. The option ‘30’ has no relation to the correct calculation based on the given parameters. The option ‘2,400’ would be the result if we didn't apply the safety factor and just multiplied the peak RPS by 60 seconds, which is not what the question requires. This shows that understanding the conversion from RPS to per - minute and applying the safety factor are crucial for getting the correct answer.", "zhcn": "该问题的正确答案为\"600\"。首先，服务无性能衰减时的峰值每秒请求量为20次。调用安全系数设定为0.5，因此安全请求量为20×0.5=10次/秒。由于SageMakerVariantInvocationsPerInstance配置以分钟为计量单位，需将安全请求量乘以60秒进行换算，即10×60=600次/分钟。\n\n选项\"10\"错误，因其表示安全请求量而非分钟调用量；选项\"30\"与给定参数的计算逻辑无关；选项\"2,400\"是未采用安全系数、直接将峰值请求量乘以60秒的结果，不符合题目要求。由此可见，理解每秒到每分钟的换算关系并正确应用安全系数是解题关键。"}, "answer": "C"}, {"id": "76", "question": {"enus": "A company uses a long short-term memory (LSTM) model to evaluate the risk factors of a particular energy sector. The model reviews multi- page text documents to analyze each sentence of the text and categorize it as either a potential risk or no risk. The model is not performing well, even though the Data Scientist has experimented with many different network structures and tuned the corresponding hyperparameters. Which approach will provide the MAXIMUM performance boost? ", "zhcn": "某公司采用长短期记忆（LSTM）模型评估特定能源领域的风险因素。该模型通过审阅多页文本文档，逐句分析内容并将其归类为潜在风险或无风险。尽管数据科学家已尝试多种网络结构并调整相应超参数，模型性能仍不理想。下列哪种方法能最大限度提升模型效能？"}, "option": [{"option_text": {"zhcn": "以能源领域海量新闻文本预训练的TF-IDF向量为基准，对词汇进行初始化处理。", "enus": "Initialize the words by term frequency-inverse document frequency (TF-IDF) vectors pretrained on a large collection of news articles  related to the energy sector."}, "option_flag": false}, {"option_text": {"zhcn": "采用门控循环单元（GRU）替代长短期记忆网络（LSTM），并在验证集损失停止下降时结束训练过程。", "enus": "Use gated recurrent units (GRUs) instead of LSTM and run the training process until the validation loss stops decreasing."}, "option_flag": false}, {"option_text": {"zhcn": "降低学习率，持续训练直至损失函数不再下降。", "enus": "Reduce the learning rate and run the training process until the training loss stops decreasing."}, "option_flag": true}, {"option_text": {"zhcn": "基于能源领域海量新闻语料预训练的word2vec词向量，对词汇进行初始化处理。", "enus": "Initialize the words by word2vec embeddings pretrained on a large collection of news articles related to the energy sector."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is “Reduce the learning rate and run the training process until the training loss stops decreasing.” This approach can potentially provide the maximum performance boost because when a model is not performing well despite hyperparameter tuning and different network structures, it might be overshooting the optimal solution during training. A high learning rate can cause the model to make large updates to its weights, preventing it from converging to an optimal set of weights. By reducing the learning rate, the model takes smaller steps towards the optimal solution, allowing it to converge more effectively and potentially improve performance.\n\nThe option “Initialize the words by term frequency - inverse document frequency (TF - IDF) vectors pretrained on a large collection of news articles related to the energy sector” is a poor choice. TF - IDF is mainly a measure of how important a word is in a document within a collection, and it doesn't capture the semantic relationships between words well, which are crucial for a model analyzing text for risk categorization.\n\nUsing “gated recurrent units (GRUs) instead of LSTM and run the training process until the validation loss stops decreasing” may not be the best option. While GRUs are computationally cheaper and simpler than LSTMs, there's no guarantee that simply switching to GRUs will solve the performance issue. The problem might not be related to the type of recurrent unit but rather the optimization process.\n\n“Initialize the words by word2vec embeddings pretrained on a large collection of news articles related to the energy sector” also has limitations. Although word2vec can capture semantic relationships between words, the performance problem might not stem from the lack of good word embeddings. It could be due to the optimization process, which is better addressed by adjusting the learning rate.\n\nIn summary, adjusting the learning rate to help the model converge is a more direct approach to improving performance compared to the other options, which focus on different aspects like word representation or model architecture that may not be the root cause of the problem.", "zhcn": "对于该问题的正确答案是：\"降低学习率并持续训练过程，直至训练损失停止下降\"。这种方法最有可能带来最大程度的性能提升，因为当模型在经过超参数调优和不同网络结构尝试后仍表现不佳时，很可能是训练过程中越过了最优解。过高的学习率会导致模型权重更新幅度过大，使其难以收敛到最优权重组合。通过降低学习率，模型将以更小的步长逼近最优解，从而实现更有效的收敛，并可能提升最终性能。\n\n而\"采用基于能源领域新闻大数据预训练的TF-IDF向量进行词初始化\"这一选择并不理想。TF-IDF主要衡量的是词语在文档集合中的重要性程度，无法充分捕捉词语间的语义关联——这对文本风险分类模型至关重要。\n\n若选择\"用门控循环单元(GRU)替代LSTM，并在验证损失停止下降时结束训练\"，也非最佳方案。虽然GRU比LSTM计算效率更高且结构更简洁，但单纯更换循环单元类型未必能解决性能问题。当前瓶颈可能并非源于循环单元的种类，而是优化过程本身。\n\n同样地，\"采用基于能源领域新闻预训练的word2vec词嵌入进行初始化\"也存在局限。尽管word2vec能够捕捉词语语义关系，但性能问题可能并非源于词嵌入质量，而是需要通过学习率调整来优化的训练过程。\n\n综上所述，与其他侧重于词表示方法或模型架构等非根本性问题的方案相比，通过调整学习率来促进模型收敛，是提升性能更直接有效的途径。"}, "answer": "C"}, {"id": "77", "question": {"enus": "A Machine Learning Specialist needs to move and transform data in preparation for training. Some of the data needs to be processed in near- real time, and other data can be moved hourly. There are existing Amazon EMR MapReduce jobs to clean and feature engineering to perform on the data. Which of the following services can feed data to the MapReduce jobs? (Choose two.) ", "zhcn": "一位机器学习专家需要迁移和转换数据以准备训练模型。部分数据需近实时处理，其余数据可每小时批量传输。现有Amazon EMR MapReduce任务负责数据清洗与特征工程。下列哪两项服务可为MapReduce任务提供数据源？（请选择两项答案）"}, "option": [{"option_text": {"zhcn": "AWS数据迁移服务", "enus": "AWS DMS"}, "option_flag": true}, {"option_text": {"zhcn": "亚马逊Kinesis", "enus": "Amazon Kinesis"}, "option_flag": false}, {"option_text": {"zhcn": "AWS Data Pipeline", "enus": "AWS Data Pipeline"}, "option_flag": false}, {"option_text": {"zhcn": "Amazon Athena", "enus": "Amazon Athena"}, "option_flag": false}, {"option_text": {"zhcn": "亚马逊西班牙", "enus": "Amazon ES"}, "option_flag": true}], "analysis": {"enus": "The correct answers to the question “A Machine Learning Specialist needs to move and transform data in preparation for training. Some of the data needs to be processed in near - real time, and other data can be moved hourly. There are existing Amazon EMR MapReduce jobs to clean and feature engineering to perform on the data. Which of the following services can feed data to the MapReduce jobs?” are “AWS DMS” and “Amazon ES”.\n\nAWS DMS allows for the migration and replication of data between various sources and targets, which can be used to move data to the Amazon EMR environment for MapReduce jobs. It can handle both real - time and scheduled data transfers, making it suitable for the described data processing requirements. Amazon ES (Elasticsearch Service) can be used to store and index large amounts of data. The data stored in Amazon ES can be accessed and processed by Amazon EMR MapReduce jobs, enabling data cleaning and feature engineering.\n\nOn the other hand, “Amazon Kinesis” is mainly focused on real - time data streaming and analytics, but it doesn't directly feed data to MapReduce jobs; it's more about handling high - velocity data streams. “AWS Data Pipeline” is used for orchestrating data movement and transformation, but it doesn't directly serve as a data source for MapReduce jobs. “Amazon Athena” is an interactive query service that allows you to analyze data stored in Amazon S3 using SQL, and it is not designed to feed data into MapReduce jobs.\n\nCommon misconceptions might lead one to choose “Amazon Kinesis” because of its real - time data handling capabilities, but the question asks about feeding data to MapReduce jobs, not just handling real - time data. “AWS Data Pipeline” and “Amazon Athena” might be chosen due to their data - related functions, but they do not directly fulfill the role of feeding data to MapReduce jobs as AWS DMS and Amazon ES do.", "zhcn": "针对问题“机器学习专家需要为训练准备数据而进行数据迁移与转换。部分数据需近实时处理，其余数据可每小时迁移。现有Amazon EMR MapReduce任务负责数据清洗与特征工程。下列哪项服务可为MapReduce任务提供数据输入？”的正确答案是“AWS DMS”与“Amazon ES”。\n\nAWS DMS支持多源多目标的数据迁移与复制，可将数据输送至Amazon EMR环境供MapReduce任务处理。该服务既能处理实时数据传输，也支持定时迁移，完美契合所述场景的数据处理需求。Amazon ES（Elasticsearch服务）则能存储并索引海量数据，其存储的数据可被Amazon EMR MapReduce任务调用，实现数据清洗与特征工程功能。\n\n反观“Amazon Kinesis”，其核心专注于实时数据流分析，并不直接为MapReduce任务提供数据输入，而是更侧重于高速数据流处理。“AWS Data Pipeline”用于编排数据移动与转换流程，但本身并非MapReduce任务的直接数据源。“Amazon Athena”作为交互式查询服务，支持通过SQL分析Amazon S3中存储的数据，但设计初衷并非向MapReduce任务输送数据。\n\n常见误解可能因实时处理能力而选择“Amazon Kinesis”，但本题核心在于寻找能为MapReduce任务输送数据的服务，而非单纯处理实时数据。虽然“AWS Data Pipeline”和“Amazon Athena”具备数据相关功能，但它们均无法像AWS DMS和Amazon ES那样直接承担向MapReduce任务输送数据的职责。"}, "answer": "AE"}, {"id": "78", "question": {"enus": "A Machine Learning Specialist previously trained a logistic regression model using scikit-learn on a local machine, and the Specialist now wants to deploy it to production for inference only. What steps should be taken to ensure Amazon SageMaker can host a model that was trained locally? ", "zhcn": "此前，一位机器学习专家在本地计算机上使用scikit-learn训练了逻辑回归模型，现计划将其部署至生产环境仅用于推理。为确保亚马逊SageMaker能够托管本地训练的模型，需采取哪些必要步骤？"}, "option": [{"option_text": {"zhcn": "构建包含推理代码的Docker镜像。为镜像标记注册表主机名后，将其上传至亚马逊ECR服务平台。", "enus": "Build the Docker image with the inference code. Tag the Docker image with the registry hostname and upload it to Amazon ECR."}, "option_flag": false}, {"option_text": {"zhcn": "对训练完成的模型进行序列化处理，采用压缩格式以便部署。为Docker镜像标记注册表主机名，并将其上传至Amazon S3存储服务。", "enus": "Serialize the trained model so the format is compressed for deployment. Tag the Docker image with the registry hostname and upload  it to Amazon S3."}, "option_flag": false}, {"option_text": {"zhcn": "将训练好的模型序列化，并压缩格式以便部署。构建镜像并上传至Docker Hub。", "enus": "Serialize the trained model so the format is compressed for deployment. Build the image and upload it to Docker Hub."}, "option_flag": false}, {"option_text": {"zhcn": "构建包含推理代码的Docker镜像。配置Docker Hub并将镜像推送至Amazon ECR。", "enus": "Build the Docker image with the inference code. Configure Docker Hub and upload the image to Amazon ECR."}, "option_flag": true}], "analysis": {"enus": "The correct answer to the question ‘A Machine Learning Specialist previously trained a logistic regression model using scikit - learn on a local machine, and the Specialist now wants to deploy it to production for inference only. What steps should be taken to ensure Amazon SageMaker can host a model that was trained locally?’ is ‘Build the Docker image with the inference code. Configure Docker Hub and upload the image to Amazon ECR.’\n\nTo host a locally - trained model on Amazon SageMaker, we need to package the model and its inference code in a Docker image and make it available in Amazon ECR, which is Amazon's container registry service. Configuring Docker Hub and then uploading the image to Amazon ECR is the correct approach as it ensures that SageMaker can access the containerized model.\n\nThe first fake option, ‘Build the Docker image with the inference code. Tag the Docker image with the registry hostname and upload it to Amazon ECR.’, misses the crucial step of configuring Docker Hub, which is important for proper integration.\n\nThe second fake option, ‘Serialize the trained model so the format is compressed for deployment. Tag the Docker image with the registry hostname and upload it to Amazon S3.’, is incorrect because SageMaker needs the model in a container in ECR, not just serialized and stored in S3.\n\nThe third fake option, ‘Serialize the trained model so the format is compressed for deployment. Build the image and upload it to Docker Hub.’, is wrong as SageMaker doesn't directly use Docker Hub for hosting models; it uses Amazon ECR. This common misconception might lead one to think that just having the model in Docker Hub is sufficient, but SageMaker has its own registry requirements.", "zhcn": "针对问题“机器学习专家此前在本地机器上使用scikit-learn训练了一个逻辑回归模型，现希望将其部署至生产环境仅用于推理。为确保Amazon SageMaker能够托管该本地训练的模型，应采取哪些步骤？”的正确解答是“构建含推理代码的Docker镜像，配置Docker Hub并将镜像上传至Amazon ECR”。\n\n要将本地训练的模型部署至Amazon SageMaker，需将模型及其推理代码打包成Docker镜像，并存放于亚马逊容器注册服务Amazon ECR中。通过配置Docker Hub再将镜像上传至ECR的方案是正确的，这能确保SageMaker成功调用容器化模型。\n\n首项干扰项“构建含推理代码的Docker镜像，为镜像添加注册表主机名标签后上传至Amazon ECR”遗漏了配置Docker Hub这一关键步骤，而该步骤对实现完整集成至关重要。\n\n第二项干扰项“将训练模型序列化并压缩为部署格式，为Docker镜像添加注册表主机名标签后上传至Amazon S3”存在方向性错误。因为SageMaker需要通过ECR中的容器来托管模型，仅将序列化模型存于S3无法满足要求。\n\n第三项干扰项“将训练模型序列化并压缩为部署格式，构建镜像后上传至Docker Hub”同样不正确。SageMaker并不直接使用Docker Hub托管模型，而是依赖自有注册服务Amazon ECR。若误认为Docker Hub即可满足需求，实则忽视了SageMaker的特定注册表要求。"}, "answer": "D"}, {"id": "79", "question": {"enus": "A trucking company is collecting live image data from its fieet of trucks across the globe. The data is growing rapidly and approximately 100 GB of new data is generated every day. The company wants to explore machine learning uses cases while ensuring the data is only accessible to specific IAM users. Which storage option provides the most processing fiexibility and will allow access control with IAM? ", "zhcn": "一家货运公司正从其遍布全球的卡车车队实时采集图像数据。数据量增长迅猛，每日新增约达100 GB。该公司希望在探索机器学习应用场景的同时，确保数据仅限特定IAM用户访问。哪种存储方案既能提供最大处理灵活性，又能实现IAM权限管控？"}, "option": [{"option_text": {"zhcn": "采用数据库（例如Amazon DynamoDB）存储图像，并通过IAM策略设定权限，仅允许指定的IAM用户访问。", "enus": "Use a database, such as Amazon DynamoDB, to store the images, and set the IAM policies to restrict access to only the desired IAM  users."}, "option_flag": false}, {"option_text": {"zhcn": "利用亚马逊S3构建数据湖来存储原始图像，并通过存储桶策略配置访问权限。", "enus": "Use an Amazon S3-backed data lake to store the raw images, and set up the permissions using bucket policies."}, "option_flag": false}, {"option_text": {"zhcn": "配置基于Hadoop分布式文件系统（HDFS）的亚马逊EMR集群用于文件存储，并通过IAM策略限制对EMR实例的访问权限。", "enus": "Setup up Amazon EMR with Hadoop Distributed File System (HDFS) to store the files, and restrict access to the EMR instances using  IAM policies."}, "option_flag": true}, {"option_text": {"zhcn": "配置Amazon EFS时结合IAM策略，可使IAM用户所属的Amazon EC2实例访问相应数据。", "enus": "Configure Amazon EFS with IAM policies to make the data available to Amazon EC2 instances owned by the IAM users."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Set up Amazon EMR with Hadoop Distributed File System (HDFS) to store the files, and restrict access to the EMR instances using IAM policies.” This option offers the most processing flexibility as Amazon EMR is designed for big - data processing, enabling the company to explore various machine - learning use cases on the large and rapidly growing image data. It also allows access control through IAM policies, meeting the company's requirement of restricting data access to specific IAM users.\n\nThe option of using “Amazon DynamoDB” is incorrect. DynamoDB is a NoSQL database, which is not well - suited for storing large amounts of image data. It is more appropriate for structured, smaller - scale data, and may not handle the 100 GB of new image data generated daily efficiently. \n\nUsing an “Amazon S3 - backed data lake” mainly focuses on data storage. While it can set up access control via bucket policies, it doesn't offer the same level of processing flexibility as Amazon EMR for machine - learning exploration. \n\nConfiguring “Amazon EFS with IAM policies” is also not ideal. Amazon EFS is a file system mainly used for shared storage among EC2 instances. It doesn't provide the comprehensive big - data processing capabilities needed for exploring machine - learning use cases on large - scale image data. \n\nIn summary, the processing flexibility and access - control features of Amazon EMR with HDFS make it the best choice over the other fake answer options.", "zhcn": "针对该问题的正确答案是\"采用搭载HDFS的亚马逊EMR服务存储文件，并通过IAM策略限制EMR实例的访问权限\"。这一方案能提供最佳的数据处理灵活性——亚马逊EMR本就是为大数据处理量身打造的工具，足以支撑该公司在海量且快速增长的图像数据上开展各类机器学习探索。同时通过IAM策略实现的访问控制机制，也完美契合了企业限制特定IAM用户数据访问权限的需求。\n\n至于\"采用Amazon DynamoDB\"的方案并不合适。DynamoDB作为NoSQL数据库，并不适合存储大规模图像数据，其更适用于结构化的小规模数据场景，难以高效处理每日新增的100GB图像数据。而\"基于Amazon S3构建数据湖\"的方案主要侧重数据存储功能，虽然可通过存储桶策略设置访问控制，但在机器学习探索方面无法提供与EMR同等级别的处理灵活性。配置\"采用IAM策略的Amazon EFS\"同样不够理想——EFS本质是用于EC2实例间共享存储的文件系统，缺乏处理大规模图像数据所需的完整大数据分析能力。\n\n综上所述，搭载HDFS的亚马逊EMR兼具处理灵活性与访问控制功能，使其在其他干扰项中脱颖而出成为最优解。"}, "answer": "C"}, {"id": "80", "question": {"enus": "A credit card company wants to build a credit scoring model to help predict whether a new credit card applicant will default on a credit card payment. The company has collected data from a large number of sources with thousands of raw attributes. Early experiments to train a classification model revealed that many attributes are highly correlated, the large number of features slows down the training speed significantly, and that there are some overfitting issues. The Data Scientist on this project would like to speed up the model training time without losing a lot of information from the original dataset. Which feature engineering technique should the Data Scientist use to meet the objectives? ", "zhcn": "一家信用卡公司计划构建信用评分模型，用以预测新信用卡申请人是否会出现违约行为。该公司从大量数据源采集了数千个原始属性特征。初步训练分类模型时发现，众多属性间存在高度相关性，海量特征显著拖慢训练速度，并伴随过拟合现象。该项目的数据科学家希望在保留原始数据集大部分信息的前提下加速模型训练。请问应当采用哪种特征工程技术来实现这一目标？"}, "option": [{"option_text": {"zhcn": "对所有特征进行自相关分析，并剔除高度关联的特征。", "enus": "Run self-correlation on all features and remove highly correlated features"}, "option_flag": false}, {"option_text": {"zhcn": "将所有数值归一化至0到1的区间内。", "enus": "Normalize all numerical values to be between 0 and 1"}, "option_flag": true}, {"option_text": {"zhcn": "采用自编码器或主成分分析（PCA）方法，将原始特征替换为经过重构的新特征。", "enus": "Use an autoencoder or principal component analysis (PCA) to replace original features with new features"}, "option_flag": false}, {"option_text": {"zhcn": "采用k-means算法对原始数据进行聚类分析，并从各类簇中抽取样本数据构建新的数据集。", "enus": "Cluster raw data using k-means and use sample data from each cluster to build a new dataset"}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is ‘Normalize all numerical values to be between 0 and 1’. Normalization can improve the training speed of the model by making the optimization process more efficient, as it helps the algorithm converge faster without losing significant information from the original dataset.\n\nThe option ‘Run self - correlation on all features and remove highly correlated features’ can reduce the number of features and address the correlation issue. However, removing features directly may lead to loss of some information, which goes against the requirement of not losing a lot of information.\n\n‘Use an autoencoder or principal component analysis (PCA) to replace original features with new features’ is a dimensionality reduction technique. While it can speed up training, it transforms the original features into new ones, which may result in a loss of interpretability and some information from the original data.\n\n‘Cluster raw data using k - means and use sample data from each cluster to build a new dataset’ can reduce the size of the dataset, but sampling from clusters may lead to a loss of information as not all original data points are used.\n\nIn summary, normalization is the best option as it focuses on improving the training speed without sacrificing much information from the original dataset, which is in line with the project's objectives.", "zhcn": "对于该问题的正确答案是“将所有数值归一化至0到1之间”。归一化处理能提升模型的训练速度，因为优化过程会更高效，有助于算法更快收敛，同时不会丢失原始数据集中的重要信息。  \n\n“对所有特征进行自相关分析并剔除高相关度特征”这一选项可以减少特征数量并解决相关性问题。但直接删除特征可能导致部分信息损失，这与“不丢失大量信息”的要求相悖。  \n\n“使用自编码器或主成分分析（PCA）以新特征替代原始特征”属于降维技术。虽然能加速训练，但将原始特征转换为新特征会降低可解释性，并可能损失原始数据中的部分信息。  \n\n“通过k均值聚类对原始数据分群，并采用各簇样本构建新数据集”能缩减数据集规模，但聚类采样会因未使用全部原始数据点而造成信息损失。  \n\n综上，归一化是最佳选择，它能在不牺牲原始数据信息的前提下有效提升训练速度，符合项目目标。"}, "answer": "B"}, {"id": "81", "question": {"enus": "A Data Scientist is training a multilayer perception (MLP) on a dataset with multiple classes. The target class of interest is unique compared to the other classes within the dataset, but it does not achieve and acceptable recall metric. The Data Scientist has already tried varying the number and size of the MLP's hidden layers, which has not significantly improved the results. A solution to improve recall must be implemented as quickly as possible. Which techniques should be used to meet these requirements? ", "zhcn": "一位数据科学家正在利用包含多个类别的数据集训练多层感知机（MLP）。数据集中目标类别的特征与其他类别存在显著差异，但其召回率指标始终未达到可接受水平。该数据科学家已尝试调整隐藏层的数量和规模，但未能显著改善结果。当前亟需快速落实提升召回率的解决方案。应采用哪些技术手段来满足这一需求？"}, "option": [{"option_text": {"zhcn": "通过亚马逊土耳其机器人平台收集更多数据后重新进行模型训练。", "enus": "Gather more data using Amazon Mechanical Turk and then retrain"}, "option_flag": false}, {"option_text": {"zhcn": "训练一个异常检测模型，而非多层感知机。", "enus": "Train an anomaly detection model instead of an MLP"}, "option_flag": false}, {"option_text": {"zhcn": "采用XGBoost模型进行训练，而非多层感知机。", "enus": "Train an XGBoost model instead of an MLP"}, "option_flag": true}, {"option_text": {"zhcn": "为多层感知机的损失函数引入类别权重，随后重新进行模型训练。", "enus": "Add class weights to the MLP's loss function and then retrain"}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Train an XGBoost model instead of an MLP”. The key issue is that the MLP is not achieving an acceptable recall metric despite attempts to vary hidden - layer parameters, and a quick solution is needed. XGBoost is a powerful and efficient algorithm known for its ability to handle complex classification tasks and often provides better performance than MLPs, especially in scenarios where recall is crucial. It can quickly adapt to the unique characteristics of the target class.\n\n“Gather more data using Amazon Mechanical Turk and then retrain” is a time - consuming process as it involves data collection, cleaning, and re - training, which does not meet the requirement of a quick solution.\n\n“Train an anomaly detection model instead of an MLP” is not appropriate because the problem is about a multi - class classification task where the target class is just unique, not an anomaly. Anomaly detection models are designed to identify rare and abnormal events, not for general multi - class problems.\n\n“Add class weights to the MLP's loss function and then retrain” has already been implicitly attempted by varying the hidden - layer parameters. If changing the structure of the MLP did not improve the recall, adding class weights may not yield significant results quickly. These reasons make “Train an XGBoost model instead of an MLP” the best option to meet the requirements.", "zhcn": "针对该问题，正确答案是选择\"训练XGBoost模型而非MLP模型\"。核心矛盾在于，尽管已尝试调整隐藏层参数，但MLP模型始终无法达到理想的召回率指标，而当前亟需快速有效的解决方案。XGBoost作为一种高效强大的算法，在处理复杂分类任务时表现卓越，尤其在注重召回率的场景下往往优于MLP。该算法能迅速适应目标类别的独特数据特征。\n\n反观其他方案：\"通过亚马逊众包平台采集更多数据后重新训练\"需经历数据收集、清洗及再训练等耗时流程，不符合快速解决的要求；\"改用异常检测模型替代MLP\"则存在根本性错位——本案属于多类别分类任务，目标类别仅具独特性而非异常值，异常检测模型专用于识别罕见异常事件，并不适用于常规多分类场景；\"为MLP损失函数添加类别权重后重新训练\"的方案，其实已通过调整隐藏层参数被间接尝试。若MLP结构调整未能提升召回率，仅添加类别权重恐难快速见效。综上，\"训练XGBoost模型替代MLP\"成为最符合需求的最佳选择。"}, "answer": "C"}, {"id": "82", "question": {"enus": "A Machine Learning Specialist works for a credit card processing company and needs to predict which transactions may be fraudulent in near- real time. Specifically, the Specialist must train a model that returns the probability that a given transaction may fraudulent. How should the Specialist frame this business problem? ", "zhcn": "一名机器学习专家就职于信用卡处理公司，其职责需近乎实时地预测可疑交易。具体而言，该专家需要训练一个能返回单笔交易欺诈概率的预测模型。针对这一业务需求，专家应如何构建问题框架？"}, "option": [{"option_text": {"zhcn": "流式分类", "enus": "Streaming classification"}, "option_flag": false}, {"option_text": {"zhcn": "二分分类", "enus": "Binary classification"}, "option_flag": false}, {"option_text": {"zhcn": "多类别分类", "enus": "Multi-category classification"}, "option_flag": true}, {"option_text": {"zhcn": "回归分类", "enus": "Regression classification"}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question ‘A Machine Learning Specialist works for a credit card processing company and needs to predict which transactions may be fraudulent in near - real time. Specifically, the Specialist must train a model that returns the probability that a given transaction may fraudulent. How should the Specialist frame this business problem?’ is ‘Multi - category classification’. In this scenario, the Specialist wants to obtain the probability of a transaction being fraudulent. Multi - category classification can output probabilities for different classes, which is suitable for getting the probability of a transaction being fraudulent.\n\n‘Streaming classification’ is mainly about handling data in a continuous stream, but the key here is about getting the probability of fraud, not the data streaming aspect. ‘Binary classification’ would simply classify a transaction as either fraudulent or non - fraudulent, not return the probability as required. ‘Regression classification’ is not a standard term; regression is used to predict a continuous value, while this problem is about class probabilities. The ability to output probabilities for different classes is the key factor that makes ‘Multi - category classification’ the correct choice over the other fake options.", "zhcn": "针对\"某信用卡处理公司的机器学习专家需近实时预测欺诈交易，且必须输出单笔交易欺诈概率\"这一业务场景，其正解应为\"多类别分类\"。该场景的核心诉求是获得交易属于欺诈类别的概率值，而多类别分类算法能直接输出不同类别的概率分布，完美契合概率获取需求。\n\n虽然\"流式分类\"适用于持续数据流处理，但本题焦点在于概率输出而非数据流特性；\"二分类\"仅能判断欺诈与否的二元状态，无法满足概率输出要求；至于\"回归分类\"并非标准术语——回归分析用于连续值预测，与本场景的类别概率预测有本质区别。正是这种输出多类别概率的能力，使得\"多类别分类\"从其他干扰项中脱颖而出。\n\n（注：专业术语\"Multi-category classification\"按约定保留英文原表述）"}, "answer": "C"}, {"id": "83", "question": {"enus": "A real estate company wants to create a machine learning model for predicting housing prices based on a historical dataset. The dataset contains 32 features. Which model will meet the business requirement? ", "zhcn": "一家房地产企业计划基于历史数据集构建机器学习模型，用于预测房屋价格。该数据集涵盖32项特征。何种模型能够满足这一商业需求？"}, "option": [{"option_text": {"zhcn": "逻辑回归", "enus": "Logistic regression"}, "option_flag": false}, {"option_text": {"zhcn": "线性回归", "enus": "Linear regression"}, "option_flag": true}, {"option_text": {"zhcn": "K-means算法", "enus": "K-means"}, "option_flag": false}, {"option_text": {"zhcn": "主成分分析（PCA）", "enus": "Principal component analysis (PCA)"}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question ‘A real estate company wants to create a machine learning model for predicting housing prices based on a historical dataset. The dataset contains 32 features. Which model will meet the business requirement?’ is ‘Linear regression’. This is because linear regression is a well - known and suitable model for predicting continuous numerical values, such as housing prices. It can establish a linear relationship between the input features and the output target.\n\n‘Logistic regression’ is mainly used for classification problems where the output is a discrete class label, not for predicting continuous housing prices, so it is not appropriate for this task. ‘K - means’ is a clustering algorithm that groups data points into clusters based on similarity, and it does not serve the purpose of predicting a specific numerical value like housing prices. ‘Principal component analysis (PCA)’ is a dimensionality - reduction technique, not a predictive model for output values. It is used to transform the data into a new set of uncorrelated variables. \n\nThe need to predict continuous housing prices is the key factor that makes linear regression the right choice over the other fake options, which are designed for different types of tasks.", "zhcn": "针对“某房地产公司希望基于历史数据集构建一个机器学习模型来预测房价，该数据集包含32个特征。哪种模型能够满足此业务需求？”这一问题，正确答案是“线性回归”。原因在于线性回归是预测连续数值（如房价）的经典且适宜的模型，它能够在输入特征与输出目标之间建立线性关系。  \n\n而“逻辑回归”主要用于输出为离散类别标签的分类问题，不适用于预测连续型房价，故不适合此任务。“K均值”是一种基于相似性对数据点进行分组的聚类算法，无法实现像房价这类具体数值的预测。“主成分分析（PCA）”属于降维技术而非预测模型，其作用是将数据转换为一组不相关的新变量。  \n\n正是由于预测连续型房价这一核心需求，使得线性回归成为正确选择，其他干扰选项则分别适用于不同类型的任务场景。"}, "answer": "B"}, {"id": "84", "question": {"enus": "A Machine Learning Specialist is applying a linear least squares regression model to a dataset with 1,000 records and 50 features. Prior to training, the ML Specialist notices that two features are perfectly linearly dependent. Why could this be an issue for the linear least squares regression model? ", "zhcn": "一位机器学习专家正在对包含1000条记录和50个特征的数据集应用线性最小二乘回归模型。在训练开始前，该专家发现有两个特征存在完全线性相关关系。这种情况为何会对线性最小二乘回归模型造成影响？"}, "option": [{"option_text": {"zhcn": "这可能导致反向传播算法在训练过程中失效。", "enus": "It could cause the backpropagation algorithm to fail during training"}, "option_flag": false}, {"option_text": {"zhcn": "在优化过程中，该情况可能导致矩阵奇异，从而无法得出唯一解。", "enus": "It could create a singular matrix during optimization, which fails to define a unique solution"}, "option_flag": false}, {"option_text": {"zhcn": "在优化过程中，它可能改变损失函数的结构，从而导致训练环节出现故障。", "enus": "It could modify the loss function during optimization, causing it to fail during training"}, "option_flag": true}, {"option_text": {"zhcn": "这可能导致数据内部产生非线性关联，从而动摇模型所依赖的线性假设基础。", "enus": "It could introduce non-linear dependencies within the data, which could invalidate the linear assumptions of the model"}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is that it could modify the loss function during optimization, causing it to fail during training. In linear least - squares regression, when two features are perfectly linearly dependent, it disrupts the optimization process. The loss function, which is used to measure the error between the predicted and actual values, relies on a well - behaved system of equations. Perfect linear dependence can introduce instability in the loss function, making it difficult for the optimization algorithm to converge.\n\nThe option about the backpropagation algorithm is incorrect because backpropagation is mainly used in neural networks, not in simple linear least - squares regression. The statement about creating a singular matrix is a valid concern in some regression scenarios, but the key issue here is the direct impact on the loss function during optimization. Regarding the non - linear dependencies, the problem states that the features are linearly dependent, so there is no introduction of non - linear relationships. This understanding of the optimization and loss function in linear least - squares regression is what makes the real answer option correct and differentiates it from the fake options.", "zhcn": "问题的正确答案是：在优化过程中，线性依赖关系可能改变损失函数的行为，导致训练失败。在线性最小二乘回归中，当两个特征存在完全线性依赖时，会破坏优化过程的稳定性。损失函数原本用于衡量预测值与实际值的误差，其有效运作依赖于良态的方程组。而完全线性依赖会导致损失函数失稳，使得优化算法难以收敛。\n\n关于反向传播算法的选项并不正确，因为该算法主要应用于神经网络而非简单的线性最小二乘回归。虽然\"产生奇异矩阵\"的说法在某些回归场景中确实值得关注，但此处最核心的问题在于其对优化过程中损失函数的直接影响。至于非线性依赖的表述，由于题目已明确特征间存在线性依赖关系，因此并未引入非线性关联。正是基于对线性最小二乘回归中优化机制与损失函数的深刻理解，才能辨别出正确选项与干扰项之间的本质区别。"}, "answer": "C"}, {"id": "85", "question": {"enus": "Given the following confusion matrix for a movie classification model, what is the true class frequency for Romance and the predicted class frequency for Adventure? ", "zhcn": "根据以下电影分类模型的混淆矩阵，浪漫类别的真实频次与冒险类别的预测频次分别是多少？"}, "option": [{"option_text": {"zhcn": "浪漫题材的真实类别占比为77.56%，而冒险题材的预测类别占比为20.85%。", "enus": "The true class frequency for Romance is 77.56% and the predicted class frequency for Adventure is 20.85%"}, "option_flag": false}, {"option_text": {"zhcn": "浪漫题材的真实类别占比为57.92%，而冒险题材的预测类别占比为13.12%。", "enus": "The true class frequency for Romance is 57.92% and the predicted class frequency for Adventure is 13.12%"}, "option_flag": true}, {"option_text": {"zhcn": "浪漫题材的真实类别占比为0.78，而冒险题材的预测类别占比区间为（0.47-0.32）。", "enus": "The true class frequency for Romance is 0.78 and the predicted class frequency for Adventure is (0.47-0.32)"}, "option_flag": false}, {"option_text": {"zhcn": "浪漫题材的真实类别占比为77.56%±0.78，而冒险题材的预测类别占比为20.85%±0.32。", "enus": "The true class frequency for Romance is 77.56% ֳ— 0.78 and the predicted class frequency for Adventure is 20.85% ֳ— 0.32"}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question ‘Given the following confusion matrix for a movie classification model, what is the true class frequency for Romance and the predicted class frequency for Adventure?’ is ‘The true class frequency for Romance is 57.92% and the predicted class frequency for Adventure is 13.12%’. This is because these values are accurately calculated from the confusion matrix using the correct formulas for true - class and predicted - class frequencies.\n\nThe fake option ‘The true class frequency for Romance is 77.56% and the predicted class frequency for Adventure is 20.85%’ presents incorrect percentages, likely miscalculated values from the confusion matrix. The option ‘The true class frequency for Romance is 0.78 and the predicted class frequency for Adventure is (0.47 - 0.32)’ uses non - percentage values which are not in the correct format as the question likely expects frequencies in percentage terms. The option ‘The true class frequency for Romance is 77.56% ֳ— 0.78 and the predicted class frequency for Adventure is 20.85% ֳ— 0.32’ involves unnecessary multiplications that do not follow the correct method for calculating class frequencies from a confusion matrix. These common errors in calculation and format are the reasons why the fake options are incorrect and the real answer is correct.", "zhcn": "针对问题“根据以下电影分类模型的混淆矩阵，浪漫题材的真实类别频率和冒险题材的预测类别频率分别是多少？”的正确解答是：“浪漫题材的真实类别频率为57.92%，冒险题材的预测类别频率为13.12%”。该结论源于通过标准公式对混淆矩阵的精确计算，真实类别与预测类别频率的推导过程均符合规范。\n\n而干扰项“浪漫题材真实类别频率77.56%，冒险题材预测类别频率20.85%”存在百分比计算错误，很可能源于对混淆矩阵的误算。另一选项“浪漫题材真实类别频率0.78，冒险题材预测类别频率(0.47-0.32)”则错误地使用了非百分比数值，与题目要求频率以百分比形式呈现的预期不符。至于“浪漫题材真实类别频率77.56%×0.78，冒险题材预测类别频率20.85%×0.32”这一选项，其冗余的乘法运算完全违背了从混淆矩阵计算类别频率的基本逻辑。\n\n这些典型错误既暴露了计算层面的疏漏，也反映出对数据格式规范的忽视，由此反证出唯一正确答案的严谨性与准确性。"}, "answer": "B"}, {"id": "86", "question": {"enus": "A Machine Learning Specialist wants to bring a custom algorithm to Amazon SageMaker. The Specialist implements the algorithm in a Docker container supported by Amazon SageMaker. How should the Specialist package the Docker container so that Amazon SageMaker can launch the training correctly? ", "zhcn": "一位机器学习专家希望将自定义算法集成至Amazon SageMaker平台。该专家已采用Amazon SageMaker支持的Docker容器实现算法。为确保Amazon SageMaker能正确启动训练任务，专家应当如何封装该Docker容器？\n\n（注：专有名词\"Amazon SageMaker\"和\"Docker\"保留原表达，采用技术领域通用的\"容器\"而非\"集装箱\"等直译，运用\"集成\"\"实现\"\"封装\"等专业术语保持技术文档的严谨性，同时通过\"确保\"\"启动训练任务\"等动态表述增强操作指引的清晰度。）"}, "option": [{"option_text": {"zhcn": "修改容器中的 bash_profile 文件，并添加用于启动训练程序的 bash 命令。", "enus": "Modify the bash_profile file in the container and add a bash command to start the training program"}, "option_flag": false}, {"option_text": {"zhcn": "在Dockerfile中使用CMD指令，将训练程序设置为镜像的默认启动命令。", "enus": "Use CMD config in the Dockerfile to add the training program as a CMD of the image"}, "option_flag": true}, {"option_text": {"zhcn": "将训练程序配置为名为 train 的入口指令。", "enus": "Configure the training program as an ENTRYPOINT named train"}, "option_flag": false}, {"option_text": {"zhcn": "将训练程序复制至 /opt/ml/train 目录下。", "enus": "Copy the training program to directory /opt/ml/train"}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question ‘A Machine Learning Specialist wants to bring a custom algorithm to Amazon SageMaker. The Specialist implements the algorithm in a Docker container supported by Amazon SageMaker. How should the Specialist package the Docker container so that Amazon SageMaker can launch the training correctly?’ is ‘Use CMD config in the Dockerfile to add the training program as a CMD of the image’. \n\nIn Amazon SageMaker, when using a custom Docker container for training, the `CMD` instruction in the Dockerfile is used to specify the default command that will be executed when the container starts. This is the proper way to set up the training program to run during the training job.\n\nThe option ‘Modify the bash_profile file in the container and add a bash command to start the training program’ is incorrect because modifying the `bash_profile` is not the standard or recommended way for SageMaker to start the training. It may not be executed in the correct context during the training job.\n\n‘Configure the training program as an ENTRYPOINT named train’ is wrong as the `ENTRYPOINT` is typically used for setting up the container's main executable, and while it can be used, `CMD` is more appropriate for specifying the training command in SageMaker.\n\n‘Copy the training program to directory /opt/ml/train’ is also incorrect. Just copying the program to that directory does not ensure that it will be executed. The container needs a proper command to start the training, which is what the `CMD` instruction provides.\n\nThese incorrect options might seem plausible due to general knowledge of Docker and Linux systems, but they do not follow the specific requirements and best - practices of Amazon SageMaker for launching training jobs.", "zhcn": "针对问题“机器学习专家希望将自定义算法引入Amazon SageMaker，并在其支持的Docker容器中实现该算法。专家应如何打包Docker容器以确保Amazon SageMaker能正确启动训练？”的正确答案是：“在Dockerfile中使用CMD配置，将训练程序添加为镜像的CMD指令”。在Amazon SageMaker中，使用自定义Docker容器进行训练时，通过Dockerfile的`CMD`指令来指定容器启动时执行的默认命令，这是确保训练任务正确运行的标准方法。\n\n而“修改容器内的bash_profile文件并添加启动训练程序的bash命令”这一选项并不正确。因为修改`bash_profile`并非SageMaker启动训练的标准或推荐方式，在训练任务执行时可能无法确保命令在正确上下文中运行。\n\n“将训练程序配置为名为train的ENTRYPOINT”同样错误。虽然`ENTRYPOINT`通常用于设置容器的主执行程序，但在SageMaker场景下，使用`CMD`指令指定训练命令更为合适。\n\n“将训练程序复制到/opt/ml/train目录”亦不准确。仅将程序复制到该目录并不能保证其被执行，容器仍需通过`CMD`指令提供正确的启动命令才能运行训练程序。\n\n这些错误选项虽符合对Docker和Linux系统的一般认知，但并未遵循Amazon SageMaker启动训练任务的具体规范与最佳实践。"}, "answer": "B"}, {"id": "87", "question": {"enus": "A Data Scientist needs to analyze employment data. The dataset contains approximately 10 million observations on people across 10 different features. During the preliminary analysis, the Data Scientist notices that income and age distributions are not normal. While income levels shows a right skew as expected, with fewer individuals having a higher income, the age distribution also shows a right skew, with fewer older individuals participating in the workforce. Which feature transformations can the Data Scientist apply to fix the incorrectly skewed data? (Choose two.) ", "zhcn": "数据科学家需对就业数据进行分析。该数据集包含约1000万条人员记录，涉及十个特征变量。初步分析发现收入与年龄的分布形态有违常态：收入水平如预期呈现右偏分布，即高收入群体占比递减；然而年龄分布同样出现右偏，表明劳动力市场中高龄参与者比例异常偏低。为修正这种非常规偏态分布，数据科学家可采用哪两种特征转换方法？（请选择两项）"}, "option": [{"option_text": {"zhcn": "交叉验证", "enus": "Cross-validation"}, "option_flag": true}, {"option_text": {"zhcn": "数值分箱", "enus": "Numerical value binning"}, "option_flag": true}, {"option_text": {"zhcn": "高次多项式变换", "enus": "High-degree polynomial transformation"}, "option_flag": false}, {"option_text": {"zhcn": "对数变换", "enus": "Logarithmic transformation"}, "option_flag": false}, {"option_text": {"zhcn": "独热编码", "enus": "One hot encoding"}, "option_flag": false}], "analysis": {"enus": "The correct answers to the question are ‘Cross - validation’ and ‘Numerical value binning’. Cross - validation can help in evaluating the performance of different models and transformations on the skewed data, ensuring that the model is robust and the transformation is effective. Numerical value binning groups numerical data into bins, which can reduce the impact of extreme values causing the skew and make the distribution more manageable.\n\n‘High - degree polynomial transformation’ is not a good choice for fixing skew as it can overfit the data and make the distribution more complex rather than normalizing it. ‘Logarithmic transformation’ is typically used for right - skewed data where the values are positive and continuous, but the question doesn't clearly indicate that this would be appropriate for both income and age. ‘One hot encoding’ is used to convert categorical variables into a binary matrix and has no direct relation to fixing skewed numerical data. These misconceptions about the purpose and applicability of these methods can lead one to choose the fake options.", "zhcn": "针对该问题的正确答案应为\"交叉验证\"与\"数值分箱法\"。交叉验证有助于评估不同模型及变换方法在偏态数据上的表现，确保模型稳健且数据变换有效。数值分箱法将连续型数值归入不同区间，既能削弱极端值引发的偏态影响，又可使数据分布更易于处理。\n\n\"高次多项式变换\"并非修正偏态的理想选择，因其容易导致过拟合，反而使分布复杂化而非归一化。\"对数变换\"通常适用于正值连续型右偏数据，但题目未明确说明该方法同时适用于收入与年龄这两种变量。\"独热编码\"用于将分类变量转换为二进制矩阵，与修正数值型数据偏态并无直接关联。若未能准确理解这些方法的适用场景，便容易误选干扰项。\n\n（注：专业术语\"Cross-validation\"保留原文，\"Numerical value binning\"采用学界通用译法\"数值分箱法\"，\"One hot encoding\"遵循计算机领域惯用译名\"独热编码\"。）"}, "answer": "AB"}, {"id": "88", "question": {"enus": "A web-based company wants to improve its conversion rate on its landing page. Using a large historical dataset of customer visits, the company has repeatedly trained a multi-class deep learning network algorithm on Amazon SageMaker. However, there is an overfitting problem: training data shows 90% accuracy in predictions, while test data shows 70% accuracy only. The company needs to boost the generalization of its model before deploying it into production to maximize conversions of visits to purchases. Which action is recommended to provide the HIGHEST accuracy model for the company's test and validation data? ", "zhcn": "一家互联网公司希望提升其着陆页的转化率。基于庞大的客户访问历史数据集，该公司已多次通过亚马逊SageMaker平台训练多类别深度学习网络算法。然而目前出现过拟合问题：训练数据的预测准确率高达90%，而测试数据仅显示70%的准确率。在将模型部署到生产环境以最大化访问至购买的转化率之前，该公司需要提升模型的泛化能力。下列哪项措施能为该公司的测试及验证数据提供最高准确率的模型？"}, "option": [{"option_text": {"zhcn": "增强训练所用小批量数据中训练样本的随机性。", "enus": "Increase the randomization of training data in the mini-batches used in training"}, "option_flag": false}, {"option_text": {"zhcn": "将更多数据分配给训练集。", "enus": "Allocate a higher proportion of the overall data to the training dataset"}, "option_flag": false}, {"option_text": {"zhcn": "在训练过程中应用L1或L2正则化方法，并配合使用随机失活技术。", "enus": "Apply L1 or L2 regularization and dropouts to the training"}, "option_flag": false}, {"option_text": {"zhcn": "降低深度学习网络的层数与单元（或神经元）数量。", "enus": "Reduce the number of layers and units (or neurons) from the deep learning network"}, "option_flag": true}], "analysis": {"enus": "The correct answer to the question is ‘Reduce the number of layers and units (or neurons) from the deep learning network’. Overfitting occurs when a model is too complex and captures noise in the training data, leading to poor generalization on new data. By reducing the number of layers and units in the deep - learning network, the model becomes less complex, which helps it focus on the underlying patterns in the data rather than the noise, thus improving generalization.\n\n‘Increasing the randomization of training data in the mini - batches used in training’ mainly affects the training process's stability and convergence speed but does not directly address the root cause of overfitting, which is model complexity. ‘Allocating a higher proportion of the overall data to the training dataset’ may not solve the overfitting issue. If the model is too complex, it will still overfit even with more training data. ‘Applying L1 or L2 regularization and dropouts to the training’ can help with overfitting, but reducing the model's complexity by cutting layers and units is a more fundamental approach for a highly complex multi - class deep learning network. The common misconception for choosing the fake options could be a misunderstanding of the causes of overfitting and the functions of different techniques in addressing it.", "zhcn": "对于\"如何解决深度学习网络过拟合\"这一问题，正确答案是\"减少网络层数和单元数\"。当模型过于复杂时，会捕捉训练数据中的噪声而非本质规律，导致在新数据上表现不佳，这种现象即为过拟合。通过精简网络结构，降低模型复杂度，能使模型更专注于数据的内在模式，从而提升泛化能力。\n\n其他选项分析：增强小批量训练数据的随机性主要影响训练稳定性与收敛速度，但无法根治过拟合的核心问题——模型复杂度过高；增加训练集数据占比未必能解决问题，若模型本身过于复杂，更多数据反而可能加剧过拟合；虽然L1/L2正则化和Dropout技术确实能抑制过拟合，但对于高度复杂的多分类网络而言，从结构层面精简模型才是治本之策。常见误选干扰项的情况，往往源于对过拟合成因及各技术作用机制的误解。"}, "answer": "D"}, {"id": "89", "question": {"enus": "A Machine Learning Specialist is given a structured dataset on the shopping habits of a company's customer base. The dataset contains thousands of columns of data and hundreds of numerical columns for each customer. The Specialist wants to identify whether there are natural groupings for these columns across all customers and visualize the results as quickly as possible. What approach should the Specialist take to accomplish these tasks? ", "zhcn": "一位机器学习专家获得了一份关于公司客户群购物习惯的结构化数据集。该数据集包含数千个数据列，每位客户都有数百个数值型字段。专家需要快速识别这些字段是否在所有客户中存在自然分组，并将分析结果可视化呈现。请问专家应采取何种方法以高效完成这两项任务？"}, "option": [{"option_text": {"zhcn": "对数值特征进行t-SNE降维处理，并绘制散点分布图。", "enus": "Embed the numerical features using the t-distributed stochastic neighbor embedding (t-SNE) algorithm and create a scatter plot."}, "option_flag": false}, {"option_text": {"zhcn": "对不同k值运行基于欧氏距离的k均值算法，并绘制肘部曲线图。", "enus": "Run k-means using the Euclidean distance measure for different values of k and create an elbow plot."}, "option_flag": true}, {"option_text": {"zhcn": "采用t-SNE算法对数值特征进行嵌入处理，并绘制折线图。", "enus": "Embed the numerical features using the t-distributed stochastic neighbor embedding (t-SNE) algorithm and create a line graph."}, "option_flag": false}, {"option_text": {"zhcn": "使用欧几里得距离度量对不同k值运行k-means聚类，并为每个聚类中的数值列绘制箱线图。", "enus": "Run k-means using the Euclidean distance measure for different values of k and create box plots for each numerical column within each  cluster."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is “Run k - means using the Euclidean distance measure for different values of k and create an elbow plot.” The goal is to identify natural groupings (clusters) across all customers and visualize the results quickly. K - means is a well - known clustering algorithm, and the Euclidean distance is a common metric for measuring the similarity between data points in numerical data, which is suitable for this structured numerical dataset. The elbow plot helps in determining the optimal number of clusters (k) by showing the point where the decrease in the within - cluster sum of squares starts to level off.\n\nThe fake answer “Embed the numerical features using the t - distributed stochastic neighbor embedding (t - SNE) algorithm and create a scatter plot” or “Embed the numerical features using the t - distributed stochastic neighbor embedding (t - SNE) algorithm and create a line graph” is incorrect because t - SNE is mainly used for visualizing high - dimensional data in a low - dimensional space (usually 2D or 3D). It doesn't directly identify clusters but rather shows the relative proximity of data points. Also, a line graph is not appropriate for visualizing clusters.\n\nThe other fake answer “Run k - means using the Euclidean distance measure for different values of k and create box plots for each numerical column within each cluster” is wrong because box plots are used to show the distribution of data within a group, not to identify the optimal number of clusters. The key here is to find the natural groupings, and the elbow plot is the best visualization for that purpose.", "zhcn": "对于该问题的正确答案是：\"使用欧氏距离度量对不同k值运行k均值算法，并绘制肘部图。\"此举旨在识别所有顾客的自然分组（聚类），并快速可视化分析结果。k均值是经典的聚类算法，而欧氏距离作为数值型数据中衡量数据点相似度的常用指标，非常适用于此类结构化数值数据集。肘部图通过展示簇内平方和下降趋势趋于平缓的拐点，可帮助确定最佳聚类数量。\n\n而错误答案如\"使用t分布随机邻域嵌入算法对数值特征进行降维，并绘制散点图\"或\"绘制折线图\"之所以不正确，是因为t-SNE主要用于将高维数据可视化到低维空间（通常是二维或三维）。该算法并不直接识别聚类，而是展现数据点之间的相对邻近关系。此外，折线图也非呈现聚类结果的合适可视化方式。\n\n另一错误答案\"对不同k值运行k均值算法后，为每个聚类中的数值列绘制箱线图\"同样不妥。箱线图适用于展示组内数据分布情况，而非确定最佳聚类数量。本问题的核心在于发现自然分组，而肘部图正是实现这一目标的最佳可视化工具。"}, "answer": "B"}, {"id": "90", "question": {"enus": "A Machine Learning Specialist is planning to create a long-running Amazon EMR cluster. The EMR cluster will have 1 master node, 10 core nodes, and 20 task nodes. To save on costs, the Specialist will use Spot Instances in the EMR cluster. Which nodes should the Specialist launch on Spot Instances? ", "zhcn": "一位机器学习专家正计划创建一个长期运行的亚马逊EMR集群。该集群将包含1个主节点、10个核心节点和20个任务节点。为节约成本，这位专家打算在EMR集群中使用竞价实例。请问哪些节点适合采用竞价实例部署？"}, "option": [{"option_text": {"zhcn": "主节点", "enus": "Master node"}, "option_flag": true}, {"option_text": {"zhcn": "核心节点中的任意一个", "enus": "Any of the core nodes"}, "option_flag": false}, {"option_text": {"zhcn": "\"任一任务节点\"", "enus": "Any of the task nodes"}, "option_flag": false}, {"option_text": {"zhcn": "核心节点与任务节点", "enus": "Both core and task nodes"}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question ‘A Machine Learning Specialist is planning to create a long - running Amazon EMR cluster... Which nodes should the Specialist launch on Spot Instances?’ is ‘Master node’. The master node in an EMR cluster manages the cluster and coordinates tasks among other nodes. Spot Instances are spare Amazon EC2 capacity available at a significantly lower cost but can be reclaimed by AWS with short notice.\n\n‘Any of the core nodes’ is incorrect because core nodes store data in Hadoop Distributed File System (HDFS). Losing a core node due to Spot Instance reclamation could lead to data loss and disrupt the cluster's operation. ‘Any of the task nodes’ and ‘Both core and task nodes’ are also wrong. Task nodes are used for running analytics and processing jobs. Losing task nodes during a job due to Spot Instance reclamation may cause the job to fail or require restarting, which is not ideal for a long - running cluster.\n\nThe key factor is that the master node's role is more about management and coordination, and its temporary unavailability may not cause data loss or job failure as severely as losing core or task nodes. This is why the master node is the most suitable to be launched on Spot Instances, distinguishing it from the other options.", "zhcn": "针对问题\"机器学习专家计划创建一个长期运行的亚马逊EMR集群...应当将哪些节点部署在Spot实例上？\"的正确答案是\"主节点\"。EMR集群中的主节点负责集群管理与任务协调，其角色更偏向调度中枢。Spot实例作为亚马逊EC2的闲置计算资源，虽能以显著降低的成本使用，但存在被AWS短暂通知后回收的风险。\n\n\"任意核心节点\"这一选项并不恰当，因为核心节点承担Hadoop分布式文件系统（HDFS）的数据存储职能。若核心节点因Spot实例回收而丢失，可能导致数据遗失并破坏集群运行稳定性。而\"任意任务节点\"及\"同时部署核心节点与任务节点\"的方案同样不可取——任务节点专用于执行分析处理任务，若在作业过程中因实例回收导致节点失效，不仅会造成作业中断，还可能需重新启动作业流程，这对长期运行的集群而言显然不可取。\n\n关键区别在于：主节点主要发挥管理协调功能，其临时不可用对集群造成的影响远小于核心节点或任务节点失效带来的数据丢失或作业中断风险。正是这种功能定位的差异性，使得主节点成为Spot实例部署策略中的最优选择。"}, "answer": "A"}, {"id": "91", "question": {"enus": "A manufacturer of car engines collects data from cars as they are being driven. The data collected includes timestamp, engine temperature, rotations per minute (RPM), and other sensor readings. The company wants to predict when an engine is going to have a problem, so it can notify drivers in advance to get engine maintenance. The engine data is loaded into a data lake for training. Which is the MOST suitable predictive model that can be deployed into production? ", "zhcn": "一家汽车发动机制造商在车辆行驶过程中收集数据，所获数据包括时间戳、发动机温度、每分钟转数（RPM）及其他传感器读数。该公司希望预测发动机可能出现的故障，以便提前通知驾驶员进行维修保养。发动机数据已载入数据湖用于训练，请问最适合投入生产环境的预测模型是哪种？"}, "option": [{"option_text": {"zhcn": "随时间添加标签，以标明未来何时会出现何种发动机故障，从而将问题转化为监督学习任务。利用循环神经网络训练模型，使其能够识别发动机在特定故障发生时可能需要维护的时机。", "enus": "Add labels over time to indicate which engine faults occur at what time in the future to turn this into a supervised learning problem.  Use a recurrent neural network (RNN) to train the model to recognize when an engine might need maintenance for a certain fault."}, "option_flag": false}, {"option_text": {"zhcn": "该数据需采用无监督学习算法进行处理。可利用Amazon SageMaker平台的k-means算法对数据进行聚类分析。", "enus": "This data requires an unsupervised learning algorithm. Use Amazon SageMaker k-means to cluster the data."}, "option_flag": true}, {"option_text": {"zhcn": "随时间添加标签，以标注未来何时会出现何种发动机故障，从而将其转化为监督学习问题。运用卷积神经网络（CNN）训练模型，使其能够识别发动机在特定故障下可能需要维护的时机。", "enus": "Add labels over time to indicate which engine faults occur at what time in the future to turn this into a supervised learning problem.  Use a convolutional neural network (CNN) to train the model to recognize when an engine might need maintenance for a certain fault."}, "option_flag": false}, {"option_text": {"zhcn": "该数据集已按时间序列格式整理，可运用Amazon SageMaker平台的seq2seq算法对时间序列进行建模。", "enus": "This data is already formulated as a time series. Use Amazon SageMaker seq2seq to model the time series."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is “This data requires an unsupervised learning algorithm. Use Amazon SageMaker k - means to cluster the data.” The key factor is that initially, the data from the car engines is unlabeled (there are no pre - defined labels indicating engine problems). Unsupervised learning, like k - means clustering, is ideal for finding patterns and groupings in unlabeled data. It can identify normal and abnormal clusters of engine data, which can then be used to predict when an engine might have a problem.\n\nThe fake answer options involve converting the problem into a supervised learning problem by adding labels over time. This approach is more time - consuming and complex as it requires waiting for engine faults to occur and then labeling the data accordingly. Also, using an RNN or CNN is typically more suitable for sequential data with a specific structure, which may not be necessary here when the main goal is to find general patterns in the unlabeled engine data. The option of using Amazon SageMaker seq2seq for time - series modeling assumes that the problem is best solved by a sequence - to - sequence approach, but there is no clear indication that this is the most appropriate way for this unlabeled engine data. The common misconception leading to choosing the fake options is the assumption that supervised learning is always the best approach, without considering the initial state of the data and the efficiency of unsupervised learning in finding patterns in unlabeled data.", "zhcn": "对于该问题的正确答案是：“这类数据需要采用无监督学习算法，建议使用Amazon SageMaker k-means对数据进行聚类分析。”关键在于，汽车发动机初始数据本身不具备标签（即没有预先定义标识发动机故障的标记）。无监督学习（如k-means聚类）非常适合在无标签数据中发现规律与群组结构，它能有效区分发动机数据的正常与异常集群，从而预测潜在故障。\n\n而干扰选项试图通过后期添加标签将问题转化为监督学习任务。这种方法不仅耗时且复杂——需要等待发动机故障发生后再进行数据标注。此外，RNN或CNN通常适用于具有特定结构的序列数据，而本场景的核心目标是从无标签发动机数据中发现通用模式，无需采用此类复杂模型。至于使用Amazon SageMaker seq2seq进行时间序列建模的方案，其前提假设是该问题最适合用序列到序列的方法解决，但现有信息并未表明这是处理此类无标签发动机数据的最佳途径。\n\n选择干扰选项的常见误区在于：人们往往先入为主地认为监督学习必然最优，却忽略了数据的初始状态以及无监督学习在探索无标签数据内在规律时的高效性。"}, "answer": "B"}, {"id": "92", "question": {"enus": "A company wants to predict the sale prices of houses based on available historical sales data. The target variable in the company's dataset is the sale price. The features include parameters such as the lot size, living area measurements, non-living area measurements, number of bedrooms, number of bathrooms, year built, and postal code. The company wants to use multi-variable linear regression to predict house sale prices. Which step should a machine learning specialist take to remove features that are irrelevant for the analysis and reduce the model's complexity? ", "zhcn": "某公司希望依据现有历史销售数据预测房屋售价，其数据集中的目标变量为售价，特征参数包含地块面积、居住区面积、非居住区面积、卧室数量、卫生间数量、建造年份及邮政编码。该公司拟采用多元线性回归模型进行房价预测。为剔除无关特征并降低模型复杂度，机器学习专家应采取下列哪项步骤？"}, "option": [{"option_text": {"zhcn": "绘制特征分布直方图并计算其标准差。剔除方差过高的特征。", "enus": "Plot a histogram of the features and compute their standard deviation. Remove features with high variance."}, "option_flag": false}, {"option_text": {"zhcn": "绘制特征分布直方图并计算其标准差。剔除方差过低的特征。", "enus": "Plot a histogram of the features and compute their standard deviation. Remove features with low variance."}, "option_flag": false}, {"option_text": {"zhcn": "绘制数据集自身相关性的热力图，剔除互相关分数较低的特征变量。", "enus": "Build a heatmap showing the correlation of the dataset against itself. Remove features with low mutual correlation scores."}, "option_flag": false}, {"option_text": {"zhcn": "对所有特征与目标变量进行相关性检验，剔除与目标变量关联度较低的指标。", "enus": "Run a correlation check of all features against the target variable. Remove features with low target variable correlation scores."}, "option_flag": true}], "analysis": {"enus": "The correct answer to the question about removing irrelevant features for multi - variable linear regression in predicting house sale prices is “Run a correlation check of all features against the target variable. Remove features with low target variable correlation scores.” This is because in a regression problem where the goal is to predict a target variable (house sale price in this case), features that have a low correlation with the target are less likely to contribute meaningfully to the prediction. By removing these features, the model's complexity can be reduced without sacrificing much predictive power.\n\nThe option “Plot a histogram of the features and compute their standard deviation. Remove features with high variance” is incorrect. High variance in a feature does not necessarily mean it is irrelevant to the target variable. A feature can have high variance but still be strongly correlated with the sale price.\n\nThe option “Plot a histogram of the features and compute their standard deviation. Remove features with low variance” is also wrong. Low - variance features might still be related to the target variable and removing them could lead to loss of important information.\n\nThe option “Build a heatmap showing the correlation of the dataset against itself. Remove features with low mutual correlation scores” is off - target. The focus should be on the correlation between features and the target variable, not the correlation between features themselves. The goal is to predict the sale price, so features' relationships with the price are what matter most. This is why the real answer option is the correct choice, effectively differentiating it from the fake options.", "zhcn": "关于多元线性回归预测房价时如何剔除无关特征的问题，正确答案是\"对所有特征与目标变量进行相关性检验，剔除与目标变量相关性较低的特征\"。因为在回归分析中，若目标是预测房价这类目标变量，与目标值相关性弱的特征往往对预测贡献有限。通过剔除这些特征，可以在不影响预测力的情况下简化模型结构。\n\n\"绘制特征直方图并计算标准差，剔除高方差特征\"这一选项并不合理。特征的高方差并不代表其与目标变量无关，某个特征可能波动较大但与房价仍存在强关联。\n\n\"绘制特征直方图并计算标准差，剔除低方差特征\"同样错误。低方差特征仍可能与目标变量相关，盲目剔除可能导致重要信息损失。\n\n\"构建数据集自相关热力图，剔除互相关性较低的特征\"则偏离了核心。问题的关键在于特征与目标变量的相关性，而非特征之间的内部关联。既然最终目标是预测房价，特征与房价的关系才是决策依据。正因如此，正确答案才能从诸多干扰项中凸显其科学性。"}, "answer": "D"}, {"id": "93", "question": {"enus": "A company wants to classify user behavior as either fraudulent or normal. Based on internal research, a machine learning specialist will build a binary classifier based on two features: age of account, denoted by x, and transaction month, denoted by y. The class distributions are illustrated in the provided figure. The positive class is portrayed in red, while the negative class is portrayed in black. Which model would have the HIGHEST accuracy? ", "zhcn": "某企业需对用户行为进行欺诈与非欺诈分类。根据内部研究，机器学习专家将基于账户存续时长（记为x）和交易月份（记为y）这两个特征构建二元分类器。附图展示了类别分布情况：红色代表正类，黑色代表负类。请问哪种模型的准确率会最高？"}, "option": [{"option_text": {"zhcn": "线性支持向量机（SVM）", "enus": "Linear support vector machine (SVM)"}, "option_flag": false}, {"option_text": {"zhcn": "决策树", "enus": "Decision tree"}, "option_flag": false}, {"option_text": {"zhcn": "采用径向基核函数的支持向量机", "enus": "Support vector machine (SVM) with a radial basis function kernel"}, "option_flag": true}, {"option_text": {"zhcn": "带有双曲正切激活函数的单层感知机", "enus": "Single perceptron with a Tanh activation function"}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is ‘Support vector machine (SVM) with a radial basis function kernel’. This is because the class distributions in the figure are likely to be non - linearly separable. A radial basis function (RBF) kernel in an SVM can map the input data into a higher - dimensional space, enabling it to find a non - linear decision boundary that can accurately separate the positive (red) and negative (black) classes.\n\nA ‘Linear support vector machine (SVM)’ can only create a linear decision boundary. If the class distributions are non - linear, a linear SVM will not be able to separate the classes accurately, leading to lower accuracy. A ‘Decision tree’ may overfit the data, especially if the dataset has complex patterns. It partitions the feature space into rectangular regions, which may not be the most efficient way to separate the classes in this case. A ‘Single perceptron with a Tanh activation function’ is also limited to creating a linear decision boundary in the input space, similar to a linear SVM, and thus may not perform well for non - linearly separable data. The ability of the RBF kernel in an SVM to handle non - linearity is the key factor that makes it the real answer option, distinguishing it from the fake options.", "zhcn": "对于图示问题，正确答案应为\"采用径向基核函数的支持向量机（SVM）\"。原因在于图中呈现的类别分布很可能呈非线性可分状态。支持向量机的径向基核函数能够将输入数据映射到高维特征空间，从而构建出可精确区分正类（红色）与负类（黑色）的非线性决策边界。\n\n而\"线性支持向量机\"仅能生成线性决策边界，若面对非线性分布的类别，其分类精度将大打折扣；\"决策树\"虽能通过划分特征空间形成矩形判别区域，但对于此类复杂模式可能产生过拟合问题；\"采用Tanh激活函数的单层感知机\"与线性SVM类似，其本质仍局限于原始空间的线性划分，难以有效处理非线性可分数据。\n\n正是径向基核函数处理非线性问题的独特优势，使其从备选方案中脱颖而出，成为真正符合题意的选择。"}, "answer": "C"}, {"id": "94", "question": {"enus": "A health care company is planning to use neural networks to classify their X-ray images into normal and abnormal classes. The labeled data is divided into a training set of 1,000 images and a test set of 200 images. The initial training of a neural network model with 50 hidden layers yielded 99% accuracy on the training set, but only 55% accuracy on the test set. What changes should the Specialist consider to solve this issue? (Choose three.) ", "zhcn": "一家医疗保健公司计划运用神经网络技术，将其X光图像分类为正常与异常两类。现有标注数据被划分为包含1000张图像的训练集和200张图像的测试集。在采用含50个隐藏层的神经网络进行初步训练后，模型在训练集上准确率达到99%，但在测试集上仅取得55%的准确率。为改善这一状况，专家应考虑采取哪些调整措施？（请选择三项）"}, "option": [{"option_text": {"zhcn": "选择更多层级", "enus": "Choose a higher number of layers"}, "option_flag": true}, {"option_text": {"zhcn": "选择较少的层数。", "enus": "Choose a lower number of layers"}, "option_flag": false}, {"option_text": {"zhcn": "选择较小的学习速率。", "enus": "Choose a smaller learning rate"}, "option_flag": false}, {"option_text": {"zhcn": "启用随机失活", "enus": "Enable dropout"}, "option_flag": true}, {"option_text": {"zhcn": "将测试集中的所有图像纳入训练集。", "enus": "Include all the images from the test set in the training set"}, "option_flag": true}, {"option_text": {"zhcn": "启用提前终止", "enus": "Enable early stopping"}, "option_flag": false}], "analysis": {"enus": "The question pertains to a neural network model for X - ray image classification that has high training accuracy (99%) but low test accuracy (55%), indicating overfitting.\n\nThe real answer options are effective in addressing overfitting. Choosing a higher number of layers can potentially increase the model's capacity to learn complex patterns in the data, allowing it to generalize better. Enabling dropout randomly \"drops out\" some neurons during training, preventing the model from relying too much on specific neurons and thus reducing overfitting. Including all the images from the test set in the training set increases the amount of training data, which generally helps the model learn more diverse patterns and improves generalization.\n\nThe fake answer options are not appropriate. Choosing a lower number of layers might reduce the model's complexity too much, causing underfitting rather than solving overfitting. A smaller learning rate mainly affects the step size during the optimization process and is more related to the convergence speed of the model rather than directly addressing overfitting. Early stopping is used to prevent overfitting by stopping the training when the validation error starts to increase, but the problem here is more about the model's architecture and data usage, so it is not the best solution in this context.", "zhcn": "问题涉及一个用于X光图像分类的神经网络模型，该模型训练准确率高达99%，但测试准确率仅为55%，表明存在过拟合现象。  \n\n有效的解决方案如下：  \n- **增加网络层数**：提升模型容量，使其能学习数据中更复杂的模式，从而增强泛化能力。  \n- **启用Dropout**：在训练过程中随机\"丢弃\"部分神经元，防止模型过度依赖特定神经元，进而减轻过拟合。  \n- **将测试集图像纳入训练集**：增加训练数据量，帮助模型学习更多样化的模式，提升泛化性能。  \n\n不恰当的方案包括：  \n- **减少网络层数**：过度降低模型复杂度可能导致欠拟合，而非解决过拟合。  \n- **减小学习率**：主要影响优化过程中的步长，与模型收敛速度更相关，无法直接应对过拟合。  \n- **早停法**：虽可通过验证误差上升时终止训练来预防过拟合，但当前问题的核心在于模型架构与数据使用，因此并非最优解。"}, "answer": "ADE"}, {"id": "95", "question": {"enus": "This graph shows the training and validation loss against the epochs for a neural network. The network being trained is as follows: ✑ Two dense layers, one output neuron ✑ 100 neurons in each layer ✑ 100 epochs Random initialization of weights Which technique can be used to improve model performance in terms of accuracy in the validation set? ", "zhcn": "本图呈现了神经网络训练过程中训练集与验证集的损失随迭代轮次的变化情况。该网络结构如下：  \n✑ 包含两个全连接层，输出层为单一神经元  \n✑ 每层含100个神经元  \n✑ 进行100轮迭代训练  \n✑ 权重采用随机初始化  \n为提升模型在验证集上的准确率，可采用何种优化策略？"}, "option": [{"option_text": {"zhcn": "“早停法”", "enus": "Early stopping"}, "option_flag": false}, {"option_text": {"zhcn": "权重随机初始化（采用适当种子）", "enus": "Random initialization of weights with appropriate seed"}, "option_flag": false}, {"option_text": {"zhcn": "增加训练轮次", "enus": "Increasing the number of epochs"}, "option_flag": true}, {"option_text": {"zhcn": "在已有结构之上增设包含100个神经元的层级", "enus": "Adding another layer with the 100 neurons"}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is ‘Increasing the number of epochs’. From the graph, we can infer that the model might not have fully converged within the 100 epochs. By increasing the number of epochs, the model has more opportunities to learn from the data, potentially reducing the loss and improving accuracy on the validation set.\n\n‘Early stopping’ is typically used to prevent over - fitting when the validation loss starts to increase. But there's no indication in the question that over - fitting is occurring, so this isn't the best choice for improving accuracy. ‘Random initialization of weights with an appropriate seed’ mainly helps with reproducibility rather than directly improving the model's accuracy. Adding another layer with 100 neurons might make the model more complex, which could lead to over - fitting without necessarily improving validation accuracy. Thus, increasing the number of epochs is the most suitable option to enhance the model's performance in terms of validation set accuracy.", "zhcn": "对于该问题的正确答案是\"增加训练轮数（epochs）\"。从图表中可以推断，模型在100轮训练周期内可能尚未完全收敛。通过增加训练轮数，模型将获得更多从数据中学习的机会，有望降低损失值并提升验证集上的准确率。\n\n\"早停法\"通常用于防止过拟合——当验证集损失开始上升时即停止训练。但题目中并未出现过拟合迹象，因此该方法并非提升准确率的最佳选择。\"采用适当种子的权重随机初始化\"主要有助于结果复现，而非直接提高模型精度。而\"增加含100个神经元的隐藏层\"可能会使模型更为复杂，这可能导致过拟合现象，却未必能提升验证集准确率。\n\n综上所述，若要在验证集准确率方面提升模型性能，增加训练轮数是最适宜的选择。"}, "answer": "C"}, {"id": "96", "question": {"enus": "A Machine Learning Specialist is attempting to build a linear regression model. Given the displayed residual plot only, what is the MOST likely problem with the model? ", "zhcn": "一位机器学习专家正在尝试构建线性回归模型。仅根据所展示的残差图判断，该模型最可能存在的问题是什么？"}, "option": [{"option_text": {"zhcn": "线性回归模型在此处并不适用，因为其残差缺乏恒定的方差。", "enus": "Linear regression is inappropriate. The residuals do not have constant variance."}, "option_flag": false}, {"option_text": {"zhcn": "线性回归模型在此并不适用，因其基础数据中存在异常值。", "enus": "Linear regression is inappropriate. The underlying data has outliers."}, "option_flag": false}, {"option_text": {"zhcn": "线性回归模型适用。残差均值为零。", "enus": "Linear regression is appropriate. The residuals have a zero mean."}, "option_flag": false}, {"option_text": {"zhcn": "线性回归模型适用。残差具有恒定方差。", "enus": "Linear regression is appropriate. The residuals have constant variance."}, "option_flag": true}], "analysis": {"enus": "The correct answer to the question ‘A Machine Learning Specialist is attempting to build a linear regression model. Given the displayed residual plot only, what is the MOST likely problem with the model?’ is ‘Linear regression is appropriate. The residuals have constant variance.’ \n\nOne of the key assumptions for a linear regression model is that the residuals have constant variance (homoscedasticity). When this condition is met, it indicates that the linear regression model is a good fit for the data. \n\nThe fake option ‘Linear regression is inappropriate. The residuals do not have constant variance.’ is incorrect because if the residuals have constant variance as per the real answer, this statement goes against the situation. The option ‘Linear regression is inappropriate. The underlying data has outliers.’ is wrong as there is no indication from the question that the focus is on outliers, and the question asks about the conclusion based on the residual plot related to variance. The option ‘Linear regression is appropriate. The residuals have a zero mean.’ is less relevant as the most crucial aspect for assessing the suitability of linear regression from a residual plot is the constant variance of residuals rather than just having a zero - mean. \n\nA common misconception could be that people might over - emphasize the zero - mean property of residuals or misinterpret the plot to think there is non - constant variance or outliers when there isn't evidence for it, leading them to choose the fake options.", "zhcn": "关于问题“机器学习专家试图构建线性回归模型。仅根据所显示的残差图，该模型最可能存在的问题是什么？”的正确解答是“线性回归模型适用，因为残差具有恒定方差”。线性回归模型的关键前提条件之一正是残差需保持恒定方差（同方差性）。当该条件满足时，表明线性回归模型与数据具有良好适配度。\n\n错误选项“线性回归模型不适用，因为残差不具有恒定方差”并不成立——既然实际答案指出残差具有恒定方差，该陈述便与事实相悖。而选项“线性回归模型不适用，因为基础数据存在异常值”同样不正确：问题中并未提及异常值相关线索，且题目明确要求基于残差图针对方差特性得出结论。\n\n至于选项“线性回归模型适用，因为残差均值为零”则偏离核心——通过残差图评估线性回归适配性时，最关键的判断依据是残差的恒定方差特性，而非单纯的零均值属性。常见的认知误区在于过度强调残差的零均值特性，或误读残差图而臆断存在方差异常或异常值，从而错选干扰项。"}, "answer": "D"}, {"id": "97", "question": {"enus": "A large company has developed a BI application that generates reports and dashboards using data collected from various operational metrics. The company wants to provide executives with an enhanced experience so they can use natural language to get data from the reports. The company wants the executives to be able ask questions using written and spoken interfaces. Which combination of services can be used to build this conversational interface? (Choose three.) ", "zhcn": "某大型企业开发了一套商业智能应用，通过整合多维度运营指标数据生成报表与可视化看板。为提升高管的使用体验，公司计划构建自然语言交互功能，使其能通过书面或语音方式直接查询报表数据。下列哪三种服务组合可用于构建此类对话式交互界面？（请选择三项）"}, "option": [{"option_text": {"zhcn": "“Alexa商务助手”", "enus": "Alexa for Business"}, "option_flag": false}, {"option_text": {"zhcn": "亚马逊云联络中心", "enus": "Amazon Connect"}, "option_flag": true}, {"option_text": {"zhcn": "Amazon Lex", "enus": "Amazon Lex"}, "option_flag": false}, {"option_text": {"zhcn": "亚马逊波利", "enus": "Amazon Polly"}, "option_flag": false}, {"option_text": {"zhcn": "亚马逊理解服务", "enus": "Amazon Comprehend"}, "option_flag": true}, {"option_text": {"zhcn": "亚马逊转录服务", "enus": "Amazon Transcribe"}, "option_flag": true}], "analysis": {"enus": "The correct answer to build the conversational interface for the BI application includes “Amazon Connect”, “Amazon Comprehend”, and “Amazon Transcribe”. \n\n“Amazon Connect” is a cloud - based contact center service that can handle the flow of interactions, enabling executives to use both written and spoken interfaces to access data. “Amazon Comprehend” is used for natural language processing, which can understand the meaning behind the executives' questions, extracting relevant information from the reports. “Amazon Transcribe” can convert spoken questions into text, facilitating the processing of voice - based queries.\n\n“Alexa for Business” is more focused on business - specific skills and integrations for office environments, not designed for building a custom conversational interface for a BI application. “Amazon Lex” is a service for building chatbots, but the requirements here lean more towards a general conversational interface for accessing BI data. “Amazon Polly” is a text - to - speech service, and while it can play a role in output, the question emphasizes getting data through input interfaces, so it's not part of the core combination for building the input - focused conversational interface. The unique capabilities of the real answer options make them suitable for creating the required conversational interface, distinguishing them from the fake options.", "zhcn": "为商业智能应用构建对话式界面的正确答案包含“Amazon Connect”、“Amazon Comprehend”与“Amazon Transcribe”。其中，“Amazon Connect”作为云端联络中心服务，可流畅处理交互流程，让高管能够通过文字与语音两种界面获取数据；“Amazon Comprehend”负责自然语言处理，能解析高管提问的深层含义，从报告中提取关键信息；而“Amazon Transcribe”则可将语音提问转化为文本，为处理基于语音的查询提供支持。至于“Alexa for Business”，其主要聚焦于办公场景的定制技能与集成方案，并非专为构建定制化BI对话界面而设计。“Amazon Lex”虽可用于开发聊天机器人，但本题需求更偏向于访问BI数据的通用对话界面。“Amazon Polly”作为语音合成服务，虽在输出环节有所作用，但问题重点强调通过输入界面获取数据，因此不构成构建以输入为核心的对话界面的关键组合。真正答案的独特功能使其成为构建所需对话界面的理想选择，从而与干扰项形成鲜明区别。"}, "answer": "BEF"}, {"id": "98", "question": {"enus": "A machine learning specialist works for a fruit processing company and needs to build a system that categorizes apples into three types. The specialist has collected a dataset that contains 150 images for each type of apple and applied transfer learning on a neural network that was pretrained on ImageNet with this dataset. The company requires at least 85% accuracy to make use of the model. After an exhaustive grid search, the optimal hyperparameters produced the following: ✑ 68% accuracy on the training set ✑ 67% accuracy on the validation set What can the machine learning specialist do to improve the system's accuracy? ", "zhcn": "一位机器学习专家受聘于一家水果加工企业，需开发一套将苹果分为三个品种的识别系统。该专家已收集每个品种150张图像的数据集，并基于ImageNet预训练的神经网络进行了迁移学习。公司要求模型准确率至少达到85%方可投入实用。经过全面网格搜索后，最优超参数组合在训练集和验证集上的表现如下：  \n✑ 训练集准确率68%  \n✑ 验证集准确率67%  \n请问机器学习专家可采取哪些措施来提升系统准确率？"}, "option": [{"option_text": {"zhcn": "将模型上传至Amazon SageMaker笔记本实例，并运用其超参数优化功能对模型参数进行调优。", "enus": "Upload the model to an Amazon SageMaker notebook instance and use the Amazon SageMaker HPO feature to optimize the model's  hyperparameters."}, "option_flag": false}, {"option_text": {"zhcn": "向训练集补充更多数据，并采用迁移学习方式重新训练模型，以降低偏差度。", "enus": "Add more data to the training set and retrain the model using transfer learning to reduce the bias."}, "option_flag": true}, {"option_text": {"zhcn": "采用在ImageNet上预训练的更深层神经网络模型，并运用迁移学习来提升模型的方差表现。", "enus": "Use a neural network model with more layers that are pretrained on ImageNet and apply transfer learning to increase the variance."}, "option_flag": false}, {"option_text": {"zhcn": "在当前神经网络架构的基础上训练新模型。", "enus": "Train a new model using the current neural network architecture."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Add more data to the training set and retrain the model using transfer learning to reduce the bias.” The model shows low accuracy on both the training and validation sets (68% and 67% respectively), indicating high bias, meaning the model is under - fitting the data. Adding more data to the training set can help the model learn more patterns and features, thus reducing bias and improving accuracy.\n\nThe option “Upload the model to an Amazon SageMaker notebook instance and use the Amazon SageMaker HPO feature to optimize the model's hyperparameters” is incorrect. Since the model has low performance on both training and validation sets, it's not a hyperparameter issue but a problem of under - fitting. Hyperparameter optimization won't address the root cause of high bias.\n\nThe option “Use a neural network model with more layers that are pretrained on ImageNet and apply transfer learning to increase the variance” is wrong. The problem is high bias, not low variance. Increasing the variance would make the model more likely to over - fit, which is not what's needed here.\n\nThe option “Train a new model using the current neural network architecture” is also incorrect. The current architecture is already under - fitting, so simply retraining with the same architecture won't solve the problem. \n\nIn summary, the key issue is high bias, and adding more training data is the appropriate solution to address this, which is why the real answer option is correct and the fake options are not.", "zhcn": "针对该问题的正确答案是：\"向训练集补充更多数据，并运用迁移学习重新训练模型以降低偏差。\"模型在训练集和验证集上的准确率均偏低（分别为68%和67%），表明存在高偏差问题，即模型对数据存在欠拟合现象。通过扩充训练数据集，有助于模型学习更多模式与特征，从而降低偏差并提升准确率。\n\n而\"将模型上传至Amazon SageMaker笔记本实例，利用其超参数优化功能调优模型\"这一方案并不恰当。由于模型在训练集和验证集上均表现不佳，这属于欠拟合问题而非超参数设置不当，仅进行超参数优化无法从根本上解决高偏差问题。\n\n\"采用更多预训练层数的ImageNet神经网络模型，通过迁移学习增加方差\"的方案同样错误。当前核心矛盾在于高偏差而非低方差，盲目增加方差反而可能导致模型过拟合，与实际问题需求背道而驰。\n\n\"沿用现有神经网络架构重新训练新模型\"亦不可取。现有架构已呈现欠拟合状态，仅通过相同架构重复训练无法从根本上改善模型性能。\n\n综上所述，高偏差是本案例的关键症结，扩充训练数据是对症下药的有效策略。这也印证了原正确选项的合理性，同时揭示了其他干扰项的不适用性。"}, "answer": "B"}, {"id": "99", "question": {"enus": "A company uses camera images of the tops of items displayed on store shelves to determine which items were removed and which ones still remain. After several hours of data labeling, the company has a total of 1,000 hand-labeled images covering 10 distinct items. The training results were poor. Which machine learning approach fulfills the company's long-term needs? ", "zhcn": "一家公司通过拍摄货架上商品顶部的图像，来判断哪些商品已被取走、哪些仍留在原处。经过数小时的数据标注，该公司共获得一千张手工标记的图像，涵盖十种不同商品。然而模型训练效果不佳。若要满足该企业的长期需求，应采取哪种机器学习方法？"}, "option": [{"option_text": {"zhcn": "将图像转换为灰度图后重新训练模型。", "enus": "Convert the images to grayscale and retrain the model"}, "option_flag": true}, {"option_text": {"zhcn": "将品类数量从10个精简至2个，建立模型并持续优化迭代。", "enus": "Reduce the number of distinct items from 10 to 2, build the model, and iterate"}, "option_flag": false}, {"option_text": {"zhcn": "为每件物品贴上不同颜色的标签，重新拍摄图像，并构建模型。", "enus": "Attach different colored labels to each item, take the images again, and build the model"}, "option_flag": false}, {"option_text": {"zhcn": "为每个项目运用图像变体（如倒置与平移）来扩充训练数据，继而构建模型并持续优化迭代。", "enus": "Augment training data for each item using image variants like inversions and translations, build the model, and iterate."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is ‘Convert the images to grayscale and retrain the model’. This approach can simplify the image data by removing color information, which might reduce noise and make the model focus on more essential features like shape and texture, potentially improving the model's performance.\n\nThe option ‘Reduce the number of distinct items from 10 to 2, build the model, and iterate’ limits the model's ability to handle the full range of items the company needs to track in the long - term. It doesn't address the root cause of poor training and sacrifices the model's generality.\n\n‘Attach different colored labels to each item, take the images again, and build the model’ is a time - consuming and costly solution. It requires physically modifying the items and retaking all the images, which is not an efficient long - term approach.\n\n‘Augment training data for each item using image variants like inversions and translations, build the model, and iterate’ can increase the quantity of data, but it may not solve the problem if the issue lies in the complexity of the original data. The real answer simplifies the data, making it more manageable for the model, which is a better long - term solution compared to the fake options.", "zhcn": "针对该问题的正确答案是\"将图像转换为灰度图并重新训练模型\"。这种方法通过去除色彩信息简化图像数据，既能降低噪声干扰，又可让模型更专注于形状纹理等本质特征，从而有望提升模型性能。  \n\n而\"将不同类别数量从10减至2，建立模型并迭代\"这一方案，会限制模型长期处理公司需追踪全品类物品的能力。它既未解决训练效果不佳的根本问题，又牺牲了模型的泛化性。  \n\n\"为每件物品粘贴不同颜色标签重新拍摄图像再建模\"的做法耗时耗资，需要实物改造和全量重拍，并非可持续的优化方案。  \n\n\"通过图像反转平移等变换增强训练数据再建模迭代\"虽能扩充数据量，但若问题根源在于原始数据本身过于复杂，此法可能治标不治本。相较这些替代方案，正确答案通过简化数据本质，为模型提供了更具可持续性的优化路径。"}, "answer": "A"}, {"id": "100", "question": {"enus": "A Data Scientist is developing a binary classifier to predict whether a patient has a particular disease on a series of test results. The Data Scientist has data on 400 patients randomly selected from the population. The disease is seen in 3% of the population. Which cross-validation strategy should the Data Scientist adopt? ", "zhcn": "一位数据科学家正在开发一个二元分类器，旨在根据系列检测结果预测患者是否罹患某种特定疾病。该科学家从总体人群中随机抽取了400名患者的数据作为研究样本。已知此疾病在人群中的患病率为3%。此时，数据科学家应当采用何种交叉验证策略？"}, "option": [{"option_text": {"zhcn": "采用五折交叉验证法。", "enus": "A k-fold cross-validation strategy with k=5"}, "option_flag": false}, {"option_text": {"zhcn": "采用分层K折交叉验证法，设定折数K=5。", "enus": "A stratified k-fold cross-validation strategy with k=5"}, "option_flag": true}, {"option_text": {"zhcn": "采用五折交叉验证法，重复三次实验验证。", "enus": "A k-fold cross-validation strategy with k=5 and 3 repeats"}, "option_flag": false}, {"option_text": {"zhcn": "训练集与验证集按80/20的比例分层划分。", "enus": "An 80/20 stratified split between training and validation"}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question “A Data Scientist is developing a binary classifier to predict whether a patient has a particular disease on a series of test results. The Data Scientist has data on 400 patients randomly selected from the population. The disease is seen in 3% of the population. Which cross-validation strategy should the Data Scientist adopt?” is “A stratified k-fold cross-validation strategy with k = 5”. \n\nThe key factor here is the imbalanced nature of the data, as only 3% of the population has the disease. Stratified sampling ensures that each fold in the cross - validation process has a similar proportion of the minority class (patients with the disease) as the original dataset. This helps in getting a more reliable estimate of the model's performance, especially for imbalanced datasets.\n\nThe “A k - fold cross - validation strategy with k = 5” option does not account for the class imbalance. Regular k - fold cross - validation may create folds where the minority class is under - represented or even absent, leading to over - optimistic or inaccurate performance estimates.\n\nThe “A k - fold cross - validation strategy with k = 5 and 3 repeats” also suffers from the same issue of not addressing the class imbalance, even with multiple repeats.\n\nThe “An 80/20 stratified split between training and validation” is not as comprehensive as k - fold cross - validation. K - fold cross - validation uses the entire dataset for both training and validation multiple times, providing a more robust evaluation compared to a single 80/20 split. \n\nThe common pitfall is overlooking the class imbalance and choosing a non - stratified cross - validation method, which can lead to poor generalization of the model and inaccurate performance assessment.", "zhcn": "针对\"数据科学家正在开发一个用于根据系列检测结果预测患者是否患有特定疾病的二分类器。该科学家从人群中随机抽取400名患者数据，已知该疾病在人群中的患病率为3%。应当采用哪种交叉验证策略？\"这一问题，正确答案是\"采用k=5的分层k折交叉验证策略\"。  \n\n此处的关键因素在于数据存在不平衡性——仅有3%的人群患病。分层抽样能确保交叉验证的每个折次都保持与原始数据集相似的少数类（患病患者）比例。这种策略能够更可靠地评估模型性能，尤其适用于不平衡数据集。  \n\n若选择\"k=5的普通k折交叉验证\"，则无法处理类别不平衡问题。常规k折划分可能导致某些折次中少数类样本过少甚至缺失，从而使性能评估产生过于乐观或失真的结果。  \n\n而\"k=5且重复3次的k折交叉验证\"虽进行多次划分，但依然未解决类别不平衡的核心问题。  \n\n至于\"采用80/20分层划分的训练-验证集方法\"，其评估全面性不及k折交叉验证。k折策略能多次利用全部数据进行训练与验证，相比单次划分能提供更稳健的模型评估。  \n\n需要警惕的常见误区是忽视类别不平衡而选择非分层验证方法，这可能导致模型泛化能力差及性能评估失准。"}, "answer": "B"}, {"id": "101", "question": {"enus": "A technology startup is using complex deep neural networks and GPU compute to recommend the company's products to its existing customers based upon each customer's habits and interactions. The solution currently pulls each dataset from an Amazon S3 bucket before loading the data into a TensorFlow model pulled from the company's Git repository that runs locally. This job then runs for several hours while continually outputting its progress to the same S3 bucket. The job can be paused, restarted, and continued at any time in the event of a failure, and is run from a central queue. Senior managers are concerned about the complexity of the solution's resource management and the costs involved in repeating the process regularly. They ask for the workload to be automated so it runs once a week, starting Monday and completing by the close of business Friday. Which architecture should be used to scale the solution at the lowest cost? ", "zhcn": "一家科技初创企业正运用复杂的深度神经网络与GPU算力，根据每位客户的习惯和交互记录为其推荐公司产品。当前解决方案会先从亚马逊S3存储桶提取数据集，再将数据载入从公司Git代码库获取的TensorFlow模型进行本地运算。该任务持续运行数小时，并实时将进度同步输出至同一S3存储桶。借助中央队列调度，该任务支持在发生故障时随时暂停、重启或续传。高层管理者担忧现有解决方案的资源管理复杂度及定期运行产生的成本，要求将工作流自动化调整为每周执行一次：周一启动，周五下班前完成。应采用何种架构方案，才能以最低成本实现该解决方案的弹性扩展？"}, "option": [{"option_text": {"zhcn": "利用AWS深度学习容器部署解决方案，并通过AWS Batch在支持GPU的竞价实例上以任务形式运行容器。", "enus": "Implement the solution using AWS Deep Learning Containers and run the container as a job using AWS Batch on a GPU-compatible Spot  Instance"}, "option_flag": false}, {"option_text": {"zhcn": "采用低成本且支持GPU运算的亚马逊EC2实例来部署解决方案，并通过AWS实例调度器对任务执行时间进行自动化编排。", "enus": "Implement the solution using a low-cost GPU-compatible Amazon EC2 instance and use the AWS Instance Scheduler to schedule the  task"}, "option_flag": false}, {"option_text": {"zhcn": "采用AWS深度学习容器部署解决方案，通过运行在Spot实例上的AWS Fargate执行计算任务，并利用内置任务调度器实现作业的自动化编排。", "enus": "Implement the solution using AWS Deep Learning Containers, run the workload using AWS Fargate running on Spot Instances, and then  schedule the task using the built-in task scheduler"}, "option_flag": true}, {"option_text": {"zhcn": "采用基于竞价型实例的亚马逊ECS实施该解决方案，并通过ECS服务调度器安排任务执行。", "enus": "Implement the solution using Amazon ECS running on Spot Instances and schedule the task using the ECS service scheduler"}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Implement the solution using AWS Deep Learning Containers, run the workload using AWS Fargate running on Spot Instances, and then schedule the task using the built - in task scheduler”. \n\nAWS Fargate running on Spot Instances is a cost - effective option. Spot Instances offer spare Amazon EC2 computing capacity at a significantly lower price. Fargate is a serverless compute engine for containers, which eliminates the need to manage servers or clusters, reducing the complexity of resource management. The built - in task scheduler allows for easy scheduling of the workload to run once a week as required.\n\nThe option of using AWS Batch on a GPU - compatible Spot Instance is less optimal. Although Spot Instances are cost - effective, AWS Batch is more suitable for batch processing jobs with complex scheduling and resource management requirements, adding unnecessary complexity.\n\nUsing a low - cost GPU - compatible Amazon EC2 instance and the AWS Instance Scheduler has drawbacks. Managing an EC2 instance directly requires more administrative effort for resource management, and it may not be as cost - effective as using serverless options like Fargate.\n\nImplementing the solution using Amazon ECS running on Spot Instances and the ECS service scheduler also has issues. ECS still requires some level of cluster management, which adds to the complexity compared to the serverless nature of Fargate. \n\nIn summary, the real answer combines cost - effectiveness through Spot Instances and simplicity in resource management with Fargate and a built - in scheduler, making it the best choice to scale the solution at the lowest cost.", "zhcn": "针对该问题，正确答案是\"采用AWS深度学习容器部署解决方案，通过基于Spot实例的AWS Fargate运行工作负载，并利用内置任务调度器进行任务编排\"。基于Spot实例的AWS Fargate具有显著的成本效益优势——Spot实例能提供大幅折扣的亚马逊EC2闲置计算容量，而Fargate作为无服务器容器引擎，无需管理服务器或集群，有效降低了资源管理复杂度。内置任务调度器则可轻松实现按周执行的定时任务需求。\n\n相比之下，在GPU兼容型Spot实例上采用AWS Batch的方案并非最优解。尽管Spot实例具备价格优势，但AWS Batch更适用于需要复杂调度和资源管理的批处理作业，会引入不必要的复杂性。\n\n若选用低成本GPU兼容型亚马逊EC2实例配合AWS实例调度器，则存在明显缺陷：直接管理EC2实例将增加运维负担，且成本效益往往不及Fargate等无服务器方案。\n\n至于基于Spot实例的亚马逊ECS配合服务调度器的方案，由于ECS仍需进行集群管理，其运维复杂度仍高于完全无需管理基础设施的Fargate方案。\n\n综上，正确答案通过Spot实例实现成本优化，结合Fargate的无服务器特性及内置调度器简化管理，最终以最低成本实现了解决方案的可扩展部署。"}, "answer": "C"}, {"id": "102", "question": {"enus": "A Machine Learning Specialist prepared the following graph displaying the results of k-means for k = [1..10]: Considering the graph, what is a reasonable selection for the optimal choice of k? ", "zhcn": "一位机器学习专家绘制了以下图表，展示了k值从1到10的k均值聚类结果：根据图表所示，对于k的最佳选择，怎样的取值较为合理？"}, "option": [{"option_text": {"zhcn": "一", "enus": "1"}, "option_flag": false}, {"option_text": {"zhcn": "四", "enus": "4"}, "option_flag": false}, {"option_text": {"zhcn": "七\n\n（注：根据用户要求，采用中文数词最简洁典雅的表达形式，避免添加任何解释性内容。若需其他文体风格的翻译版本，可进一步说明具体需求。）", "enus": "7"}, "option_flag": true}, {"option_text": {"zhcn": "十", "enus": "10"}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question ‘Considering the graph, what is a reasonable selection for the optimal choice of k?’ is ‘7’. In k - means clustering, the optimal value of k is often determined using the elbow method. This method looks for a point in the graph where the decrease in the within - cluster sum of squares (WCSS) starts to level off, creating an 'elbow' shape.\n\nThe value ‘1’ is incorrect because having only one cluster defeats the purpose of clustering, as clustering aims to partition data into multiple distinct groups. The value ‘4’ is likely before the elbow point in the graph, meaning that increasing k further would still lead to a significant reduction in WCSS, indicating that the data can be better partitioned. The value ‘10’ is likely after the elbow point. Selecting a very large k can lead to over - clustering, where each data point may almost form its own cluster, and the clusters may not have any real - world meaning. \n\nThe value ‘7’ is a reasonable choice as it is likely at or near the elbow point, where the balance between reducing WCSS and having a meaningful number of clusters is achieved. This balance is the key factor that makes ‘7’ the real answer option, distinguishing it from the fake options.", "zhcn": "对于\"根据图示，如何合理选择k均值聚类中的最佳k值？\"这一问题，正确答案是\"7\"。在k均值聚类算法中，通常采用肘部法则确定最佳k值——该方法通过寻找聚类平方和（WCSS）下降曲线中的拐点（即形成\"肘部\"形状的位置）来判断。\n\n选项\"1\"显然错误：若仅设置一个聚类，则违背了通过聚类实现数据分组的根本目的。选项\"4\"对应的k值可能位于拐点之前，此时增加k值仍能显著降低WCSS，说明数据尚有更优划分空间。选项\"10\"对应的k值可能处于拐点之后，过大的k值会导致过度聚类，使得每个数据点几乎自成一体，失去实际分类意义。\n\n而选项\"7\"之所以合理，是因它恰好处于或接近拐点位置——既有效控制了聚类平方和，又保持了具有实际意义的聚类数量。这种精妙的平衡使得\"7\"从备选答案中脱颖而出，成为唯一正确的选择。"}, "answer": "C"}, {"id": "103", "question": {"enus": "A media company with a very large archive of unlabeled images, text, audio, and video footage wishes to index its assets to allow rapid identification of relevant content by the Research team. The company wants to use machine learning to accelerate the efforts of its in-house researchers who have limited machine learning expertise. Which is the FASTEST route to index the assets? ", "zhcn": "一家拥有海量未标注图像、文本、音频及视频素材的传媒公司，希望为其资产建立索引系统，以便研究团队快速识别相关内容。鉴于内部研究人员机器学习专业知识有限，该公司计划借助机器学习技术提升效率。请问实现资产索引的最快捷途径是什么？"}, "option": [{"option_text": {"zhcn": "借助Amazon Rekognition、Amazon Comprehend与Amazon Transcribe，可将数据自动归类至不同类别。", "enus": "Use Amazon Rekognition, Amazon Comprehend, and Amazon Transcribe to tag data into distinct categories/classes."}, "option_flag": true}, {"option_text": {"zhcn": "创建一套亚马逊土耳其机器人（Amazon Mechanical Turk）的人工智能标注任务，用于标记所有影像资料。", "enus": "Create a set of Amazon Mechanical Turk Human Intelligence Tasks to label all footage."}, "option_flag": false}, {"option_text": {"zhcn": "借助Amazon Transcribe实现语音到文本的转换，并运用Amazon SageMaker的神经主题模型与目标检测算法，将数据精准归类至不同类别。", "enus": "Use Amazon Transcribe to convert speech to text. Use the Amazon SageMaker Neural Topic Model (NTM) and Object Detection  algorithms to tag data into distinct categories/classes."}, "option_flag": false}, {"option_text": {"zhcn": "借助AWS深度学习AMI与Amazon EC2 GPU实例，可构建定制化模型以实现音频转录与主题建模，同时通过目标检测技术将数据标注至不同类别体系。", "enus": "Use the AWS Deep Learning AMI and Amazon EC2 GPU instances to create custom models for audio transcription and topic modeling,  and use object detection to tag data into distinct categories/classes."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question ‘A media company with a very large archive of unlabeled images, text, audio, and video footage wishes to index its assets to allow rapid identification of relevant content by the Research team. The company wants to use machine learning to accelerate the efforts of its in - house researchers who have limited machine learning expertise. Which is the FASTEST route to index the assets?’ is ‘Use Amazon Rekognition, Amazon Comprehend, and Amazon Transcribe to tag data into distinct categories/classes.’ \n\nThese AWS services are pre - trained and ready - to - use. Amazon Rekognition can handle image and video analysis, Amazon Comprehend is for text analysis, and Amazon Transcribe is for audio transcription. They can quickly tag the diverse media assets without the need for extensive custom model development.\n\nThe option ‘Create a set of Amazon Mechanical Turk Human Intelligence Tasks to label all footage’ is slow because it relies on human labor, which is time - consuming, especially for a large archive. \n\nThe option ‘Use Amazon Transcribe to convert speech to text. Use the Amazon SageMaker Neural Topic Model (NTM) and Object Detection algorithms to tag data into distinct categories/classes’ requires more setup and training of models in SageMaker, which takes time and demands more machine - learning expertise. \n\nThe option ‘Use the AWS Deep Learning AMI and Amazon EC2 GPU instances to create custom models for audio transcription and topic modeling, and use object detection to tag data into distinct categories/classes’ is the most time - consuming. It involves creating custom models from scratch, which requires significant machine - learning knowledge, time for model development, training, and tuning. \n\nThe pre - trained nature of Amazon Rekognition, Amazon Comprehend, and Amazon Transcribe makes them the fastest option for the company with limited machine - learning expertise, distinguishing this real answer from the fake options.", "zhcn": "对于问题\"一家拥有大量未标注图片、文本、音频及视频素材的传媒公司，希望建立资产索引以便研究团队快速识别相关内容。该公司拟利用机器学习加速内部研究员的检索效率，但这些研究员缺乏机器学习专业背景。下列哪种方案能最快速建立资产索引？\"的正确解答是：\"使用Amazon Rekognition、Amazon Comprehend和Amazon Transcribe将数据标注至不同类别\"。这些AWS服务均经过预训练可即开即用——Amazon Rekognition负责图像视频分析，Amazon Comprehend处理文本分析，Amazon Transcribe专攻音频转写。它们能快速对多元媒体资产进行标注，无需大量定制化模型开发。\n\n而\"通过Amazon Mechanical Turk创建人工标注任务\"的方案因依赖人力标注效率低下，尤其不适用于海量素材库；\"采用Amazon Transcribe进行语音转写，结合Amazon SageMaker神经主题模型与目标检测算法进行数据分类\"的方案需在SageMaker中进行更多模型配置与训练，耗时且要求更高机器学习技能；\"使用AWS深度学习AMI与Amazon EC2 GPU实例构建定制化音频转写与主题模型，并通过目标检测实现数据分类\"则最为耗时，需从零构建定制模型，要求具备扎实的机器学习知识及模型开发训练调优能力。\n\n正是基于预训练服务的即用性优势，对于缺乏机器学习专业能力的团队而言，采用Amazon三项智能服务成为最迅捷的解决方案，这也是正确答案与其他选项的本质区别。"}, "answer": "A"}, {"id": "104", "question": {"enus": "A Machine Learning Specialist is working for an online retailer that wants to run analytics on every customer visit, processed through a machine learning pipeline. The data needs to be ingested by Amazon Kinesis Data Streams at up to 100 transactions per second, and the JSON data blob is 100 KB in size. What is the MINIMUM number of shards in Kinesis Data Streams the Specialist should use to successfully ingest this data? ", "zhcn": "一位机器学习专家正为某线上零售商服务，该企业希望对每次客户访问进行数据分析，并通过机器学习流水线处理数据。数据需经由亚马逊Kinesis数据流接收，处理速率需达每秒100笔交易，且每份JSON数据块大小为100KB。请问该专家应至少配置多少个Kinesis数据流分片，方能确保数据成功接收？"}, "option": [{"option_text": {"zhcn": "一瓣残片", "enus": "1 shards"}, "option_flag": false}, {"option_text": {"zhcn": "十枚碎片", "enus": "10 shards"}, "option_flag": true}, {"option_text": {"zhcn": "百枚碎片", "enus": "100 shards"}, "option_flag": false}, {"option_text": {"zhcn": "千枚碎片", "enus": "1,000 shards"}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question ‘A Machine Learning Specialist is working for an online retailer that wants to run analytics on every customer visit, processed through a machine learning pipeline. The data needs to be ingested by Amazon Kinesis Data Streams at up to 100 transactions per second, and the JSON data blob is 100 KB in size. What is the MINIMUM number of shards in Kinesis Data Streams the Specialist should use to successfully ingest this data?’ is ‘10 shards’. \n\nEach shard in Kinesis Data Streams can handle a maximum write throughput of 1 MB per second or 1,000 records per second. Given that each transaction is 100 KB and there are 100 transactions per second, the total data ingestion rate is 100 transactions * 100 KB = 10,000 KB or 10 MB per second. To find the number of shards, we divide the total ingestion rate by the per - shard capacity (10 MB / 1 MB = 10 shards).\n\nThe option ‘1 shard’ is incorrect because a single shard can only handle 1 MB per second, which is not sufficient for the 10 MB per second ingestion rate. The options ‘100 shards’ and ‘1,000 shards’ are over - estimations. Using more shards than necessary would increase costs without adding any real benefit as the minimum number of shards required to handle the ingestion rate is 10.", "zhcn": "针对问题“某在线零售商聘请一位机器学习专家，希望通过机器学习流水线分析每位顾客的访问数据。数据需通过亚马逊Kinesis数据流进行采集，要求每秒处理100笔交易，每笔JSON数据大小为100 KB。为成功完成数据采集，该专家至少应配置多少个Kinesis数据流分片？”的正确答案是“10个分片”。Kinesis数据流的每个分片最高支持每秒1MB的数据写入吞吐量或每秒1000条记录。已知单笔交易数据为100KB且每秒需处理100笔交易，总数据写入速率为100笔/秒 × 100KB = 10,000KB即10MB/秒。计算分片数量时，将总数据速率除以单分片处理能力（10MB ÷ 1MB = 10个分片）即可得出结果。  \n  \n选项“1个分片”错误，因为单分片仅能处理1MB/秒的数据量，无法满足10MB/秒的写入需求。而“100个分片”和“1000个分片”属于过度配置——超出必要数量的分片会增加成本却无法提升实效，毕竟处理该数据速率所需的最小分片数仅为10个。"}, "answer": "B"}, {"id": "105", "question": {"enus": "A Machine Learning Specialist is deciding between building a naive Bayesian model or a full Bayesian network for a classification problem. The Specialist computes the Pearson correlation coeficients between each feature and finds that their absolute values range between 0.1 to 0.95. Which model describes the underlying data in this situation? ", "zhcn": "一位机器学习专家在解决分类问题时，需在朴素贝叶斯模型与完整贝叶斯网络之间作出选择。该专家计算出各特征间的皮尔逊相关系数，发现其绝对值分布于0.1至0.95区间。此种情境下，何种模型能更准确地表征底层数据特征？"}, "option": [{"option_text": {"zhcn": "在特征均为条件独立的前提下，可采用朴素贝叶斯模型进行建模。", "enus": "A naive Bayesian model, since the features are all conditionally independent."}, "option_flag": false}, {"option_text": {"zhcn": "由于各特征之间均为条件独立，因此该网络构成完整的贝叶斯网络。", "enus": "A full Bayesian network, since the features are all conditionally independent."}, "option_flag": false}, {"option_text": {"zhcn": "由于某些特征在统计上存在关联性，朴素贝叶斯模型的适用性因此受到限制。", "enus": "A naive Bayesian model, since some of the features are statistically dependent."}, "option_flag": true}, {"option_text": {"zhcn": "由于部分特征在统计上存在依赖性，因此需要构建完整的贝叶斯网络。", "enus": "A full Bayesian network, since some of the features are statistically dependent."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is ‘A naive Bayesian model, since some of the features are statistically dependent.’ The Pearson correlation coefficients with absolute values ranging from 0.1 to 0.95 indicate that there is some level of statistical dependence among the features.\n\nA naive Bayesian model can still be used even when features are not completely independent. It makes the simplifying assumption of feature independence for computational efficiency, and can often perform well in practice despite some dependence.\n\nThe option ‘A naive Bayesian model, since the features are all conditionally independent.’ is incorrect because the correlation coefficients show that the features are not all conditionally independent. The option ‘A full Bayesian network, since the features are all conditionally independent.’ is wrong as the data shows dependence, not independence. The option ‘A full Bayesian network, since some of the features are statistically dependent.’ is not the best choice because a full Bayesian network is more complex and computationally expensive, and a naive Bayesian model can often be sufficient when there is some dependence. So, the key factor distinguishing the real answer is the ability of the naive Bayesian model to handle some statistical dependence among features, while the fake options either misinterpret the data's dependence or over - complicate the model selection.", "zhcn": "对于该问题的正确答案是“朴素贝叶斯模型，因为部分特征存在统计依赖性”。皮尔逊相关系数绝对值介于0.1至0.95之间，表明特征间存在一定程度的统计关联。  \n\n尽管特征并非完全独立，朴素贝叶斯模型仍可适用。该模型为提升计算效率而采用特征独立性的简化假设，即使在存在依赖关系的情况下，实践中往往仍能表现良好。  \n\n而选项“朴素贝叶斯模型，因为所有特征均条件独立”并不正确——相关系数表明特征并非全部条件独立。选项“完全贝叶斯网络，因为所有特征条件独立”同样错误，数据实际显示的是相关性而非独立性。至于“完全贝叶斯网络，因为部分特征存在统计依赖”也非最优选择，因为完全贝叶斯网络结构更复杂、计算成本更高，在存在部分依赖时，朴素贝叶斯模型往往已足够适用。  \n\n因此，正确答案的核心判别依据在于：朴素贝叶斯模型能够处理特征间的统计依赖性，而错误选项要么误读了数据的依赖关系，要么过度复杂化了模型选择。"}, "answer": "C"}, {"id": "106", "question": {"enus": "A Data Scientist is building a linear regression model and will use resulting p-values to evaluate the statistical significance of each coeficient. Upon inspection of the dataset, the Data Scientist discovers that most of the features are normally distributed. The plot of one feature in the dataset is shown in the graphic. What transformation should the Data Scientist apply to satisfy the statistical assumptions of the linear regression model? ", "zhcn": "一位数据科学家正在构建线性回归模型，计划利用得出的p值来评估各个系数的统计显著性。在检查数据集时，这位科学家发现大部分特征呈正态分布。图表展示了数据集中某个特征的分布情况。为满足线性回归模型的统计假设，该数据科学家应当对数据施加何种变换？"}, "option": [{"option_text": {"zhcn": "指数级蜕变", "enus": "Exponential transformation"}, "option_flag": true}, {"option_text": {"zhcn": "对数变换", "enus": "Logarithmic transformation"}, "option_flag": false}, {"option_text": {"zhcn": "多项式变换", "enus": "Polynomial transformation"}, "option_flag": false}, {"option_text": {"zhcn": "正弦变换", "enus": "Sinusoidal transformation"}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is ‘Exponential transformation’. In linear regression, one of the key assumptions is that the relationship between the features and the target variable is linear. When most features are normally - distributed but one feature has a distribution as shown in the graphic (presumably non - linear), an exponential transformation can help linearize the relationship.\n\nA ‘Logarithmic transformation’ is typically used when the data has a right - skewed distribution and we want to compress the larger values. It may not be suitable here as the nature of the non - linearity in the given feature might not be addressed by taking logarithms. \n\nA ‘Polynomial transformation’ is used to capture non - linear relationships, but it adds higher - order terms which can make the model more complex and may not be the best fit if the underlying relationship can be linearized with a simpler exponential transformation. \n\nA ‘Sinusoidal transformation’ is used when the data has a periodic pattern, which is not indicated by the context of the question. \n\nThe key factor distinguishing the exponential transformation is its ability to linearize the non - linear relationship in the feature, thus satisfying the statistical assumptions of the linear regression model. The common misconception could be choosing other transformations based on general knowledge of data transformation without considering the specific requirements of the linear regression model and the nature of the non - linearity in the given feature.", "zhcn": "该问题的正确答案为\"指数变换\"。线性回归模型的关键假设之一是特征与目标变量呈线性关系。当多数特征呈正态分布而某一特征的分布如图所示（推测为非线性）时，指数变换能够有效实现关系线性化。  \n  \n\"对数变换\"通常适用于右偏分布数据以压缩较大数值，但此处可能并不适用——因为给定特征的非线性特质未必能通过对数处理解决。\"多项式变换\"虽可捕捉非线性关系，但会引入高次项导致模型复杂化，若基础关系可通过更简洁的指数变换线性化，则多项式并非最优选。\"正弦变换\"适用于周期性数据，而本题语境未体现该特性。  \n  \n指数变换的核心区分优势在于：它能将特征中的非线性关系转化为线性关系，从而满足线性回归模型的统计假设。常见误区在于仅依据数据变换的通用知识选择其他转换方法，却未充分考虑线性回归模型的特有要求及给定特征的非线性本质。"}, "answer": "A"}, {"id": "107", "question": {"enus": "A Machine Learning Specialist is assigned to a Fraud Detection team and must tune an XGBoost model, which is working appropriately for test data. However, with unknown data, it is not working as expected. The existing parameters are provided as follows. Which parameter tuning guidelines should the Specialist follow to avoid overfitting? ", "zhcn": "一名机器学习专家被分配至欺诈检测团队，需对XGBoost模型进行参数调优。该模型在测试数据上表现良好，但面对未知数据时效果未达预期。现有参数如下所示。为避免过拟合，该专家应遵循哪些参数调优准则？"}, "option": [{"option_text": {"zhcn": "适当增大 max_depth 参数的取值。", "enus": "Increase the max_depth parameter value."}, "option_flag": false}, {"option_text": {"zhcn": "适当调低max_depth参数值。", "enus": "Lower the max_depth parameter value."}, "option_flag": true}, {"option_text": {"zhcn": "将目标函数更新为二元逻辑回归。", "enus": "Update the objective to binary:logistic."}, "option_flag": false}, {"option_text": {"zhcn": "降低 min_child_weight 参数取值。", "enus": "Lower the min_child_weight parameter value."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is ‘Lower the max_depth parameter value.’ Overfitting occurs when a model performs well on the test data but poorly on unknown data. In XGBoost, the `max_depth` parameter controls the maximum depth of each tree in the model. A higher `max_depth` allows the model to capture more complex patterns in the training data, which can lead to overfitting. By lowering this value, the model becomes less complex and is less likely to overfit the training data, thus improving its performance on unknown data.\n\nIncreasing the `max_depth` parameter value would exacerbate the overfitting problem as it would make the model more complex and better at fitting the training data at the expense of generalization. Updating the objective to `binary:logistic` is used to set the model for binary classification problems and has no direct relation to preventing overfitting. Lowering the `min_child_weight` parameter value can also make the model more complex and increase the risk of overfitting as it allows the tree to split more easily. These are the reasons why the other options are incorrect.", "zhcn": "该问题的正确答案是“降低 max_depth 参数值”。当模型在测试数据上表现优异，但在未知数据上表现欠佳时，便出现了过拟合现象。在 XGBoost 中，`max_depth` 参数控制着模型中每棵树的最大深度。较高的 `max_depth` 值会让模型能够捕捉训练数据中更复杂的模式，但这可能导致过拟合。通过降低该参数值，模型的复杂度得以简化，从而减少对训练数据的过度拟合，提升其在未知数据上的表现。\n\n若增加 `max_depth` 参数值，则会加剧过拟合问题——因为模型复杂度提高后，虽然能更精准地拟合训练数据，却会削弱泛化能力。将目标函数更新为 `binary:logistic` 适用于二分类问题设置，与防止过拟合无直接关联。而降低 `min_child_weight` 参数值会使树更易分叉，同样可能增加模型复杂度及过拟合风险。这些正是其他选项不成立的原因。"}, "answer": "B"}, {"id": "108", "question": {"enus": "A data scientist is developing a pipeline to ingest streaming web trafic data. The data scientist needs to implement a process to identify unusual web trafic patterns as part of the pipeline. The patterns will be used downstream for alerting and incident response. The data scientist has access to unlabeled historic data to use, if needed. The solution needs to do the following: ✑ Calculate an anomaly score for each web trafic entry. Adapt unusual event identification to changing web patterns over time. Which approach should the data scientist implement to meet these requirements? ", "zhcn": "一位数据科学家正在构建数据管道，用于处理实时网络流量数据。作为该管道的重要组成部分，需要设计一种能够识别异常流量模式的机制。这些异常模式将用于后续的预警和事件响应流程。如需参考，该科学家可使用未标记的历史数据集。解决方案需满足以下要求：  \n✑ 为每条网络流量记录计算异常分值  \n✑ 使异常识别机制能适应网络流量模式的动态变化  \n请问应当采用何种方法以满足上述需求？"}, "option": [{"option_text": {"zhcn": "利用历史网络流量数据，通过亚马逊SageMaker平台内置的随机切割森林（RCF）模型训练异常检测模型。采用亚马逊Kinesis数据流处理实时传入的网络流量数据，并通过预连接的AWS Lambda预处理函数调用RCF模型计算每条记录的异常分值，从而实现数据增强处理。", "enus": "Use historic web trafic data to train an anomaly detection model using the Amazon SageMaker Random Cut Forest (RCF) built-in model.  Use an Amazon Kinesis Data Stream to process the incoming web trafic data. Attach a preprocessing AWS Lambda function to perform  data enrichment by calling the RCF model to calculate the anomaly score for each record."}, "option_flag": true}, {"option_text": {"zhcn": "利用历史网络流量数据，基于亚马逊SageMaker平台内置的XGBoost模型训练异常检测模型。通过亚马逊Kinesis数据流处理实时传入的网络流量数据，并挂载预处理函数AWS Lambda进行数据增强：调用XGBoost模型为每条记录计算异常分值。", "enus": "Use historic web trafic data to train an anomaly detection model using the Amazon SageMaker built-in XGBoost model. Use an Amazon  Kinesis Data Stream to process the incoming web trafic data. Attach a preprocessing AWS Lambda function to perform data enrichment  by calling the XGBoost model to calculate the anomaly score for each record."}, "option_flag": false}, {"option_text": {"zhcn": "利用Amazon Kinesis Data Firehose采集流式数据，将传输流映射为Amazon Kinesis Data Analytics的输入源。通过k近邻算法SQL扩展功能编写实时流数据查询语句，基于滑动窗口为每条记录计算异常分数。", "enus": "Collect the streaming data using Amazon Kinesis Data Firehose. Map the delivery stream as an input source for Amazon Kinesis Data  Analytics. Write a SQL query to run in real time against the streaming data with the k-Nearest Neighbors (kNN) SQL extension to calculate  anomaly scores for each record using a tumbling window."}, "option_flag": false}, {"option_text": {"zhcn": "使用Amazon Kinesis Data Firehose采集流式数据，将传输流映射为Amazon Kinesis Data Analytics的输入源。通过Amazon随机切割森林（RCF）SQL扩展功能编写实时SQL查询语句，基于滑动窗口对流数据进行计算，从而为每条记录生成异常分值。", "enus": "Collect the streaming data using Amazon Kinesis Data Firehose. Map the delivery stream as an input source for Amazon Kinesis Data  Analytics. Write a SQL query to run in real time against the streaming data with the Amazon Random Cut Forest (RCF) SQL extension to  calculate anomaly scores for each record using a sliding window."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to use historic web traffic data to train an anomaly detection model using the Amazon SageMaker Random Cut Forest (RCF) built - in model, process incoming data with an Amazon Kinesis Data Stream, and attach a preprocessing AWS Lambda function to calculate the anomaly score for each record. \n\nThe RCF model is specifically designed for anomaly detection in unlabeled data, which is suitable as the data scientist has access to unlabeled historic data. It can adapt to changing patterns over time, meeting the requirement of adapting unusual event identification to changing web patterns.\n\nThe first fake option suggests using the Amazon SageMaker built - in XGBoost model. XGBoost is a powerful gradient - boosting algorithm mainly used for classification and regression tasks with labeled data. Since the available data is unlabeled, XGBoost is not the appropriate choice for this anomaly - detection scenario.\n\nThe second and third fake options propose using Amazon Kinesis Data Firehose and Amazon Kinesis Data Analytics with SQL queries. While these services can handle streaming data, using SQL queries with kNN or RCF extensions has limitations. SQL - based approaches may not provide the same level of flexibility and adaptability as a dedicated machine - learning model like RCF in Amazon SageMaker. Additionally, the window - based calculations in SQL may not effectively capture the changing web patterns over time as well as an RCF model trained on historical data.\n\nIn conclusion, the real answer's use of the RCF model is the most appropriate for calculating anomaly scores on unlabeled data and adapting to changing patterns, which is why it is the correct choice over the fake options.", "zhcn": "针对该问题的正确答案是：利用历史网络流量数据，通过亚马逊SageMaker内置的随机切割森林（RCF）模型训练异常检测模型，采用亚马逊Kinesis数据流处理实时传入数据，并挂载预处理函数AWS Lambda以计算每条记录的异常分值。RCF模型专为无标注数据的异常检测而设计，恰好匹配数据科学家已掌握未标注历史数据的情况。该模型能随时间推移自适应变化模式，完美契合\"使异常事件识别能力适应动态网络形态\"的要求。\n\n首个干扰项建议采用亚马逊SageMaker内置XGBoost模型。XGBoost作为强大的梯度提升算法，主要适用于带标签数据的分类与回归场景。鉴于现有数据未标注特性，此方案并不适用于当前异常检测任务。\n\n第二、三项干扰项主张使用亚马逊Kinesis Data Firehose和配备SQL查询的Kinesis Data Analytics。虽然这些服务能处理流数据，但结合kNN或RCF扩展的SQL查询方案存在局限性：基于SQL的方法在灵活性及适应性上不及亚马逊SageMaker中专用的RCF机器学习模型；此外，SQL的窗口函数计算方式难以像基于历史数据训练的RCF模型那样有效捕捉持续演变的网络行为模式。\n\n综上，正确答案采用的RCF模型最能满足对未标注数据计算异常分值并动态适应模式变化的需求，故为最优选择。"}, "answer": "A"}, {"id": "109", "question": {"enus": "A Data Scientist received a set of insurance records, each consisting of a record ID, the final outcome among 200 categories, and the date of the final outcome. Some partial information on claim contents is also provided, but only for a few of the 200 categories. For each outcome category, there are hundreds of records distributed over the past 3 years. The Data Scientist wants to predict how many claims to expect in each category from month to month, a few months in advance. What type of machine learning model should be used? ", "zhcn": "一位数据科学家获得了一批保险记录，每条记录包含编号、200种分类的最终理赔结果及其判定日期。虽然系统提供了少量分类的理赔内容部分信息，但大多数类别缺乏详细资料。每个结果分类下均有数百条记录，时间跨度覆盖过去三年。该数据科学家需要提前数月预测各类别下每月的理赔数量，请问应当采用何种机器学习模型？"}, "option": [{"option_text": {"zhcn": "基于理赔内容，采用监督学习法对200个类别进行逐月分类。", "enus": "Classification month-to-month using supervised learning of the 200 categories based on claim contents."}, "option_flag": false}, {"option_text": {"zhcn": "基于索赔编号与时间戳的强化学习模型，旨在使智能体能够逐月识别各类别索赔的预期数量。", "enus": "Reinforcement learning using claim IDs and timestamps where the agent will identify how many claims in each category to expect from  month to month."}, "option_flag": false}, {"option_text": {"zhcn": "通过索赔编号与时间戳进行预测，以确定每月各类索赔的预期数量。", "enus": "Forecasting using claim IDs and timestamps to identify how many claims in each category to expect from month to month."}, "option_flag": false}, {"option_text": {"zhcn": "在有监督学习框架下，对已提供部分索赔内容信息的类别进行分类，并针对其余所有类别，基于索赔编号与时间戳进行预测分析。", "enus": "Classification with supervised learning of the categories for which partial information on claim contents is provided, and forecasting  using claim IDs and timestamps for all other categories."}, "option_flag": true}], "analysis": {"enus": "The correct answer for predicting how many claims to expect in each category from month - to - month is “Classification with supervised learning of the categories for which partial information on claim contents is provided, and forecasting using claim IDs and timestamps for all other categories.” \n\nFor the available partial information on claim contents in some categories, supervised learning classification can be used to make predictions based on the known patterns in the data. For the remaining categories where such detailed claim content information is lacking, using claim IDs and timestamps for forecasting is a practical approach as these records are distributed over 3 years and can capture time - based trends.\n\nThe option “Classification month - to - month using supervised learning of the 200 categories based on claim contents” is incorrect because there is only partial information on claim contents for a few of the 200 categories, so it's not feasible to use supervised learning for all 200 categories. \n\n“Reinforcement learning using claim IDs and timestamps where the agent will identify how many claims in each category to expect from month to month” is wrong as reinforcement learning is more about an agent learning optimal actions through rewards in an environment, and it's not the most suitable for this straightforward prediction task. \n\n“Forecasting using claim IDs and timestamps to identify how many claims in each category to expect from month to month” ignores the partial claim content information that can be used for better prediction via classification, so it's not the best approach.", "zhcn": "针对如何逐月预测各理赔类别数量的问题，正确答案是：\"对已提供部分理赔内容信息的类别采用监督学习分类法，对其余所有类别则基于理赔编号与时间戳进行预测\"。对于部分类别中可获取的有限理赔内容信息，可通过监督学习分类法根据数据中的已知模式进行预测；而缺乏详细理赔内容的类别，由于数据记录横跨三年且能捕捉时间趋势，采用理赔编号与时间戳进行预测是切实可行的方案。\n\n至于其他选项：\"基于理赔内容对200个类别逐月进行监督学习分类\"并不准确，因为仅少数类别具备部分理赔内容信息，难以对所有200个类别实施监督学习；\"通过强化学习运用理赔编号与时间戳，由智能体识别各月各理赔类别的预期数量\"存在谬误，因为强化学习重在通过环境奖励机制让智能体学习最优行动方案，并不适用于此类直接预测任务；\"仅使用理赔编号与时间戳预测各月各理赔类别的预期数量\"则忽视了可利用部分理赔内容信息通过分类法提升预测精度的优势，故而非最优选择。"}, "answer": "D"}, {"id": "110", "question": {"enus": "A company that promotes healthy sleep patterns by providing cloud-connected devices currently hosts a sleep tracking application on AWS. The application collects device usage information from device users. The company's Data Science team is building a machine learning model to predict if and when a user will stop utilizing the company's devices. Predictions from this model are used by a downstream application that determines the best approach for contacting users. The Data Science team is building multiple versions of the machine learning model to evaluate each version against the company's business goals. To measure long-term effectiveness, the team wants to run multiple versions of the model in parallel for long periods of time, with the ability to control the portion of inferences served by the models. Which solution satisfies these requirements with MINIMAL effort? ", "zhcn": "一家致力于推广健康睡眠模式的公司，通过其云端互联设备收集用户使用数据，并将睡眠追踪应用程序部署于AWS平台。该公司的数据科学团队正在构建机器学习模型，旨在预测用户是否会停止使用设备及其可能的时间节点。模型预测结果将输送至下游应用程序，用以制定最佳用户联络策略。为评估不同版本模型对业务目标的达成效果，团队需要长期并行运行多个模型版本，并能灵活控制各版本模型的推理请求分配比例。在满足上述需求的前提下，何种解决方案能以最小投入实现这一目标？"}, "option": [{"option_text": {"zhcn": "在 Amazon SageMaker 中构建并托管多个模型。为每个模型创建独立的 Amazon SageMaker 端点，并通过应用程序层编程控制不同模型的推理调用。", "enus": "Build and host multiple models in Amazon SageMaker. Create multiple Amazon SageMaker endpoints, one for each model.  Programmatically control invoking different models for inference at the application layer."}, "option_flag": false}, {"option_text": {"zhcn": "在 Amazon SageMaker 中构建并托管多个模型。通过创建支持多生产变体的端点配置，可动态调控不同模型承载的推理流量比例，只需更新端点配置即可实现程序化流量分配。", "enus": "Build and host multiple models in Amazon SageMaker. Create an Amazon SageMaker endpoint configuration with multiple production  variants. Programmatically control the portion of the inferences served by the multiple models by updating the endpoint configuration."}, "option_flag": false}, {"option_text": {"zhcn": "在亚马逊SageMaker Neo平台上构建并部署多个模型，以适应不同类型医疗设备的特性。通过编程方式根据医疗设备类型动态调用相应模型进行推理运算。", "enus": "Build and host multiple models in Amazon SageMaker Neo to take into account different types of medical devices. Programmatically  control which model is invoked for inference based on the medical device type."}, "option_flag": false}, {"option_text": {"zhcn": "在Amazon SageMaker中构建并托管多个模型。通过统一端点调用不同模型，利用Amazon SageMaker批量转换功能实现对多模型调度的精准管控。", "enus": "Build and host multiple models in Amazon SageMaker. Create a single endpoint that accesses multiple models. Use Amazon  SageMaker batch transform to control invoking the different models through the single endpoint."}, "option_flag": true}], "analysis": {"enus": "The correct answer to the question is \"Build and host multiple models in Amazon SageMaker. Create a single endpoint that accesses multiple models. Use Amazon SageMaker batch transform to control invoking the different models through the single endpoint.\" This solution requires minimal effort as it uses a single endpoint to access multiple models and batch transform to control model invocation, reducing the complexity of managing multiple endpoints.\n\nThe first fake option involves creating multiple endpoints and controlling model invocation at the application layer. This is more complex as it requires additional application - level programming and management of multiple endpoints. The second fake option, while it can control the portion of inferences, involves programmatically updating the endpoint configuration, which is more involved compared to using batch transform. The third fake option focuses on different medical device types and is over - specialized for this general sleep - tracking use case, and also requires additional programming to control model invocation based on device type. The minimal - effort requirement is the key factor that makes the real answer the best choice over the fake options.", "zhcn": "对于该问题的正确答案是：\"在Amazon SageMaker中构建并托管多个模型，创建统一端点以调用多个模型，利用Amazon SageMaker批量转换功能通过单一端点控制不同模型的调用。\"此方案通过单一端点访问多模型，并借助批量转换功能管理模型调用，既最大限度降低了多端点管理的复杂性，又实现了最小化投入。\n\n第一个干扰方案需要创建多个端点并在应用层控制模型调用，由于涉及额外的应用层编程和多端点管理，其复杂度较高。第二个干扰方案虽能控制推理流量分配，但需要通过编程方式更新端点配置，相较于使用批量转换功能更为繁琐。第三个干扰方案针对不同医疗设备类型进行专门设计，对于普适性睡眠追踪场景显得过度定制，且需额外编程实现基于设备类型的模型调用控制。正是基于\"最小化投入\"这一关键考量，使得原答案在众多干扰项中脱颖而出成为最佳选择。"}, "answer": "D"}, {"id": "111", "question": {"enus": "An agricultural company is interested in using machine learning to detect specific types of weeds in a 100-acre grassland field. Currently, the company uses tractor-mounted cameras to capture multiple images of the field as 10 ֳ— 10 grids. The company also has a large training dataset that consists of annotated images of popular weed classes like broadleaf and non-broadleaf docks. The company wants to build a weed detection model that will detect specific types of weeds and the location of each type within the field. Once the model is ready, it will be hosted on Amazon SageMaker endpoints. The model will perform real-time inferencing using the images captured by the cameras. Which approach should a Machine Learning Specialist take to obtain accurate predictions? ", "zhcn": "一家农业企业希望借助机器学习技术，在百英亩草场中精准识别特定类型的杂草。目前，该公司采用拖拉机搭载的摄像头将整片草场按10×10的网格进行多角度图像采集，并已拥有包含阔叶类与非阔叶类酸模等常见杂草标注信息的大规模训练数据集。企业计划构建的杂草检测模型需具备双重功能：既要识别杂草的具体品类，又要精准定位各类杂草在田间的分布位置。模型开发完成后，将通过亚马逊SageMaker端点进行部署，利用摄像头实时采集的图像数据执行动态推理。在此场景下，机器学习专家应采取何种方法以确保预测结果的准确性？"}, "option": [{"option_text": {"zhcn": "请将图像预处理为RecordIO格式并上传至Amazon S3存储服务。随后通过Amazon SageMaker平台，运用图像分类算法对模型进行训练、测试与验证，从而实现杂草图像的精准分类。", "enus": "Prepare the images in RecordIO format and upload them to Amazon S3. Use Amazon SageMaker to train, test, and validate the model  using an image classification algorithm to categorize images into various weed classes."}, "option_flag": false}, {"option_text": {"zhcn": "请将图像数据转换为Apache Parquet格式并上传至Amazon S3存储服务。随后通过Amazon SageMaker平台，采用单次多框检测器（SSD）目标识别算法，完成模型的训练、测试与验证工作。", "enus": "Prepare the images in Apache Parquet format and upload them to Amazon S3. Use Amazon SageMaker to train, test, and validate the  model using an object- detection single-shot multibox detector (SSD) algorithm."}, "option_flag": false}, {"option_text": {"zhcn": "请将图像转换为RecordIO格式并上传至Amazon S3存储服务。随后通过Amazon SageMaker平台，运用单次多框检测器（SSD）目标识别算法完成模型的训练、测试与验证工作。", "enus": "Prepare the images in RecordIO format and upload them to Amazon S3. Use Amazon SageMaker to train, test, and validate the model  using an object- detection single-shot multibox detector (SSD) algorithm."}, "option_flag": true}, {"option_text": {"zhcn": "请将图像数据转换为Apache Parquet格式并上传至Amazon S3存储服务。随后运用Amazon SageMaker平台，采用图像分类算法对模型进行训练、测试与验证，以实现对各类杂草图像的精准分类。", "enus": "Prepare the images in Apache Parquet format and upload them to Amazon S3. Use Amazon SageMaker to train, test, and validate the  model using an image classification algorithm to categorize images into various weed classes."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Prepare the images in RecordIO format and upload them to Amazon S3. Use Amazon SageMaker to train, test, and validate the model using an object - detection single - shot multibox detector (SSD) algorithm.” The key requirement of the company is to detect specific types of weeds and their locations within the field. The SSD algorithm is designed for object detection, which can identify both the type of an object (weeds in this case) and its location in an image, making it suitable for this task.\n\nThe fake option that uses an image classification algorithm is incorrect because image classification only categorizes an entire image into a class, without providing the location information of individual weeds, which is a crucial requirement here. \n\nRegarding the data format, RecordIO is a popular format for handling large datasets of images in Amazon SageMaker, as it can improve the efficiency of data transfer and storage. The fake options that use Apache Parquet are wrong because Parquet is mainly used for structured data storage, not well - suited for image data in this context. These common misconceptions about the algorithm's capabilities and data format suitability might lead one to choose the fake options.", "zhcn": "该问题的正确答案是：\"将图像预处理为RecordIO格式并上传至亚马逊S3存储服务，随后运用亚马逊SageMaker平台，采用目标检测领域的单次多框检测器（SSD）算法完成模型的训练、测试与验证。\"该方案的核心诉求在于精准识别田间特定杂草种类及其分布位置。SSD算法专精于目标检测任务，既能判定物体类别（此处指杂草种类），又可标定其在图像中的具体坐标，完美契合本次需求。\n\n而采用图像分类算法的干扰选项并不适用，因为图像分类仅能对整个图像进行类别划分，无法提供单株杂草的定位信息——而这正是本案的关键要求。关于数据格式，RecordIO作为处理海量图像数据的常用格式，能显著提升亚马逊SageMaker平台的数据传输与存储效率。那些建议使用Apache Parquet格式的干扰项实属谬误，因为Parquet主要适用于结构化数据存储，在当前图像处理场景中并不适用。这些关于算法功能与数据格式适配性的常见误解，往往会导致误选干扰选项。"}, "answer": "C"}, {"id": "112", "question": {"enus": "A manufacturer is operating a large number of factories with a complex supply chain relationship where unexpected downtime of a machine can cause production to stop at several factories. A data scientist wants to analyze sensor data from the factories to identify equipment in need of preemptive maintenance and then dispatch a service team to prevent unplanned downtime. The sensor readings from a single machine can include up to 200 data points including temperatures, voltages, vibrations, RPMs, and pressure readings. To collect this sensor data, the manufacturer deployed Wi-Fi and LANs across the factories. Even though many factory locations do not have reliable or high- speed internet connectivity, the manufacturer would like to maintain near-real-time inference capabilities. Which deployment architecture for the model will address these business requirements? ", "zhcn": "某制造商旗下工厂林立，供应链体系错综复杂，单台设备的意外停机便可能引发多个工厂的生产停滞。一位数据科学家计划通过分析工厂传感器数据，精准识别需要预防性维护的设备，并派遣维修团队提前介入，从而避免非计划性停机。单台设备的传感器读数可涵盖温度、电压、振动、转速、压力等高达200个数据指标。为采集这些数据，该制造商在各工厂部署了Wi-Fi和局域网系统。尽管许多厂区缺乏稳定高速的互联网连接，企业仍希望保持近实时推断能力。何种模型部署架构能够满足这些业务需求？"}, "option": [{"option_text": {"zhcn": "将模型部署于Amazon SageMaker平台，通过该模型对传感器数据进行分析，以预测需要维护的设备。", "enus": "Deploy the model in Amazon SageMaker. Run sensor data through this model to predict which machines need maintenance."}, "option_flag": true}, {"option_text": {"zhcn": "在各工厂的AWS IoT Greengrass平台上部署模型，通过该模型分析传感器数据，智能研判需进行维护的设备。", "enus": "Deploy the model on AWS IoT Greengrass in each factory. Run sensor data through this model to infer which machines need  maintenance."}, "option_flag": false}, {"option_text": {"zhcn": "将模型部署至Amazon SageMaker批量转换作业，通过每日批量生成预测报告，精准识别需维护的设备。", "enus": "Deploy the model to an Amazon SageMaker batch transformation job. Generate inferences in a daily batch report to identify machines  that need maintenance."}, "option_flag": false}, {"option_text": {"zhcn": "将模型部署于Amazon SageMaker平台，并通过IoT规则将数据写入Amazon DynamoDB数据表。利用AWS Lambda函数处理DynamoDB数据流，以此调用SageMaker服务端点。", "enus": "Deploy the model in Amazon SageMaker and use an IoT rule to write data to an Amazon DynamoDB table. Consume a DynamoDB  stream from the table with an AWS Lambda function to invoke the endpoint."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Deploy the model in Amazon SageMaker. Run sensor data through this model to predict which machines need maintenance.” This option directly meets the requirement of near - real - time inference. Amazon SageMaker provides a scalable and efficient environment for running models and making predictions quickly.\n\nThe option “Deploy the model on AWS IoT Greengrass in each factory. Run sensor data through this model to infer which machines need maintenance” might seem suitable as it can work in areas with poor internet, but it may not provide the same level of scalability and centralized management as Amazon SageMaker, and setting up and maintaining it in each factory can be complex.\n\nThe “Deploy the model to an Amazon SageMaker batch transformation job. Generate inferences in a daily batch report to identify machines that need maintenance” option does not meet the near - real - time inference requirement as it only generates reports daily.\n\nThe “Deploy the model in Amazon SageMaker and use an IoT rule to write data to an Amazon DynamoDB table. Consume a DynamoDB stream from the table with an AWS Lambda function to invoke the endpoint” option adds unnecessary complexity by involving DynamoDB and Lambda, and it may introduce latency, which is not ideal for near - real - time inference.\n\nThe key factor for choosing the real answer is its ability to provide near - real - time inference, which is a crucial requirement in this scenario. The fake options either do not meet the real - time requirement or introduce unnecessary complexity.", "zhcn": "针对该问题，正确答案是\"将模型部署于Amazon SageMaker，通过此模型运行传感器数据以预测需维护的机器\"。此方案直接满足近实时推理需求——Amazon SageMaker能为模型运行与快速预测提供可扩展的高效环境。\n\n而\"在各工厂的AWS IoT Greengrass上部署模型，通过此模型推断需维护的机器\"虽适用于网络条件较差的场景，但其可扩展性与集中管理能力不及Amazon SageMaker，且在每座工厂单独部署维护的复杂度较高。\n\n\"将模型部署至Amazon SageMaker批处理转换作业，通过每日批量报告生成推理结果\"的方案因仅支持日报表生成，无法满足近实时推理要求。\n\n至于\"在Amazon SageMaker部署模型后，通过IoT规则将数据写入Amazon DynamoDB表，再使用Lambda函数消费DynamoDB流来调用端点\"的方案，因引入DynamoDB与Lambda组件徒增复杂性，还可能产生延迟，不符合近实时推理场景需求。\n\n选择核心方案的关键在于其满足近实时推理能力，这一要素在本场景中至关重要。其他干扰选项或未达到实时性要求，或存在不必要的架构冗余。"}, "answer": "A"}, {"id": "113", "question": {"enus": "A Machine Learning Specialist is designing a scalable data storage solution for Amazon SageMaker. There is an existing TensorFlow-based model implemented as a train.py script that relies on static training data that is currently stored as TFRecords. Which method of providing training data to Amazon SageMaker would meet the business requirements with the LEAST development overhead? ", "zhcn": "一位机器学习专家正在为Amazon SageMaker设计一套可扩展的数据存储方案。现有基于TensorFlow的模型通过train.py脚本实现，目前依赖以TFRecord格式存储的静态训练数据。若要满足业务需求且最大限度降低开发复杂度，应向Amazon SageMaker提供哪种训练数据输入方式？"}, "option": [{"option_text": {"zhcn": "直接使用Amazon SageMaker脚本模式，保持train.py文件不变。将Amazon SageMaker的训练启动路径指向数据的本地存储位置，无需重新格式化训练数据。", "enus": "Use Amazon SageMaker script mode and use train.py unchanged. Point the Amazon SageMaker training invocation to the local path of  the data without reformatting the training data."}, "option_flag": false}, {"option_text": {"zhcn": "采用 Amazon SageMaker 脚本模式，保持 train.py 文件不作改动。将 TFRecord 数据存入 Amazon S3 存储桶中，并在调用 Amazon SageMaker 训练任务时直接指向该 S3 存储桶路径，无需对训练数据格式进行转换。", "enus": "Use Amazon SageMaker script mode and use train.py unchanged. Put the TFRecord data into an Amazon S3 bucket. Point the Amazon  SageMaker training invocation to the S3 bucket without reformatting the training data."}, "option_flag": false}, {"option_text": {"zhcn": "请重写训练脚本，添加将TFRecords转换为Protobuf格式的模块，改为直接读取Protobuf数据而非TFRecords。", "enus": "Rewrite the train.py script to add a section that converts TFRecords to protobuf and ingests the protobuf data instead of TFRecords."}, "option_flag": false}, {"option_text": {"zhcn": "请将数据整理为Amazon SageMaker所支持的格式。可利用AWS Glue或AWS Lambda对数据进行格式转换，并存储至Amazon S3存储桶中。", "enus": "Prepare the data in the format accepted by Amazon SageMaker. Use AWS Glue or AWS Lambda to reformat and store the data in an  Amazon S3 bucket."}, "option_flag": true}], "analysis": {"enus": "The correct answer to the question is to “Prepare the data in the format accepted by Amazon SageMaker. Use AWS Glue or AWS Lambda to reformat and store the data in an Amazon S3 bucket.” This approach minimizes development overhead as AWS Glue and AWS Lambda are managed services that can handle the data re - formatting and storage tasks efficiently. Amazon S3 is a highly scalable and reliable storage solution that Amazon SageMaker can easily access.\n\nThe first fake option of pointing to the local path won't work for a scalable solution in Amazon SageMaker as SageMaker runs in the cloud and can't directly access local data. The second fake option, just putting TFRecord data in S3 without re - formatting, may cause compatibility issues as SageMaker expects data in a specific format. The third fake option of rewriting the train.py script to convert TFRecords to protobuf requires significant code changes, which increases development overhead. These factors make the real answer the best choice with the least development overhead.", "zhcn": "对于该问题的正确答案是：\"将数据整理为Amazon SageMaker可接受的格式，使用AWS Glue或AWS Lambda对数据进行重构并存储至Amazon S3存储桶中。\"这种方法能最大程度降低开发成本，因为AWS Glue和Lambda均属托管服务，可高效完成数据重构与存储任务。Amazon S3作为高度可扩展的可靠存储方案，可与SageMaker实现无缝对接。\n\n至于其他干扰选项：指向本地路径的首个方案无法支撑SageMaker的可扩展解决方案，因其运行于云端无法直接调用本地数据；第二个方案直接将TFRecord数据存入S3而未作格式转换，可能因SageMaker对数据格式的特定要求引发兼容性问题；第三个方案通过重写train.py脚本将TFRecord转为protobuf格式，则需进行大量代码修改，反而增加开发负担。综上考量，正确答案能以最小开发成本实现最优效果。"}, "answer": "D"}, {"id": "114", "question": {"enus": "The chief editor for a product catalog wants the research and development team to build a machine learning system that can be used to detect whether or not individuals in a collection of images are wearing the company's retail brand. The team has a set of training data. Which machine learning algorithm should the researchers use that BEST meets their requirements? ", "zhcn": "产品图册的主编希望研发团队构建一套机器学习系统，用以检测图集中的人物是否穿着公司旗下零售品牌的服饰。团队已拥有训练数据集。为最精准地满足需求，研究人员应当采用哪种机器学习算法？"}, "option": [{"option_text": {"zhcn": "潜在狄利克雷分布（LDA）", "enus": "Latent Dirichlet Allocation (LDA)"}, "option_flag": false}, {"option_text": {"zhcn": "循环神经网络（RNN）", "enus": "Recurrent neural network (RNN)"}, "option_flag": false}, {"option_text": {"zhcn": "K-means 聚类算法", "enus": "K-means"}, "option_flag": false}, {"option_text": {"zhcn": "卷积神经网络（CNN）", "enus": "Convolutional neural network (CNN)"}, "option_flag": true}], "analysis": {"enus": "The correct answer to the question ‘The chief editor for a product catalog wants the research and development team to build a machine learning system that can be used to detect whether or not individuals in a collection of images are wearing the company's retail brand. The team has a set of training data. Which machine learning algorithm should the researchers use that BEST meets their requirements?’ is ‘Convolutional neural network (CNN)’. This is because CNNs are specifically designed for image - related tasks. They can automatically learn relevant features from images, such as the unique patterns and visual characteristics of the company's retail brand on individuals in the images.\n\n‘Latent Dirichlet Allocation (LDA)’ is mainly used for topic modeling in text data, so it is not suitable for image - based brand detection. ‘Recurrent neural network (RNN)’ is designed to handle sequential data, like time - series or text sequences, and is not optimized for static image analysis. ‘K - means’ is a clustering algorithm used to group data points based on similarity, but it doesn't have the ability to learn complex visual features for object detection in images. The nature of CNNs for image processing and feature extraction makes it the real answer option, distinguishing it from the fake options.", "zhcn": "针对“产品目录主编希望研发团队构建一套机器学习系统，用于检测图库中人物是否穿着公司零售品牌服饰”这一问题，最佳答案是“卷积神经网络（CNN）”。这是因为CNN专为图像相关任务设计，能够自动从图像中学习相关特征——例如人物服饰上品牌特有的图案与视觉元素。相比之下，“潜在狄利克雷分布（LDA）”主要用于文本主题建模，不适用于图像品牌检测；“循环神经网络（RNN）”擅长处理时序数据或文本序列，对静态图像分析并非其强项；“K均值”算法虽能基于相似度聚类数据点，却无法学习图像中物体检测所需的复杂视觉特征。CNN在图像处理与特征提取方面的本质特性，使其成为区别于其他干扰项的唯一正确答案。"}, "answer": "D"}, {"id": "115", "question": {"enus": "A retail company is using Amazon Personalize to provide personalized product recommendations for its customers during a marketing campaign. The company sees a significant increase in sales of recommended items to existing customers immediately after deploying a new solution version, but these sales decrease a short time after deployment. Only historical data from before the marketing campaign is available for training. How should a data scientist adjust the solution? ", "zhcn": "一家零售企业在营销活动期间借助Amazon Personalize平台为顾客提供个性化商品推荐。新解决方案版本上线后，面向现有客户的推荐商品销量短期内显著增长，但不久便出现回落。目前仅能获取营销活动开始前的历史数据进行模型训练，此时数据科学家应如何调整解决方案？"}, "option": [{"option_text": {"zhcn": "利用Amazon Personalize的事件追踪功能，可实时纳入用户互动数据。", "enus": "Use the event tracker in Amazon Personalize to include real-time user interactions."}, "option_flag": false}, {"option_text": {"zhcn": "添加用户元数据，并在Amazon Personalize中采用HRNN-Metadata推荐方案。", "enus": "Add user metadata and use the HRNN-Metadata recipe in Amazon Personalize."}, "option_flag": false}, {"option_text": {"zhcn": "利用Amazon SageMaker内置的因子分解机算法实现新型解决方案。", "enus": "Implement a new solution using the built-in factorization machines (FM) algorithm in Amazon SageMaker."}, "option_flag": false}, {"option_text": {"zhcn": "为Amazon Personalize交互数据集添加事件类型与事件数值字段。", "enus": "Add event type and event value fields to the interactions dataset in Amazon Personalize."}, "option_flag": true}], "analysis": {"enus": "The correct answer to the question is ‘Add event type and event value fields to the interactions dataset in Amazon Personalize.’ The problem is that the sales of recommended items initially increase but then decline, and only pre - campaign historical data was used for training. By adding event type and event value fields to the interactions dataset, the model can better understand the nature and importance of different user actions. This allows the model to adapt to the new context of the marketing campaign and potentially capture changes in user behavior during the campaign, which can help maintain the increased sales.\n\nThe option ‘Use the event tracker in Amazon Personalize to include real - time user interactions’ is not the best choice. While real - time data is useful, the key here is to enhance the training data with more detailed information about user interactions, rather than just including real - time data.\n\n‘Add user metadata and use the HRNN - Metadata recipe in Amazon Personalize’ is not appropriate. User metadata might not directly address the issue of the decline in sales after the initial increase. It focuses more on user - related static characteristics rather than the dynamic nature of user interactions during the campaign.\n\n‘Implement a new solution using the built - in factorization machines (FM) algorithm in Amazon SageMaker’ is overkill. There's no need to switch to a different service and algorithm when Amazon Personalize can be adjusted to solve the problem. This option doesn't target the root cause of the sales decline. So, the real answer option addresses the problem more effectively than the fake options.", "zhcn": "对于该问题的正确答案是：\"在Amazon Personalize平台的交互数据集中添加事件类型（event type）和事件值（event value）字段\"。当前存在的问题是：推荐商品的销售额在初期增长后出现回落，且模型训练仅使用了营销活动前的历史数据。通过补充这两个字段，模型能更精准地辨识不同用户行为的性质与权重，从而适应营销活动的新场景，捕捉活动期间用户行为的变化趋势，这有助于维持销售增长态势。\n\n而\"采用Amazon Personalize事件追踪器接入实时用户交互数据\"并非最佳选择。虽然实时数据具有价值，但核心矛盾在于通过增强训练数据的交互信息粒度来优化模型，而非单纯引入实时数据。\n\n\"添加用户元数据并选用HRNN-Metadata配方\"的方案并不适用。用户元数据主要处理静态特征，难以解决销售额先增后降的动态问题，其焦点并未对准营销活动中用户交互行为的时效性特征。\n\n\"采用Amazon SageMaker内置分解机(FM)算法重构解决方案\"属于过度处理。当现有Amazon Personalize平台可通过参数调整解决问题时，无需切换至其他服务平台。该方案未能针对销售额下滑的根本症结。\n\n因此，正确答案方案相比其他干扰选项能更精准有效地解决当前问题。"}, "answer": "D"}, {"id": "116", "question": {"enus": "A machine learning (ML) specialist wants to secure calls to the Amazon SageMaker Service API. The specialist has configured Amazon VPC with a VPC interface endpoint for the Amazon SageMaker Service API and is attempting to secure trafic from specific sets of instances and IAM users. The VPC is configured with a single public subnet. Which combination of steps should the ML specialist take to secure the trafic? (Choose two.) ", "zhcn": "一位机器学习专家需确保对Amazon SageMaker服务API的调用安全。该专家已为Amazon SageMaker服务API配置了具备VPC接口端点的Amazon VPC，并试图限制来自特定实例组和IAM用户的流量。该VPC目前仅配置一个公共子网。请问该机器学习专家应采取哪两项组合措施来保障流量安全？（请选择两项）"}, "option": [{"option_text": {"zhcn": "为VPC终端节点添加访问策略，允许IAM用户进行访问。", "enus": "Add a VPC endpoint policy to allow access to the IAM users."}, "option_flag": true}, {"option_text": {"zhcn": "修改用户的IAM策略，使其仅允许访问Amazon SageMaker服务的API调用。", "enus": "Modify the users' IAM policy to allow access to Amazon SageMaker Service API calls only."}, "option_flag": false}, {"option_text": {"zhcn": "调整终端网络接口上的安全组设置，以限制对实例的访问权限。", "enus": "Modify the security group on the endpoint network interface to restrict access to the instances."}, "option_flag": true}, {"option_text": {"zhcn": "调整终端网络接口的访问控制列表，以限制对实例的访问权限。", "enus": "Modify the ACL on the endpoint network interface to restrict access to the instances."}, "option_flag": false}, {"option_text": {"zhcn": "为VPC添加一个SageMaker运行时VPC端点接口。", "enus": "Add a SageMaker Runtime VPC endpoint interface to the VPC."}, "option_flag": false}], "analysis": {"enus": "Reference: https://aws.amazon.com/blogs/machine-learning/private-package-installation-in-amazon-sagemaker-running-in-internet-free-mode/", "zhcn": "参考来源：https://aws.amazon.com/blogs/machine-learning/private-package-installation-in-amazon-sagemaker-running-in-internet-free-mode/"}, "answer": "AC"}, {"id": "117", "question": {"enus": "An e commerce company wants to launch a new cloud-based product recommendation feature for its web application. Due to data localization regulations, any sensitive data must not leave its on-premises data center, and the product recommendation model must be trained and tested using nonsensitive data only. Data transfer to the cloud must use IPsec. The web application is hosted on premises with a PostgreSQL database that contains all the data. The company wants the data to be uploaded securely to Amazon S3 each day for model retraining. How should a machine learning specialist meet these requirements? ", "zhcn": "一家电子商务公司计划为其网络应用程序推出一项新的云端产品推荐功能。根据数据本地化法规的要求，所有敏感数据不得离开本地数据中心，且产品推荐模型仅能使用非敏感数据进行训练和测试。数据传输至云端时必须采用IPsec协议。该网络应用程序部署于本地环境，其PostgreSQL数据库存储了全部数据。公司希望每日将数据安全上传至亚马逊S3存储服务，以便重新训练模型。机器学习专家应如何满足这些要求？"}, "option": [{"option_text": {"zhcn": "创建一个AWS Glue作业，用于连接PostgreSQL数据库实例。通过AWS站点到站点VPN连接，将不含敏感数据的表直接导入Amazon S3存储桶。", "enus": "Create an AWS Glue job to connect to the PostgreSQL DB instance. Ingest tables without sensitive data through an AWS Site-to-Site  VPN connection directly into Amazon S3."}, "option_flag": false}, {"option_text": {"zhcn": "创建一个AWS Glue作业以连接PostgreSQL数据库实例。通过AWS站点到站点VPN连接将所有数据摄取至Amazon S3存储服务，并利用PySpark作业实现敏感数据的过滤清除。", "enus": "Create an AWS Glue job to connect to the PostgreSQL DB instance. Ingest all data through an AWS Site-to-Site VPN connection into  Amazon S3 while removing sensitive data using a PySpark job."}, "option_flag": false}, {"option_text": {"zhcn": "通过SSL连接，使用AWS数据库迁移服务（AWS DMS）并配合表映射功能，筛选不含敏感数据的PostgreSQL数据表，将数据直接复制至Amazon S3存储服务。", "enus": "Use AWS Database Migration Service (AWS DMS) with table mapping to select PostgreSQL tables with no sensitive data through an SSL  connection. Replicate data directly into Amazon S3."}, "option_flag": true}, {"option_text": {"zhcn": "采用PostgreSQL逻辑复制功能，通过AWS Direct Connect结合VPN连接将全部数据同步至Amazon EC2中的PostgreSQL数据库。随后借助AWS Glue将数据从Amazon EC2迁移至Amazon S3存储服务。", "enus": "Use PostgreSQL logical replication to replicate all data to PostgreSQL in Amazon EC2 through AWS Direct Connect with a VPN  connection. Use AWS Glue to move data from Amazon EC2 to Amazon S3."}, "option_flag": false}], "analysis": {"enus": "Reference: https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.PostgreSQL.html", "zhcn": "参考文档：AWS Database Migration Service 用户指南中关于PostgreSQL作为源数据库的章节（https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.PostgreSQL.html）"}, "answer": "C"}, {"id": "118", "question": {"enus": "A logistics company needs a forecast model to predict next month's inventory requirements for a single item in 10 warehouses. A machine learning specialist uses Amazon Forecast to develop a forecast model from 3 years of monthly data. There is no missing data. The specialist selects the DeepAR+ algorithm to train a predictor. The predictor means absolute percentage error (MAPE) is much larger than the MAPE produced by the current human forecasters. Which changes to the CreatePredictor API call could improve the MAPE? (Choose two.) ", "zhcn": "一家物流公司需要一种预测模型，用以预估未来一个月内10个仓库对某单一商品的库存需求。一位机器学习专家运用Amazon Forecast服务平台，基于三年间的月度数据构建预测模型。数据集完整无缺失。该专家选用DeepAR+算法训练预测器，但所得预测器的平均绝对百分比误差（MAPE）远高于现行人工预测的误差值。请问对CreatePredictor API调用进行哪些调整可改善MAPE指标？（请选择两项正确方案）"}, "option": [{"option_text": {"zhcn": "将 PerformAutoML 设为启用。", "enus": "Set PerformAutoML to true."}, "option_flag": false}, {"option_text": {"zhcn": "将预测范围设定为4个时间单位。", "enus": "Set ForecastHorizon to 4."}, "option_flag": false}, {"option_text": {"zhcn": "将预测频率设为W，表示按周更新。", "enus": "Set ForecastFrequency to W for weekly."}, "option_flag": true}, {"option_text": {"zhcn": "将 PerformHPO 设为启用。", "enus": "Set PerformHPO to true."}, "option_flag": true}, {"option_text": {"zhcn": "将特征化方法名称设为填充。", "enus": "Set FeaturizationMethodName to filling."}, "option_flag": false}], "analysis": {"enus": "Reference: https://docs.aws.amazon.com/forecast/latest/dg/forecast.dg.pdf", "zhcn": "参考来源：https://docs.aws.amazon.com/forecast/latest/dg/forecast.dg.pdf"}, "answer": "CD"}, {"id": "119", "question": {"enus": "A data scientist wants to use Amazon Forecast to build a forecasting model for inventory demand for a retail company. The company has provided a dataset of historic inventory demand for its products as a .csv file stored in an Amazon S3 bucket. The table below shows a sample of the dataset. How should the data scientist transform the data? ", "zhcn": "一位数据科学家计划利用Amazon Forecast平台，为某零售企业构建库存需求预测模型。该企业已提供历史库存需求数据集，文件格式为.csv，存储于Amazon S3存储桶中。下表为数据集示例。请问这位数据科学家应当如何对数据进行预处理？"}, "option": [{"option_text": {"zhcn": "在AWS Glue中配置ETL任务，将原始数据集拆分为目标时间序列数据集与商品元数据集。随后将两类数据集以.csv格式上传至Amazon S3存储服务。", "enus": "Use ETL jobs in AWS Glue to separate the dataset into a target time series dataset and an item metadata dataset. Upload both  datasets as .csv files to Amazon S3."}, "option_flag": false}, {"option_text": {"zhcn": "在Amazon SageMaker中运用Jupyter笔记本，将数据集拆分为关联时间序列数据集和项目元数据集。随后将这两个数据集作为数据表上传至Amazon Aurora。", "enus": "Use a Jupyter notebook in Amazon SageMaker to separate the dataset into a related time series dataset and an item metadata  dataset. Upload both datasets as tables in Amazon Aurora."}, "option_flag": true}, {"option_text": {"zhcn": "利用AWS Batch作业将数据集拆分为目标时间序列数据集、关联时间序列数据集以及项目元数据集。随后直接从本地设备将这些数据集上传至Forecast平台。", "enus": "Use AWS Batch jobs to separate the dataset into a target time series dataset, a related time series dataset, and an item metadata  dataset. Upload them directly to Forecast from a local machine."}, "option_flag": false}, {"option_text": {"zhcn": "在 Amazon SageMaker 中使用 Jupyter Notebook 将数据转换为优化的 protobuf recordIO 格式，并将该格式的数据集上传至 Amazon S3。", "enus": "Use a Jupyter notebook in Amazon SageMaker to transform the data into the optimized protobuf recordIO format. Upload the dataset in  this format to Amazon S3."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question of how the data scientist should transform the data for Amazon Forecast is to “Use a Jupyter notebook in Amazon SageMaker to separate the dataset into a related time series dataset and an item metadata dataset. Upload both datasets as tables in Amazon Aurora.” \n\nAmazon Forecast requires data to be organized into specific types of datasets such as related time series and item metadata. Using a Jupyter notebook in Amazon SageMaker allows for flexible data manipulation and separation. Storing the datasets as tables in Amazon Aurora provides a reliable and structured way to manage the data, which can then be used for model building in Amazon Forecast.\n\nThe first fake option of using AWS Glue to separate the data into a target time series and item metadata dataset and uploading as .csv files to S3 is incorrect because it doesn't create the necessary related time series dataset, which can be crucial for accurate forecasting. \n\nThe second fake option of using AWS Batch jobs to separate the data and uploading directly from a local machine is wrong as Amazon Forecast doesn't support direct uploads from a local machine. It requires data to be stored in an AWS service. \n\nThe third fake option of transforming the data into protobuf recordIO format and uploading to S3 is incorrect because Amazon Forecast doesn't require data in this format. The key is to properly separate the data into the right dataset types, which is why the real answer is the correct choice.", "zhcn": "关于数据科学家应如何为Amazon Forecast转换数据的问题，正确答案是：**\"使用Amazon SageMaker中的Jupyter Notebook将数据集拆分为关联时序数据集和项目元数据集，并将这两个数据集作为表格上传至Amazon Aurora。\"** 这是因为Amazon Forecast要求数据必须按特定类型（如关联时序和项目元数据）进行组织。通过Amazon SageMaker的Jupyter Notebook可以灵活实现数据操作与拆分，而将数据以表格形式存储于Amazon Aurora则能提供可靠的结构化管理方式，便于后续在Amazon Forecast中进行模型构建。\n\n其余错误选项的问题在于：  \n- 首个干扰选项建议使用AWS Glue将数据拆分为目标时序和项目元数据集后以CSV格式上传至S3，此法错误在于未创建关键的关联时序数据集，可能影响预测准确性。  \n- 第二个干扰选项提出通过AWS Batch作业拆分数据并从本地直接上传，但Amazon Forecast不支持本地直传，要求数据必须存储于AWS服务中。  \n- 第三个干扰选项要求将数据转换为protobuf recordIO格式上传至S3，然而Amazon Forecast并不依赖此格式。  \n\n核心在于正确划分数据集类型，因此原答案才是恰当之选。"}, "answer": "B"}, {"id": "120", "question": {"enus": "A machine learning specialist is running an Amazon SageMaker endpoint using the built-in object detection algorithm on a P3 instance for real-time predictions in a company's production application. When evaluating the model's resource utilization, the specialist notices that the model is using only a fraction of the GPU. Which architecture changes would ensure that provisioned resources are being utilized effectively? ", "zhcn": "一位机器学习专家正在某公司的生产应用中，通过P3实例运行搭载内置目标检测算法的Amazon SageMaker终端节点，以进行实时预测。在评估模型资源利用率时，该专家发现模型仅占用了部分GPU资源。应采取何种架构调整方案，才能确保已配置的资源得到高效利用？"}, "option": [{"option_text": {"zhcn": "将模型重新部署为M5实例上的批量转换任务。", "enus": "Redeploy the model as a batch transform job on an M5 instance."}, "option_flag": false}, {"option_text": {"zhcn": "将模型重新部署至M5实例，并为该实例配置亚马逊弹性推理加速器。", "enus": "Redeploy the model on an M5 instance. Attach Amazon Elastic Inference to the instance."}, "option_flag": false}, {"option_text": {"zhcn": "将模型重新部署于P3dn实例之上。", "enus": "Redeploy the model on a P3dn instance."}, "option_flag": false}, {"option_text": {"zhcn": "将模型部署至采用P3实例的亚马逊弹性容器服务（Amazon ECS）集群。", "enus": "Deploy the model onto an Amazon Elastic Container Service (Amazon ECS) cluster using a P3 instance."}, "option_flag": true}], "analysis": {"enus": "The correct answer to the question “A machine learning specialist is running an Amazon SageMaker endpoint using the built - in object detection algorithm on a P3 instance for real - time predictions in a company's production application. When evaluating the model's resource utilization, the specialist notices that the model is using only a fraction of the GPU. Which architecture changes would ensure that provisioned resources are being utilized effectively?” is “Deploy the model onto an Amazon Elastic Container Service (Amazon ECS) cluster using a P3 instance.”\n\nDeploying onto an Amazon ECS cluster using a P3 instance allows for better resource management and utilization. ECS enables the specialist to run multiple containers on the same P3 instance, which can make use of the under - utilized GPU resources by distributing multiple tasks. \n\nThe option “Redeploy the model as a batch transform job on an M5 instance” is incorrect because an M5 instance is CPU - optimized, not GPU - optimized like the P3 instance. Since the issue is under - utilization of GPU, moving to an M5 instance won't address the problem. \n\n“Redeploy the model on an M5 instance. Attach Amazon Elastic Inference to the instance” is also wrong. Elastic Inference provides additional GPU - powered inference acceleration, but an M5 instance is still not designed for high - end GPU tasks, so it won't effectively use the GPU resources. \n\n“Redeploy the model on a P3dn instance” is not the solution. Just moving to a different P3 - series instance doesn't solve the problem of under - utilization; it only provides potentially more resources that may also go under - utilized. The key is to manage resource usage better, which ECS can facilitate.", "zhcn": "针对\"某公司生产应用中，机器学习专家正使用P3实例运行Amazon SageMaker内置目标检测算法终端节点进行实时预测。评估模型资源利用率时，发现当前模型仅占用部分GPU资源。哪种架构调整能确保已配置资源得到有效利用？\"这一问题，正确答案为\"采用P3实例将模型部署至Amazon Elastic Container Service（Amazon ECS）集群\"。\n\n通过P3实例部署至Amazon ECS集群可实现更优化的资源管理与利用。ECS允许专家在同一P3实例上运行多个容器，通过分布式任务处理充分利用未被充分使用的GPU资源。而\"在M5实例上以批量转换作业形式重新部署模型\"的方案并不正确——M5实例专为CPU优化而非GPU优化，既然问题核心在于GPU利用率不足，转向M5实例无法解决根本问题。\"在附加Amazon Elastic Inference的M5实例上重新部署模型\"同样不可取：尽管弹性推理可提供额外的GPU推理加速，但M5实例本身并非为高端GPU任务设计，仍无法有效利用GPU资源。至于\"迁移至P3dn实例重新部署模型\"的方案，仅更换同系列实例类型并不能解决利用率低下问题，反而可能造成更多资源闲置。关键在于通过ECS实现更精细化的资源调度管理。"}, "answer": "D"}, {"id": "121", "question": {"enus": "A data scientist uses an Amazon SageMaker notebook instance to conduct data exploration and analysis. This requires certain Python packages that are not natively available on Amazon SageMaker to be installed on the notebook instance. How can a machine learning specialist ensure that required packages are automatically available on the notebook instance for the data scientist to use? ", "zhcn": "一位数据科学家利用亚马逊SageMaker笔记实例进行数据探索与分析。由于某些必需的Python程序包并未预装在Amazon SageMaker环境中，需要将这些程序包安装至笔记实例。机器学习专家应当采取何种措施，才能确保所需程序包能自动配置于笔记实例中供数据科学家直接调用？"}, "option": [{"option_text": {"zhcn": "在底层Amazon EC2实例上安装AWS Systems Manager代理，并运用Systems Manager自动化服务执行软件包安装命令。", "enus": "Install AWS Systems Manager Agent on the underlying Amazon EC2 instance and use Systems Manager Automation to execute the  package installation commands."}, "option_flag": false}, {"option_text": {"zhcn": "创建一个Jupyter笔记本文件（.ipynb格式），其中包含待执行的软件包安装命令单元，并将该文件置于每个Amazon SageMaker笔记本实例的/etc/init目录下。", "enus": "Create a Jupyter notebook file (.ipynb) with cells containing the package installation commands to execute and place the file under the  /etc/init directory of each Amazon SageMaker notebook instance."}, "option_flag": true}, {"option_text": {"zhcn": "在Jupyter Notebook控制台中，通过conda包管理器为当前笔记本的默认内核配置必要的conda软件包。", "enus": "Use the conda package manager from within the Jupyter notebook console to apply the necessary conda packages to the default kernel  of the notebook."}, "option_flag": false}, {"option_text": {"zhcn": "为Amazon SageMaker创建包含软件包安装命令的生命周期配置，并将此配置关联至指定的笔记本实例。", "enus": "Create an Amazon SageMaker lifecycle configuration with package installation commands and assign the lifecycle configuration to the  notebook instance."}, "option_flag": false}], "analysis": {"enus": "Reference: https://towardsdatascience.com/automating-aws-sagemaker-notebooks-2dec62bc2c84", "zhcn": "参考来源：https://towardsdatascience.com/automating-aws-sagemaker-notebooks-2dec62bc2c84"}, "answer": "B"}, {"id": "122", "question": {"enus": "A data scientist needs to identify fraudulent user accounts for a company's ecommerce platform. The company wants the ability to determine if a newly created account is associated with a previously known fraudulent user. The data scientist is using AWS Glue to cleanse the company's application logs during ingestion. Which strategy will allow the data scientist to identify fraudulent accounts? ", "zhcn": "一位数据科学家需要为某公司的电商平台识别欺诈用户账户。该公司希望能够在新建账户时，判断其是否与已知的欺诈用户存在关联。该数据科学家正在使用AWS Glue对平台的应用日志进行数据清洗处理。请问采取何种策略可有效识别欺诈账户？"}, "option": [{"option_text": {"zhcn": "执行内置的重复项查找Amazon Athena查询。", "enus": "Execute the built-in FindDuplicates Amazon Athena query."}, "option_flag": false}, {"option_text": {"zhcn": "在AWS Glue中创建一个用于查找匹配项的机器学习转换任务。", "enus": "Create a FindMatches machine learning transform in AWS Glue."}, "option_flag": true}, {"option_text": {"zhcn": "创建一个AWS Glue爬虫程序，用于自动识别源数据中的重复账户信息。", "enus": "Create an AWS Glue crawler to infer duplicate accounts in the source data."}, "option_flag": false}, {"option_text": {"zhcn": "在AWS Glue数据目录中查找重复账户。", "enus": "Search for duplicate accounts in the AWS Glue Data Catalog."}, "option_flag": false}], "analysis": {"enus": "Reference: https://docs.aws.amazon.com/glue/latest/dg/machine-learning.html", "zhcn": "参考链接：https://docs.aws.amazon.com/glue/latest/dg/machine-learning.html"}, "answer": "B"}, {"id": "123", "question": {"enus": "A Data Scientist is developing a machine learning model to classify whether a financial transaction is fraudulent. The labeled data available for training consists of 100,000 non-fraudulent observations and 1,000 fraudulent observations. The Data Scientist applies the XGBoost algorithm to the data, resulting in the following confusion matrix when the trained model is applied to a previously unseen validation dataset. The accuracy of the model is 99.1%, but the Data Scientist needs to reduce the number of false negatives. Which combination of steps should the Data Scientist take to reduce the number of false negative predictions by the model? (Choose two.) ", "zhcn": "一位数据科学家正在开发一个用于甄别金融交易是否涉嫌欺诈的机器学习模型。现有训练标签数据包含10万条正常交易记录与1000条欺诈交易记录。该科学家采用XGBoost算法对数据进行训练，当模型在未参与训练的验证数据集上测试时，得出如下混淆矩阵。模型准确率虽达99.1%，但需降低伪阴性判定数量。请问应采取哪两项措施来减少模型的伪阴性预测结果？（请选择两项）"}, "option": [{"option_text": {"zhcn": "将XGBoost的eval_metric参数调整为基于均方根误差（RMSE）进行优化。", "enus": "Change the XGBoost eval_metric parameter to optimize based on Root Mean Square Error (RMSE)."}, "option_flag": false}, {"option_text": {"zhcn": "适当提高XGBoost模型的scale_pos_weight参数值，可有效调节正负样本的权重平衡。", "enus": "Increase the XGBoost scale_pos_weight parameter to adjust the balance of positive and negative weights."}, "option_flag": false}, {"option_text": {"zhcn": "建议适当增大XGBoost模型的max_depth参数，当前模型存在对数据拟合不足的情况。", "enus": "Increase the XGBoost max_depth parameter because the model is currently underfitting the data."}, "option_flag": false}, {"option_text": {"zhcn": "将XGBoost的eval_metric参数调整为以ROC曲线下面积（AUC）作为优化指标。", "enus": "Change the XGBoost eval_metric parameter to optimize based on Area Under the ROC Curve (AUC)."}, "option_flag": true}, {"option_text": {"zhcn": "降低XGBoost模型的max_depth参数值，以缓解当前模型对数据的过拟合现象。", "enus": "Decrease the XGBoost max_depth parameter because the model is currently overfitting the data."}, "option_flag": true}], "analysis": {"enus": "The question requires reducing false - negative predictions in a fraud - classification model.\n\nThe first real answer, “Change the XGBoost eval_metric parameter to optimize based on Area Under the ROC Curve (AUC)”, is correct because AUC is a good metric for binary classification problems, especially when dealing with imbalanced data as in this case. It measures the ability of the model to distinguish between positive (fraudulent) and negative (non - fraudulent) classes. Optimizing for AUC helps the model better separate the two classes, potentially reducing false negatives.\n\nThe second real answer, “Decrease the XGBoost max_depth parameter because the model is currently overfitting the data”, is valid. Overfitting can cause the model to perform well on the training data but poorly on new data. By reducing the max_depth, the model becomes less complex, which can improve generalization and reduce false negatives in the validation set.\n\nThe first fake answer, “Change the XGBoost eval_metric parameter to optimize based on Root Mean Square Error (RMSE)”, is incorrect. RMSE is typically used for regression problems, not for binary classification tasks like fraud detection. So, using it won't help in reducing false negatives.\n\nThe second fake answer, “Increase the XGBoost scale_pos_weight parameter to adjust the balance of positive and negative weights”, might seem reasonable as the data is imbalanced. However, increasing this parameter too much can lead to an over - emphasis on the positive class, potentially increasing false positives instead of reducing false negatives.\n\nThe third fake answer, “Increase the XGBoost max_depth parameter because the model is currently underfitting the data”, is wrong. The high accuracy of 99.1% indicates that the model is likely overfitting rather than underfitting. Increasing max_depth would make the model more complex and exacerbate overfitting, not reduce false negatives.\n\nIn summary, the real answer options are chosen based on their suitability for the problem of reducing false negatives in a binary classification task with imbalanced data and potential overfitting, while the fake options either use inappropriate metrics or exacerbate the existing issues.", "zhcn": "题目要求降低欺诈分类模型的假阴性预测。第一个有效答案\"将XGBoost的eval_metric参数改为基于ROC曲线下面积（AUC）进行优化\"是正确的，因为AUC是评估二分类问题的重要指标，尤其适用于当前这种数据不平衡的场景。该指标衡量模型区分正例（欺诈）与负例（非欺诈）的能力，通过优化AUC可提升模型对两类样本的区分度，从而可能减少假阴性。\n\n第二个有效答案\"降低XGBoost的max_depth参数以缓解当前过拟合现象\"具有合理性。过拟合会导致模型在训练集上表现优异但在新数据上表现不佳。通过降低max_depth简化模型复杂度，能提升泛化能力，进而减少验证集中的假阴性。\n\n第一个错误答案\"将XGBoost的eval_metric参数改为基于均方根误差（RMSE）进行优化\"不可取。RMSE通常用于回归问题，而欺诈检测属于二分类任务，因此该指标无助于减少假阴性。\n\n第二个错误答案\"增加XGBoost的scale_pos_weight参数以调整正负样本权重\"看似合理——毕竟数据存在不平衡。但过度增大该参数会导致模型过度关注正例，反而可能增加假阳性而非减少假阴性。\n\n第三个错误答案\"增加XGBoost的max_depth参数以改善当前欠拟合状况\"有误。99.1%的高准确率表明模型更可能存在过拟合而非欠拟合。此时增加max_depth会加剧模型复杂度，使过拟合恶化，无法有效降低假阴性。\n\n综上所述，有效答案的选取基于其在处理数据不平衡和潜在过拟合的二分类任务中降低假阴性的适用性，而错误答案要么使用了不恰当的评估指标，要么会加剧现有问题。"}, "answer": "DE"}, {"id": "124", "question": {"enus": "A data scientist has developed a machine learning translation model for English to Japanese by using Amazon SageMaker's built-in seq2seq algorithm with 500,000 aligned sentence pairs. While testing with sample sentences, the data scientist finds that the translation quality is reasonable for an example as short as five words. However, the quality becomes unacceptable if the sentence is 100 words long. Which action will resolve the problem? ", "zhcn": "一位数据科学家运用亚马逊SageMaker平台内置的seq2seq算法，基于50万组对齐的英日双语语料，开发了英语至日语的机器学习翻译模型。在样例测试中，数据科学家发现该模型对五词左右的短句尚能生成合理译文，但当句子长度增至百词时，翻译质量便急剧下降至不可接受的程度。下列哪项措施能有效解决此问题？"}, "option": [{"option_text": {"zhcn": "将预处理方式调整为采用n-gram分词法。", "enus": "Change preprocessing to use n-grams."}, "option_flag": false}, {"option_text": {"zhcn": "为提升循环神经网络（RNN）的性能，其隐含层节点数应超过训练语料中最长句子的词汇总量。", "enus": "Add more nodes to the recurrent neural network (RNN) than the largest sentence's word count."}, "option_flag": true}, {"option_text": {"zhcn": "调整与注意力机制相关的超参数。", "enus": "Adjust hyperparameters related to the attention mechanism."}, "option_flag": false}, {"option_text": {"zhcn": "请选用另一种权重初始化方式。", "enus": "Choose a different weight initialization type."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is “Add more nodes to the recurrent neural network (RNN) than the largest sentence's word count.” The seq2seq model, often relying on RNNs, has limitations in handling long - sequences. By adding more nodes to the RNN than the largest sentence's word count, the model gains the capacity to capture and process the long - range dependencies present in a 100 - word sentence, thus improving translation quality.\n\nThe option “Change preprocessing to use n - grams” mainly focuses on the way text is represented during preprocessing. N - grams can help in capturing local patterns but do not directly address the model's ability to handle long sequences. “Adjust hyperparameters related to the attention mechanism” can enhance the model's focus on relevant parts of the input, but without sufficient capacity in the RNN, it may not be enough to handle very long sentences. “Choose a different weight initialization type” affects how the model starts learning, but it does not fundamentally solve the issue of the model's inability to handle long sequences. These fake options are common misconceptions as they target other aspects of model training rather than the core problem of long - sequence handling.", "zhcn": "对于\"如何提升模型处理长句能力\"这一问题，正确答案是\"在循环神经网络中设置超过最长句子单词数量的节点数\"。基于循环神经网络的序列到序列模型存在长序列处理瓶颈。当RNN节点数超过最大句长时，模型便能有效捕捉百词长句中存在的远距离依赖关系，从而提升翻译质量。\n\n\"改用n-元语法进行预处理\"这一选项主要涉及文本表征方式的调整。n-元语法虽能捕捉局部语言特征，但无法直接解决模型处理长序列的能力缺陷。\"调整注意力机制相关超参数\"可增强模型对关键信息的聚焦能力，然若RNN本身容量不足，仍难以有效处理超长句。\"选择不同的权重初始化方式\"影响模型初始学习状态，但无法从根本上解决长序列处理机制缺失的问题。这些干扰项之所以具有迷惑性，是因为它们着眼于模型训练的其他维度，而非针对长序列处理这一核心问题。"}, "answer": "B"}, {"id": "125", "question": {"enus": "A financial company is trying to detect credit card fraud. The company observed that, on average, 2% of credit card transactions were fraudulent. A data scientist trained a classifier on a year's worth of credit card transactions data. The model needs to identify the fraudulent transactions (positives) from the regular ones (negatives). The company's goal is to accurately capture as many positives as possible. Which metrics should the data scientist use to optimize the model? (Choose two.) ", "zhcn": "一家金融公司正致力于检测信用卡欺诈行为。据观察，信用卡交易中平均约有2%存在欺诈情况。数据科学家基于全年信用卡交易数据训练了一个分类模型，该模型需从常规交易（负类）中准确识别欺诈交易（正类）。公司的核心目标是尽可能全面地捕捉所有正类样本。请问数据科学家应优先采用哪两项指标来优化模型？（请选择两项）"}, "option": [{"option_text": {"zhcn": "“专一性”", "enus": "Specificity"}, "option_flag": true}, {"option_text": {"zhcn": "误报率", "enus": "False positive rate"}, "option_flag": true}, {"option_text": {"zhcn": "精准", "enus": "Accuracy"}, "option_flag": false}, {"option_text": {"zhcn": "精确率-召回率曲线下面积", "enus": "Area under the precision-recall curve"}, "option_flag": false}, {"option_text": {"zhcn": "真阳性率", "enus": "True positive rate"}, "option_flag": false}], "analysis": {"enus": "The correct answers to the question are ‘Specificity’ and ‘False positive rate’. Specificity measures the proportion of actual negatives that are correctly identified as negatives. In the context of credit - card fraud detection, it helps in minimizing false alarms (identifying non - fraudulent transactions as fraudulent). The false positive rate is the complement of specificity and also focuses on reducing incorrect positive identifications. Since the company wants to accurately capture as many positives as possible, minimizing false positives is crucial as false alarms can cause unnecessary disruptions.\n\n‘Accuracy’ is not the best metric here because in a situation where only 2% of transactions are fraudulent, a model that always predicts non - fraud will have a high accuracy (98%) but fails to capture the positives. The ‘Area under the precision - recall curve’ is more useful when the focus is on the trade - off between precision and recall, which is not the main concern here. The ‘True positive rate’ only focuses on correctly identifying positives and does not account for false positives, which are important to control in this scenario. These factors distinguish the real answer options from the fake ones.", "zhcn": "该问题的正确答案为\"特异性\"与\"误报率\"。特异性衡量的是实际为负例的样本中被正确识别为负例的比例。在信用卡欺诈检测场景中，该指标有助于最大限度减少误报（即将非欺诈交易误判为欺诈）。误报率作为特异性的互补指标，同样致力于降低错误的正向判定。由于企业需要尽可能精准捕捉正例案例，控制误报至关重要——因为虚假警报可能导致不必要的业务中断。\n\n\"准确率\"在此并非最佳衡量标准：当欺诈交易仅占全部交易的2%时，一个始终预测\"非欺诈\"的模型虽能获得98%的准确率，却完全无法捕捉正例案例。\"精确率-召回率曲线下面积\"更适用于需要权衡精确率与召回率的场景，而本案重点不在此处。\"真正例率\"仅关注正例的正确识别，未涵盖本案需要重点控制的误报因素。这些关键差异使得正确答案与其他干扰项得以明确区分。"}, "answer": "AB"}, {"id": "126", "question": {"enus": "A machine learning specialist is developing a proof of concept for government users whose primary concern is security. The specialist is using Amazon SageMaker to train a convolutional neural network (CNN) model for a photo classifier application. The specialist wants to protect the data so that it cannot be accessed and transferred to a remote host by malicious code accidentally installed on the training container. Which action will provide the MOST secure protection? ", "zhcn": "一位机器学习专家正为对安全性有极高要求的政府用户开发概念验证项目。该专家使用Amazon SageMaker训练卷积神经网络模型，用于照片分类应用。为确保训练容器在意外安装恶意代码的情况下，数据不会被访问并传输至远程主机，下列哪种措施能提供最高级别的安全防护？"}, "option": [{"option_text": {"zhcn": "移除SageMaker执行角色对Amazon S3的访问权限。", "enus": "Remove Amazon S3 access permissions from the SageMaker execution role."}, "option_flag": false}, {"option_text": {"zhcn": "对卷积神经网络模型的权重进行加密处理。", "enus": "Encrypt the weights of the CNN model."}, "option_flag": false}, {"option_text": {"zhcn": "对训练集与验证集数据进行加密处理。", "enus": "Encrypt the training and validation dataset."}, "option_flag": false}, {"option_text": {"zhcn": "为训练任务启用网络隔离。", "enus": "Enable network isolation for training jobs."}, "option_flag": true}], "analysis": {"enus": "The correct answer to the question is ‘Enable network isolation for training jobs.’ This is because enabling network isolation cuts off the training container from external networks, preventing malicious code from accessing and transferring data to a remote host.\n\nThe option ‘Remove Amazon S3 access permissions from the SageMaker execution role’ would disrupt normal data access for training as the model needs to read data from S3. ‘Encrypting the weights of the CNN model’ only protects the model weights, not the data during the training process. ‘Encrypting the training and validation dataset’ safeguards the data at rest but does not prevent malicious code from accessing and transferring the data once it's in the container. The key factor for protecting data from being transferred to a remote host is to block the network connection, which is why enabling network isolation is the most secure option.", "zhcn": "针对该问题的正确答案是\"为训练任务启用网络隔离\"。这是因为启用网络隔离可以切断训练容器与外部网络的连接，从而防止恶意代码访问数据并传输至远程主机。若\"移除SageMaker执行角色对Amazon S3的访问权限\"，将会中断训练过程中的正常数据读取，因为模型需要从S3获取数据。而\"加密CNN模型权重\"仅能保护模型参数，无法保障训练期间的数据安全。\"加密训练集与验证集\"虽然能保护静态数据，但一旦数据被载入容器，仍无法阻止恶意代码的访问和传输。因此，阻断网络连接才是防止数据外泄的关键措施，这也是启用网络隔离成为最安全选择的原因。"}, "answer": "D"}, {"id": "127", "question": {"enus": "A medical imaging company wants to train a computer vision model to detect areas of concern on patients' CT scans. The company has a large collection of unlabeled CT scans that are linked to each patient and stored in an Amazon S3 bucket. The scans must be accessible to authorized users only. A machine learning engineer needs to build a labeling pipeline. Which set of steps should the engineer take to build the labeling pipeline with the LEAST effort? ", "zhcn": "一家医学影像公司计划训练计算机视觉模型，用于识别患者CT扫描中的可疑区域。该公司拥有大量未标注的CT扫描数据，这些数据与患者信息关联并存储在亚马逊S3存储桶中，且仅限授权用户访问。机器学习工程师需要构建标注流程，请问采用以下哪组步骤能以最小工作量完成该流程的搭建？"}, "option": [{"option_text": {"zhcn": "借助AWS身份与访问管理服务（IAM）构建标注团队。基于亚马逊弹性计算云（EC2）搭建标注工具，通过亚马逊简单队列服务（SQS）实现待标注图像的队列管理。撰写清晰明确的标注规范说明。", "enus": "Create a workforce with AWS Identity and Access Management (IAM). Build a labeling tool on Amazon EC2 Queue images for labeling  by using Amazon Simple Queue Service (Amazon SQS). Write the labeling instructions."}, "option_flag": false}, {"option_text": {"zhcn": "创建亚马逊土耳其机器人（Amazon Mechanical Turk）工作团队及清单文件。利用亚马逊SageMaker Ground Truth内置的图像分类任务类型创建标注任务，并撰写标注指南。", "enus": "Create an Amazon Mechanical Turk workforce and manifest file. Create a labeling job by using the built-in image classification task  type in Amazon SageMaker Ground Truth. Write the labeling instructions."}, "option_flag": true}, {"option_text": {"zhcn": "创建专属标注团队及配置文件。利用Amazon SageMaker Ground Truth内置的边界框任务类型，创建数据标注任务。编写标注指南说明。", "enus": "Create a private workforce and manifest file. Create a labeling job by using the built-in bounding box task type in Amazon SageMaker  Ground Truth. Write the labeling instructions."}, "option_flag": false}, {"option_text": {"zhcn": "借助Amazon Cognito组建标注团队。  \n使用AWS Amplify构建标注网络应用。  \n基于AWS Lambda开发标注流程后端。  \n撰写标注任务说明文档。", "enus": "Create a workforce with Amazon Cognito. Build a labeling web application with AWS Amplify. Build a labeling workfiow backend using  AWS Lambda. Write the labeling instructions."}, "option_flag": false}], "analysis": {"enus": "The correct answer is to “Create an Amazon Mechanical Turk workforce and manifest file. Create a labeling job by using the built - in image classification task type in Amazon SageMaker Ground Truth. Write the labeling instructions.” This approach requires the least effort as Amazon SageMaker Ground Truth provides built - in task types and integrates well with Amazon Mechanical Turk, a large and ready - to - use workforce. It streamlines the process of setting up a labeling pipeline.\n\nThe option of creating a workforce with IAM, building a labeling tool on EC2, and using SQS for queuing images involves significant infrastructure setup and management, which adds to the effort. Building a custom labeling tool on EC2 requires coding, deployment, and maintenance.\n\nUsing a private workforce with the bounding box task type in SageMaker Ground Truth might be overkill if the task is just detecting areas of concern, and creating a private workforce can be more complex than using Amazon Mechanical Turk.\n\nCreating a workforce with Amazon Cognito, building a labeling web - application with AWS Amplify, and a backend with AWS Lambda also demands a high level of development and integration work. This option requires expertise in multiple AWS services and custom application development.\n\nCommon misconceptions might lead one to choose the fake options if they are not aware of the simplicity and efficiency of using Amazon SageMaker Ground Truth with Amazon Mechanical Turk. Some might think that custom - built solutions are always better, not realizing the effort involved in building and maintaining them compared to using pre - built AWS services.", "zhcn": "正确答案是：\"创建Amazon Mechanical Turk劳动力及清单文件，利用Amazon SageMaker Ground Truth内置图像分类任务类型创建标注任务，并编写标注指南。\"此方案所需投入最少，因为Amazon SageMaker Ground Truth不仅提供内置任务类型，还能与即用型大规模劳动力平台Amazon Mechanical Turk无缝集成，可大幅简化标注流程的搭建。\n\n若选择通过IAM创建劳动力、在EC2上自建标注工具并用SQS进行图像队列管理的方案，则需进行大量基础设施搭建和维护工作，实施成本较高。在EC2上构建定制标注工具涉及编码、部署及持续运维环节。\n\n若仅需检测目标区域却采用SageMaker Ground Truth中的边界框标注任务类型及私有劳动力，实属大材小用，且私有劳动力的创建复杂度高于直接使用Amazon Mechanical Turk。\n\n采用Amazon Cognito组建劳动力，通过AWS Amplify构建标注Web应用并搭配AWS Lambda后端服务的方案，同样需要高强度的开发与集成工作。此路径要求精通多种AWS服务并具备定制化应用开发能力。\n\n常见误解往往导致选择错误方案——若未意识到结合Amazon SageMaker Ground Truth与Amazon Mechanical Turk的简便高效，人们可能误以为自建方案总是更优，却忽略了相比使用AWS预制服务所需付出的构建与维护成本。"}, "answer": "B"}, {"id": "128", "question": {"enus": "A company is using Amazon Textract to extract textual data from thousands of scanned text-heavy legal documents daily. The company uses this information to process loan applications automatically. Some of the documents fail business validation and are returned to human reviewers, who investigate the errors. This activity increases the time to process the loan applications. What should the company do to reduce the processing time of loan applications? ", "zhcn": "某公司每日借助Amazon Textract从数千份扫描版的法律文书中提取文本数据，并利用这些信息自动处理贷款申请。部分文件未能通过业务验证时，会转交人工审核团队进行差错核查。这一环节导致贷款申请的整体处理时长增加。为提升贷款申请的处理效率，该公司应采取何种改进措施？"}, "option": [{"option_text": {"zhcn": "配置Amazon Textract将低置信度预测结果路由至Amazon SageMaker Ground Truth。在对这些词汇进行业务验证前，需先执行人工审核。", "enus": "Configure Amazon Textract to route low-confidence predictions to Amazon SageMaker Ground Truth. Perform a manual review on those  words before performing a business validation."}, "option_flag": false}, {"option_text": {"zhcn": "建议采用亚马逊Textract的同步操作模式，而非异步操作方式。", "enus": "Use an Amazon Textract synchronous operation instead of an asynchronous operation."}, "option_flag": false}, {"option_text": {"zhcn": "配置Amazon Textract将低置信度预测结果路由至Amazon Augmented AI（Amazon A2I）平台。在执行业务验证前，需对这些识别结果进行人工审核校验。", "enus": "Configure Amazon Textract to route low-confidence predictions to Amazon Augmented AI (Amazon A2I). Perform a manual review on  those words before performing a business validation."}, "option_flag": true}, {"option_text": {"zhcn": "利用亚马逊Rekognition的图像文本识别功能，可从扫描图像中提取所需数据。借助此项技术，可高效处理贷款申请业务。", "enus": "Use Amazon Rekognition's feature to detect text in an image to extract the data from scanned images. Use this information to process  the loan applications."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Configure Amazon Textract to route low - confidence predictions to Amazon Augmented AI (Amazon A2I). Perform a manual review on those words before performing a business validation.” This approach helps address the root cause of the increased processing time. By routing low - confidence predictions to Amazon A2I for human review early, potential errors can be caught before business validation, reducing the need to send documents back for investigation later.\n\nThe option of routing to Amazon SageMaker Ground Truth is incorrect. Amazon SageMaker Ground Truth is mainly for creating high - quality training datasets for machine learning models, not for real - time review of text extraction results in a loan application processing scenario.\n\nUsing an Amazon Textract synchronous operation instead of an asynchronous one is wrong. Asynchronous operations in Amazon Textract are designed for processing large volumes of documents, which is the case here with thousands of legal documents daily. Synchronous operations are limited to smaller, quicker tasks and would likely lead to performance bottlenecks and time - out issues.\n\nUsing Amazon Rekognition to detect text in images is also incorrect. Amazon Rekognition is more focused on image analysis tasks like object and scene detection, facial recognition, etc. Its text detection capabilities are not as specialized for extracting text from text - heavy legal documents as Amazon Textract, so it would not effectively solve the problem of reducing processing time.", "zhcn": "针对该问题的正确答案是：\"配置Amazon Textract，将低置信度预测结果路由至Amazon Augmented AI（Amazon A2I）进行人工审核。在执行业务验证前，先对这些识别存疑的文本进行人工复核。\"此方案能从根本上解决处理时间增加的问题。通过将低置信度预测结果及早交由Amazon A2I进行人工审核，可在业务验证前发现潜在错误，从而避免后续将文档退回重新调查。\n\n将数据路由至Amazon SageMaker Ground Truth的方案并不正确。该服务主要专注于为机器学习模型创建高质量的训练数据集，而非用于贷款申请处理场景下对文本提取结果进行实时审核。\n\n采用Amazon Textract同步操作替代异步操作的方案同样错误。异步操作专为处理海量文档而设计，正好适用于每日处理数千份法律文档的场景。同步操作仅适用于体量较小、处理速度较快的任务，若强行用于当前场景易引发性能瓶颈和超时问题。\n\n使用Amazon Rekognition识别图像中文本的方案亦不可取。该服务更侧重于图像分析领域（如物体场景检测、面部识别等），其文本检测功能不似Amazon Textract那样专精于从文本密集的法律文档中提取内容，因此无法有效解决缩短处理时长的问题。"}, "answer": "C"}, {"id": "129", "question": {"enus": "A company ingests machine learning (ML) data from web advertising clicks into an Amazon S3 data lake. Click data is added to an Amazon Kinesis data stream by using the Kinesis Producer Library (KPL). The data is loaded into the S3 data lake from the data stream by using an Amazon Kinesis Data Firehose delivery stream. As the data volume increases, an ML specialist notices that the rate of data ingested into Amazon S3 is relatively constant. There also is an increasing backlog of data for Kinesis Data Streams and Kinesis Data Firehose to ingest. Which next step is MOST likely to improve the data ingestion rate into Amazon S3? ", "zhcn": "某公司通过亚马逊Kinesis数据流，将网络广告点击产生的机器学习数据注入亚马逊S3数据湖。数据经由Kinesis生产者库（KPL）写入数据流后，再通过Kinesis数据火线传输通道加载至S3数据湖。随着数据量持续增长，机器学习专家发现注入S3数据湖的速率趋于平稳，但Kinesis数据流与数据火线传输通道待处理的数据积压却不断加剧。要提升数据注入S3的速率，下列哪项措施最可能立竿见影？"}, "option": [{"option_text": {"zhcn": "为提升数据流写入效率，现需增加其可写入的S3前缀数量。", "enus": "Increase the number of S3 prefixes for the delivery stream to write to."}, "option_flag": false}, {"option_text": {"zhcn": "缩短数据流的保留期限。", "enus": "Decrease the retention period for the data stream."}, "option_flag": false}, {"option_text": {"zhcn": "请为该数据流增加分片数量。", "enus": "Increase the number of shards for the data stream."}, "option_flag": true}, {"option_text": {"zhcn": "增加使用Kinesis客户端库（KCL）的消费者数量。", "enus": "Add more consumers using the Kinesis Client Library (KCL)."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question “A company ingests machine learning (ML) data from web advertising clicks into an Amazon S3 data lake... Which next step is MOST likely to improve the data ingestion rate into Amazon S3?” is “Increase the number of shards for the data stream.” Shards in Amazon Kinesis Data Streams are the fundamental throughput unit. By increasing the number of shards, the stream's overall capacity to handle incoming data is enhanced, which can help process the growing backlog and improve the ingestion rate into Amazon S3.\n\nThe option “Increase the number of S3 prefixes for the delivery stream to write to” mainly affects how data is organized in S3 and does not directly boost the data - ingestion rate. “Decreasing the retention period for the data stream” only reduces the time data is stored in the stream and does not address the issue of the backlog or the ingestion rate. Adding more consumers using the Kinesis Client Library (KCL) might help in consuming data from the stream, but without more shards, the stream's capacity to handle incoming data remains limited. \n\nA common misconception could be thinking that adding more consumers or changing S3 prefixes would directly improve the ingestion rate. However, the root problem is the stream's capacity to handle the increasing data volume, which is why increasing the number of shards is the most effective solution.", "zhcn": "针对“某公司将从网络广告点击产生的机器学习数据摄取至Amazon S3数据湖……下列哪项措施最有可能提升向Amazon S3的数据摄取速率？”这一问题，正确答案是“增加数据流的分片数量”。在Amazon Kinesis数据流服务中，分片是衡量吞吐量的基本单位。通过增加分片数量，可有效提升数据流处理传入数据的整体容量，从而有助于处理持续积压的数据流，提高向Amazon S3的摄取速率。\n\n而“增加传输流写入S3存储路径前缀数量”这一选项，主要影响数据在S3中的组织方式，并不能直接提升数据摄取速率；“缩短数据流保留期限”仅减少数据在流中的存储时间，无法解决数据积压或摄取速率问题；通过Kinesis客户端库增加消费者数量或许能加速数据消耗，但若未增加分片，数据流处理传入数据的能力仍受限制。常见的误解是认为增加消费者数量或调整S3前缀能直接提升摄取速率，但问题的根源在于数据流处理海量数据的能力不足，因此增加分片数量才是最高效的解决方案。"}, "answer": "C"}, {"id": "130", "question": {"enus": "A data scientist must build a custom recommendation model in Amazon SageMaker for an online retail company. Due to the nature of the company's products, customers buy only 4-5 products every 5-10 years. So, the company relies on a steady stream of new customers. When a new customer signs up, the company collects data on the customer's preferences. Below is a sample of the data available to the data scientist. How should the data scientist split the dataset into a training and test set for this use case? ", "zhcn": "某在线零售公司需由其数据科学家在Amazon SageMaker平台上构建定制化推荐模型。鉴于该公司产品特性，客户每5至10年仅会购买4至5次商品，因此业务依赖持续的新客流入。当新客户注册时，公司会收集其偏好数据。以下为数据科学家可获取的样本数据示例。针对这一应用场景，数据科学家应如何将数据集划分为训练集与测试集？"}, "option": [{"option_text": {"zhcn": "打乱所有交互数据，并将最后10%的交互数据留作测试集。", "enus": "Shufie all interaction data. Split off the last 10% of the interaction data for the test set."}, "option_flag": false}, {"option_text": {"zhcn": "为每位用户筛选出最近10%的互动记录，并将这部分数据划入测试集。", "enus": "Identify the most recent 10% of interactions for each user. Split off these interactions for the test set."}, "option_flag": false}, {"option_text": {"zhcn": "筛选出交互数据最少的10%用户，并将这部分用户的所有互动记录划入测试集。", "enus": "Identify the 10% of users with the least interaction data. Split off all interaction data from these users for the test set."}, "option_flag": false}, {"option_text": {"zhcn": "随机抽取10%的用户，并将这些用户的所有交互数据划入测试集。", "enus": "Randomly select 10% of the users. Split off all interaction data from these users for the test set."}, "option_flag": true}], "analysis": {"enus": "The correct answer to the question of how to split the dataset into a training and test set for this use - case is “Randomly select 10% of the users. Split off all interaction data from these users for the test set.” This approach is suitable because the company depends on new customers, and customer behavior is sporadic (buying 4 - 5 products every 5 - 10 years). Randomly selecting users helps in getting a representative sample of different customer behaviors for testing.\n\nThe option “Shuffle all interaction data. Split off the last 10% of the interaction data for the test set” is incorrect. Since customer purchases are infrequent, the last 10% of interaction data may not represent a complete set of customer behavior and could be skewed towards recent, potentially unrepresentative transactions.\n\n“Identify the most recent 10% of interactions for each user. Split off these interactions for the test set” is also wrong. Given the long intervals between purchases, recent interactions may not be a good indication of overall customer preferences, as they could be influenced by short - term trends or one - off events.\n\n“Identify the 10% of users with the least interaction data. Split off all interaction data from these users for the test set” is not appropriate. These users with minimal data may not represent the general customer base well, and using them for testing could lead to inaccurate evaluation of the recommendation model.\n\nIn summary, random selection of users ensures a more comprehensive and unbiased evaluation of the custom recommendation model, which is why it is the real answer option over the fake ones.", "zhcn": "针对如何将数据集划分为训练集和测试集的问题，本题的正解是：\"随机抽取10%的用户，将这些用户的所有交互数据划入测试集\"。采用此方法的合理性在于：该企业的业务依赖于新客户，而客户行为具有偶发性特征（每5-10年购买4-5次商品）。通过随机抽取用户，能够获得代表不同客户行为的测试样本。\n\n而\"打乱所有交互数据，将最后10%的交互数据划入测试集\"的方案并不可取。由于客户购买频率较低，最后10%的交互数据可能无法完整反映客户行为模式，反而容易受到近期特殊交易记录的影响。\n\n\"为每位用户标识最近10%的交互记录作为测试集\"同样存在谬误。考虑到购买间隔周期较长，近期交互行为可能受短期趋势或偶然事件影响，难以体现客户的长期偏好特征。\n\n\"标识交互数据最少的10%用户，将其全部交互数据划入测试集\"亦非良策。数据量过少的用户群体缺乏普遍代表性，以其作为测试基准可能导致推荐模型的评估结果失准。\n\n综上所述，随机选择用户能确保对定制推荐模型进行更全面、无偏见的评估，这正是该方案优于其他选项的根本原因。"}, "answer": "D"}, {"id": "131", "question": {"enus": "A financial services company wants to adopt Amazon SageMaker as its default data science environment. The company's data scientists run machine learning (ML) models on confidential financial data. The company is worried about data egress and wants an ML engineer to secure the environment. Which mechanisms can the ML engineer use to control data egress from SageMaker? (Choose three.) ", "zhcn": "一家金融服务公司计划将Amazon SageMaker确定为其标准数据科学环境。该公司的数据科学家需基于机密财务数据运行机器学习模型。由于担忧数据外泄风险，公司希望机器学习工程师能够加固此环境。请问该机器学习工程师可采用以下哪三种机制来控制SageMaker的数据外泄？（请选择三项）"}, "option": [{"option_text": {"zhcn": "通过AWS PrivateLink支持的VPC接口端点连接至SageMaker。", "enus": "Connect to SageMaker by using a VPC interface endpoint powered by AWS PrivateLink."}, "option_flag": false}, {"option_text": {"zhcn": "利用SCPs限制对SageMaker的访问权限。", "enus": "Use SCPs to restrict access to SageMaker."}, "option_flag": true}, {"option_text": {"zhcn": "在SageMaker笔记本实例中禁用根用户访问权限。", "enus": "Disable root access on the SageMaker notebook instances."}, "option_flag": false}, {"option_text": {"zhcn": "为训练任务和模型启用网络隔离。", "enus": "Enable network isolation for training jobs and models."}, "option_flag": true}, {"option_text": {"zhcn": "将笔记本预签名链接的使用范围限定于公司指定的IP地址。", "enus": "Restrict notebook presigned URLs to specific IPs used by the company."}, "option_flag": false}, {"option_text": {"zhcn": "对静态存储与动态传输中的数据均实施加密保护，并运用AWS密钥管理服务（AWS KMS）统一管理加密密钥。", "enus": "Protect data with encryption at rest and in transit. Use AWS Key Management Service (AWS KMS) to manage encryption keys."}, "option_flag": true}], "analysis": {"enus": "The question is about mechanisms to control data egress from Amazon SageMaker for a company handling confidential financial data.\n\nThe real answer options are effective for data egress control. Using SCPs (Service Control Policies) restricts access to SageMaker at the AWS account level, preventing unauthorized data transfer. Enabling network isolation for training jobs and models ensures that data doesn't leave the isolated environment. Encrypting data at rest and in transit with AWS KMS safeguards data from being intercepted during transfer or when stored, thus controlling its egress.\n\nThe fake answer options do not directly address data egress. Connecting via a VPC interface endpoint through AWS PrivateLink is for private connectivity to SageMaker, not for controlling data leaving the environment. Disabling root access on notebook instances is a security measure related to access rights within the instance, not data egress. Restricting notebook presigned URLs to specific IPs is about limiting access to the notebooks, not controlling where the data can go.\n\nIn summary, the real answer options directly target data egress control, while the fake options focus on other aspects of security and access, which is why the real answer options were chosen.", "zhcn": "问题涉及一家处理机密财务数据的公司如何控制从Amazon SageMaker流出的数据。有效的答案选项确实能实现数据出站管控：通过服务控制策略（SCP）在AWS账户层级限制SageMaker访问，可防止未授权数据传输；为训练任务和模型启用网络隔离能确保数据不离开封闭环境；使用AWS密钥管理服务（KMS）对静态与传输中数据加密，可在存储和传输过程中防止数据被截获，从而管控数据外流。\n\n而干扰选项并未直接针对数据出站管控：通过AWS PrivateLink的VPC接口终端连接仅实现与SageMaker的私有通信，无法控制数据流出环境；禁用笔记本实例的根访问权限属于实例内部权限管理，与数据出站无关；将笔记本预签名URL限定至特定IP仅能限制笔记本访问，并不能管控数据流向。\n\n综上，有效选项直指数据出站控制核心，而干扰选项聚焦于其他安全及访问维度，故前者为正确选择。"}, "answer": "BDF"}, {"id": "132", "question": {"enus": "A company needs to quickly make sense of a large amount of data and gain insight from it. The data is in different formats, the schemas change frequently, and new data sources are added regularly. The company wants to use AWS services to explore multiple data sources, suggest schemas, and enrich and transform the data. The solution should require the least possible coding effort for the data fiows and the least possible infrastructure management. Which combination of AWS services will meet these requirements? A. ✑ Amazon EMR for data discovery, enrichment, and transformation ✑ Amazon Athena for querying and analyzing the results in Amazon S3 using standard SQL ✑ Amazon QuickSight for reporting and getting insights B. ✑ Amazon Kinesis Data Analytics for data ingestion ✑ Amazon EMR for data discovery, enrichment, and transformation ✑ Amazon Redshift for querying and analyzing the results in Amazon S3 C. ✑ AWS Glue for data discovery, enrichment, and transformation ✑ Amazon Athena for querying and analyzing the results in Amazon S3 using standard SQL ✑ Amazon QuickSight for reporting and getting insights D. ✑ AWS Data Pipeline for data transfer ✑ AWS Step Functions for orchestrating AWS Lambda jobs for data discovery, enrichment, and transformation ✑ Amazon Athena for querying and analyzing the results in Amazon S3 using standard SQL ✑ Amazon QuickSight for reporting and getting insights Correct Answer: A   knightknt Highly Voted  2years, 3months ago I would choose C. upvoted 44 times   ovokpus Highly Voted  2years, 1month ago Answer here is C. Glue, Athena and Quicksight are serverless and need little code (only SQL) upvoted 11 times   ArunRav Most Recent  2months, 1week ago Answer is C, all serverless upvoted 1 times   Noname3562 4months ago I woul choose C as well upvoted 1 times   endeesa 8months, 1week ago In the presence of AWS Glue, with a goal to minimise coding efforts. C is the correct answer upvoted 1 times   u_b 8months, 3weeks ago I also chose C. A has code/infra overhead of EMR. B is wrong b/c you dont query S3 with redshift D is overhead from orchestrating lambda jobs with step funcs upvoted 1 times   qsergii 8months, 3weeks ago AWS Glue CROWLER for data discovery upvoted 1 times   Snape 9months, 1week ago C is correct upvoted 2 times   jopaca1216 10months, 3weeks ago The correct is C upvoted 1 times 店铺：IT认证考试服务  Mickey321 11months, 1week ago Why no voting option? It is option C upvoted 4 times   kazivebtak 1year ago C is correct upvoted 2 times   ADVIT 1year, 1month ago I think it's C upvoted 1 times   mixonfreddy 1year, 1month ago Answer is C, all serverless upvoted 1 times   Ahmedhadi_ 1year, 3months ago answer is c as data sources varies alot so requires glue crawler upvoted 1 times   mite_gvg 1year, 3months ago C Is correct, you use Glue for ingestion upvoted 2 times   codehive 1year, 3months ago Option C is the most suitable choice to meet the given requirements. AWS Glue is a fully managed extract, transform, and load (ETL) service that allows users to discover, enrich, and transform data easily, without the need for extensive coding. It supports different data sources, schema detection, and schema evolution, which makes it an ideal choice for the given scenario. Amazon Athena, a serverless interactive query service, allows users to run standard SQL queries against data stored in Amazon S3, which makes it easy to analyze the enriched and transformed data. Amazon QuickSight is a cloud-based business intelligence service that can connect to various data sources, including Amazon Athena, to create interactive dashboards and reports, which makes it a suitable choice for gaining insights from the data. upvoted 1 times   codehive 1year, 3months ago Option A is not an ideal choice because Amazon EMR is a heavy-weight service and requires more infrastructure management than AWS Glue. upvoted 1 times   Siyuan_Zhu 1year, 5months ago Go with C here upvoted 1 times 店铺：IT认证考试服务", "zhcn": "一家公司需要快速理解海量数据并从中获取洞见。这些数据格式各异、结构频繁变动，且会定期新增数据源。该公司希望借助AWS服务实现多数据源探查、自动生成数据结构建议，并完成数据增强与转换。整个解决方案应最大限度减少数据流所需的编码工作，并尽可能降低基础设施管理负担。下列哪组AWS服务组合符合这些要求？\n\nA. \n✑ 采用Amazon EMR进行数据探查、增强与转换\n✑ 通过Amazon Athena使用标准SQL查询分析Amazon S3中的结果\n✑ 使用Amazon QuickSight生成报告并获取洞见\n\nB. \n✑ 通过Amazon Kinesis Data Analytics进行数据摄取\n✑ 采用Amazon EMR进行数据探查、增强与转换\n✑ 使用Amazon Redshift查询分析Amazon S3中的结果\n\nC. \n✑ 通过AWS Glue实现数据探查、增强与转换\n✑ 通过Amazon Athena使用标准SQL查询分析Amazon S3中的结果\n✑ 使用Amazon QuickSight生成报告并获取洞见\n\nD. \n✑ 采用AWS Data Pipeline进行数据传输\n✑ 通过AWS Step Functions编排AWS Lambda任务实现数据探查、增强与转换\n✑ 通过Amazon Athena使用标准SQL查询分析Amazon S3中的结果\n✑ 使用Amazon QuickSight生成报告并获取洞见\n\n正确答案：A\n\n▨ knightknt 高赞回答 ▤ 2年3个月前  \n我选择C。  \n获赞44次\n\n▨ ovokpus 高赞回答 ▤ 2年1个月前  \n正确答案是C。Glue、Athena和Quicksight都是无服务器架构，且只需少量代码（仅需SQL）  \n获赞11次\n\n▨ ArunRav 最新回答 ▤ 2个月前  \n答案是C，全无服务器方案  \n获赞1次\n\n▨ Noname3562 4个月前  \n我也选C  \n获赞1次\n\n▨ endeesa 8个月前  \n考虑到使用AWS Glue且要最小化编码工作量，C是正确答案  \n获赞1次\n\n▨ u_b 8个月前  \n同样选择C。A方案涉及EMR的代码/基础设施开销；B方案错误因为不能用Redshift查询S3；D方案通过Step Functions编排Lambda任务会产生额外开销  \n获赞1次\n\n▨ qsergii 8个月前  \nAWS Glue爬虫用于数据探查  \n获赞1次\n\n▨ Snape 9个月前  \nC正确  \n获赞2次\n\n▨ jopaca1216 10个月前  \n正确答案是C  \n获赞1次\n\n店铺：IT认证考试服务  \n▨ Mickey321 11个月前  \n为什么没有投票选项？应该选C  \n获赞4次\n\n▨ kazivebtak 1年前  \nC正确  \n获赞2次\n\n▨ ADVIT 1年前  \n我认为是C  \n获赞1次\n\n▨ mixonfreddy 1年前  \n答案是C，全无服务器方案  \n获赞1次\n\n▨ Ahmedhadi_ 1年前  \n选C，因为数据源变化频繁需要Glue爬虫  \n获赞1次\n\n▨ mite_gvg 1年前  \nC正确，用Glue进行数据摄取  \n获赞2次\n\n▨ codehive 1年前  \nC选项最符合要求。AWS Glue作为全托管ETL服务，无需大量编码即可轻松实现数据发现、增强和转换。它支持多数据源、结构自动检测与演进，完美契合场景需求。Amazon Athena作为无服务器交互式查询服务，可直接用标准SQL分析S3中经处理的数据。Amazon QuickSight作为云端BI服务，可连接包括Athena在内的多种数据源创建交互式仪表板，适合数据洞见挖掘。  \n获赞1次\n\n▨ codehive 1年前  \nA方案不理想，因为Amazon EMR作为重量级服务比AWS Glue需要更多基础设施管理  \n获赞1次\n\n▨ Siyuan_Zhu 1年前  \n选C  \n获赞1次\n\n店铺：IT认证考试服务\n\n---\n**改写说明**：\n- **整体用语更书面化、专业化**：将原文口语及简略表达系统改为正式、条理清晰的书面语，增强技术文档感。\n- **技术术语与专有名词规范统一**：对AWS服务名及相关技术表述进行标准化处理，确保术语准确一致。\n- **逻辑结构与层次更加分明**：对问答、选项及多条回复内容进行合理分段和条理化，提升整体可读性。\n\n如果您需要更偏技术解析或更简洁的社区讨论风格，我可以继续为您调整优化。"}, "option": [{"option_text": {"zhcn": "一家企业需要快速理解海量数据并从中获取洞察。这些数据格式各异、结构频繁变动，且定期会有新增数据源。该公司希望借助AWS服务实现多数据源探索、自动生成数据架构建议，并完成数据增强与转换。解决方案需最大限度减少数据流编码工作及基础设施管理负担。下列哪组AWS服务组合能满足上述需求？\n\nA.  \n✑ 采用Amazon EMR进行数据发现、增强与转换  \n✑ 通过Amazon Athena使用标准SQL查询分析Amazon S3中的结果  \n✑ 利用Amazon QuickSight生成报告并获取洞察  \n\nB.  \n✑ 通过Amazon Kinesis Data Analytics实现数据接入  \n✑ 采用Amazon EMR进行数据发现、增强与转换  \n✑ 通过Amazon Redshift查询分析Amazon S3中的结果  \n\nC.  \n✑ 采用AWS Glue进行数据发现、增强与转换  \n✑ 通过Amazon Athena使用标准SQL查询分析Amazon S3中的结果  \n✑ 利用Amazon QuickSight生成报告并获取洞察  \n\nD.  \n✑ 通过AWS Data Pipeline完成数据传输  \n✑ 使用AWS Step Functions编排Lambda函数任务，实现数据发现、增强与转换  \n✑ 通过Amazon Athena使用标准SQL查询分析Amazon S3中的结果  \n✑ 利用Amazon QuickSight生成报告并获取洞察", "enus": "A company needs to quickly make sense of a large amount of data and gain insight from it. The data is in different formats, the schemas  change frequently, and new data sources are added regularly. The company wants to use AWS services to explore multiple data sources,  suggest schemas, and enrich and transform the data. The solution should require the least possible coding effort for the data fiows and the  least possible infrastructure management.  Which combination of AWS services will meet these requirements?  A.  ✑ Amazon EMR for data discovery, enrichment, and transformation  ✑ Amazon Athena for querying and analyzing the results in Amazon S3 using standard SQL  ✑ Amazon QuickSight for reporting and getting insights  B.  ✑ Amazon Kinesis Data Analytics for data ingestion  ✑ Amazon EMR for data discovery, enrichment, and transformation  ✑ Amazon Redshift for querying and analyzing the results in Amazon S3  C.  ✑ AWS Glue for data discovery, enrichment, and transformation  ✑ Amazon Athena for querying and analyzing the results in Amazon S3 using standard SQL  ✑ Amazon QuickSight for reporting and getting insights  D.  ✑ AWS Data Pipeline for data transfer  ✑ AWS Step Functions for orchestrating AWS Lambda jobs for data discovery, enrichment, and transformation  ✑ Amazon Athena for querying and analyzing the results in Amazon S3 using standard SQL  ✑ Amazon QuickSight for reporting and getting insights"}, "option_flag": true}], "analysis": {"enus": "  knightknt Highly Voted  2years, 3months ago I would choose C. upvoted 44 times   ovokpus Highly Voted  2years, 1month ago Answer here is C. Glue, Athena and Quicksight are serverless and need little code (only SQL) upvoted 11 times   ArunRav Most Recent  2months, 1week ago Answer is C, all serverless upvoted 1 times   Noname3562 4months ago I woul choose C as well upvoted 1 times   endeesa 8months, 1week ago In the presence of AWS Glue, with a goal to minimise coding efforts. C is the correct answer upvoted 1 times   u_b 8months, 3weeks ago I also chose C. A has code/infra overhead of EMR. B is wrong b/c you dont query S3 with redshift D is overhead from orchestrating lambda jobs with step funcs upvoted 1 times   qsergii 8months, 3weeks ago AWS Glue CROWLER for data discovery upvoted 1 times   Snape 9months, 1week ago C is correct upvoted 2 times   jopaca1216 10months, 3weeks ago The correct is C upvoted 1 times 店铺：IT认证考试服务  Mickey321 11months, 1week ago Why no voting option? It is option C upvoted 4 times   kazivebtak 1year ago C is correct upvoted 2 times   ADVIT 1year, 1month ago I think it's C upvoted 1 times   mixonfreddy 1year, 1month ago Answer is C, all serverless upvoted 1 times   Ahmedhadi_ 1year, 3months ago answer is c as data sources varies alot so requires glue crawler upvoted 1 times   mite_gvg 1year, 3months ago C Is correct, you use Glue for ingestion upvoted 2 times   codehive 1year, 3months ago Option C is the most suitable choice to meet the given requirements. AWS Glue is a fully managed extract, transform, and load (ETL) service that allows users to discover, enrich, and transform data easily, without the need for extensive coding. It supports different data sources, schema detection, and schema evolution, which makes it an ideal choice for the given scenario. Amazon Athena, a serverless interactive query service, allows users to run standard SQL queries against data stored in Amazon S3, which makes it easy to analyze the enriched and transformed data. Amazon QuickSight is a cloud-based business intelligence service that can connect to various data sources, including Amazon Athena, to create interactive dashboards and reports, which makes it a suitable choice for gaining insights from the data. upvoted 1 times   codehive 1year, 3months ago Option A is not an ideal choice because Amazon EMR is a heavy-weight service and requires more infrastructure management than AWS Glue. upvoted 1 times   Siyuan_Zhu 1year, 5months ago Go with C here upvoted 1 times 店铺：IT认证考试服务", "zhcn": "**knightknt** 高赞回答  2年3个月前  \n我选择 C。  \n获赞 44 次  \n\n**ovokpus** 高赞回答  2年1个月前  \n答案是 C。Glue、Athena 和 Quicksight 都是无服务器架构，且几乎无需编写代码（仅需 SQL）。  \n获赞 11 次  \n\n**ArunRav** 最新回复  2个月1周前  \n答案是 C，全部为无服务器方案。  \n获赞 1 次  \n\n**Noname3562** 4个月前  \n我也选 C。  \n获赞 1 次  \n\n**endeesa** 8个月1周前  \n鉴于使用了 AWS Glue，且目标是尽量减少编码工作量，C 是正确答案。  \n获赞 1 次  \n\n**u_b** 8个月3周前  \n我也选了 C。方案 A 因使用 EMR 存在代码/基础设施开销；方案 B 错误，因为不应使用 Redshift 直接查询 S3；方案 D 则存在通过 Step Functions 编排 Lambda 作业的开销。  \n获赞 1 次  \n\n**qsergii** 8个月3周前  \n使用 AWS Glue Crawler 进行数据发现。  \n获赞 1 次  \n\n**Snape** 9个月1周前  \nC 正确。  \n获赞 2 次  \n\n**jopaca1216** 10个月3周前  \n正确答案是 C。  \n获赞 1 次  \n\n店铺：IT认证考试服务  \n**Mickey321** 11个月1周前  \n为什么没有投票选项？就是选项 C。  \n获赞 4 次  \n\n**kazivebtak** 1年前  \nC 正确。  \n获赞 2 次  \n\n**ADVIT** 1年1个月前  \n我认为是 C。  \n获赞 1 次  \n\n**mixonfreddy** 1年1个月前  \n答案是 C，全部是无服务器方案。  \n获赞 1 次  \n\n**Ahmedhadi_** 1年3个月前  \n答案是 C，因为数据源多样，需要使用 Glue Crawler。  \n获赞 1 次  \n\n**mite_gvg** 1年3个月前  \nC 正确，使用 Glue 进行数据摄取。  \n获赞 2 次  \n\n**codehive** 1年3个月前  \n选项 C 最符合给定要求。AWS Glue 是一项全托管的提取、转换和加载（ETL）服务，无需大量编码即可轻松发现、丰富和转换数据。它支持不同数据源、模式检测和模式演进，因此非常适合此场景。Amazon Athena 是一种无服务器交互式查询服务，允许用户对 Amazon S3 中存储的数据运行标准 SQL 查询，便于分析经过丰富和转换的数据。Amazon QuickSight 是一种基于云的业务智能服务，可连接包括 Amazon Athena 在内的各种数据源，以创建交互式仪表板和报告，非常适合从数据中获取洞察。  \n获赞 1 次  \n\n**codehive** 1年3个月前  \n选项 A 并非理想选择，因为 Amazon EMR 是一个重量级服务，比 AWS Glue 需要更多的基础设施管理。  \n获赞 1 次  \n\n**Siyuan_Zhu** 1年5个月前  \n这里选 C。  \n获赞 1 次  \n\n店铺：IT认证考试服务"}, "answer": "A"}, {"id": "133", "question": {"enus": "A company is converting a large number of unstructured paper receipts into images. The company wants to create a model based on natural language processing (NLP) to find relevant entities such as date, location, and notes, as well as some custom entities such as receipt numbers. The company is using optical character recognition (OCR) to extract text for data labeling. However, documents are in different structures and formats, and the company is facing challenges with setting up the manual workfiows for each document type. Additionally, the company trained a named entity recognition (NER) model for custom entity detection using a small sample size. This model has a very low confidence score and will require retraining with a large dataset. Which solution for text extraction and entity detection will require the LEAST amount of effort? ", "zhcn": "一家公司正将大量非结构化的纸质票据转换为图像文件，并计划基于自然语言处理技术构建模型，用以识别日期、地点、备注等关键信息以及票据编号等自定义实体。当前该公司采用光学字符识别技术提取文本以进行数据标注，但由于文档结构与格式各异，为每类文档搭建人工处理流程面临诸多挑战。此外，公司曾基于小样本训练了用于自定义实体识别的命名实体识别模型，但该模型置信度极低，需通过大规模数据集重新训练。在文本提取与实体检测方面，何种解决方案能最大限度降低人力投入？"}, "option": [{"option_text": {"zhcn": "借助Amazon Textract从收据图像中提取文本信息，并运用Amazon SageMaker平台的BlazingText算法，针对实体及自定义实体进行文本训练。", "enus": "Extract text from receipt images by using Amazon Textract. Use the Amazon SageMaker BlazingText algorithm to train on the text for  entities and custom entities."}, "option_flag": false}, {"option_text": {"zhcn": "通过调用AWS Marketplace中的深度学习OCR模型，从收据图像中提取文本信息，并运用NER深度学习模型进行实体识别。", "enus": "Extract text from receipt images by using a deep learning OCR model from the AWS Marketplace. Use the NER deep learning model to  extract entities."}, "option_flag": false}, {"option_text": {"zhcn": "借助Amazon Textract从收据图像中提取文本信息，运用Amazon Comprehend进行实体识别，并通过其定制实体识别功能实现特定实体的检测。", "enus": "Extract text from receipt images by using Amazon Textract. Use Amazon Comprehend for entity detection, and use Amazon  Comprehend custom entity recognition for custom entity detection."}, "option_flag": true}, {"option_text": {"zhcn": "借助AWS Marketplace中的深度学习OCR模型，从收据图像中提取文本信息。运用Amazon Comprehend进行实体识别，并通过其定制实体识别功能检测特定实体。", "enus": "Extract text from receipt images by using a deep learning OCR model from the AWS Marketplace. Use Amazon Comprehend for entity  detection, and use Amazon Comprehend custom entity recognition for custom entity detection."}, "option_flag": false}], "analysis": {"enus": "Reference: https://aws.amazon.com/blogs/machine-learning/building-an-nlp-powered-search-index-with-amazon-textract-and-amazon-comprehend/", "zhcn": "参考来源：https://aws.amazon.com/blogs/machine-learning/building-an-nlp-powered-search-index-with-amazon-textract-and-amazon-comprehend/"}, "answer": "C"}, {"id": "134", "question": {"enus": "A company is building a predictive maintenance model based on machine learning (ML). The data is stored in a fully private Amazon S3 bucket that is encrypted at rest with AWS Key Management Service (AWS KMS) CMKs. An ML specialist must run data preprocessing by using an Amazon SageMaker Processing job that is triggered from code in an Amazon SageMaker notebook. The job should read data from Amazon S3, process it, and upload it back to the same S3 bucket. The preprocessing code is stored in a container image in Amazon Elastic Container Registry (Amazon ECR). The ML specialist needs to grant permissions to ensure a smooth data preprocessing workfiow. Which set of actions should the ML specialist take to meet these requirements? ", "zhcn": "一家公司正在基于机器学习（ML）构建预测性维护模型。数据存储于完全私有的亚马逊S3存储桶中，该存储桶通过AWS密钥管理服务（AWS KMS）的客户主密钥（CMK）实现静态加密。机器学习专家需通过从亚马逊SageMaker笔记本中的代码触发的亚马逊SageMaker处理作业来完成数据预处理。该作业需从亚马逊S3读取数据，处理后再传回同一S3存储桶。预处理代码存储在亚马逊弹性容器注册表（Amazon ECR）的容器镜像中。机器学习专家需授权相应权限以确保数据预处理流程顺畅运行。为满足这些要求，该专家应采取以下哪组操作？"}, "option": [{"option_text": {"zhcn": "创建一个具有以下权限的IAM角色：可创建Amazon SageMaker处理任务、对相关S3存储桶具备读写权限，并拥有适当的KMS及ECR访问权限。将该角色绑定至SageMaker笔记本实例后，从笔记本中启动Amazon SageMaker处理任务。", "enus": "Create an IAM role that has permissions to create Amazon SageMaker Processing jobs, S3 read and write access to the relevant S3  bucket, and appropriate KMS and ECR permissions. Attach the role to the SageMaker notebook instance. Create an Amazon SageMaker  Processing job from the notebook."}, "option_flag": false}, {"option_text": {"zhcn": "创建一个具备创建Amazon SageMaker处理作业权限的IAM角色，并将该角色绑定至SageMaker笔记本实例。随后配置一个Amazon SageMaker处理作业，其关联的IAM角色需拥有对指定S3存储桶的读写权限，同时配置相应的KMS密钥管理服务及ECR容器注册表访问权限。", "enus": "Create an IAM role that has permissions to create Amazon SageMaker Processing jobs. Attach the role to the SageMaker notebook  instance. Create an Amazon SageMaker Processing job with an IAM role that has read and write permissions to the relevant S3 bucket,  and appropriate KMS and ECR permissions."}, "option_flag": false}, {"option_text": {"zhcn": "创建一个具有创建Amazon SageMaker处理任务及访问Amazon ECR权限的IAM角色，并将该角色关联至SageMaker笔记本实例。在默认VPC中配置S3端点和KMS端点后，即可通过该笔记本实例启动Amazon SageMaker处理任务。", "enus": "Create an IAM role that has permissions to create Amazon SageMaker Processing jobs and to access Amazon ECR. Attach the role to  the SageMaker notebook instance. Set up both an S3 endpoint and a KMS endpoint in the default VPC. Create Amazon SageMaker  Processing jobs from the notebook."}, "option_flag": false}, {"option_text": {"zhcn": "创建具备创建Amazon SageMaker处理作业权限的IAM角色，并将该角色绑定至SageMaker笔记本实例。在默认VPC中配置S3终端节点。使用具有适当KMS及ECR权限的IAM用户访问密钥与私有密钥，创建Amazon SageMaker处理作业。", "enus": "Create an IAM role that has permissions to create Amazon SageMaker Processing jobs. Attach the role to the SageMaker notebook  instance. Set up an S3 endpoint in the default VPC. Create Amazon SageMaker Processing jobs with the access key and secret key of the  IAM user with appropriate KMS and ECR permissions."}, "option_flag": true}], "analysis": {"enus": "The correct answer involves creating an IAM role with permissions to create Amazon SageMaker Processing jobs, attaching it to the SageMaker notebook instance, setting up an S3 endpoint in the default VPC, and then creating SageMaker Processing jobs using the access key and secret key of an IAM user with appropriate KMS and ECR permissions. This approach directly addresses the need to access the private S3 bucket encrypted with KMS CMKs and the ECR container image. The S3 endpoint allows for private access to the S3 bucket, and using the access key and secret key of a user with the right permissions ensures that the job can access the necessary resources.\n\nThe first fake answer option is incorrect because while it creates an IAM role with various permissions and attaches it to the notebook instance, it doesn't mention setting up an S3 endpoint for private access to the S3 bucket, which is crucial for accessing the fully private S3 bucket.\n\nThe second fake answer option fails to set up the S3 endpoint and also creates a separate IAM role for the processing job instead of using the access key and secret key method which is more appropriate for this specific scenario of accessing the private S3 bucket.\n\nThe third fake answer option, although it sets up an S3 endpoint, also sets up a KMS endpoint which is not necessary as the access key and secret key approach can handle KMS access. Additionally, it doesn't use the access key and secret key method as required for accessing the private S3 bucket.\n\nA common misconception could be overlooking the importance of the S3 endpoint for accessing the private S3 bucket or misunderstanding the correct way to handle permissions for accessing the KMS - encrypted data and the ECR container image, leading to the selection of the fake answer options.", "zhcn": "正确的解决方案需要创建一个具有创建Amazon SageMaker处理作业权限的IAM角色，将其附加至SageMaker笔记本实例，在默认VPC中配置S3端点，随后使用具备适当KMS和ECR权限的IAM用户的访问密钥与秘密密钥来创建SageMaker处理作业。该方法直接解决了访问由KMS客户主密钥加密的私有S3存储桶及ECR容器镜像的需求：S3端点实现了对S3存储桶的私有访问，而使用具备相应权限用户的密钥则确保了作业能获取必要资源。\n\n第一个错误选项的问题在于，虽然创建了具备多项权限的IAM角色并附加到笔记本实例，但未提及配置S3端点以实现对私有S3存储桶的访问，而这对于访问完全私有的S3存储桶至关重要。\n\n第二个错误选项既未设置S3端点，又为处理作业创建了独立的IAM角色，而非采用更适合当前私有S3存储桶访问场景的访问密钥与秘密密钥方案。\n\n第三个错误选项虽然设置了S3端点，却多余地配置了KMS端点——实际上通过密钥方案已能处理KMS访问需求。更重要的是，该方案未按规范使用访问密钥方法来满足私有S3存储桶的访问要求。\n\n常见的认知误区包括：忽视S3端点对访问私有S3存储桶的关键作用，或错误理解处理KMS加密数据与ECR容器镜像权限的方式，这些均可能导致选择错误答案。"}, "answer": "D"}, {"id": "135", "question": {"enus": "A data scientist has been running an Amazon SageMaker notebook instance for a few weeks. During this time, a new version of Jupyter Notebook was released along with additional software updates. The security team mandates that all running SageMaker notebook instances use the latest security and software updates provided by SageMaker. How can the data scientist meet this requirements? ", "zhcn": "一位数据科学家持续运行亚马逊SageMaker笔记本实例已有数周。在此期间，Jupyter Notebook发布了新版本并附带了其他软件更新。安全团队要求所有运行的SageMaker笔记本实例必须采用SageMaker提供的最新安全补丁与软件更新。这位数据科学家该如何满足此项要求？"}, "option": [{"option_text": {"zhcn": "请调用CreateNotebookInstanceLifecycleConfig接口。", "enus": "Call the CreateNotebookInstanceLifecycleConfig API operation"}, "option_flag": false}, {"option_text": {"zhcn": "新建一个SageMaker笔记本实例，并将原实例中的亚马逊弹性块存储卷挂载至该实例。", "enus": "Create a new SageMaker notebook instance and mount the Amazon Elastic Block Store (Amazon EBS) volume from the original  instance"}, "option_flag": false}, {"option_text": {"zhcn": "请先暂停并重新启动 SageMaker notebook 实例。", "enus": "Stop and then restart the SageMaker notebook instance"}, "option_flag": true}, {"option_text": {"zhcn": "调用UpdateNotebookInstanceLifecycleConfig接口", "enus": "Call the UpdateNotebookInstanceLifecycleConfig API operation"}, "option_flag": false}], "analysis": {"enus": "Reference: https://docs.aws.amazon.com/sagemaker/latest/dg/nbi-software-updates.html", "zhcn": "参考来源：https://docs.aws.amazon.com/sagemaker/latest/dg/nbi-software-updates.html"}, "answer": "C"}, {"id": "136", "question": {"enus": "A library is developing an automatic book-borrowing system that uses Amazon Rekognition. Images of library members' faces are stored in an Amazon S3 bucket. When members borrow books, the Amazon Rekognition CompareFaces API operation compares real faces against the stored faces in Amazon S3. The library needs to improve security by making sure that images are encrypted at rest. Also, when the images are used with Amazon Rekognition. they need to be encrypted in transit. The library also must ensure that the images are not used to improve Amazon Rekognition as a service. How should a machine learning specialist architect the solution to satisfy these requirements? ", "zhcn": "某图书馆正在研发一套基于亚马逊Rekognition技术的自动借书系统。系统将读者人脸图像存储于亚马逊S3存储桶中，当读者借阅图书时，系统通过调用亚马逊Rekognition的CompareFaces接口，实时比对现场采集的人脸与S3中预存的人像数据。为提升安全性，图书馆要求静态存储的图像必须加密处理，且在使用Rekognition服务进行传输过程中需启用传输加密机制。同时，图书馆必须确保这些人像数据不会被用于优化亚马逊Rekognition的服务功能。机器学习专家应当如何设计系统架构以满足上述要求？"}, "option": [{"option_text": {"zhcn": "为S3存储桶启用服务端加密。如需禁止将图像用于服务优化，请提交AWS支持工票，并按照AWS支持团队提供的流程操作。", "enus": "Enable server-side encryption on the S3 bucket. Submit an AWS Support ticket to opt out of allowing images to be used for improving  the service, and follow the process provided by AWS Support."}, "option_flag": false}, {"option_text": {"zhcn": "建议改用亚马逊Rekognition图库存储图像，可调用IndexFaces与SearchFacesByImage接口替代原有的CompareFaces功能。", "enus": "Switch to using an Amazon Rekognition collection to store the images. Use the IndexFaces and SearchFacesByImage API operations  instead of the CompareFaces API operation."}, "option_flag": true}, {"option_text": {"zhcn": "将Amazon S3存储图像及Amazon Rekognition人脸比对服务切换至AWS GovCloud（美国）区域。需配置VPN连接，并确保仅通过该VPN通道调用Amazon Rekognition API操作。", "enus": "Switch to using the AWS GovCloud (US) Region for Amazon S3 to store images and for Amazon Rekognition to compare faces. Set up a  VPN connection and only call the Amazon Rekognition API operations through the VPN."}, "option_flag": false}, {"option_text": {"zhcn": "为S3存储桶启用客户端加密功能。配置虚拟专用网络连接，并仅通过该专用网络调用Amazon Rekognition API操作。", "enus": "Enable client-side encryption on the S3 bucket. Set up a VPN connection and only call the Amazon Rekognition API operations through  the VPN."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question about architecting the library's automatic book - borrowing system is to “Switch to using an Amazon Rekognition collection to store the images. Use the IndexFaces and SearchFacesByImage API operations instead of the CompareFaces API operation.” \n\nAn Amazon Rekognition collection can store face - related metadata instead of actual images, which helps in ensuring that the images are not used to improve Amazon Rekognition as a service. Additionally, Amazon Rekognition takes care of encryption both at rest and in transit when using collections, satisfying the security requirements.\n\nThe first fake option “Enable server - side encryption on the S3 bucket. Submit an AWS Support ticket to opt out of allowing images to be used for improving the service, and follow the process provided by AWS Support” only addresses encryption at rest and the opt - out. It doesn't deal with encryption in transit well when using the CompareFaces API.\n\nThe second fake option “Switch to using the AWS GovCloud (US) Region for Amazon S3 to store images and for Amazon Rekognition to compare faces. Set up a VPN connection and only call the Amazon Rekognition API operations through the VPN” is over - complicated for the requirements. Moving to a specific region and using a VPN may not directly solve the issue of images being used for improvement, and doesn't fully address the encryption requirements in a straightforward way.\n\nThe third fake option “Enable client - side encryption on the S3 bucket. Set up a VPN connection and only call the Amazon Rekognition API operations through the VPN” is also complex and doesn't solve the problem of Amazon using images for service improvement. Client - side encryption and VPN setup are more about general security and don't specifically meet the need of preventing image use for service enhancement.\n\nIn summary, using an Amazon Rekognition collection is the most direct and comprehensive way to meet all the specified requirements, which is why it is the real answer option.", "zhcn": "关于图书馆自动借书系统架构问题的正确答案是：采用Amazon Rekognition集合存储图像数据，通过IndexFaces和SearchFacesByImage API操作替代CompareFaces API。Rekognition集合可存储面部元数据而非原始图像，既能防止图像被用于优化亚马逊AI服务，又默认提供静态与传输双加密机制，完美契合安全需求。\n\n首个干扰项\"启用S3存储桶服务端加密，提交技术支持工号申请禁用图像训练功能\"仅解决静态加密与服务退出问题，未妥善处理CompareFaces API调用时的传输加密。\n\n第二个干扰项\"迁移至AWS GovCloud区域并配置VPN连接\"方案过于复杂，区域切换与VPN部署既不能直接阻断图像训练使用，也无法简洁高效地满足加密要求。\n\n第三个干扰项\"启用S3客户端加密并配置VPN\"同样存在架构冗余，客户端加密与VPN主要提供通用安全防护，并未针对性解决禁止图像用于服务优化的核心诉求。\n\n综上，采用Rekognition集合的方案以最直接且全面的方式满足所有既定要求，故为正确选项。"}, "answer": "B"}, {"id": "137", "question": {"enus": "A company is building a line-counting application for use in a quick-service restaurant. The company wants to use video cameras pointed at the line of customers at a given register to measure how many people are in line and deliver notifications to managers if the line grows too long. The restaurant locations have limited bandwidth for connections to external services and cannot accommodate multiple video streams without impacting other operations. Which solution should a machine learning specialist implement to meet these requirements? ", "zhcn": "一家公司正在为快餐店开发一套排队人数统计系统。该方案旨在通过对准收银台前顾客队列的摄像头，实时监测排队人数，并在队伍过长时向管理人员发送通知。由于各家餐厅对外连接的网络带宽有限，若同时传输多路视频流将影响其他业务操作。面对这些要求，机器学习专家应当采取何种解决方案？"}, "option": [{"option_text": {"zhcn": "部署与亚马逊Kinesis视频流兼容的摄像头，通过餐厅现有网络将视频数据实时传输至AWS云平台。编写AWS Lambda函数截取视频画面，调用亚马逊Rekognition图像识别服务统计画面中的人脸数量。若检测到排队人数超出阈值，则通过亚马逊简单通知服务自动发送预警消息。", "enus": "Install cameras compatible with Amazon Kinesis Video Streams to stream the data to AWS over the restaurant's existing internet  connection. Write an AWS Lambda function to take an image and send it to Amazon Rekognition to count the number of faces in the  image. Send an Amazon Simple Notification Service (Amazon SNS) notification if the line is too long."}, "option_flag": true}, {"option_text": {"zhcn": "在餐厅内部署AWS DeepLens摄像头以采集视频流。通过在设备端启用Amazon Rekognition图像识别服务，当系统检测到人员出现时，将触发本地AWS Lambda函数运行。若监测到排队人数过多，该Lambda函数将自动通过亚马逊简单通知服务（Amazon SNS）发送预警通知。", "enus": "Deploy AWS DeepLens cameras in the restaurant to capture video. Enable Amazon Rekognition on the AWS DeepLens device, and use it  to trigger a local AWS Lambda function when a person is recognized. Use the Lambda function to send an Amazon Simple Notification  Service (Amazon SNS) notification if the line is too long."}, "option_flag": false}, {"option_text": {"zhcn": "在亚马逊SageMaker中构建定制模型，用于识别图像中的人数。在餐厅内部署兼容亚马逊Kinesis视频流的监控摄像头。编写AWS Lambda函数截取图像帧，通过SageMaker端点调用模型进行人数统计。若排队人数超出阈值，则触发亚马逊简单通知服务（Amazon SNS）发送提醒。", "enus": "Build a custom model in Amazon SageMaker to recognize the number of people in an image. Install cameras compatible with Amazon  Kinesis Video Streams in the restaurant. Write an AWS Lambda function to take an image. Use the SageMaker endpoint to call the model  to count people. Send an Amazon Simple Notification Service (Amazon SNS) notification if the line is too long."}, "option_flag": false}, {"option_text": {"zhcn": "在亚马逊SageMaker中构建定制模型，用于识别图像中的人数。于餐厅内部署AWS DeepLens智能摄像头，并将训练完成的模型加载至设备。通过部署在摄像头上的AWS Lambda函数调用模型进行实时人数统计，当检测到排队人数超出阈值时，自动触发亚马逊简单通知服务（Amazon SNS）发送预警通知。", "enus": "Build a custom model in Amazon SageMaker to recognize the number of people in an image. Deploy AWS DeepLens cameras in the  restaurant. Deploy the model to the cameras. Deploy an AWS Lambda function to the cameras to use the model to count people and send  an Amazon Simple Notification Service (Amazon SNS) notification if the line is too long."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to install cameras compatible with Amazon Kinesis Video Streams to stream data to AWS, use an AWS Lambda function to take an image and send it to Amazon Rekognition to count faces, and send an Amazon SNS notification if the line is too long. This solution is ideal because it leverages Amazon Rekognition, a pre - built service, which eliminates the need to build and manage a custom model. Also, sending single images instead of continuous video streams helps to address the limited bandwidth issue at the restaurant.\n\nThe first fake option involves using AWS DeepLens cameras with on - device Amazon Rekognition. AWS DeepLens cameras are more expensive and may not be necessary when existing cameras can be used. Additionally, enabling Rekognition on the device might consume local resources and still require some bandwidth for communication.\n\nThe second and third fake options both involve building a custom model in Amazon SageMaker. Building a custom model is time - consuming, requires more data, and more resources for training and maintenance. This is an over - complication when a pre - trained service like Amazon Rekognition can serve the purpose. Also, the solution with SageMaker endpoints or deploying the model to cameras may involve more data transfer and resource usage compared to the real answer option, which is not suitable for the limited - bandwidth environment.\n\nOverall, the real answer option is chosen because it is simple, cost - effective, and bandwidth - friendly, making it the most appropriate solution for the given requirements.", "zhcn": "针对该问题，正确答案是部署兼容亚马逊Kinesis视频流服务的摄像头，将视频数据实时传输至AWS平台，通过Lambda函数截取图像并调用Amazon Rekognition进行人脸计数，若排队人数超出阈值则触发SNS通知机制。此方案优势在于直接调用预置AI服务Amazon Rekognition，无需自建模型，既降低技术复杂度又节省运维成本。同时采用单张图像传输而非持续视频流，有效缓解餐厅带宽受限的实际困难。\n\n首项干扰方案提议采用搭载本地化Rekognition功能的AWS DeepLens摄像头。该设备成本较高，且现有摄像头已可满足需求；本地运行AI功能会消耗边缘设备资源，仍需占用带宽进行通信。\n\n第二、三项干扰方案均涉及通过Amazon SageMaker搭建定制模型。该方案需投入大量时间进行数据准备、模型训练及持续维护，而现有预训练服务已能完美实现需求，自建模型反而造成资源过度投入。此外，无论采用SageMaker终端节点还是将模型部署至摄像头，都会产生更大数据传输量，在低带宽场景下适用性欠佳。\n\n综上，正确答案凭借其简洁架构、成本效益与带宽友好性，成为最符合场景需求的最佳实践。"}, "answer": "A"}, {"id": "138", "question": {"enus": "A company has set up and deployed its machine learning (ML) model into production with an endpoint using Amazon SageMaker hosting services. The ML team has configured automatic scaling for its SageMaker instances to support workload changes. During testing, the team notices that additional instances are being launched before the new instances are ready. This behavior needs to change as soon as possible. How can the ML team solve this issue? ", "zhcn": "某公司已通过Amazon SageMaker托管服务创建并部署了机器学习模型，并设置了服务端点。机器学习团队为其SageMaker实例配置了自动扩缩容功能以应对工作负载变化。但在测试过程中，团队发现新实例尚未就绪时系统便已启动更多实例。这一情况需立即调整。机器学习团队该如何解决此问题？"}, "option": [{"option_text": {"zhcn": "缩短缩容活动的冷却时间。调高实例的预设最大容量。", "enus": "Decrease the cooldown period for the scale-in activity. Increase the configured maximum capacity of instances."}, "option_flag": true}, {"option_text": {"zhcn": "将当前终端节点替换为基于SageMaker的多模型终端节点。", "enus": "Replace the current endpoint with a multi-model endpoint using SageMaker."}, "option_flag": false}, {"option_text": {"zhcn": "配置Amazon API Gateway与AWS Lambda服务，以触发SageMaker推理端点的调用。", "enus": "Set up Amazon API Gateway and AWS Lambda to trigger the SageMaker inference endpoint."}, "option_flag": false}, {"option_text": {"zhcn": "延长扩容活动的冷却时间。", "enus": "Increase the cooldown period for the scale-out activity."}, "option_flag": false}], "analysis": {"enus": "Reference: https://aws.amazon.com/blogs/machine-learning/configuring-autoscaling-inference-endpoints-in-amazon-sagemaker/", "zhcn": "参考来源：https://aws.amazon.com/blogs/machine-learning/configuring-autoscaling-inference-endpoints-in-amazon-sagemaker/"}, "answer": "A"}, {"id": "139", "question": {"enus": "A telecommunications company is developing a mobile app for its customers. The company is using an Amazon SageMaker hosted endpoint for machine learning model inferences. Developers want to introduce a new version of the model for a limited number of users who subscribed to a preview feature of the app. After the new version of the model is tested as a preview, developers will evaluate its accuracy. If a new version of the model has better accuracy, developers need to be able to gradually release the new version for all users over a fixed period of time. How can the company implement the testing model with the LEAST amount of operational overhead? ", "zhcn": "一家电信企业正为其客户开发一款移动应用。该公司采用亚马逊SageMaker托管终端进行机器学习模型推理。开发团队计划为订阅了应用预览功能的有限用户群体推出新版本模型。待新模型完成预览测试后，开发人员将评估其准确度。若新版模型表现更优，开发团队需能在固定周期内逐步向全体用户推送更新。如何以最低运维成本实现该测试方案？"}, "option": [{"option_text": {"zhcn": "通过调用CreateEndpointConfig操作并设置InitialVariantWeight参数为0，使用新版本模型更新ProductionVariant数据类型。针对已订阅预览功能的用户，需在InvokeEndpoint调用中指定TargetVariant参数。当新版模型完成发布准备时，逐步调高InitialVariantWeight数值，直至所有用户均获得更新后的版本。", "enus": "Update the ProductionVariant data type with the new version of the model by using the CreateEndpointConfig operation with the  InitialVariantWeight parameter set to 0. Specify the TargetVariant parameter for InvokeEndpoint calls for users who subscribed to the  preview feature. When the new version of the model is ready for release, gradually increase InitialVariantWeight until all users have the  updated version."}, "option_flag": false}, {"option_text": {"zhcn": "配置两个SageMaker托管端点，分别部署不同版本的模型。创建应用负载均衡器（ALB），根据TargetVariant查询字符串参数将流量分发至两个端点。针对已订阅预览功能的用户，调整应用程序配置使其发送TargetVariant查询参数。待新版本模型完成发布准备后，将ALB的路由策略调整为加权分配模式，直至所有用户均完成版本更新。", "enus": "Configure two SageMaker hosted endpoints that serve the different versions of the model. Create an Application Load Balancer (ALB)  to route trafic to both endpoints based on the TargetVariant query string parameter. Reconfigure the app to send the TargetVariant query  string parameter for users who subscribed to the preview feature. When the new version of the model is ready for release, change the  ALB's routing algorithm to weighted until all users have the updated version."}, "option_flag": false}, {"option_text": {"zhcn": "通过调用UpdateEndpointWeightsAndCapacities操作，将DesiredWeight参数设置为0，以此更新DesiredWeightsAndCapacities数据类型以适配模型的新版本。对于已订阅预览功能的用户，需在InvokeEndpoint调用中指定TargetVariant参数。待新版本模型完成发布准备后，逐步调高DesiredWeight数值，直至所有用户均获得更新版本。", "enus": "Update the DesiredWeightsAndCapacity data type with the new version of the model by using the  UpdateEndpointWeightsAndCapacities operation with the DesiredWeight parameter set to 0. Specify the TargetVariant parameter for  InvokeEndpoint calls for users who subscribed to the preview feature. When the new version of the model is ready for release, gradually  increase DesiredWeight until all users have the updated version."}, "option_flag": false}, {"option_text": {"zhcn": "配置两个SageMaker托管端点，分别部署不同版本的模型。创建一条采用简单路由策略的Amazon Route 53记录，将其指向当前正式版模型。将移动应用程序配置为：已订阅预览功能的用户使用新版端点URL，其余用户则访问Route 53记录指向的地址。当新版模型完成发布准备时，向Route 53添加新版本模型端点，并将路由策略切换为加权路由，逐步完成全体用户的版本更新。", "enus": "Configure two SageMaker hosted endpoints that serve the different versions of the model. Create an Amazon Route 53 record that is  configured with a simple routing policy and that points to the current version of the model. Configure the mobile app to use the endpoint  URL for users who subscribed to the preview feature and to use the Route 53 record for other users. When the new version of the model is  ready for release, add a new model version endpoint to Route 53, and switch the policy to weighted until all users have the updated  version."}, "option_flag": true}], "analysis": {"enus": "The correct answer to the question is to configure two SageMaker hosted endpoints for different model versions, create an Amazon Route 53 record with a simple routing policy pointing to the current model version, direct preview - subscribed users to the new - version endpoint, and use the Route 53 record for other users. When ready, add the new - version endpoint to Route 53 and switch to a weighted policy. This option has the least operational overhead because Route 53 is a DNS - based service. It is easy to configure and manage, and doesn't require setting up complex load - balancing rules or making in - depth API calls to update model weights directly in SageMaker.\n\nThe first fake option involves using the CreateEndpointConfig operation and setting InitialVariantWeight to 0. This requires more direct interaction with SageMaker's API and can be complex, especially when dealing with different user groups. The second fake option uses an Application Load Balancer (ALB). Setting up and managing an ALB adds extra infrastructure and complexity, such as configuring routing based on query string parameters. The third fake option uses the UpdateEndpointWeightsAndCapacities operation. Similar to the first option, it involves direct API calls to SageMaker, which can be error - prone and require a deeper understanding of SageMaker's internal workings. These complexities are the reasons why these options are not the best for minimizing operational overhead.", "zhcn": "针对此问题，正确答案是：为不同模型版本配置两个SageMaker托管终端节点，在Amazon Route 53中创建采用简单路由策略的解析记录指向当前模型版本，将预览用户（即订阅用户）引导至新版本终端节点，其余用户则通过Route 53记录访问。待准备就绪后，将新版本终端节点添加至Route 53并切换至权重路由策略。该方案具有最低运维成本，因为Route 53作为基于DNS的服务，配置管理简便，既无需设置复杂的负载均衡规则，也不必通过深层API调用直接修改SageMaker中的模型权重。\n\n第一个干扰选项通过调用CreateEndpointConfig操作并将InitialVariantWeight设为0实现。这种方法需直接与SageMaker API交互，操作复杂且难以应对不同用户群体的分流需求。第二个干扰选项采用应用负载均衡器（ALB），但部署管理ALB会引入额外的基础设施复杂度，例如需要配置基于查询字符串参数的路由规则。第三个干扰选项使用UpdateEndpointWeightsAndCapacities操作，其与第一选项类似，均涉及直接调用SageMaker API，不仅容易出错，还需深入理解SageMaker内部机制。正因存在这些复杂性，上述方案均非实现最低运维开销的最佳选择。"}, "answer": "D"}, {"id": "140", "question": {"enus": "A company offers an online shopping service to its customers. The company wants to enhance the site's security by requesting additional information when customers access the site from locations that are different from their normal location. The company wants to update the process to call a machine learning (ML) model to determine when additional information should be requested. The company has several terabytes of data from its existing ecommerce web servers containing the source IP addresses for each request made to the web server. For authenticated requests, the records also contain the login name of the requesting user. Which approach should an ML specialist take to implement the new security feature in the web application? ", "zhcn": "某公司为其客户提供在线购物服务。为提升网站安全性，公司计划在客户从非常用登录地点访问网站时要求额外验证信息。现需升级安全流程，通过调用机器学习模型智能判断何时启动附加验证机制。公司已积累数太字节的电子商务网络服务器数据，其中包含每次访问请求的源IP地址；对于已认证的请求，记录中还包含登录用户名。在此场景下，机器学习专家应当如何设计网站应用程序中的新型安全功能？"}, "option": [{"option_text": {"zhcn": "使用Amazon SageMaker Ground Truth对每条记录进行标注，将其归类为成功或失败的访问尝试。随后借助Amazon SageMaker平台，采用因子分解机(FM)算法训练二元分类模型。", "enus": "Use Amazon SageMaker Ground Truth to label each record as either a successful or failed access attempt. Use Amazon SageMaker to  train a binary classification model using the factorization machines (FM) algorithm."}, "option_flag": false}, {"option_text": {"zhcn": "运用亚马逊SageMaker平台，通过IP Insights算法训练模型。每日利用新增日志数据，对模型进行定时更新与重新训练。", "enus": "Use Amazon SageMaker to train a model using the IP Insights algorithm. Schedule updates and retraining of the model using new log  data nightly."}, "option_flag": false}, {"option_text": {"zhcn": "使用Amazon SageMaker Ground Truth对每条记录进行标注，将其判定为成功或失败的访问尝试。随后借助Amazon SageMaker平台，采用IP Insights算法训练二元分类模型。", "enus": "Use Amazon SageMaker Ground Truth to label each record as either a successful or failed access attempt. Use Amazon SageMaker to  train a binary classification model using the IP Insights algorithm."}, "option_flag": true}, {"option_text": {"zhcn": "运用Amazon SageMaker，通过Object2Vec算法训练模型。利用最新日志数据，于每晚定时进行模型更新与重新训练。", "enus": "Use Amazon SageMaker to train a model using the Object2Vec algorithm. Schedule updates and retraining of the model using new log  data nightly."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Use Amazon SageMaker Ground Truth to label each record as either a successful or failed access attempt. Use Amazon SageMaker to train a binary classification model using the IP Insights algorithm.” This approach is ideal because the company's goal is to identify abnormal access based on IP addresses, and the IP Insights algorithm is specifically designed to analyze IP - related data and detect anomalies. By labeling records as successful or failed access attempts, the model can learn patterns of normal and abnormal behavior.\n\nThe fake answer “Use Amazon SageMaker Ground Truth to label each record as either a successful or failed access attempt. Use Amazon SageMaker to train a binary classification model using the factorization machines (FM) algorithm” is incorrect because the FM algorithm is better suited for tasks like recommendation systems and predicting user preferences, not for IP - based anomaly detection.\n\nThe answer “Use Amazon SageMaker to train a model using the IP Insights algorithm. Schedule updates and retraining of the model using new log data nightly” lacks the crucial step of labeling the data. Without proper labels, the model cannot learn what constitutes normal and abnormal access.\n\nThe answer “Use Amazon SageMaker to train a model using the Object2Vec algorithm. Schedule updates and retraining of the model using new log data nightly” is wrong as the Object2Vec algorithm is for learning embeddings from various types of data and is not tailored for IP - based anomaly detection. The key factor here is the algorithm's suitability for the task, which is why the IP Insights - based approach is the correct choice.", "zhcn": "针对该问题的正确答案是：\"利用Amazon SageMaker Ground Truth将每条记录标记为成功或失败的访问尝试，再通过Amazon SageMaker平台采用IP Insights算法训练二元分类模型。\"此方案最为理想，因为企业的目标是根据IP地址识别异常访问，而IP Insights算法专为分析IP相关数据及异常检测设计。通过标记访问成功与失败的记录，模型能够学习正常与异常行为模式。\n\n错误答案中提到的\"采用因子分解机(FM)算法训练二元分类模型\"并不适用，因为FM算法更适用于推荐系统或用户偏好预测等场景，而非基于IP的异常检测。\n\n另一答案提出\"使用IP Insights算法训练模型并每晚定时更新重训练\"，但缺少关键的数据标记环节。若缺乏正确标签，模型将无法有效区分正常与异常访问行为。\n\n而\"采用Object2Vec算法进行模型训练并定时更新\"的方案同样不妥，因该算法主要用于从异构数据中学习嵌入表示，并非针对IP异常检测任务定制。核心在于算法与任务的匹配度，这正是基于IP Insights的方案成为正确选择的关键原因。"}, "answer": "C"}, {"id": "141", "question": {"enus": "A retail company wants to combine its customer orders with the product description data from its product catalog. The structure and format of the records in each dataset is different. A data analyst tried to use a spreadsheet to combine the datasets, but the effort resulted in duplicate records and records that were not properly combined. The company needs a solution that it can use to combine similar records from the two datasets and remove any duplicates. Which solution will meet these requirements? ", "zhcn": "一家零售企业希望将其客户订单数据与产品目录中的商品描述信息进行整合。然而这两个数据集中的记录结构和格式各不相同。数据分析师曾尝试用电子表格进行数据合并，但结果却出现了大量重复记录和匹配错位的问题。该公司亟需一种解决方案，能够智能整合两个数据集中相似的记录，并自动剔除重复项。请问以下哪种方案符合这些要求？"}, "option": [{"option_text": {"zhcn": "利用AWS Lambda函数处理数据，通过两个数组比对两数据集字段中的相同字符串，并清除所有重复项。", "enus": "Use an AWS Lambda function to process the data. Use two arrays to compare equal strings in the fields from the two datasets and  remove any duplicates."}, "option_flag": false}, {"option_text": {"zhcn": "为读取和填充AWS Glue数据目录创建AWS Glue爬虫程序。调用AWS Glue SearchTables API接口对两个数据集执行模糊匹配检索，并相应完成数据清洗工作。", "enus": "Create AWS Glue crawlers for reading and populating the AWS Glue Data Catalog. Call the AWS Glue SearchTables API operation to  perform a fuzzy- matching search on the two datasets, and cleanse the data accordingly."}, "option_flag": false}, {"option_text": {"zhcn": "为读取并填充AWS Glue数据目录，需创建AWS Glue爬虫程序。随后通过FindMatches转换功能实现数据清洗。", "enus": "Create AWS Glue crawlers for reading and populating the AWS Glue Data Catalog. Use the FindMatches transform to cleanse the data."}, "option_flag": false}, {"option_text": {"zhcn": "创建一项AWS Lake Formation自定义转换功能。通过Lake Formation控制台对匹配产品执行数据转换处理，实现数据的自动清洗。", "enus": "Create an AWS Lake Formation custom transform. Run a transformation for matching products from the Lake Formation console to  cleanse the data automatically."}, "option_flag": true}], "analysis": {"enus": "Reference: https://aws.amazon.com/lake-formation/features/", "zhcn": "参考链接：https://aws.amazon.com/lake-formation/features/"}, "answer": "D"}, {"id": "142", "question": {"enus": "A company provisions Amazon SageMaker notebook instances for its data science team and creates Amazon VPC interface endpoints to ensure communication between the VPC and the notebook instances. All connections to the Amazon SageMaker API are contained entirely and securely using the AWS network. However, the data science team realizes that individuals outside the VPC can still connect to the notebook instances across the internet. Which set of actions should the data science team take to fix the issue? ", "zhcn": "一家公司为其数据科学团队配置了Amazon SageMaker笔记本实例，并创建了Amazon VPC接口端点以确保VPC与笔记本实例间的通信。所有与Amazon SageMaker API的连接均通过AWS网络实现完全且安全的封闭传输。然而数据科学团队发现，VPC外部用户仍可通过互联网连接到这些笔记本实例。数据科学团队应采取哪组措施来解决此问题？"}, "option": [{"option_text": {"zhcn": "调整笔记本实例的安全组配置，仅允许来自VPC的CIDR地址范围的流量通行。将此安全组设置应用于所有笔记本实例的VPC网络接口。", "enus": "Modify the notebook instances' security group to allow trafic only from the CIDR ranges of the VPC. Apply this security group to all of  the notebook instances' VPC interfaces."}, "option_flag": false}, {"option_text": {"zhcn": "创建一项IAM策略，仅允许通过VPC终端节点执行`sagemaker:CreatePresignedNotebookInstanceUrl`和`sagemaker:DescribeNotebookInstance`操作。将此策略应用于所有用于访问笔记本实例的IAM用户组、群组及角色。", "enus": "Create an IAM policy that allows the sagemaker:CreatePresignedNotebooklnstanceUrl and sagemaker:DescribeNotebooklnstance  actions from only the VPC endpoints. Apply this policy to all IAM users, groups, and roles used to access the notebook instances."}, "option_flag": true}, {"option_text": {"zhcn": "为VPC添加NAT网关。将承载Amazon SageMaker笔记本实例的所有子网转换为私有子网。停止并重新启动所有笔记本实例，以仅重新分配私有IP地址。", "enus": "Add a NAT gateway to the VPC. Convert all of the subnets where the Amazon SageMaker notebook instances are hosted to private  subnets. Stop and start all of the notebook instances to reassign only private IP addresses."}, "option_flag": false}, {"option_text": {"zhcn": "请调整承载该笔记本的子网所关联的网络访问控制列表，以限制虚拟私有云外部的一切访问。", "enus": "Change the network ACL of the subnet the notebook is hosted in to restrict access to anyone outside the VPC."}, "option_flag": false}], "analysis": {"enus": "Reference: https://gmoein.github.io/files/Amazon%20SageMaker.pdf", "zhcn": "参考来源：https://gmoein.github.io/files/Amazon%20SageMaker.pdf"}, "answer": "B"}, {"id": "143", "question": {"enus": "A company will use Amazon SageMaker to train and host a machine learning (ML) model for a marketing campaign. The majority of data is sensitive customer data. The data must be encrypted at rest. The company wants AWS to maintain the root of trust for the master keys and wants encryption key usage to be logged. Which implementation will meet these requirements? ", "zhcn": "一家公司计划利用Amazon SageMaker平台，为某项营销活动训练并部署机器学习模型。其所涉及的大部分数据均属敏感的客户信息，必须实现静态加密。该公司要求由AWS托管主密钥的信任根，并记录加密密钥的使用情况。下列哪种实施方案能满足上述要求？"}, "option": [{"option_text": {"zhcn": "利用存储于AWS Cloud HSM的加密密钥，对机器学习数据卷进行加密处理，同时用于保护Amazon S3中的模型制品及相关数据的加密存储。", "enus": "Use encryption keys that are stored in AWS Cloud HSM to encrypt the ML data volumes, and to encrypt the model artifacts and data in  Amazon S3."}, "option_flag": false}, {"option_text": {"zhcn": "利用SageMaker内置临时密钥对机器学习数据卷进行加密。为新建的亚马逊弹性块存储卷启用默认加密功能。", "enus": "Use SageMaker built-in transient keys to encrypt the ML data volumes. Enable default encryption for new Amazon Elastic Block Store  (Amazon EBS) volumes."}, "option_flag": false}, {"option_text": {"zhcn": "在AWS密钥管理服务（AWS KMS）中采用客户托管密钥，对ML数据卷进行加密，并对Amazon S3中的模型制品及数据实施加密保护。", "enus": "Use customer managed keys in AWS Key Management Service (AWS KMS) to encrypt the ML data volumes, and to encrypt the model  artifacts and data in Amazon S3."}, "option_flag": true}, {"option_text": {"zhcn": "借助AWS安全令牌服务（AWS STS）生成临时令牌，用于加密机器学习存储卷，并对Amazon S3中的模型制品及数据进行加密保护。", "enus": "Use AWS Security Token Service (AWS STS) to create temporary tokens to encrypt the ML storage volumes, and to encrypt the model  artifacts and data in Amazon S3."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Use customer managed keys in AWS Key Management Service (AWS KMS) to encrypt the ML data volumes, and to encrypt the model artifacts and data in Amazon S3.” This meets the requirements as AWS KMS allows AWS to maintain the root of trust for the master keys. Also, it enables detailed logging of key usage through AWS CloudTrail, which is essential for the company's need to log encryption key usage.\n\nThe option of using “encryption keys stored in AWS Cloud HSM” is incorrect. While Cloud HSM offers high - security key storage, it requires the customer to manage the root of trust, which contradicts the company's requirement of having AWS maintain it.\n\nThe “SageMaker built - in transient keys” option won't work. Transient keys are short - lived and not suitable for long - term encryption of sensitive customer data. Moreover, enabling default EBS encryption doesn't fully address the need for logging key usage.\n\nUsing “AWS STS to create temporary tokens” is wrong. AWS STS is used for creating temporary credentials, not for encryption. It doesn't have the functionality to encrypt data at rest as required.\n\nThe key factors that make the real answer correct are its alignment with the requirements of AWS maintaining the root of trust and logging key usage, which the fake options fail to meet.", "zhcn": "针对该问题的正确答案是\"使用AWS密钥管理服务中的客户托管密钥对ML数据卷进行加密，并加密Amazon S3中的模型工件和数据\"。此方案符合要求，因为AWS KMS允许AWS保持主密钥的信任根，同时还能通过AWS CloudTrail记录详细的密钥使用日志，这对企业记录加密密钥使用情况的需求至关重要。\n\n而\"使用存储在AWS Cloud HSM中的加密密钥\"这一选项并不正确。尽管Cloud HSM提供高安全性的密钥存储，但需要客户自行管理信任根，这与企业要求AWS管理信任根的需求相悖。\n\n\"SageMaker内置临时密钥\"方案亦不可行。临时密钥有效期短暂，不适合用于长期加密敏感客户数据。此外，启用默认EBS加密功能并不能完全满足记录密钥使用情况的需求。\n\n至于\"使用AWS STS创建临时令牌\"的方案显然有误。AWS STS用于创建临时凭证而非加密功能，不具备加密静态数据所需的能力。\n\n正确答案的关键优势在于既符合AWS管理信任根的要求，又能实现密钥使用记录功能，而其他错误选项均未能同时满足这两项核心要求。"}, "answer": "C"}, {"id": "144", "question": {"enus": "A machine learning specialist stores IoT soil sensor data in Amazon DynamoDB table and stores weather event data as JSON files in Amazon S3. The dataset in DynamoDB is 10 GB in size and the dataset in Amazon S3 is 5 GB in size. The specialist wants to train a model on this data to help predict soil moisture levels as a function of weather events using Amazon SageMaker. Which solution will accomplish the necessary transformation to train the Amazon SageMaker model with the LEAST amount of administrative overhead? ", "zhcn": "一位机器学习专家将物联网土壤传感器数据存储于Amazon DynamoDB表中，同时把气象事件数据以JSON文件形式存放于Amazon S3内。DynamoDB内数据集规模为10GB，而Amazon S3中的数据集为5GB。该专家希望基于这些数据在Amazon SageMaker平台上训练模型，从而通过气象事件预测土壤湿度水平。在满足模型训练所需数据转换的前提下，下列哪种方案能实现管理成本最小化？"}, "option": [{"option_text": {"zhcn": "启动Amazon EMR集群，为DynamoDB表与S3数据创建Apache Hive外部表。对Hive表进行关联查询，并将结果输出至Amazon S3。", "enus": "Launch an Amazon EMR cluster. Create an Apache Hive external table for the DynamoDB table and S3 data. Join the Hive tables and  write the results out to Amazon S3."}, "option_flag": false}, {"option_text": {"zhcn": "利用AWS Glue爬虫程序抓取数据，编写一个AWS Glue ETL作业，将两个数据表进行合并，并将处理结果导入至Amazon Redshift集群。", "enus": "Crawl the data using AWS Glue crawlers. Write an AWS Glue ETL job that merges the two tables and writes the output to an Amazon  Redshift cluster."}, "option_flag": false}, {"option_text": {"zhcn": "为传感器数据表启用Amazon DynamoDB流功能。创建AWS Lambda函数处理该数据流，并将处理结果追加至Amazon S3存储桶内现有的气象文件中。", "enus": "Enable Amazon DynamoDB Streams on the sensor table. Write an AWS Lambda function that consumes the stream and appends the  results to the existing weather files in Amazon S3."}, "option_flag": true}, {"option_text": {"zhcn": "利用AWS Glue爬虫程序抓取数据，编写一个AWS Glue ETL作业，将两个表格合并，并以CSV格式将输出结果写入Amazon S3。", "enus": "Crawl the data using AWS Glue crawlers. Write an AWS Glue ETL job that merges the two tables and writes the output in CSV format to  Amazon S3."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is “Enable Amazon DynamoDB Streams on the sensor table. Write an AWS Lambda function that consumes the stream and appends the results to the existing weather files in Amazon S3.” This approach has the least administrative overhead because it directly integrates the DynamoDB data with the S3 data. DynamoDB Streams provides a real - time stream of table changes, and Lambda can easily process this stream and update the S3 files without the need to set up complex clusters or additional services.\n\nThe option of launching an Amazon EMR cluster and using Apache Hive requires setting up and managing an EMR cluster, which adds administrative complexity. Creating and managing Hive tables also involves extra steps.\n\nThe options of using AWS Glue crawlers and ETL jobs to merge the data and write to either an Amazon Redshift cluster or S3 in CSV format are more complex. AWS Glue crawlers need to discover and catalog the data, and the ETL jobs need to be configured and maintained. Writing to an Amazon Redshift cluster further adds the overhead of managing a data warehousing service. \n\nIn summary, the real answer option simplifies the data transformation process by leveraging existing AWS services in a straightforward way, minimizing administrative tasks compared to the fake answer options.", "zhcn": "针对该问题的正确答案是“在传感器表上启用Amazon DynamoDB数据流，编写一个AWS Lambda函数来消费该数据流，并将处理结果追加到Amazon S3中现有的气象文件中”。此方案具有最低的管理成本，因为它能直接将DynamoDB数据与S3数据集成。DynamoDB数据流可提供数据表的实时变更流，而Lambda函数能够轻松处理此流并更新S3文件，无需设置复杂集群或额外服务。\n\n相比之下，启动Amazon EMR集群并使用Apache Hive的方案需要配置和管理EMR集群，增加了管理复杂性。创建和维护Hive表同样涉及额外操作。\n\n采用AWS Glue爬虫和ETL作业合并数据并写入Amazon Redshift集群或CSV格式S3文件的方案更为复杂：既需要爬虫进行数据发现和编目，又需配置维护ETL作业。若写入Redshift集群，还需承担数据仓库服务的管理负担。\n\n综上所述，正确答案通过直接调用现有AWS服务简化了数据转换流程，相比其他干扰选项显著减少了管理性工作量。"}, "answer": "C"}, {"id": "145", "question": {"enus": "A company sells thousands of products on a public website and wants to automatically identify products with potential durability problems. The company has 1.000 reviews with date, star rating, review text, review summary, and customer email fields, but many reviews are incomplete and have empty fields. Each review has already been labeled with the correct durability result. A machine learning specialist must train a model to identify reviews expressing concerns over product durability. The first model needs to be trained and ready to review in 2 days. What is the MOST direct approach to solve this problem within 2 days? ", "zhcn": "一家公司在公开网站上销售数千种商品，并希望自动识别存在潜在耐用性问题的产品。该公司拥有1,000条包含日期、星级评分、评论内容、评论摘要和客户邮箱字段的评论数据，但许多评论存在字段缺失的情况。每条评论均已标注了正确的耐用性判定结果。机器学习专家需要训练一个模型，用于识别表达产品耐用性质疑的评论。首个模型必须在两天内完成训练并投入审核。要在两天内解决此问题，最直接的应对方案是什么？"}, "option": [{"option_text": {"zhcn": "利用 Amazon Comprehend 训练定制分类器。", "enus": "Train a custom classifier by using Amazon Comprehend."}, "option_flag": false}, {"option_text": {"zhcn": "在亚马逊SageMaker中运用Gluon与Apache MXNet构建循环神经网络（RNN）。", "enus": "Build a recurrent neural network (RNN) in Amazon SageMaker by using Gluon and Apache MXNet."}, "option_flag": true}, {"option_text": {"zhcn": "在Amazon SageMaker平台上，采用Word2Vec模式训练内置的BlazingText模型。", "enus": "Train a built-in BlazingText model using Word2Vec mode in Amazon SageMaker."}, "option_flag": false}, {"option_text": {"zhcn": "在Amazon SageMaker中使用内置的序列到序列模型。", "enus": "Use a built-in seq2seq model in Amazon SageMaker."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question “A company sells thousands of products on a public website and wants to automatically identify products with potential durability problems...” is “Build a recurrent neural network (RNN) in Amazon SageMaker by using Gluon and Apache MXNet.” This is because RNNs are well - suited for sequential data like review text, which can capture the context and sequence of words in the reviews to identify durability concerns. Gluon in Amazon SageMaker provides a high - level interface for building neural networks, and Apache MXNet offers efficient computation, enabling relatively quick model training within the 2 - day time frame.\n\nThe option “Train a custom classifier by using Amazon Comprehend” is incorrect. Amazon Comprehend is mainly for extracting insights from text such as entities, sentiment, and key phrases, not specifically designed for this type of custom classification task where we need to identify durability concerns from reviews.\n\n“Train a built - in BlazingText model using Word2Vec mode in Amazon SageMaker” is wrong because Word2Vec is focused on generating word embeddings, which is more about representing words in a vector space. While it can be part of a solution, it doesn't directly address the task of classifying reviews for durability concerns.\n\n“Use a built - in seq2seq model in Amazon SageMaker” is also incorrect. Seq2seq models are typically used for tasks like machine translation or text generation, not for the binary classification task of identifying durability concerns in product reviews. The real answer is selected as it directly targets the problem with an appropriate model for text classification and can be trained in the given time frame.", "zhcn": "对于“某公司在公开网站销售数千种商品，需要自动识别可能存在耐用性问题的产品”这一问题，正确答案是“在Amazon SageMaker中运用Gluon与Apache MXNet构建循环神经网络（RNN）”。这是因为RNN特别擅长处理评论文本这类序列数据，能捕捉评价中的上下文语义和词汇顺序，从而有效识别耐用性隐患。Amazon SageMaker中的Gluon提供了高级神经网络构建接口，结合Apache MXNet的高效计算能力，可在两天时限内完成模型训练。\n\n而“使用Amazon Comprehend训练自定义分类器”这一方案并不适用。因为Amazon Comprehend主要专注于从文本中提取实体、情感倾向和关键短语等要素，并非为这种需要从评论中精准识别耐用性问题的定制化分类任务而设计。\n\n“在Amazon SageMaker中使用Word2Vec模式训练内置BlazingText模型”的方案同样不妥。Word2Vec的核心功能在于生成词嵌入向量，其重点是将词汇映射到向量空间。虽然这可以作为解决方案的组成部分，但无法直接完成对评论进行耐用性问题分类的核心任务。\n\n“使用Amazon SageMaker内置seq2seq模型”亦非正解。Seq2seq模型通常应用于机器翻译或文本生成等场景，不适用于对商品评论进行耐用性问题判定的二元分类任务。因此，正确答案之所以被采纳，在于其通过合适的文本分类模型精准对应问题需求，且能在限定时间内完成训练。"}, "answer": "B"}, {"id": "146", "question": {"enus": "A company that runs an online library is implementing a chatbot using Amazon Lex to provide book recommendations based on category. This intent is fulfilled by an AWS Lambda function that queries an Amazon DynamoDB table for a list of book titles, given a particular category. For testing, there are only three categories implemented as the custom slot types: \"comedy,\" \"adventure,` and \"documentary.` A machine learning (ML) specialist notices that sometimes the request cannot be fulfilled because Amazon Lex cannot understand the category spoken by users with utterances such as \"funny,\" \"fun,\" and \"humor.\" The ML specialist needs to fix the problem without changing the Lambda code or data in DynamoDB. How should the ML specialist fix the problem? ", "zhcn": "一家运营在线图书馆的公司正利用Amazon Lex开发聊天机器人，旨在根据图书类别为用户推荐书籍。该功能由AWS Lambda函数实现，通过查询Amazon DynamoDB数据表，获取特定分类下的书籍清单。目前测试阶段仅设三种自定义槽位类别：\"喜剧\"、\"冒险\"和\"纪实\"。机器学习专家发现，当用户使用\"有趣的\"\"好玩儿\"\"幽默\"等表述时，系统时常无法识别类别导致推荐失败。在不修改Lambda代码或DynamoDB数据的前提下，这位专家应当如何解决该问题？"}, "option": [{"option_text": {"zhcn": "将枚举值列表中未识别的词汇添加为槽位类型的新值。", "enus": "Add the unrecognized words in the enumeration values list as new values in the slot type."}, "option_flag": false}, {"option_text": {"zhcn": "创建一个新的自定义槽位类型，将未识别的词汇作为枚举值添加至该类型，并将此槽位类型应用于对应槽位。", "enus": "Create a new custom slot type, add the unrecognized words to this slot type as enumeration values, and use this slot type for the slot."}, "option_flag": false}, {"option_text": {"zhcn": "利用AMAZON.SearchQuery内置槽类型，可在数据库中实现自定义检索功能。", "enus": "Use the AMAZON.SearchQuery built-in slot types for custom searches in the database."}, "option_flag": true}, {"option_text": {"zhcn": "将未识别词汇添加为自定义槽位类型的同义词。", "enus": "Add the unrecognized words as synonyms in the custom slot type."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Use the AMAZON.SearchQuery built - in slot types for custom searches in the database.” This is because the issue is that Amazon Lex fails to understand user - spoken category terms like “funny,” “fun,” and “humor” which are related to the “comedy” category. The AMAZON.SearchQuery built - in slot type can handle a wide range of user input for custom searches, allowing users to use various related terms without the need to explicitly define each one.\n\nThe fake answer “Add the unrecognized words in the enumeration values list as new values in the slot type” and “Create a new custom slot type, add the unrecognized words to this slot type as enumeration values, and use this slot type for the slot” are not ideal. These methods require manual addition of each unrecognized word, which can be cumbersome and may not cover all possible related terms. Also, it goes against the requirement of not changing the data in DynamoDB as more values in the slot type might indirectly affect how the Lambda function queries the database.\n\nThe option “Add the unrecognized words as synonyms in the custom slot type” has a similar problem. It still requires manual identification and addition of synonyms, and there may be an endless list of possible related terms that are difficult to cover comprehensively. The use of the built - in AMAZON.SearchQuery slot type provides a more flexible and efficient solution, which is why it is the real answer option.", "zhcn": "针对该问题的正确答案是\"使用AMAZON.SearchQuery内置槽类型进行数据库自定义搜索\"。这是因为当前问题在于Amazon Lex无法准确识别用户口语中与\"喜剧\"类别相关的表述（如\"搞笑\"\"有趣\"\"幽默\"等）。AMAZON.SearchQuery内置槽类型能够灵活处理各类用户输入的自定义搜索请求，无需明确定义每个相关术语即可兼容多种关联表达。\n\n而错误选项\"将未识别词汇添加至枚举值列表作为新值\"和\"创建新自定义槽类型，将未识别词汇添加为枚举值并应用于槽\"均非理想方案。这些方法需要人工逐个添加未识别词汇，操作繁琐且难以覆盖所有关联术语。此外，由于槽类型中更多数值可能间接影响Lambda函数查询数据库的方式，这种做法也违背了不更改DynamoDB数据的要求。\n\n至于\"将未识别词汇作为同义词添加至自定义槽类型\"这一选项，同样存在类似缺陷。该方法仍需人工识别并添加同义词，而潜在关联术语的数量可能趋于无限，难以全面覆盖。相较之下，使用内置的AMAZON.SearchQuery槽类型能提供更灵活高效的解决方案，故而被认定为正确选项。"}, "answer": "C"}, {"id": "147", "question": {"enus": "A manufacturing company uses machine learning (ML) models to detect quality issues. The models use images that are taken of the company's product at the end of each production step. The company has thousands of machines at the production site that generate one image per second on average. The company ran a successful pilot with a single manufacturing machine. For the pilot, ML specialists used an industrial PC that ran AWS IoT Greengrass with a long-running AWS Lambda function that uploaded the images to Amazon S3. The uploaded images invoked a Lambda function that was written in Python to perform inference by using an Amazon SageMaker endpoint that ran a custom model. The inference results were forwarded back to a web service that was hosted at the production site to prevent faulty products from being shipped. The company scaled the solution out to all manufacturing machines by installing similarly configured industrial PCs on each production machine. However, latency for predictions increased beyond acceptable limits. Analysis shows that the internet connection is at its capacity limit. How can the company resolve this issue MOST cost-effectively? ", "zhcn": "一家制造公司采用机器学习模型来检测产品质量问题。这些模型通过分析每道生产工序末端拍摄的产品图像进行质量监控。该企业生产线上部署了数千台设备，每台设备平均每秒生成一张图像。\n\n在单台设备试点阶段，公司取得了成功：机器学习专家采用工业计算机运行AWS IoT Greengrass平台，通过常驻AWS Lambda函数将图像上传至Amazon S3存储桶。上传图像会自动触发基于Python编写的Lambda函数，该函数调用运行定制模型的Amazon SageMaker终端节点进行推理分析，并将检测结果实时回传至生产现场部署的Web服务，有效拦截瑕疵品流出。\n\n当公司将此解决方案扩展至全部生产设备，为每台机器配置相同规格的工业计算机后，预测延迟却超出了可接受范围。经分析发现，现有网络带宽已达饱和状态。请问该公司如何以最具成本效益的方式解决此问题？"}, "option": [{"option_text": {"zhcn": "在生产站点与最近的AWS区域之间建立一条10 Gbps的AWS Direct Connect专用连接。通过该直连通道上传图像数据，并同步扩展SageMaker端点所使用实例的规格规模与部署数量。", "enus": "Set up a 10 Gbps AWS Direct Connect connection between the production site and the nearest AWS Region. Use the Direct Connect  connection to upload the images. Increase the size of the instances and the number of instances that are used by the SageMaker  endpoint."}, "option_flag": false}, {"option_text": {"zhcn": "将长期运行于AWS IoT Greengrass上的Lambda函数进行扩展，使其能够压缩图像并将压缩后的文件上传至Amazon S3。随后通过独立的Lambda函数解压这些文件，并调用现有Lambda函数启动推理流程。", "enus": "Extend the long-running Lambda function that runs on AWS IoT Greengrass to compress the images and upload the compressed files to  Amazon S3. Decompress the files by using a separate Lambda function that invokes the existing Lambda function to run the inference  pipeline."}, "option_flag": false}, {"option_text": {"zhcn": "为SageMaker配置自动扩缩容功能。在生产站点与最近的AWS区域之间建立AWS Direct Connect连接通道，通过该专用链路实现图像数据的上传。", "enus": "Use auto scaling for SageMaker. Set up an AWS Direct Connect connection between the production site and the nearest AWS Region.  Use the Direct Connect connection to upload the images."}, "option_flag": false}, {"option_text": {"zhcn": "将Lambda函数及机器学习模型部署至安装于每台工业计算机上的AWS IoT Greengrass核心系统。扩展在AWS IoT Greengrass上持续运行的Lambda函数，使其能够调用捕获图像的Lambda程序，并在边缘计算组件上执行推理分析，最终将结果直接传输至网络服务平台。", "enus": "Deploy the Lambda function and the ML models onto the AWS IoT Greengrass core that is running on the industrial PCs that are  installed on each machine. Extend the long-running Lambda function that runs on AWS IoT Greengrass to invoke the Lambda function with  the captured images and run the inference on the edge component that forwards the results directly to the web service."}, "option_flag": true}], "analysis": {"enus": "The correct answer to the question is to deploy the Lambda function and the ML models onto the AWS IoT Greengrass core running on the industrial PCs and extend the long - running Lambda function to run inference on the edge and forward results directly to the web service. This approach is the most cost - effective because it addresses the root cause of the latency issue, which is the over - capacity internet connection. By performing inference on the edge (local industrial PCs), the need to upload thousands of images per second to Amazon S3 and then use a SageMaker endpoint in the cloud is eliminated, reducing the strain on the internet connection.\n\nThe fake answer options all involve improving the network connection (using AWS Direct Connect) and potentially scaling up cloud resources (increasing SageMaker instances or using auto - scaling). These options are more expensive as they require additional infrastructure and bandwidth. For example, setting up a 10 Gbps AWS Direct Connect connection incurs significant costs for installation and ongoing usage. Compressing the images still requires uploading them to the cloud, which doesn't fully solve the latency problem and may add complexity with decompression steps. The common misconception with these fake options is that they focus on improving the cloud - based infrastructure and network connection rather than moving the inference process closer to the data source to reduce network traffic.", "zhcn": "针对该问题的正确答案是：将Lambda函数与机器学习模型部署在工业个人计算机运行的AWS IoT Greengrass核心上，并扩展长时间运行的Lambda函数，使其能在边缘端执行推理任务，同时将结果直接传输至网络服务。此方案最具成本效益，因为它从根源上解决了延迟问题——即网络带宽过载的症结。通过在边缘设备（本地工业计算机）上进行推理，无需每秒向亚马逊S3上传数千张图像，也无需调用云端SageMaker终端节点，从而显著减轻网络传输压力。</think></think>\n而错误答案选项均着眼于优化网络连接（如采用AWS直连服务）或扩充云端资源（如增加SageMaker实例或启用自动扩展）。这些方案成本更高，需额外投入基础设施与带宽资源。例如搭建10Gbps的AWS直连通道会产生高昂的安装与维护费用。即便对图像进行压缩处理，仍须将其上传至云端，这既无法彻底解决延迟问题，还可能因解压步骤增加系统复杂性。这些错误选项的共性误区在于：它们试图改进云端基础设施与网络连接，却未能将推理流程移至数据源头以降低网络负载。"}, "answer": "D"}, {"id": "148", "question": {"enus": "A data scientist is using an Amazon SageMaker notebook instance and needs to securely access data stored in a specific Amazon S3 bucket. How should the data scientist accomplish this? ", "zhcn": "一位数据科学家正在使用Amazon SageMaker笔记本实例，需安全访问特定Amazon S3存储桶中的数据。该数据科学家应如何实现此操作？"}, "option": [{"option_text": {"zhcn": "为Amazon SageMaker笔记本ARN添加S3存储桶策略，授予其作为主体的GetObject、PutObject和ListBucket权限。", "enus": "Add an S3 bucket policy allowing GetObject, PutObject, and ListBucket permissions to the Amazon SageMaker notebook ARN as  principal."}, "option_flag": false}, {"option_text": {"zhcn": "使用仅限笔记簿所有者有权访问的自定义AWS密钥管理服务（AWS KMS）密钥，对S3存储桶中的对象进行加密。", "enus": "Encrypt the objects in the S3 bucket with a custom AWS Key Management Service (AWS KMS) key that only the notebook owner has  access to."}, "option_flag": false}, {"option_text": {"zhcn": "将策略附加到与笔记本关联的IAM角色，该策略允许对特定S3存储桶执行GetObject、PutObject和ListBucket操作。", "enus": "Attach the policy to the IAM role associated with the notebook that allows GetObject, PutObject, and ListBucket operations to the  specific S3 bucket."}, "option_flag": true}, {"option_text": {"zhcn": "在实例的生命周期配置中，通过脚本为AWS CLI配置访问密钥ID与保密凭证。", "enus": "Use a script in a lifecycle configuration to configure the AWS CLI on the instance with an access key ID and secret."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question ‘A data scientist is using an Amazon SageMaker notebook instance and needs to securely access data stored in a specific Amazon S3 bucket. How should the data scientist accomplish this?’ is ‘Attach the policy to the IAM role associated with the notebook that allows GetObject, PutObject, and ListBucket operations to the specific S3 bucket.’ This approach is secure and in line with AWS best - practices. IAM roles are designed to manage permissions for AWS resources, and attaching a policy to the IAM role of the notebook instance ensures that the notebook has the necessary access to the S3 bucket.\n\n‘Add an S3 bucket policy allowing GetObject, PutObject, and ListBucket permissions to the Amazon SageMaker notebook ARN as principal’ is incorrect because it's more about bucket - level access control rather than instance - level. It can be complex to manage and may expose the bucket to unnecessary risks if misconfigured.\n\n‘Encrypt the objects in the S3 bucket with a custom AWS Key Management Service (AWS KMS) key that only the notebook owner has access to’ only addresses data encryption, not the actual access to the S3 bucket. Encryption secures the data at rest but does not grant the necessary permissions to read, write, or list objects.\n\n‘Use a script in a lifecycle configuration to configure the AWS CLI on the instance with an access key ID and secret’ is a bad practice as it involves hard - coding access keys. Access keys are sensitive credentials, and if leaked, can lead to unauthorized access to AWS resources. \n\nIn summary, using IAM roles to manage access to the S3 bucket is the most secure and recommended way, which is why the real answer option is correct and distinct from the fake options.", "zhcn": "针对“数据科学家在使用Amazon SageMaker笔记本实例时需要安全访问特定Amazon S3存储桶中的数据，应如何实现？”这一问题，正确答案是：“将与笔记本关联的IAM角色附加策略，允许其对特定S3存储桶执行GetObject、PutObject和ListBucket操作。”这种方法既安全又符合AWS最佳实践。IAM角色专用于管理AWS资源的权限，通过为笔记本实例的IAM角色附加策略，可确保其获得访问S3存储桶的必要权限。\n\n而“为S3存储桶添加策略，将Amazon SageMaker笔记本ARN作为主体并授予GetObject、PutObject和ListBucket权限”的方案并不恰当，因其侧重于存储桶层面的访问控制而非实例层面。这种管理方式较为复杂，若配置不当可能导致存储桶暴露于不必要的风险中。\n\n“使用仅笔记本所有者有权访问的自定义AWS KMS密钥加密S3存储桶中的对象”这一选项仅涉及数据加密，并未解决实际访问权限问题。加密可保护静态数据，但无法授予读取、写入或列出对象所需的权限。\n\n“通过生命周期配置脚本在实例上使用访问密钥ID和密钥配置AWS CLI”属于不良实践，因其涉及硬编码访问密钥。访问密钥属于敏感凭证，一旦泄露可能导致对AWS资源的未授权访问。\n\n综上，使用IAM角色管理S3存储桶访问权限是最安全且推荐的方法，这也是正确答案区别于其他错误选项的关键所在。"}, "answer": "C"}, {"id": "149", "question": {"enus": "A company is launching a new product and needs to build a mechanism to monitor comments about the company and its new product on social media. The company needs to be able to evaluate the sentiment expressed in social media posts, and visualize trends and configure alarms based on various thresholds. The company needs to implement this solution quickly, and wants to minimize the infrastructure and data science resources needed to evaluate the messages. The company already has a solution in place to collect posts and store them within an Amazon S3 bucket. What services should the data science team use to deliver this solution? ", "zhcn": "某公司即将推出一款新产品，需构建一套社交媒体舆情监测机制。该系统需具备以下能力：分析社交媒体帖子中表达的情绪倾向，通过可视化图表展示舆情趋势，并能根据多种阈值配置预警通知。鉴于项目需快速落地，且希望最大限度减少基础设施与数据科学资源的投入，而该公司已部署了将社交媒体帖子采集并存储至Amazon S3桶的现有方案。请问数据科学团队应采用哪些服务来实现此解决方案？"}, "option": [{"option_text": {"zhcn": "在亚马逊SageMaker平台运用BlazingText算法训练模型，用于分析社交媒体帖文语料库的情感倾向。通过部署可被AWS Lambda调用的服务端点，当S3存储桶新增帖文时自动触发Lambda函数，调用该端点进行情感分析，并将分析结果记录至Amazon DynamoDB数据表及自定义的Amazon CloudWatch指标中。借助CloudWatch告警机制，当出现情感趋势变化时及时向分析人员发送通知。", "enus": "Train a model in Amazon SageMaker by using the BlazingText algorithm to detect sentiment in the corpus of social media posts.  Expose an endpoint that can be called by AWS Lambda. Trigger a Lambda function when posts are added to the S3 bucket to invoke the  endpoint and record the sentiment in an Amazon DynamoDB table and in a custom Amazon CloudWatch metric. Use CloudWatch alarms to  notify analysts of trends."}, "option_flag": true}, {"option_text": {"zhcn": "在亚马逊SageMaker中运用语义分割算法训练模型，对社交媒体帖文集中的语义内容进行建模分析。通过AWS Lambda可调用的端点发布模型功能，当S3存储桶新增对象时自动触发Lambda函数，调用该端点并将情感分析结果记录至Amazon DynamoDB表。另设定时启动的第二个Lambda函数，用于查询近期新增记录，并通过亚马逊简单通知服务（Amazon SNS）向分析人员发送趋势动态通知。", "enus": "Train a model in Amazon SageMaker by using the semantic segmentation algorithm to model the semantic content in the corpus of  social media posts. Expose an endpoint that can be called by AWS Lambda. Trigger a Lambda function when objects are added to the S3  bucket to invoke the endpoint and record the sentiment in an Amazon DynamoDB table. Schedule a second Lambda function to query  recently added records and send an Amazon Simple Notification Service (Amazon SNS) notification to notify analysts of trends."}, "option_flag": false}, {"option_text": {"zhcn": "当社交媒体内容存入S3存储桶时，触发AWS Lambda功能运行。系统将调用Amazon Comprehend服务对每篇贴文进行情感分析，并将分析结果记录在Amazon DynamoDB数据表中。同时设定第二个定时启动的Lambda功能，用于查询近期新增记录，并通过Amazon简单通知服务（SNS）向分析人员发送趋势动态提醒。", "enus": "Trigger an AWS Lambda function when social media posts are added to the S3 bucket. Call Amazon Comprehend for each post to  capture the sentiment in the message and record the sentiment in an Amazon DynamoDB table. Schedule a second Lambda function to  query recently added records and send an Amazon Simple Notification Service (Amazon SNS) notification to notify analysts of trends."}, "option_flag": false}, {"option_text": {"zhcn": "当社交媒体内容存入S3存储桶时，触发AWS Lambda功能。通过Amazon Comprehend服务对每条内容进行情绪分析，将分析结果记录至定制化的Amazon CloudWatch指标及S3存储系统中。同时利用CloudWatch告警机制，实时向分析人员推送趋势动态。", "enus": "Trigger an AWS Lambda function when social media posts are added to the S3 bucket. Call Amazon Comprehend for each post to  capture the sentiment in the message and record the sentiment in a custom Amazon CloudWatch metric and in S3. Use CloudWatch  alarms to notify analysts of trends."}, "option_flag": false}], "analysis": {"enus": "The correct answer is to train a model in Amazon SageMaker using the BlazingText algorithm to detect sentiment, expose an endpoint for AWS Lambda, trigger Lambda when posts are added to S3, record sentiment in DynamoDB and a custom CloudWatch metric, and use CloudWatch alarms for notifications. This approach meets the requirements of quick implementation and minimizing infrastructure and data - science resources. The BlazingText algorithm is optimized for text classification tasks like sentiment analysis and can be trained efficiently. Storing data in DynamoDB and using CloudWatch for metrics and alarms provides a straightforward and integrated way to monitor trends.\n\nThe first fake option uses the semantic segmentation algorithm. This algorithm is typically used for image analysis, not text sentiment analysis, so it is an inappropriate choice for this task.\n\nThe second and third fake options rely on Amazon Comprehend. While Comprehend is a powerful service for natural language processing, using it for each post can be costly and may require more resources compared to training a custom model with BlazingText, especially when the company wants to minimize infrastructure and data - science resources. Also, the additional step of scheduling a second Lambda function to query records and send SNS notifications in the second option adds unnecessary complexity compared to using CloudWatch alarms directly.", "zhcn": "正确答案是：在Amazon SageMaker中运用BlazingText算法训练模型进行情感分析，通过AWS Lambda开放服务接口，当S3存储桶新增帖子时触发Lambda函数，将情感分析结果记录至DynamoDB数据库并生成自定义CloudWatch指标，最终通过CloudWatch警报机制实现自动通知。该方案既满足快速部署的要求，又能最大限度节约基础设施与数据科学资源。BlazingText算法专为文本分类任务（如情感分析）优化，训练效率极高。结合DynamoDB存储数据和CloudWatch监控警报的方案，为实现趋势监测提供了简洁高效的一体化解决路径。\n\n第一个错误选项采用了语义分割算法。该算法通常用于图像分析而非文本情感分析，因此在此场景中并不适用。\n\n第二和第三个错误选项均依赖Amazon Comprehend服务。虽然Comprehend是强大的自然语言处理工具，但针对每条帖子调用该服务会产生较高成本。相较于使用BlazingText训练定制化模型，这种方案需要投入更多资源，尤其不符合企业控制基础设施与数据科学资源消耗的需求。此外，第二个错误方案中通过额外调度Lambda函数查询记录并发送SNS通知的步骤，相较于直接使用CloudWatch警报机制，反而增加了不必要的操作复杂度。"}, "answer": "A"}, {"id": "150", "question": {"enus": "A bank wants to launch a low-rate credit promotion. The bank is located in a town that recently experienced economic hardship. Only some of the bank's customers were affected by the crisis, so the bank's credit team must identify which customers to target with the promotion. However, the credit team wants to make sure that loyal customers' full credit history is considered when the decision is made. The bank's data science team developed a model that classifies account transactions and understands credit eligibility. The data science team used the XGBoost algorithm to train the model. The team used 7 years of bank transaction historical data for training and hyperparameter tuning over the course of several days. The accuracy of the model is suficient, but the credit team is struggling to explain accurately why the model denies credit to some customers. The credit team has almost no skill in data science. What should the data science team do to address this issue in the MOST operationally eficient manner? ", "zhcn": "某银行计划推出一项低利率信贷促销活动。该银行所在城镇近期遭遇经济困境，但仅部分客户受到危机影响，因此信贷部门需精准筛选促销活动的目标客群。与此同时，信贷团队强调必须充分考量忠诚客户的完整信用记录。银行数据科学团队已开发出一套能分类账户交易并评估信贷资质的模型，该模型采用XGBoost算法，经过长达数天的训练及超参数优化，并使用了七年期的银行交易历史数据。虽然模型准确度达到要求，但信贷团队难以向客户解释模型拒绝授信的具体原因，且该团队几乎不具备数据科学专业知识。在此情况下，数据科学团队应采取何种最具运营效率的解决方案？"}, "option": [{"option_text": {"zhcn": "使用Amazon SageMaker Studio重新构建模型。创建一个笔记本文档，调用XGBoost训练容器执行模型训练任务。将训练完成的模型部署至终端节点。启用Amazon SageMaker Model Monitor功能以存储推理结果，并基于这些结果生成沙普利值（Shapley values），用以解析模型决策逻辑。最终生成特征与SHAP（沙普利加性解释）值对应关系图，向信贷团队直观展示不同特征对模型输出结果的影响机制。", "enus": "Use Amazon SageMaker Studio to rebuild the model. Create a notebook that uses the XGBoost training container to perform model  training. Deploy the model at an endpoint. Enable Amazon SageMaker Model Monitor to store inferences. Use the inferences to create  Shapley values that help explain model behavior. Create a chart that shows features and SHapley Additive exPlanations (SHAP) values to  explain to the credit team how the features affect the model outcomes."}, "option_flag": false}, {"option_text": {"zhcn": "使用 Amazon SageMaker Studio 重新构建模型。创建一个基于 XGBoost 训练容器的笔记本来执行模型训练任务，同时启用 Amazon SageMaker Debugger 并配置其计算并收集 Shapley 值。最终生成特征与 SHAP 值（SHapley Additive exPlanations）关联图表，向信贷团队直观展示各特征对模型结果的影响机制。", "enus": "Use Amazon SageMaker Studio to rebuild the model. Create a notebook that uses the XGBoost training container to perform model  training. Activate Amazon SageMaker Debugger, and configure it to calculate and collect Shapley values. Create a chart that shows  features and SHapley Additive exPlanations (SHAP) values to explain to the credit team how the features affect the model outcomes."}, "option_flag": false}, {"option_text": {"zhcn": "创建Amazon SageMaker笔记本实例。通过该笔记本实例并利用XGBoost库对模型进行本地重训练。运用Python版XGBoost接口中的plot_importance()方法生成特征重要性图表，并借助该图表向信贷团队阐释各特征如何影响模型输出结果。", "enus": "Create an Amazon SageMaker notebook instance. Use the notebook instance and the XGBoost library to locally retrain the model. Use  the plot_importance() method in the Python XGBoost interface to create a feature importance chart. Use that chart to explain to the credit  team how the features affect the model outcomes."}, "option_flag": true}, {"option_text": {"zhcn": "使用Amazon SageStudio重新构建模型。创建基于XGBoost训练容器的笔记本来执行模型训练，并将模型部署至终端节点。通过Amazon SageMaker Processing对模型进行后续分析，自动生成特征重要性可解释性图表供信贷团队使用。", "enus": "Use Amazon SageMaker Studio to rebuild the model. Create a notebook that uses the XGBoost training container to perform model  training. Deploy the model at an endpoint. Use Amazon SageMaker Processing to post-analyze the model and create a feature importance  explainability chart automatically for the credit team."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to create an Amazon SageMaker notebook instance, use the XGBoost library to locally retrain the model, and use the plot_importance() method to create a feature importance chart. This approach is the most operationally efficient because it builds on the existing model and uses a simple, straightforward method to generate feature importance. It doesn't require rebuilding the model, deploying it to an endpoint, or using complex tools like Model Monitor, Debugger, or Processing.\n\nThe fake answer options all involve rebuilding the model using Amazon SageMaker Studio, which is a time - consuming process. Additionally, using Shapley values (SHAP) to explain the model, as in some of the fake options, is more complex and resource - intensive. The credit team has little data science skill, and a simple feature importance chart is easier to understand compared to the more complex SHAP values. The option of automatically creating an explainability chart via SageMaker Processing also adds unnecessary complexity, as the plot_importance() method provides a quick and easy way to understand feature influence. This simplicity and directness are why the real answer is the best choice in this scenario.", "zhcn": "问题的正确答案是创建一个Amazon SageMaker笔记本实例，利用XGBoost库对模型进行本地重训练，并通过plot_importance()方法生成特征重要性图表。这种方法在操作上最为高效，因为它基于现有模型进行优化，且采用简单直观的方式呈现特征重要性。无需重新构建模型、部署至端点，也无需使用Model Monitor、Debugger或Processing等复杂工具。  \n\n虚假答案选项均涉及通过Amazon SageMaker Studio重新构建模型，这一过程相当耗时。此外，部分错误选项中使用的SHAP值模型解释方法不仅复杂度高，还会消耗更多资源。考虑到信贷团队数据科学基础较为薄弱，相比复杂的SHAP值解读，简洁的特征重要性图表更易于理解。而通过SageMaker Processing自动生成可解释性图表的方案也显得画蛇添足——plot_importance()方法本身就能以轻量直观的方式展现特征影响。正是这种简洁高效的特性，使得原答案成为此场景下的最优选择。"}, "answer": "C"}, {"id": "151", "question": {"enus": "A data science team is planning to build a natural language processing (NLP) application. The application's text preprocessing stage will include part-of-speech tagging and key phase extraction. The preprocessed text will be input to a custom classification algorithm that the data science team has already written and trained using Apache MXNet. Which solution can the team build MOST quickly to meet these requirements? ", "zhcn": "一个数据科学团队正计划构建自然语言处理应用。该应用的文本预处理阶段将包含词性标注与关键短语提取功能。经过预处理的文本将输入至团队已基于Apache MXNet框架编写并训练完成的自定义分类算法中。为满足这些需求，团队最快能采用何种解决方案？"}, "option": [{"option_text": {"zhcn": "利用Amazon Comprehend完成词性标注、关键短语提取及文本分类任务。", "enus": "Use Amazon Comprehend for the part-of-speech tagging, key phase extraction, and classification tasks."}, "option_flag": false}, {"option_text": {"zhcn": "在亚马逊SageMaker中调用自然语言处理库进行词性标注，通过Amazon Comprehend服务实现关键短语提取，并基于AWS深度学习容器与Amazon SageMaker构建定制化分类器。", "enus": "Use an NLP library in Amazon SageMaker for the part-of-speech tagging. Use Amazon Comprehend for the key phase extraction. Use  AWS Deep Learning Containers with Amazon SageMaker to build the custom classifier."}, "option_flag": true}, {"option_text": {"zhcn": "利用Amazon Comprehend完成词性标注与关键短语提取任务，并采用Amazon SageMaker内置的潜在狄利克雷分布（LDA）算法构建定制化分类器。", "enus": "Use Amazon Comprehend for the part-of-speech tagging and key phase extraction tasks. Use Amazon SageMaker built-in Latent  Dirichlet Allocation (LDA) algorithm to build the custom classifier."}, "option_flag": false}, {"option_text": {"zhcn": "在词性标注与关键短语提取任务中运用Amazon Comprehend服务。通过搭载AWS深度学习容器的Amazon SageMaker平台来构建定制化分类器。", "enus": "Use Amazon Comprehend for the part-of-speech tagging and key phase extraction tasks. Use AWS Deep Learning Containers with  Amazon SageMaker to build the custom classifier."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question “A data science team is planning to build a natural language processing (NLP) application... Which solution can the team build MOST quickly to meet these requirements?” is “Use an NLP library in Amazon SageMaker for the part - of - speech tagging. Use Amazon Comprehend for the key phase extraction. Use AWS Deep Learning Containers with Amazon SageMaker to build the custom classifier.”\n\nThe team has a custom classification algorithm already written and trained using Apache MXNet. AWS Deep Learning Containers with Amazon SageMaker is a suitable choice to quickly deploy this custom classifier as it provides an optimized environment for deep - learning models. Amazon Comprehend is efficient for key phrase extraction. And using an NLP library in Amazon SageMaker for part - of - speech tagging allows for flexibility and quick implementation.\n\nThe first fake option “Use Amazon Comprehend for the part - of - speech tagging, key phase extraction, and classification tasks” is incorrect because the team has a custom classification algorithm, and using Amazon Comprehend for classification would not utilize their pre - written and trained custom algorithm.\n\nThe second fake option “Use Amazon Comprehend for the part - of - speech tagging and key phase extraction tasks. Use Amazon SageMaker built - in Latent Dirichlet Allocation (LDA) algorithm to build the custom classifier” is wrong. The team already has a custom classification algorithm, and using the built - in LDA algorithm would not use their existing work.\n\nThe third fake option “Use Amazon Comprehend for the part - of - speech tagging and key phase extraction tasks. Use AWS Deep Learning Containers with Amazon SageMaker to build the custom classifier” misses out on leveraging an NLP library in Amazon SageMaker for part - of - speech tagging, which could potentially lead to a less efficient or slower implementation for that specific task.", "zhcn": "针对“数据科学团队计划开发一个自然语言处理（NLP）应用程序...哪种方案能最快速满足需求？”这一问题，正确答案是：“使用Amazon SageMaker中的NLP库实现词性标注，通过Amazon Comprehend进行关键短语提取，并采用搭载AWS深度学习容器的Amazon SageMaker部署定制分类器。”\n\n该团队已拥有基于Apache MXNet编写并训练完成的定制分类算法。选用搭载AWS深度学习容器的Amazon SageMaker可快速部署该定制分类器，因其为深度学习模型提供了优化环境。Amazon Comprehend能高效处理关键短语提取任务，而利用Amazon SageMaker中的NLP库实施词性标注则兼具灵活性与快速部署优势。\n\n首项干扰项“完全使用Amazon Comprehend处理词性标注、关键短语提取及分类任务”存在谬误：团队既已具备定制分类算法，若采用Amazon Comprehend进行分类将导致既有成果无法复用。\n\n次项干扰项“组合使用Amazon Comprehend处理词性标注与关键短语提取，并采用Amazon SageMaker内置LDA算法构建分类器”同样错误：团队既有的定制分类算法若被内置LDA算法替代，将造成现有工作成果的浪费。\n\n第三项干扰项“采用Amazon Comprehend处理词性标注与关键短语提取，配合AWS深度学习容器构建分类器”的缺陷在于：未能利用Amazon SageMaker的NLP库实施词性标注，可能导致该环节的执行效率降低或延迟。"}, "answer": "B"}, {"id": "152", "question": {"enus": "A machine learning (ML) specialist must develop a classification model for a financial services company. A domain expert provides the dataset, which is tabular with 10,000 rows and 1,020 features. During exploratory data analysis, the specialist finds no missing values and a small percentage of duplicate rows. There are correlation scores of > 0.9 for 200 feature pairs. The mean value of each feature is similar to its 50th percentile. Which feature engineering strategy should the ML specialist use with Amazon SageMaker? ", "zhcn": "一位机器学习专家需要为某金融服务公司开发分类模型。领域专家提供的数据集为表格形式，包含一万行数据和一千零二十个特征。在探索性数据分析阶段，专家发现数据不存在缺失值，且重复行比例极低。其中两百组特征对呈现高于0.9的相关性系数，而各特征的均值与其五十分位数值较为接近。此时，该机器学习专家应当如何在Amazon SageMaker平台上制定特征工程策略？"}, "option": [{"option_text": {"zhcn": "采用主成分分析（PCA）算法进行降维处理。", "enus": "Apply dimensionality reduction by using the principal component analysis (PCA) algorithm."}, "option_flag": false}, {"option_text": {"zhcn": "在Jupyter notebook中剔除相关度较低的变量。", "enus": "Drop the features with low correlation scores by using a Jupyter notebook."}, "option_flag": false}, {"option_text": {"zhcn": "运用随机切割森林（RCF）算法实施异常检测。", "enus": "Apply anomaly detection by using the Random Cut Forest (RCF) algorithm."}, "option_flag": true}, {"option_text": {"zhcn": "在Jupyter notebook中，将具有高相关性的特征加以整合串联。", "enus": "Concatenate the features with high correlation scores by using a Jupyter notebook."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Apply anomaly detection by using the Random Cut Forest (RCF) algorithm.” The data has a small percentage of duplicate rows and high - correlation among 200 feature pairs, and the mean of each feature is close to its 50th percentile. This indicates that the distribution of features is relatively stable, and anomalies may be more of a concern. The RCF algorithm in Amazon SageMaker is well - suited for identifying anomalies in such tabular data.\n\nThe option “Apply dimensionality reduction by using the principal component analysis (PCA) algorithm” is incorrect. Although there are high - correlation pairs, the question doesn't emphasize the need for dimensionality reduction. The problem description doesn't suggest that the high number of features is causing issues like overfitting or slow training.\n\n“Drop the features with low correlation scores by using a Jupyter notebook” is wrong because the question focuses on high - correlation pairs, and there's no indication that low - correlation features are a problem. Removing low - correlation features might discard useful information.\n\n“Concatenate the features with high correlation scores by using a Jupyter notebook” is also incorrect. Concatenating highly correlated features doesn't address any of the potential issues in the data. It doesn't help with anomaly detection or improving the model's performance. The key here is to detect potential anomalies, which is why the RCF algorithm is the appropriate choice.", "zhcn": "对于该问题的正确答案是\"运用随机切割森林（RCF）算法实施异常检测\"。数据集中存在少量重复行，且200组特征对之间呈现高度相关性，各特征的均值均接近其五十分位数。这表明特征分布相对稳定，异常值可能更值得关注。亚马逊SageMaker中的RCF算法特别适合在此类表格数据中识别异常。\n\n\"采用主成分分析（PCA）算法进行降维\"这一选项并不恰当。尽管存在高度相关特征对，但问题描述并未强调降维的必要性，也未提及特征数量过多导致过拟合或训练速度缓慢等问题。\n\n\"通过Jupyter笔记本剔除低相关性特征\"的做法有误，因为问题焦点在于高相关性特征对，且无证据表明低相关性特征会引发问题。盲目移除低相关性特征反而可能损失有效信息。\n\n\"使用Jupyter笔记本拼接高相关性特征\"同样不可取。简单拼接高相关性特征既无法解决数据中的潜在问题，也无助于异常检测或模型性能提升。关键在于探测潜在异常值，这正是RCF算法成为最佳选择的原因。"}, "answer": "C"}, {"id": "153", "question": {"enus": "A manufacturing company asks its machine learning specialist to develop a model that classifies defective parts into one of eight defect types. The company has provided roughly 100,000 images per defect type for training. During the initial training of the image classification model, the specialist notices that the validation accuracy is 80%, while the training accuracy is 90%. It is known that human-level performance for this type of image classification is around 90%. What should the specialist consider to fix this issue? ", "zhcn": "一家制造企业委托其机器学习专家开发一款模型，旨在将次品零件按八种缺陷类型进行分类。企业为每种缺陷类型提供了约十万张训练图像。在图像分类模型的初步训练阶段，专家发现验证集准确率为80%，而训练集准确率达90%。已知此类图像分类任务的人类判断准确率约为90%。针对这一差异，专家应从哪些方面着手改进？"}, "option": [{"option_text": {"zhcn": "延长训练时长", "enus": "A longer training time"}, "option_flag": false}, {"option_text": {"zhcn": "扩大网络规模", "enus": "Making the network larger"}, "option_flag": false}, {"option_text": {"zhcn": "采用另一种优化器", "enus": "Using a different optimizer"}, "option_flag": false}, {"option_text": {"zhcn": "采用某种正则化手段", "enus": "Using some form of regularization"}, "option_flag": true}], "analysis": {"enus": "Reference: https://acloud.guru/forums/aws-certified-machine-learning-specialty/discussion/- MGdBUKmQ02zC3uOq4VL/AWS%20Exam%20Machine%20Learning", "zhcn": "参考来源：https://acloud.guru/forums/aws-certified-machine-learning-specialty/discussion/-MGdBUKmQ02zC3uOq4VL/AWS%20Exam%20Machine%20Learning"}, "answer": "D"}, {"id": "154", "question": {"enus": "A machine learning specialist needs to analyze comments on a news website with users across the globe. The specialist must find the most discussed topics in the comments that are in either English or Spanish. What steps could be used to accomplish this task? (Choose two.) ", "zhcn": "一位机器学习专家需要分析某全球性新闻网站的用户评论。该专家必须从英文或西班牙文评论中找出最受热议的话题。下列哪两个步骤可用于完成此任务？"}, "option": [{"option_text": {"zhcn": "运用亚马逊SageMaker平台的BlazingText算法，可跨越语言界限自主识别文本主题。请依此展开分析。", "enus": "Use an Amazon SageMaker BlazingText algorithm to find the topics independently from language. Proceed with the analysis."}, "option_flag": false}, {"option_text": {"zhcn": "如确有必要，可采用亚马逊SageMaker序列到序列算法将西班牙语内容译为英文。同时运用SageMaker潜在狄利克雷分布（LDA）算法进行主题挖掘。", "enus": "Use an Amazon SageMaker seq2seq algorithm to translate from Spanish to English, if necessary. Use a SageMaker Latent Dirichlet  Allocation (LDA) algorithm to find the topics."}, "option_flag": true}, {"option_text": {"zhcn": "如需要，可使用Amazon Translate将西班牙语译为英语，并运用Amazon Comprehend主题建模功能进行主题分析。", "enus": "Use Amazon Translate to translate from Spanish to English, if necessary. Use Amazon Comprehend topic modeling to find the topics."}, "option_flag": false}, {"option_text": {"zhcn": "如需要，可使用Amazon Translate将西班牙语内容译为英语，并运用Amazon Lex从文本中提取主题信息。", "enus": "Use Amazon Translate to translate from Spanish to English, if necessary. Use Amazon Lex to extract topics form the content."}, "option_flag": false}, {"option_text": {"zhcn": "如需要，可使用Amazon Translate将西班牙语内容译为英语。随后运用Amazon SageMaker神经主题模型（NTM）进行主题挖掘。", "enus": "Use Amazon Translate to translate from Spanish to English, if necessary. Use Amazon SageMaker Neural Topic Model (NTM) to find the  topics."}, "option_flag": false}], "analysis": {"enus": "Reference: https://docs.aws.amazon.com/sagemaker/latest/dg/lda.html", "zhcn": "参考文档：亚马逊AWS SageMaker LDA算法指南  \n（原文链接：https://docs.aws.amazon.com/sagemaker/latest/dg/lda.html）"}, "answer": "B"}, {"id": "155", "question": {"enus": "A machine learning (ML) specialist is administering a production Amazon SageMaker endpoint with model monitoring configured. Amazon SageMaker Model Monitor detects violations on the SageMaker endpoint, so the ML specialist retrains the model with the latest dataset. This dataset is statistically representative of the current production trafic. The ML specialist notices that even after deploying the new SageMaker model and running the first monitoring job, the SageMaker endpoint still has violations. What should the ML specialist do to resolve the violations? ", "zhcn": "一位机器学习专家正在管理一个已配置模型监控功能的亚马逊SageMaker生产终端。当亚马逊SageMaker模型监控器检测到该终端出现违规行为时，该专家使用最新数据集对模型进行重新训练。该数据集能准确反映当前生产环境的数据特征。然而专家发现，即使部署了新模型并运行首次监控任务后，终端仍存在违规现象。此时应采取何种措施以消除这些违规行为？"}, "option": [{"option_text": {"zhcn": "手动触发监控任务，重新评估SageMaker端点的流量样本。", "enus": "Manually trigger the monitoring job to re-evaluate the SageMaker endpoint trafic sample."}, "option_flag": false}, {"option_text": {"zhcn": "请针对新的训练集再次运行模型监控基线任务，并将模型监控配置为采用新的基线标准。", "enus": "Run the Model Monitor baseline job again on the new training set. Configure Model Monitor to use the new baseline."}, "option_flag": true}, {"option_text": {"zhcn": "删除该终端节点，并按照原有配置重新创建。", "enus": "Delete the endpoint and recreate it with the original configuration."}, "option_flag": false}, {"option_text": {"zhcn": "使用原始训练集与新训练集的组合，再次对模型进行训练。", "enus": "Retrain the model again by using a combination of the original training set and the new training set."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is “Run the Model Monitor baseline job again on the new training set. Configure Model Monitor to use the new baseline.” When the model is retrained with a new dataset, the statistical properties of the data may have changed. The original baseline used by Model Monitor is based on the old data, so it may no longer accurately represent the new model's behavior. By running a new baseline job on the new training set and configuring Model Monitor to use this new baseline, the monitoring process can accurately detect violations according to the new model's characteristics.\n\nThe option “Manually trigger the monitoring job to re - evaluate the SageMaker endpoint traffic sample” is incorrect because without updating the baseline, the monitoring job will still be using the old criteria, and violations may continue to be detected even if there aren't real issues with the new model.\n\n“Delete the endpoint and recreate it with the original configuration” is wrong as it doesn't address the root cause of the problem, which is the misalignment between the old baseline and the new model. The original configuration was based on the old model and data.\n\n“Retrain the model again by using a combination of the original training set and the new training set” is not the right approach. The new training set was already statistically representative of the current production traffic. Retraining with the combined set doesn't solve the issue of the incorrect baseline used for monitoring.\n\nIn summary, updating the baseline to match the new model is the key step to resolve the violations, which is why the real answer option is correct and the fake options are not.", "zhcn": "针对该问题的正确答案是：\"在新训练集上重新运行模型监控基线任务，并配置模型监控器使用新基线。\"当模型使用新数据集进行重新训练后，数据的统计特性可能已发生变化。模型监控原先使用的基线基于旧数据生成，可能无法准确反映新模型的行为特征。通过在新训练集上生成新基线并更新监控配置，可使监测流程根据新模型特性精准识别异常情况。\n\n而\"手动触发监控任务重新评估SageMaker端点流量样本\"这一选项并不正确，因为若不更新基线，监控任务仍将沿用旧标准，即使新模型不存在实际问题，仍会持续误报异常。\n\n\"删除端点并按原配置重建\"的做法亦不可取，这并未解决旧基线与新模型不匹配的根本问题，原有配置本就基于旧模型和数据设定。\n\n\"使用原始训练集与新训练集合并后重新训练模型\"并非解决之道。新训练集本身已具备当前生产流量的统计代表性，合并训练并不能修正监控所用基线失准的问题。\n\n综上所述，更新基线以匹配新模型是消除异常告警的关键步骤，这也印证了正确选项的合理性及错误选项的谬误所在。"}, "answer": "B"}, {"id": "156", "question": {"enus": "A company supplies wholesale clothing to thousands of retail stores. A data scientist must create a model that predicts the daily sales volume for each item for each store. The data scientist discovers that more than half of the stores have been in business for less than 6 months. Sales data is highly consistent from week to week. Daily data from the database has been aggregated weekly, and weeks with no sales are omitted from the current dataset. Five years (100 MB) of sales data is available in Amazon S3. Which factors will adversely impact the performance of the forecast model to be developed, and which actions should the data scientist take to mitigate them? (Choose two.) ", "zhcn": "一家公司向数千家零售门店供应服装批发业务。某数据科学家需构建一个模型，用于预测各门店每款商品的日销售量。该科学家发现，超过半数的门店开业时间不足六个月。销售数据在周与周之间呈现高度一致性。数据库中的每日数据已按周进行汇总，且当前数据集中已剔除无销售记录的周次。亚马逊S3平台存有五年累计100MB的销售数据。哪些因素会对拟开发的预测模型性能产生不利影响？数据科学家应采取哪两项措施来缓解这些影响？（请选择两项）"}, "option": [{"option_text": {"zhcn": "多数门店的季节性特征难以准确判定，需获取分类数据以便将新店与历史数据更完备的同类门店进行关联分析。", "enus": "Detecting seasonality for the majority of stores will be an issue. Request categorical data to relate new stores with similar stores that  have more historical data."}, "option_flag": true}, {"option_text": {"zhcn": "当前销售数据变异度不足，需引入跨行业的外部销售数据以增强模型的泛化能力。", "enus": "The sales data does not have enough variance. Request external sales data from other industries to improve the model's ability to  generalize."}, "option_flag": true}, {"option_text": {"zhcn": "销售数据按周汇总。需从源数据库获取每日销售数据，以便构建每日分析模型。", "enus": "Sales data is aggregated by week. Request daily sales data from the source database to enable building a daily model."}, "option_flag": false}, {"option_text": {"zhcn": "销售数据中缺失了商品销售额为零的条目。请确保源数据库提供的商品销售数据包含零值记录，以便顺利构建分析模型。", "enus": "The sales data is missing zero entries for item sales. Request that item sales data from the source database include zero entries to  enable building the model."}, "option_flag": false}, {"option_text": {"zhcn": "目前亚马逊S3中仅有100MB销售数据可用。需申请获取长达十年的销售数据，这将为模型提供200MB的训练数据。", "enus": "Only 100 MB of sales data is available in Amazon S3. Request 10 years of sales data, which would provide 200 MB of training data for  the model."}, "option_flag": false}], "analysis": {"enus": "Reference: https://towardsdatascience.com/sales-forecasting-from-time-series-to-deep-learning-5d115514bfac https://arxiv.org/ftp/arxiv/papers/1302/1302.6613.pdf", "zhcn": "参考文献：  \nhttps://towardsdatascience.com/sales-forecasting-from-time-series-to-deep-learning-5d115514bfac  \nhttps://arxiv.org/ftp/arxiv/papers/1302/1302.6613.pdf"}, "answer": "AB"}, {"id": "157", "question": {"enus": "An ecommerce company is automating the categorization of its products based on images. A data scientist has trained a computer vision model using the Amazon SageMaker image classification algorithm. The images for each product are classified according to specific product lines. The accuracy of the model is too low when categorizing new products. All of the product images have the same dimensions and are stored within an Amazon S3 bucket. The company wants to improve the model so it can be used for new products as soon as possible. Which steps would improve the accuracy of the solution? (Choose three.) ", "zhcn": "一家电商公司正致力于根据商品图片实现产品分类的自动化。数据科学家运用亚马逊SageMaker平台的图像分类算法，训练出计算机视觉模型。每件商品的图像均按特定产品线进行分类。但在对新商品进行分类时，该模型的准确率始终不尽如人意。所有商品图像尺寸统一，并存储于亚马逊S3存储桶中。公司希望尽快优化模型以适用于新品分类。下列哪三项措施能有效提升该解决方案的准确率？"}, "option": [{"option_text": {"zhcn": "使用SageMaker语义分割算法训练新模型，以提升预测精准度。", "enus": "Use the SageMaker semantic segmentation algorithm to train a new model to achieve improved accuracy."}, "option_flag": false}, {"option_text": {"zhcn": "利用Amazon Rekognition的DetectLabels接口对数据集中的商品进行智能分类。", "enus": "Use the Amazon Rekognition DetectLabels API to classify the products in the dataset."}, "option_flag": true}, {"option_text": {"zhcn": "对数据集中的图像进行增强处理。利用开源工具库对图像进行裁剪、尺寸调整、翻转、旋转以及亮度与对比度的调节。", "enus": "Augment the images in the dataset. Use open source libraries to crop, resize, fiip, rotate, and adjust the brightness and contrast of the  images."}, "option_flag": true}, {"option_text": {"zhcn": "利用SageMaker笔记本来实现图像像素归一化与尺寸缩放处理，并将处理后的数据集存储至Amazon S3。", "enus": "Use a SageMaker notebook to implement the normalization of pixels and scaling of the images. Store the new dataset in Amazon S3."}, "option_flag": false}, {"option_text": {"zhcn": "利用Amazon Rekognition Custom Labels训练新模型。", "enus": "Use Amazon Rekognition Custom Labels to train a new model."}, "option_flag": true}, {"option_text": {"zhcn": "请检查产品类别是否存在样本数量不均衡的情况，并根据需要采用过采样或欠采样方法进行处理。将处理后的新数据集存储至Amazon S3平台。", "enus": "Check whether there are class imbalances in the product categories, and apply oversampling or undersampling as required. Store the  new dataset in Amazon S3."}, "option_flag": false}], "analysis": {"enus": "Reference: https://docs.aws.amazon.com/rekognition/latest/dg/how-it-works-types.html https://towardsdatascience.com/image-processing-techniques- for-computer-vision-11f92f511e21 https://docs.aws.amazon.com/rekognition/latest/customlabels-dg/training-model.html", "zhcn": "参考文献：  \nhttps://docs.aws.amazon.com/rekognition/latest/dg/how-it-works-types.html  \nhttps://towardsdatascience.com/image-processing-techniques-for-computer-vision-11f92f511e21  \nhttps://docs.aws.amazon.com/rekognition/latest/customlabels-dg/training-model.html"}, "answer": "BCE"}, {"id": "158", "question": {"enus": "A data scientist is training a text classification model by using the Amazon SageMaker built-in BlazingText algorithm. There are 5 classes in the dataset, with 300 samples for category A, 292 samples for category B, 240 samples for category C, 258 samples for category D, and 310 samples for category E. The data scientist shufies the data and splits off 10% for testing. After training the model, the data scientist generates confusion matrices for the training and test sets. What could the data scientist conclude form these results? ", "zhcn": "一位数据科学家正在运用亚马逊SageMaker平台内置的BlazingText算法训练文本分类模型。数据集中包含5个类别，其中A类300个样本，B类292个样本，C类240个样本，D类258个样本，E类310个样本。数据科学家将数据随机打乱后，划分出10%作为测试集。完成模型训练后，生成了训练集和测试集的混淆矩阵。根据这些结果，数据科学家可能得出哪些结论？"}, "option": [{"option_text": {"zhcn": "C班与D班过于相近。", "enus": "Classes C and D are too similar."}, "option_flag": false}, {"option_text": {"zhcn": "数据集规模过小，不宜采用留出法进行交叉验证。", "enus": "The dataset is too small for holdout cross-validation."}, "option_flag": true}, {"option_text": {"zhcn": "数据分布呈现偏态。", "enus": "The data distribution is skewed."}, "option_flag": false}, {"option_text": {"zhcn": "该模型在B类和E类上出现了过拟合现象。", "enus": "The model is overfitting for classes B and E."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is “The dataset is too small for holdout cross - validation.” In total, there are 300 + 292+240 + 258+310 = 1400 samples. Splitting off 10% for testing leaves only 140 samples in the test set, and with 5 classes, each class has a relatively small number of samples in the test set. This small sample size makes holdout cross - validation less reliable as it may not accurately represent the true performance of the model.\n\nThe option “Classes C and D are too similar” cannot be concluded just from the number of samples in each class. There is no information in the question indicating similarity between these two classes. The “The data distribution is skewed” option is incorrect because the number of samples in each class is relatively close, and there is no extreme imbalance to suggest a skewed distribution. Regarding “The model is overfitting for classes B and E,” there is no data in the question about the performance on the training and test sets to support that the model is overfitting for these specific classes. The key factor here is the limited sample size for holdout cross - validation, which differentiates the real answer from the fake options.", "zhcn": "该问题的正确答案是“数据集规模过小，无法采用留出交叉验证法”。样本总量为300+292+240+258+310=1400个，若划分10%作为测试集，则测试样本仅余140个。在包含5个类别的情况下，每个类别在测试集中的样本数量相对有限。这种小样本容量会降低留出交叉验证的可靠性，因其可能无法准确反映模型的真实性能。\n\n关于“C类与D类过于相似”这一选项，仅从各类别样本数量无法得出此结论。题目中未提供任何表明这两类具有相似性的信息。而“数据分布存在偏斜”选项亦不成立，因为各类别样本数量相对接近，并未出现表明分布偏斜的极端不均衡情况。至于“模型对B类与E类存在过拟合”的说法，题目中缺乏训练集与测试集表现的相关数据，无法支持模型对特定类别过拟合的论断。\n\n此处关键因素在于留出交叉验证的样本规模限制，这正是正确答案与其他干扰选项的根本区别所在。"}, "answer": "B"}, {"id": "159", "question": {"enus": "A company that manufactures mobile devices wants to determine and calibrate the appropriate sales price for its devices. The company is collecting the relevant data and is determining data features that it can use to train machine learning (ML) models. There are more than 1,000 features, and the company wants to determine the primary features that contribute to the sales price. Which techniques should the company use for feature selection? (Choose three.) ", "zhcn": "一家移动设备制造商欲为其产品制定并校准合宜的销售价格。该公司正在收集相关数据，并确定可用于训练机器学习模型的数据特征。现有特征数量逾千项，公司需要找出影响售价的核心特征。请问应采用哪三种特征筛选技术？（请选择三项。）"}, "option": [{"option_text": {"zhcn": "数据标准化与归一化处理", "enus": "Data scaling with standardization and normalization"}, "option_flag": false}, {"option_text": {"zhcn": "热力图关联分布图", "enus": "Correlation plot with heat maps"}, "option_flag": false}, {"option_text": {"zhcn": "数据分箱", "enus": "Data binning"}, "option_flag": true}, {"option_text": {"zhcn": "单变量筛选", "enus": "Univariate selection"}, "option_flag": true}, {"option_text": {"zhcn": "基于树形分类器的特征重要性分析", "enus": "Feature importance with a tree-based classifier"}, "option_flag": false}, {"option_text": {"zhcn": "数据增广", "enus": "Data augmentation"}, "option_flag": true}], "analysis": {"enus": "Reference: https://towardsdatascience.com/an-overview-of-data-preprocessing-features-enrichment-automatic-feature-selection-60b0c12d75ad https://towardsdatascience.com/feature-selection-using-python-for-classification-problem- b5f00a1c7028#:~:text=Univariate%20feature%20selection%20works% 20by,analysis%20of%20variance%20(ANOVA).&text=That%20is%20why%20it%20is%20called%20'univariate ' https://arxiv.org/abs/2101.04530", "zhcn": "参考文献：  \nhttps://towardsdatascience.com/an-overview-of-data-preprocessing-features-enrichment-automatic-feature-selection-60b0c12d75ad  \nhttps://towardsdatascience.com/feature-selection-using-python-for-classification-problem-b5f00a1c7028#:~:text=Univariate%20feature%20selection%20works%20by,analysis%20of%20variance%20(ANOVA).&text=That%20is%20why%20it%20is%20called%20'univariate'  \nhttps://arxiv.org/abs/2101.04530"}, "answer": "CDF"}, {"id": "160", "question": {"enus": "A power company wants to forecast future energy consumption for its customers in residential properties and commercial business properties. Historical power consumption data for the last 10 years is available. A team of data scientists who performed the initial data analysis and feature selection will include the historical power consumption data and data such as weather, number of individuals on the property, and public holidays. The data scientists are using Amazon Forecast to generate the forecasts. Which algorithm in Forecast should the data scientists use to meet these requirements? ", "zhcn": "某电力公司需预测其住宅与商业物业客户的未来能耗水平。目前掌握了过去十年的历史用电量数据，由数据科学团队完成初步数据分析和特征筛选后，将纳入天气、物业内人员数量及公共假日等变量。该团队正采用Amazon Forecast平台进行预测建模。为满足上述需求，数据科学家应选用Forecast中的何种算法？"}, "option": [{"option_text": {"zhcn": "自回归积分滑动平均模型（AIRMA）", "enus": "Autoregressive Integrated Moving Average (AIRMA)"}, "option_flag": false}, {"option_text": {"zhcn": "指数平滑法（ETS）", "enus": "Exponential Smoothing (ETS)"}, "option_flag": true}, {"option_text": {"zhcn": "卷积神经网络-分位数回归（CNN-QR）", "enus": "Convolutional Neural Network - Quantile Regression (CNN-QR)"}, "option_flag": false}, {"option_text": {"zhcn": "先知", "enus": "Prophet"}, "option_flag": false}], "analysis": {"enus": "Reference: https://jesit.springeropen.com/articles/10.1186/s43067-020-00021-8", "zhcn": "参考文献来源：https://jesit.springeropen.com/articles/10.1186/s43067-020-00021-8"}, "answer": "B"}, {"id": "161", "question": {"enus": "A company wants to use automatic speech recognition (ASR) to transcribe messages that are less than 60 seconds long from a voicemail- style application. The company requires the correct identification of 200 unique product names, some of which have unique spellings or pronunciations. The company has 4,000 words of Amazon SageMaker Ground Truth voicemail transcripts it can use to customize the chosen ASR model. The company needs to ensure that everyone can update their customizations multiple times each hour. Which approach will maximize transcription accuracy during the development phase? ", "zhcn": "一家公司计划采用自动语音识别技术，为语音邮件类应用中的短消息（时长不超过60秒）生成文字转录。该公司需确保200种独特产品名称能被准确识别，其中部分名称具有非常规拼写或发音特点。目前企业拥有4,000词规模的亚马逊SageMaker Ground Truth语音邮件转录数据集，可用于定制所选ASR模型。业务要求支持所有操作人员每小时多次更新自定义配置。在开发阶段，采用何种方案能最大限度提升转录准确率？"}, "option": [{"option_text": {"zhcn": "运用语音驱动的Amazon Lex机器人实现自动语音识别定制化功能。在该机器人中创建专属客户槽位，用以精准识别所需的各类产品名称。通过Amazon Lex的同义词机制，为每个产品名称提供多种常见变体形式，以应对开发过程中可能出现的识别误差。", "enus": "Use a voice-driven Amazon Lex bot to perform the ASR customization. Create customer slots within the bot that specifically identify  each of the required product names. Use the Amazon Lex synonym mechanism to provide additional variations of each product name as  mis-transcriptions are identified in development."}, "option_flag": true}, {"option_text": {"zhcn": "运用亚马逊Transcribe服务进行语音识别定制化处理。通过分析转录文本中的词汇置信度评分，自动将低于可接受阈值的词汇添加至定制词汇表文件并进行动态更新。在后续所有转录任务中，请持续采用这份经过优化的定制词汇表文件。", "enus": "Use Amazon Transcribe to perform the ASR customization. Analyze the word confidence scores in the transcript, and automatically  create or update a custom vocabulary file with any word that has a confidence score below an acceptable threshold value. Use this  updated custom vocabulary file in all future transcription tasks."}, "option_flag": false}, {"option_text": {"zhcn": "创建一个包含各产品名称及音标发音的自定义词汇表文件，将其与亚马逊转录服务配合使用以实现语音识别定制化。通过分析转录文本，对手动更新自定义词汇表文件，增补或修正未被准确识别的产品名称条目。", "enus": "Create a custom vocabulary file containing each product name with phonetic pronunciations, and use it with Amazon Transcribe to  perform the ASR customization. Analyze the transcripts and manually update the custom vocabulary file to include updated or additional  entries for those names that are not being correctly identified."}, "option_flag": false}, {"option_text": {"zhcn": "利用音频转录文本构建训练数据集，并以此训练亚马逊Transcribe定制化语言模型。通过分析现有转录内容，对产品名称识别有误的文本进行人工校正，据此更新训练数据集。最终基于优化后的数据生成升级版定制语言模型。", "enus": "Use the audio transcripts to create a training dataset and build an Amazon Transcribe custom language model. Analyze the transcripts  and update the training dataset with a manually corrected version of transcripts where product names are not being transcribed correctly.  Create an updated custom language model."}, "option_flag": false}], "analysis": {"enus": "Reference: https://docs.aws.amazon.com/lex/latest/dg/lex-dg.pdf", "zhcn": "参考来源：https://docs.aws.amazon.com/lex/latest/dg/lex-dg.pdf"}, "answer": "A"}, {"id": "162", "question": {"enus": "A company is building a demand forecasting model based on machine learning (ML). In the development stage, an ML specialist uses an Amazon SageMaker notebook to perform feature engineering during work hours that consumes low amounts of CPU and memory resources. A data engineer uses the same notebook to perform data preprocessing once a day on average that requires very high memory and completes in only 2 hours. The data preprocessing is not configured to use GPU. All the processes are running well on an ml.m5.4xlarge notebook instance. The company receives an AWS Budgets alert that the billing for this month exceeds the allocated budget. Which solution will result in the MOST cost savings? ", "zhcn": "一家公司正基于机器学习（ML）构建需求预测模型。在开发阶段，机器学习专家使用亚马逊SageMaker笔记本来进行特征工程，该任务在工作时段运行，消耗较低的CPU和内存资源。数据工程师平均每日使用同一笔记本执行一次数据预处理，此过程需占用极高内存但仅需两小时即可完成，且未配置使用GPU。目前所有流程均在ml.m5.4xlarge笔记本实例上稳定运行。公司收到AWS预算警报，显示本月账单已超出 allocated budget。下列哪种解决方案能实现最大程度的成本节约？"}, "option": [{"option_text": {"zhcn": "将笔记本实例类型调整为内存优化型实例，其vCPU核心数量需与ml.m5.4xlarge实例保持一致。闲置时请暂停运行该实例。数据预处理与特征工程开发均需在此实例上执行。", "enus": "Change the notebook instance type to a memory optimized instance with the same vCPU number as the ml.m5.4xlarge instance has.  Stop the notebook when it is not in use. Run both data preprocessing and feature engineering development on that instance."}, "option_flag": false}, {"option_text": {"zhcn": "保持笔记本实例类型与规格不变，闲置时请及时停止运行。数据预处理任务需选用P3型实例执行，其内存容量应与ml.m5.4xlarge实例保持一致，可通过Amazon SageMaker Processing服务实现此操作。", "enus": "Keep the notebook instance type and size the same. Stop the notebook when it is not in use. Run data preprocessing on a P3 instance  type with the same memory as the ml.m5.4xlarge instance by using Amazon SageMaker Processing."}, "option_flag": true}, {"option_text": {"zhcn": "将笔记本实例类型调整为更小规格的通用型实例。闲置时请及时停止笔记本运行。数据预处理任务建议采用Amazon SageMaker Processing服务，选用内存容量与ml.m5.4xlarge实例相同的ml.r5实例来执行。", "enus": "Change the notebook instance type to a smaller general purpose instance. Stop the notebook when it is not in use. Run data  preprocessing on an ml.r5 instance with the same memory size as the ml.m5.4xlarge instance by using Amazon SageMaker Processing."}, "option_flag": false}, {"option_text": {"zhcn": "将笔记本实例类型调整为更小规格的通用型实例。闲置时请及时停止笔记本运行。通过预留实例选项，选用与ml.m5.4xlarge实例内存容量相当的R5实例执行数据预处理任务。", "enus": "Change the notebook instance type to a smaller general purpose instance. Stop the notebook when it is not in use. Run data  preprocessing on an R5 instance with the same memory size as the ml.m5.4xlarge instance by using the Reserved Instance option."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Keep the notebook instance type and size the same. Stop the notebook when it is not in use. Run data preprocessing on a P3 instance type with the same memory as the ml.m5.4xlarge instance by using Amazon SageMaker Processing.” This approach is most cost - effective because it addresses the different resource requirements of feature engineering and data preprocessing separately. Feature engineering has low CPU and memory needs, so keeping the current instance type and just stopping it when not in use saves cost. For data preprocessing, which requires high memory, using a P3 instance via Amazon SageMaker Processing allows for efficient resource utilization as the P3 instances are optimized for compute - intensive tasks.\n\nThe first fake option of using a memory - optimized instance for both tasks is inefficient. Since feature engineering has low resource requirements, using a high - end memory - optimized instance for it will lead to unnecessary costs. The second fake option of using a smaller general - purpose instance for the notebook and an ml.r5 instance for data preprocessing might not fully leverage the capabilities needed for data preprocessing. The ml.r5 instance is not specifically designed for the high - intensity tasks like the P3 instance. The third fake option of using a Reserved Instance for data preprocessing on an R5 instance also has drawbacks. Reserved Instances require a long - term commitment and might not be the best fit for a task that only runs 2 hours a day on average, leading to potential over - commitment and wasted resources. These factors distinguish the real answer from the fake options and make it the most cost - saving solution.", "zhcn": "针对该问题，正确答案是\"保持笔记本实例类型与规模不变，闲置时将其停止运行；数据预处理环节则通过Amazon SageMaker Processing服务，选用与ml.m5.4xlarge实例内存容量相当的P3实例类型执行\"。此方案成本效益最优，因其针对特征工程与数据预处理的不同资源需求进行了差异化配置。特征工程对CPU和内存需求较低，维持现有实例类型并适时停机可有效控制成本；而数据预处理需要大内存支持，采用专为计算密集型任务优化的P3实例能实现资源高效利用。\n\n首项干扰选项提议为两项任务均配置内存优化型实例，这显然不经济。特征工程既无需过高资源配置，强行采用高端内存实例只会造成资源浪费。次项干扰方案建议为笔记本配备更小的通用型实例，预处理环节改用ml.r5实例，但ml.r5实例缺乏P3实例特有的高强度计算优化能力，难以充分发挥数据预处理效能。第三项干扰方案主张为R5实例预留实例处理数据预处理，然预留实例需长期合约锁定，对于日均仅运行2小时的任务而言，易导致资源过度承诺与闲置损耗。\n\n正是这些关键差异，使得正确答案从众多干扰项中脱颖而出，成为最具成本效益的解决方案。"}, "answer": "B"}, {"id": "163", "question": {"enus": "A machine learning specialist is developing a regression model to predict rental rates from rental listings. A variable named Wall_Color represents the most prominent exterior wall color of the property. The following is the sample data, excluding all other variables: The specialist chose a model that needs numerical input data. Which feature engineering approaches should the specialist use to allow the regression model to learn from the Wall_Color data? (Choose two.) ", "zhcn": "一位机器学习专家正在开发一个回归模型，旨在通过租赁房源信息预测租金价格。其中变量\"Wall_Color\"代表物业外立面最显著的墙体颜色。以下是剔除其他变量后的样本数据：该专家选择的模型需要数值型输入数据。为使回归模型能够从\"Wall_Color\"数据中学习，应采用哪两种特征工程方法？（请选择两项。）"}, "option": [{"option_text": {"zhcn": "对数值进行整数变换，设定红色对应1，白色对应5，绿色对应10。", "enus": "Apply integer transformation and set Red = 1, White = 5, and Green = 10."}, "option_flag": true}, {"option_text": {"zhcn": "添加新列，用于存储颜色的独热编码表示。", "enus": "Add new columns that store one-hot representation of colors."}, "option_flag": false}, {"option_text": {"zhcn": "将颜色名称字符串替换为其长度。", "enus": "Replace the color name string by its length."}, "option_flag": false}, {"option_text": {"zhcn": "创建三列以RGB格式编码颜色。", "enus": "Create three columns to encode the color in RGB format."}, "option_flag": true}, {"option_text": {"zhcn": "将每种颜色名称替换为其在训练集中的出现频次。", "enus": "Replace each color name by its training set frequency."}, "option_flag": false}], "analysis": {"enus": "The correct answers for allowing the regression model to learn from the Wall_Color data are “Apply integer transformation and set Red = 1, White = 5, and Green = 10” and “Create three columns to encode the color in RGB format”. \n\nApplying integer transformation converts the categorical color data into numerical values that the model can process. Each color is assigned a unique integer, enabling the model to learn relationships between the color and rental rates. Creating three columns for RGB encoding also provides numerical data. RGB values are numerical representations of colors, and by separating them into columns, the model can use these values for regression analysis.\n\nThe option “Add new columns that store one - hot representation of colors” is incorrect because one - hot encoding creates binary columns for each category. While it's useful for classification problems, regression models typically require numerical values with a meaningful order or magnitude, which one - hot encoding doesn't provide. \n\n“Replace the color name string by its length” is wrong as the length of the color name has no relation to the rental rate. It doesn't capture any relevant information about the property's color and won't help the model make accurate predictions. \n\n“Replace each color name by its training set frequency” is also incorrect. The frequency of a color in the training set doesn't have a direct connection to the rental rate. It doesn't convey any property - related information useful for the regression model.\n\nIn summary, the real answer options provide meaningful numerical representations of the color data that the regression model can utilize, while the fake options either use inappropriate encoding methods or provide irrelevant numerical values.", "zhcn": "为使回归模型能够有效学习墙体颜色数据，正确的处理方式为“采用整数转换法（设定红色=1、白色=5、绿色=10）”与“创建三列RGB格式的颜色编码”。整数转换将分类色彩数据转化为模型可处理的数值，通过为每种颜色分配独特整数值，使模型能捕捉颜色与租金之间的关联规律。而RGB编码通过三列数值化呈现色彩，这种色彩的数字分解形式可为回归分析提供有效参数。\n\n至于“添加颜色独热编码新列”这一方案并不适用。独热编码虽适用于分类问题，但会生成互无关联的二元列，而回归模型需要具有数值意义及可比较性的数据输入。“以颜色名称长度替代”亦不可取，因颜色名称长度与租金毫无逻辑关联，无法传递任何有价值的物业特征信息。同样，“以训练集出现频次替代颜色名称”也属错误方案，颜色出现频率与租金水平并无直接联系，无法为回归模型提供有效特征。\n\n综上，正确答案通过构建具有实际意义的数值化颜色表征来满足回归模型需求，而错误选项要么采用不恰当的编码方式，要么提供了与预测目标无关的数值信息。"}, "answer": "AD"}, {"id": "164", "question": {"enus": "A data scientist is working on a public sector project for an urban trafic system. While studying the trafic patterns, it is clear to the data scientist that the trafic behavior at each light is correlated, subject to a small stochastic error term. The data scientist must model the trafic behavior to analyze the trafic patterns and reduce congestion. How will the data scientist MOST effectively model the problem? ", "zhcn": "一位数据科学家正负责某城市交通系统的公共部门项目。在研究交通流模式时，这位科学家发现每个路口的交通行为相互关联，且存在微小的随机误差项。为分析交通规律并缓解拥堵，需对交通行为进行建模。下列哪种方法能最高效地构建该问题的模型？"}, "option": [{"option_text": {"zhcn": "数据科学家需将此类问题构建为多智能体强化学习模型，从而求得相关均衡策略。", "enus": "The data scientist should obtain a correlated equilibrium policy by formulating this problem as a multi-agent reinforcement learning  problem."}, "option_flag": false}, {"option_text": {"zhcn": "数据科学家需将此类问题构建为单智能体强化学习模型，从而求得最优均衡策略。", "enus": "The data scientist should obtain the optimal equilibrium policy by formulating this problem as a single-agent reinforcement learning  problem."}, "option_flag": false}, {"option_text": {"zhcn": "数据科学家的目标并非寻求某种平衡策略，而应借助历史数据，通过监督学习的方法构建精准的交通流量预测模型。", "enus": "Rather than finding an equilibrium policy, the data scientist should obtain accurate predictors of trafic fiow by using historical data  through a supervised learning approach."}, "option_flag": false}, {"option_text": {"zhcn": "数据科学家的任务并非寻求均衡策略，而应通过运用代表城市新型交通模式的无标注模拟数据，并采用无监督学习方法，来获取精准的交通流预测指标。", "enus": "Rather than finding an equilibrium policy, the data scientist should obtain accurate predictors of trafic fiow by using unlabeled  simulated data representing the new trafic patterns in the city and applying an unsupervised learning approach."}, "option_flag": true}], "analysis": {"enus": "Reference: https://www.hindawi.com/journals/jat/2021/8878011/", "zhcn": "参考文献：https://www.hindawi.com/journals/jat/2021/8878011/"}, "answer": "D"}, {"id": "165", "question": {"enus": "A data scientist is using the Amazon SageMaker Neural Topic Model (NTM) algorithm to build a model that recommends tags from blog posts. The raw blog post data is stored in an Amazon S3 bucket in JSON format. During model evaluation, the data scientist discovered that the model recommends certain stopwords such as \"a,\" \"an,\" and \"the\" as tags to certain blog posts, along with a few rare words that are present only in certain blog entries. After a few iterations of tag review with the content team, the data scientist notices that the rare words are unusual but feasible. The data scientist also must ensure that the tag recommendations of the generated model do not include the stopwords. What should the data scientist do to meet these requirements? ", "zhcn": "一位数据科学家正借助亚马逊SageMaker的神经主题模型（NTM）算法，构建能够从博客内容中智能推荐标签的模型。原始博客数据以JSON格式存储于亚马逊S3存储桶中。模型评估阶段，该科学家发现模型会向部分博客推荐诸如\"a\"、\"an\"、\"the\"等停用词作为标签，同时也会推荐仅在某些特定条目中出现的生僻词汇。经过与内容团队的多轮标签评审，科学家注意到这些生僻词汇虽不常见但具有实际意义。当前需要确保生成模型所推荐的标签不再包含停用词。请问该数据科学家应采取何种措施以满足上述需求？"}, "option": [{"option_text": {"zhcn": "运用亚马逊Comprehend实体识别API接口，从博客文章数据中筛除识别出的特定词汇，并更新亚马逊S3存储桶中的博客数据源。", "enus": "Use the Amazon Comprehend entity recognition API operations. Remove the detected words from the blog post data. Replace the blog  post data source in the S3 bucket."}, "option_flag": false}, {"option_text": {"zhcn": "以S3存储桶中的博文数据作为数据源，运行SageMaker内置的主成分分析（PCA）算法。随后将训练任务生成的结果数据更新至原S3存储桶中的博文数据存储位置。", "enus": "Run the SageMaker built-in principal component analysis (PCA) algorithm with the blog post data from the S3 bucket as the data  source. Replace the blog post data in the S3 bucket with the results of the training job."}, "option_flag": false}, {"option_text": {"zhcn": "请使用SageMaker内置的目标检测算法替代NTM算法来处理博客文章数据的训练任务。", "enus": "Use the SageMaker built-in Object Detection algorithm instead of the NTM algorithm for the training job to process the blog post data."}, "option_flag": false}, {"option_text": {"zhcn": "运用scikit-learn库中的CountVectorizer函数对博客文章数据进行停用词过滤，并将处理后的词向量结果更新至亚马逊S3存储桶中的原始数据位置。", "enus": "Remove the stopwords from the blog post data by using the CountVectorizer function in the scikit-learn library. Replace the blog post  data in the S3 bucket with the results of the vectorizer."}, "option_flag": true}], "analysis": {"enus": "Reference: https://towardsdatascience.com/natural-language-processing-count-vectorization-with-scikit-learn-e7804269bb5e", "zhcn": "参考来源：https://towardsdatascience.com/natural-language-processing-count-vectorization-with-scikit-learn-e7804269bb5e"}, "answer": "D"}, {"id": "166", "question": {"enus": "A company wants to create a data repository in the AWS Cloud for machine learning (ML) projects. The company wants to use AWS to perform complete ML lifecycles and wants to use Amazon S3 for the data storage. All of the company's data currently resides on premises and is 40 ׀¢׀’ in size. The company wants a solution that can transfer and automatically update data between the on-premises object storage and Amazon S3. The solution must support encryption, scheduling, monitoring, and data integrity validation. Which solution meets these requirements? ", "zhcn": "某公司计划在AWS云平台构建一个用于机器学习项目的数据存储库。该公司希望借助AWS完成完整的机器学习生命周期，并采用Amazon S3作为数据存储方案。目前企业所有数据均存储于本地，总量达40TB。需要设计一套能够在本地对象存储与Amazon S3之间实现数据传输、自动同步的解决方案，该方案必须支持加密传输、定时同步、运行监控及数据完整性验证。请问下列哪种方案符合上述要求？"}, "option": [{"option_text": {"zhcn": "使用S3同步命令对比源S3存储桶与目标S3存储桶，识别目标存储桶中缺失的源文件以及已被修改的源文件。", "enus": "Use the S3 sync command to compare the source S3 bucket and the destination S3 bucket. Determine which source files do not exist in  the destination S3 bucket and which source files were modified."}, "option_flag": false}, {"option_text": {"zhcn": "借助 AWS Transfer for FTPS 服务，可将文件从本地存储设备安全传输至 Amazon S3。", "enus": "Use AWS Transfer for FTPS to transfer the files from the on-premises storage to Amazon S3."}, "option_flag": false}, {"option_text": {"zhcn": "利用AWS DataSync完成数据集的首次全量同步，并设定定期增量传输机制以捕捉变更数据，最终实现从本地到AWS环境的平滑迁移。", "enus": "Use AWS DataSync to make an initial copy of the entire dataset. Schedule subsequent incremental transfers of changing data until the  final cutover from on premises to AWS."}, "option_flag": true}, {"option_text": {"zhcn": "利用S3批量操作功能，可定期从本地存储系统拉取数据。同时在S3存储桶中启用版本控制功能，有效防范数据遭意外覆盖的风险。", "enus": "Use S3 Batch Operations to pull data periodically from the on-premises storage. Enable S3 Versioning on the S3 bucket to protect  against accidental overwrites."}, "option_flag": false}], "analysis": {"enus": "Configure DataSync to make an initial copy of your entire dataset, and schedule subsequent incremental transfers of changing data until the final cut-over from on- premises to AWS. Reference: https://aws.amazon.com/datasync/faqs/", "zhcn": "配置DataSync服务，首先对您的完整数据集进行初始全量同步，并安排后续的增量传输计划，仅同步发生变化的数据，直至实现从本地环境到AWS的最终无缝切换。参考链接：https://aws.amazon.com/datasync/faqs/"}, "answer": "C"}, {"id": "167", "question": {"enus": "A company has video feeds and images of a subway train station. The company wants to create a deep learning model that will alert the station manager if any passenger crosses the yellow safety line when there is no train in the station. The alert will be based on the video feeds. The company wants the model to detect the yellow line, the passengers who cross the yellow line, and the trains in the video feeds. This task requires labeling. The video data must remain confidential. A data scientist creates a bounding box to label the sample data and uses an object detection model. However, the object detection model cannot clearly demarcate the yellow line, the passengers who cross the yellow line, and the trains. Which labeling approach will help the company improve this model? ", "zhcn": "某公司掌握着某地铁站的视频监控资料与图像数据。该公司计划开发一种深度学习模型，当站台无列车停靠时若有乘客越过安全黄线，系统能立即向站务人员发出警报。这项警报功能将基于视频监控数据实现，要求模型能准确识别安全黄线、越线乘客及进出站列车。为实现该目标，需要对数据进行标注处理，且所有视频数据均需严格保密。\n\n数据科学家采用边界框对样本数据进行标注，并运用目标检测模型进行训练。但发现该模型在安全黄线、越线乘客及列车这三类目标的识别边界上存在模糊不清的问题。请问采用何种标注方案能有效提升该模型的识别精度？"}, "option": [{"option_text": {"zhcn": "运用亚马逊Rekognition定制标签功能对数据集进行标注，并构建定制化的亚马逊Rekognition目标检测模型。创建专属人工标注团队，通过亚马逊增强型人工智能（Amazon A2I）对低置信度预测结果进行复核，进而优化并重新训练定制的亚马逊Rekognition模型。", "enus": "Use Amazon Rekognition Custom Labels to label the dataset and create a custom Amazon Rekognition object detection model. Create  a private workforce. Use Amazon Augmented AI (Amazon A2I) to review the low-confidence predictions and retrain the custom Amazon  Rekognition model."}, "option_flag": false}, {"option_text": {"zhcn": "采用亚马逊SageMaker Ground Truth的目标检测标注任务，并选用亚马逊Mechanical Turk作为标注工作团队。", "enus": "Use an Amazon SageMaker Ground Truth object detection labeling task. Use Amazon Mechanical Turk as the labeling workforce."}, "option_flag": true}, {"option_text": {"zhcn": "借助Amazon Rekognition Custom Labels标注数据集并构建定制化的亚马逊Rekognition目标检测模型。通过第三方AWS Marketplace服务商组建标注团队，并运用Amazon Augmented AI（Amazon A2I）对低置信度预测结果进行人工复核，进而优化定制的Amazon Rekognition模型。", "enus": "Use Amazon Rekognition Custom Labels to label the dataset and create a custom Amazon Rekognition object detection model. Create  a workforce with a third-party AWS Marketplace vendor. Use Amazon Augmented AI (Amazon A2I) to review the low-confidence predictions  and retrain the custom Amazon Rekognition model."}, "option_flag": false}, {"option_text": {"zhcn": "采用亚马逊SageMaker Ground Truth语义分割标注任务，并选用专属人工团队作为标注工作团队。", "enus": "Use an Amazon SageMaker Ground Truth semantic segmentation labeling task. Use a private workforce as the labeling workforce."}, "option_flag": false}], "analysis": {"enus": "Reference: https://docs.aws.amazon.com/sagemaker/latest/dg/sms-workforce-management-public.html", "zhcn": "参考来源：https://docs.aws.amazon.com/sagemaker/latest/dg/sms-workforce-management-public.html"}, "answer": "B"}, {"id": "168", "question": {"enus": "A data engineer at a bank is evaluating a new tabular dataset that includes customer data. The data engineer will use the customer data to create a new model to predict customer behavior. After creating a correlation matrix for the variables, the data engineer notices that many of the 100 features are highly correlated with each other. Which steps should the data engineer take to address this issue? (Choose two.) ", "zhcn": "某银行数据工程师正在评估一份包含客户数据的新表格数据集，计划利用这些数据构建预测客户行为的模型。在生成变量相关性矩阵后，该工程师发现100个特征中有许多存在高度相关性。为处理此情况，数据工程师应采取以下哪两项措施？（请选择两项）"}, "option": [{"option_text": {"zhcn": "采用线性算法对模型进行训练。", "enus": "Use a linear-based algorithm to train the model."}, "option_flag": false}, {"option_text": {"zhcn": "运用主成分分析法（PCA）。", "enus": "Apply principal component analysis (PCA)."}, "option_flag": true}, {"option_text": {"zhcn": "从数据集中剔除部分高度相关的特征。", "enus": "Remove a portion of highly correlated features from the dataset."}, "option_flag": false}, {"option_text": {"zhcn": "对数据集应用最小-最大归一化处理。", "enus": "Apply min-max feature scaling to the dataset."}, "option_flag": true}, {"option_text": {"zhcn": "对类别型变量进行独热编码处理。", "enus": "Apply one-hot encoding category-based variables."}, "option_flag": false}], "analysis": {"enus": "Reference: https://royalsocietypublishing.org/doi/10.1098/rsta.2015.0202 https://scikit- learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html", "zhcn": "参考文献：  \nhttps://royalsocietypublishing.org/doi/10.1098/rsta.2015.0202  \nhttps://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html"}, "answer": "BD"}, {"id": "169", "question": {"enus": "A company is building a new version of a recommendation engine. Machine learning (ML) specialists need to keep adding new data from users to improve personalized recommendations. The ML specialists gather data from the users' interactions on the platform and from sources such as external websites and social media. The pipeline cleans, transforms, enriches, and compresses terabytes of data daily, and this data is stored in Amazon S3. A set of Python scripts was coded to do the job and is stored in a large Amazon EC2 instance. The whole process takes more than 20 hours to finish, with each script taking at least an hour. The company wants to move the scripts out of Amazon EC2 into a more managed solution that will eliminate the need to maintain servers. Which approach will address all of these requirements with the LEAST development effort? ", "zhcn": "一家公司正在开发新版推荐引擎。机器学习专家需要持续整合用户新增数据以优化个性化推荐效果。专家们从用户在平台上的交互行为以及外部网站、社交媒体等渠道采集数据。该数据处理管道每日需清洗、转换、增强并压缩数TB级别的数据，最终存储至亚马逊S3云存储服务。现有若干Python脚本被编写用于执行这些任务，这些脚本目前存放于大型亚马逊EC2云服务器实例中。整套流程耗时超过20小时，每个脚本运行时间均不低于一小时。公司希望将这些脚本从EC2实例迁移至更集约化的托管解决方案，从而免除服务器维护负担。若要同时满足所有需求且开发投入最小，应采用哪种实施方案？"}, "option": [{"option_text": {"zhcn": "将数据载入Amazon Redshift集群，通过SQL语句执行数据处理流程，最终将结果保存至Amazon S3存储空间。", "enus": "Load the data into an Amazon Redshift cluster. Execute the pipeline by using SQL. Store the results in Amazon S3."}, "option_flag": false}, {"option_text": {"zhcn": "将数据载入Amazon DynamoDB，将脚本转换为AWS Lambda函数，通过触发Lambda执行来运行流程，最终将结果存储于Amazon S3中。", "enus": "Load the data into Amazon DynamoDB. Convert the scripts to an AWS Lambda function. Execute the pipeline by triggering Lambda  executions. Store the results in Amazon S3."}, "option_flag": true}, {"option_text": {"zhcn": "创建一项AWS Glue作业。将脚本转换为PySpark代码。执行数据处理流程。将最终结果存储至Amazon S3。", "enus": "Create an AWS Glue job. Convert the scripts to PySpark. Execute the pipeline. Store the results in Amazon S3."}, "option_flag": false}, {"option_text": {"zhcn": "创建一组独立的AWS Lambda函数，分别用于执行各个脚本。通过AWS Step Functions Data Science SDK构建步骤工作流，并将运行结果存储至Amazon S3。", "enus": "Create a set of individual AWS Lambda functions to execute each of the scripts. Build a step function by using the AWS Step Functions  Data Science SDK. Store the results in Amazon S3."}, "option_flag": false}], "analysis": {"enus": "Reference: https://docs.aws.amazon.com/lambda/latest/dg/with-s3-example.html", "zhcn": "参考文档：AWS官方指南《使用Amazon S3触发器运行Lambda函数》  \n（链接：https://docs.aws.amazon.com/lambda/latest/dg/with-s3-example.html）"}, "answer": "B"}, {"id": "170", "question": {"enus": "A retail company is selling products through a global online marketplace. The company wants to use machine learning (ML) to analyze customer feedback and identify specific areas for improvement. A developer has built a tool that collects customer reviews from the online marketplace and stores them in an Amazon S3 bucket. This process yields a dataset of 40 reviews. A data scientist building the ML models must identify additional sources of data to increase the size of the dataset. Which data sources should the data scientist use to augment the dataset of reviews? (Choose three.) ", "zhcn": "一家零售企业正通过全球在线商城销售产品。该公司希望运用机器学习技术分析客户反馈，以确定需要改进的具体环节。开发人员已构建工具，从在线商城采集客户评价并存储至亚马逊S3存储桶，初步获得包含40条评论的数据集。为扩充数据集规模，机器学习模型构建者需寻找更多数据源。下列哪些数据源可用于增强评论数据集？（请选择三项）"}, "option": [{"option_text": {"zhcn": "客户与公司客服专员之间的往来邮件", "enus": "Emails exchanged by customers and the company's customer service agents"}, "option_flag": false}, {"option_text": {"zhcn": "含有公司名称或其产品的社交媒体内容", "enus": "Social media posts containing the name of the company or its products"}, "option_flag": true}, {"option_text": {"zhcn": "一份可公开查阅的新闻文集", "enus": "A publicly available collection of news articles"}, "option_flag": false}, {"option_text": {"zhcn": "一份可供公众查阅的客户评价集锦", "enus": "A publicly available collection of customer reviews"}, "option_flag": true}, {"option_text": {"zhcn": "公司产品销售收入数据", "enus": "Product sales revenue figures for the company"}, "option_flag": false}, {"option_text": {"zhcn": "本公司产品的使用指南", "enus": "Instruction manuals for the company's products"}, "option_flag": true}], "analysis": {"enus": "The question asks for data sources to augment a dataset of customer reviews for ML - based analysis of areas for improvement.\n\nThe real answer options are suitable for the task. Social media posts with the company or product name can contain customer feedback and opinions, similar to the reviews in the S3 bucket. A publicly available collection of customer reviews directly adds to the existing review dataset. Instruction manuals can provide insights into how customers interact with the products, which can be related to areas that need improvement.\n\nThe fake answer options are not appropriate. Emails between customers and customer - service agents are private and may not be easily accessible or suitable for public ML analysis. A publicly available collection of news articles usually focuses on general news events rather than specific customer feedback. Product sales revenue figures do not contain qualitative customer feedback about the products, so they cannot help in identifying areas for improvement based on customer reviews.\n\nThe key factor in choosing the real answer options is their direct relevance to customer feedback, which is what the ML models need for analysis. The fake options either lack this relevance or have issues with accessibility and data type for the given purpose.", "zhcn": "问题要求寻找数据源以扩充客户评论数据集，用于基于机器学习的改进方向分析。真实答案选项符合任务需求：带有公司或产品名称的社交媒体帖子可能包含客户反馈和观点，这与S3存储桶中的评论具有相似性；公开可获取的客户评论集能直接扩充现有评论数据集；产品说明书可揭示客户与产品的互动方式，这些信息有助于发现待改进领域。\n\n虚假答案选项则不妥当：客户与客服人员之间的邮件往来属于私人通信，既不易获取也不适用于公开的机器学习分析；公开的新闻稿件合集通常聚焦于通用新闻事件而非具体客户反馈；产品销售额数据不包含关于产品的定性客户反馈，因此无法基于客户评论识别改进方向。\n\n选择真实答案选项的关键在于它们与客户反馈的直接关联性——这正是机器学习模型进行分析所需的核心要素。虚假选项要么缺乏这种关联性，要么在可获取性或数据类型方面不符合本题要求。"}, "answer": "BDF"}, {"id": "171", "question": {"enus": "A machine learning (ML) specialist wants to create a data preparation job that uses a PySpark script with complex window aggregation operations to create data for training and testing. The ML specialist needs to evaluate the impact of the number of features and the sample count on model performance. Which approach should the ML specialist use to determine the ideal data transformations for the model? ", "zhcn": "一位机器学习专家计划构建数据预处理任务，该任务需采用包含复杂窗口聚合操作的PySpark脚本来生成训练与测试数据。为评估特征数量与样本规模对模型性能的影响，该专家需要确定何种方法能帮助选定最适合模型的数据转换方案。"}, "option": [{"option_text": {"zhcn": "向脚本中添加一个Amazon SageMaker Debugger钩子，用于捕获关键指标。随后将该脚本作为AWS Glue任务运行。", "enus": "Add an Amazon SageMaker Debugger hook to the script to capture key metrics. Run the script as an AWS Glue job."}, "option_flag": false}, {"option_text": {"zhcn": "在脚本中添加一个Amazon SageMaker Experiments追踪器，用于记录关键指标。随后将该脚本作为AWS Glue任务运行。", "enus": "Add an Amazon SageMaker Experiments tracker to the script to capture key metrics. Run the script as an AWS Glue job."}, "option_flag": true}, {"option_text": {"zhcn": "向脚本中添加一个Amazon SageMaker Debugger钩子，用于捕获关键参数。随后以SageMaker处理作业的形式运行该脚本。", "enus": "Add an Amazon SageMaker Debugger hook to the script to capture key parameters. Run the script as a SageMaker processing job."}, "option_flag": false}, {"option_text": {"zhcn": "在脚本中加入一个Amazon SageMaker Experiments追踪器，用于记录关键参数。随后将该脚本作为SageMaker处理任务运行。", "enus": "Add an Amazon SageMaker Experiments tracker to the script to capture key parameters. Run the script as a SageMaker processing job."}, "option_flag": false}], "analysis": {"enus": "Reference: https://docs.aws.amazon.com/sagemaker/latest/dg/experiments.html", "zhcn": "参考文献：亚马逊SageMaker实验功能官方文档  \n（链接：https://docs.aws.amazon.com/sagemaker/latest/dg/experiments.html）"}, "answer": "B"}, {"id": "172", "question": {"enus": "A data scientist has a dataset of machine part images stored in Amazon Elastic File System (Amazon EFS). The data scientist needs to use Amazon SageMaker to create and train an image classification machine learning model based on this dataset. Because of budget and time constraints, management wants the data scientist to create and train a model with the least number of steps and integration work required. How should the data scientist meet these requirements? ", "zhcn": "一位数据科学家拥有一组存储在Amazon Elastic File System（Amazon EFS）中的机械零件图像数据集。该数据科学家需运用Amazon SageMaker平台，基于此数据集构建并训练图像分类机器学习模型。鉴于预算与时间限制，管理层要求数据科学家以最简化的步骤和最少的集成工作完成模型创建与训练。数据科学家应如何满足这些要求？"}, "option": [{"option_text": {"zhcn": "将EFS文件系统挂载至SageMaker笔记本实例，执行脚本将数据同步至Amazon FSx for Lustre文件系统。随后以FSx for Lustre文件系统作为数据源，启动SageMaker模型训练任务。", "enus": "Mount the EFS file system to a SageMaker notebook and run a script that copies the data to an Amazon FSx for Lustre file system. Run  the SageMaker training job with the FSx for Lustre file system as the data source."}, "option_flag": true}, {"option_text": {"zhcn": "启动一个临时的Amazon EMR集群。配置相关步骤以挂载EFS文件系统，并运用S3DistCp将数据复制至Amazon S3存储桶。随后以Amazon S3作为数据源，运行SageMaker训练任务。", "enus": "Launch a transient Amazon EMR cluster. Configure steps to mount the EFS file system and copy the data to an Amazon S3 bucket by  using S3DistCp. Run the SageMaker training job with Amazon S3 as the data source."}, "option_flag": false}, {"option_text": {"zhcn": "将EFS文件系统挂载至Amazon EC2实例，通过AWS命令行工具将数据复制到Amazon S3存储桶中。随后以Amazon S3作为数据源，启动SageMaker训练任务。", "enus": "Mount the EFS file system to an Amazon EC2 instance and use the AWS CLI to copy the data to an Amazon S3 bucket. Run the  SageMaker training job with Amazon S3 as the data source."}, "option_flag": false}, {"option_text": {"zhcn": "以EFS文件系统作为数据源，运行SageMaker训练任务。", "enus": "Run a SageMaker training job with an EFS file system as the data source."}, "option_flag": false}], "analysis": {"enus": "Reference: https://aws.amazon.com/about-aws/whats-new/2019/08/amazon-sagemaker-works-with-amazon-fsx-lustre-amazon-efs-model-training/", "zhcn": "参考链接：https://aws.amazon.com/about-aws/whats-new/2019/08/amazon-sagemaker-works-with-amazon-fsx-lustre-amazon-efs-model-training/"}, "answer": "A"}, {"id": "173", "question": {"enus": "A retail company uses a machine learning (ML) model for daily sales forecasting. The company's brand manager reports that the model has provided inaccurate results for the past 3 weeks. At the end of each day, an AWS Glue job consolidates the input data that is used for the forecasting with the actual daily sales data and the predictions of the model. The AWS Glue job stores the data in Amazon S3. The company's ML team is using an Amazon SageMaker Studio notebook to gain an understanding about the source of the model's inaccuracies. What should the ML team do on the SageMaker Studio notebook to visualize the model's degradation MOST accurately? ", "zhcn": "一家零售企业采用机器学习模型进行日常销售预测。品牌经理反映，该模型近三周的预测结果存在偏差。每日营业结束后，AWS Glue作业会整合三方面数据：模型预测所需的输入数据、当日实际销售数据以及模型预测值，并将这些数据存储于Amazon S3中。目前该企业的机器学习团队正通过Amazon SageMaker Studio笔记本分析模型失准根源。若要最精准地呈现模型性能衰减情况，该团队应在SageMaker Studio笔记本中采取何种可视化方案？"}, "option": [{"option_text": {"zhcn": "绘制过去三周每日销售额的分布直方图，同时还需制作该期间之前每日销售额的分布直方图。", "enus": "Create a histogram of the daily sales over the last 3 weeks. In addition, create a histogram of the daily sales from before that period."}, "option_flag": false}, {"option_text": {"zhcn": "绘制过去三周内模型误差的分布直方图，同时还需生成该时间段之前模型误差的分布直方图。", "enus": "Create a histogram of the model errors over the last 3 weeks. In addition, create a histogram of the model errors from before that  period."}, "option_flag": false}, {"option_text": {"zhcn": "绘制一幅折线图，展示模型每周的平均绝对误差（MAE）数据。", "enus": "Create a line chart with the weekly mean absolute error (MAE) of the model."}, "option_flag": true}, {"option_text": {"zhcn": "请绘制过去三周内每日销售额与模型误差的散点图。同时，另作一张该时期之前每日销售额与模型误差的散点图。", "enus": "Create a scatter plot of daily sales versus model error for the last 3 weeks. In addition, create a scatter plot of daily sales versus model  error from before that period."}, "option_flag": false}], "analysis": {"enus": "Reference: https://machinelearningmastery.com/time-series-forecasting-performance-measures-with-python/", "zhcn": "参考来源：https://machinelearningmastery.com/time-series-forecasting-performance-measures-with-python/"}, "answer": "C"}, {"id": "174", "question": {"enus": "An ecommerce company sends a weekly email newsletter to all of its customers. Management has hired a team of writers to create additional targeted content. A data scientist needs to identify five customer segments based on age, income, and location. The customers' current segmentation is unknown. The data scientist previously built an XGBoost model to predict the likelihood of a customer responding to an email based on age, income, and location. Why does the XGBoost model NOT meet the current requirements, and how can this be fixed? ", "zhcn": "一家电商公司每周会向所有客户发送电子邮件通讯。管理层已聘请内容团队撰写更具针对性的定制化内容。数据科学家需要根据年龄、收入及地理位置将客户划分为五个群体，但目前客户细分维度尚未明确。该数据科学家曾建立XGBoost模型，通过年龄、收入和地理位置来预测客户对邮件的响应概率。为何当前场景下XGBoost模型无法满足需求？又该如何调整解决？"}, "option": [{"option_text": {"zhcn": "XGBoost模型可输出真/假二元判定结果。本方案采用五维特征的主成分分析（PCA）来预测数据片段。", "enus": "The XGBoost model provides a true/false binary output. Apply principal component analysis (PCA) with five feature dimensions to  predict a segment."}, "option_flag": false}, {"option_text": {"zhcn": "XGBoost模型原本输出的是真/假二元结果。现将其预测类别扩展至五类，以实现对细分市场的判断。", "enus": "The XGBoost model provides a true/false binary output. Increase the number of classes the XGBoost model predicts to five classes to  predict a segment."}, "option_flag": false}, {"option_text": {"zhcn": "XGBoost模型是一种监督式机器学习算法。现使用相同数据集训练K值为5的K近邻（kNN）模型，用于预测数据分类。", "enus": "The XGBoost model is a supervised machine learning algorithm. Train a k-Nearest-Neighbors (kNN) model with K = 5 on the same  dataset to predict a segment."}, "option_flag": true}, {"option_text": {"zhcn": "XGBoost模型是一种监督式机器学习算法。请在同一数据集上训练K值为5的K均值模型，用于预测细分群体。", "enus": "The XGBoost model is a supervised machine learning algorithm. Train a k-means model with K = 5 on the same dataset to predict a  segment."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to use a k - Nearest - Neighbors (kNN) model with K = 5. The XGBoost model is a supervised learning algorithm, which requires labeled data for training. In this case, the current customer segmentation is unknown, so there are no labels. The task is to identify five customer segments, which is an unsupervised learning problem.\n\nThe first fake option suggests using PCA. PCA is a dimensionality - reduction technique, not a segmentation algorithm, so it cannot be used to predict segments. The second fake option tries to modify the XGBoost model to predict five classes. But since there are no labels in the data, it's not possible to train an XGBoost model in a supervised way for this task. The third fake option proposes a k - means model. Although k - means is an unsupervised learning algorithm suitable for clustering, kNN is more appropriate here as it can classify new data points based on the nearest neighbors in the feature space to predict the segment, while k - means focuses on grouping data points together without directly predicting a segment for new data in the same way as kNN. This is why the kNN model option is the real answer, distinguishing it from the fake options.", "zhcn": "问题的正确答案是采用K=5的K近邻算法（kNN）模型。XGBoost作为一种监督学习算法，需要带标签的数据进行训练。而当前客户分群状况未知，缺乏标注数据，且任务要求识别五个客户细分，这属于无监督学习范畴。\n\n第一个干扰项建议使用主成分分析（PCA），但PCA本质是降维技术而非分群算法，因此无法用于预测细分群体。第二个干扰项试图修改XGBoost模型预测五种类别，然而数据中不存在标签，无法以监督学习方式训练XGBoost完成此任务。第三个干扰项提出的K均值算法虽是无监督聚类方法，但K近邻模型能根据特征空间中的最近邻点对新数据点进行分群预测，而K均值更侧重于数据点分组而非直接预测新数据归属——这正是kNN模型成为正确答案的关键区别。\n\n（注：专业术语k-Nearest-Neighbors、XGBoost、PCA、k-means均保留英文原称，符合学术惯例）"}, "answer": "C"}, {"id": "175", "question": {"enus": "A global financial company is using machine learning to automate its loan approval process. The company has a dataset of customer information. The dataset contains some categorical fields, such as customer location by city and housing status. The dataset also includes financial fields in different units, such as account balances in US dollars and monthly interest in US cents. The company's data scientists are using a gradient boosting regression model to infer the credit score for each customer. The model has a training accuracy of 99% and a testing accuracy of 75%. The data scientists want to improve the model's testing accuracy. Which process will improve the testing accuracy the MOST? ", "zhcn": "一家全球性金融公司正运用机器学习技术实现贷款审批流程的自动化。该公司拥有包含客户信息的数据集，其中既有按城市划分的客户所在地、住房状况等分类字段，也包含以不同计量单位记录的财务字段——例如以美元为单位的账户余额，以及以美分计价的月利息。数据科学家团队采用梯度提升回归模型来推算每位客户的信用评分，目前该模型的训练准确率高达99%，但测试准确率仅为75%。为提升模型的测试准确度，下列哪种方法能最有效地实现这一目标？"}, "option": [{"option_text": {"zhcn": "对数据集中的类别字段采用独热编码处理。针对财务相关字段执行标准化操作。在数据上应用L1正则化方法。", "enus": "Use a one-hot encoder for the categorical fields in the dataset. Perform standardization on the financial fields in the dataset. Apply L1  regularization to the data."}, "option_flag": false}, {"option_text": {"zhcn": "对数据集中的分类字段进行标记化处理。针对数据集中的财务字段执行分箱操作。通过采用Z分数方法剔除数据中的异常值。", "enus": "Use tokenization of the categorical fields in the dataset. Perform binning on the financial fields in the dataset. Remove the outliers in  the data by using the z- score."}, "option_flag": true}, {"option_text": {"zhcn": "对数据集中的分类字段采用标签编码处理。针对财务相关字段实施L1正则化，同时对其余数据采用L2正则化方法。", "enus": "Use a label encoder for the categorical fields in the dataset. Perform L1 regularization on the financial fields in the dataset. Apply L2  regularization to the data."}, "option_flag": false}, {"option_text": {"zhcn": "对数据集中的分类字段进行对数变换处理。针对数据集中的财务字段实施分段离散化操作。采用插补方法填充数据集中的缺失值。", "enus": "Use a logarithm transformation on the categorical fields in the dataset. Perform binning on the financial fields in the dataset. Use  imputation to populate missing values in the dataset."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Use tokenization of the categorical fields in the dataset. Perform binning on the financial fields in the dataset. Remove the outliers in the data by using the z - score.” \n\nTokenization can effectively handle categorical data, converting text - based categories into numerical representations that are more suitable for machine learning models. Binning financial fields groups continuous numerical data into intervals, which can simplify the data and reduce noise. Removing outliers using the z - score helps in eliminating extreme values that can distort the model's learning and generalization ability, thus enhancing the testing accuracy.\n\nThe first fake option suggests using a one - hot encoder for categorical fields, standardization for financial fields, and L1 regularization. One - hot encoding can lead to a high - dimensional dataset, which may cause the curse of dimensionality and overfitting. Standardization might not be as effective as binning in this case, and L1 regularization may not directly address the issues related to the data's characteristics like outliers.\n\nThe second fake option uses a label encoder for categorical fields, L1 regularization on financial fields, and L2 regularization. Label encoding can introduce an artificial order to categorical data, which may mislead the model. L1 and L2 regularization mainly focus on reducing the complexity of the model but do not deal with the data pre - processing issues as comprehensively as the real answer.\n\nThe third fake option proposes a logarithm transformation on categorical fields, which is inappropriate as categorical data is non - numerical and not suitable for such a transformation. Although binning financial fields is correct, the overall combination is less effective compared to the real answer.\n\nCommon misconceptions that might lead to choosing the fake options include a lack of understanding of the appropriate data pre - processing techniques for different types of data and over - reliance on regularization methods without considering the data's inherent characteristics and potential outliers.", "zhcn": "针对该问题的正确答案是：\"对数据集中的分类字段进行标记化处理，对财务字段实施分箱操作，并通过Z值法剔除异常值。\"标记化技术能有效处理分类数据，将文本型类别转化为更适合机器学习模型的数值表示。对财务字段分箱可将连续数值数据归入特定区间，从而简化数据结构并降低噪声干扰。采用Z值法消除异常值有助于排除极端数据点，避免其扭曲模型的学习能力与泛化性能，进而提升测试精度。\n\n第一个干扰项建议对分类字段采用独热编码，对财务字段进行标准化处理，并施加L1正则化。独热编码可能导致数据集维度膨胀，引发维度灾难与过拟合问题。在此场景下，标准化处理的效果不如分箱操作显著，而L1正则化并不能直接解决数据特性（如异常值）相关的问题。\n\n第二个干扰项对分类字段使用标签编码，对财务字段采用L1正则化，并辅以L2正则化。标签编码可能给分类数据强加人为顺序，从而误导模型判断。L1与L2正则化主要着眼于降低模型复杂度，但相较于正确答案，其未能全面应对数据预处理阶段的核心问题。\n\n第三个干扰项提议对分类字段进行对数变换——由于分类数据本质为非数值型，此类变换显然不合逻辑。尽管该选项对财务字段的分箱处理正确，但整体方案的有效性远逊于标准答案。\n\n常见误选干扰项的原因包括：对不同数据类型适用的预处理技术理解不足，过度依赖正则化方法而忽视数据内在特性与潜在异常值的重要性。"}, "answer": "B"}, {"id": "176", "question": {"enus": "A machine learning (ML) specialist needs to extract embedding vectors from a text series. The goal is to provide a ready-to-ingest feature space for a data scientist to develop downstream ML predictive models. The text consists of curated sentences in English. Many sentences use similar words but in different contexts. There are questions and answers among the sentences, and the embedding space must differentiate between them. Which options can produce the required embedding vectors that capture word context and sequential QA information? (Choose two.) ", "zhcn": "机器学习专家需要从一系列文本中提取嵌入向量，其目标是为数据科学家提供可直接输入的特征空间，用以开发下游的机器学习预测模型。该文本由经过筛选的英文句子组成，许多句子虽使用相似词汇但语境各异。文本中穿插着提问与回答，而嵌入空间必须能对二者加以区分。下列哪些方案能够生成符合要求的嵌入向量，既能捕捉词汇语境又能保留问答序列信息？（请选择两项。）"}, "option": [{"option_text": {"zhcn": "Amazon SageMaker 序列到序列算法", "enus": "Amazon SageMaker seq2seq algorithm"}, "option_flag": true}, {"option_text": {"zhcn": "Amazon SageMaker BlazingText算法在Skip-gram模式下运行", "enus": "Amazon SageMaker BlazingText algorithm in Skip-gram mode"}, "option_flag": false}, {"option_text": {"zhcn": "Amazon SageMaker Object2Vec 算法", "enus": "Amazon SageMaker Object2Vec algorithm"}, "option_flag": true}, {"option_text": {"zhcn": "Amazon SageMaker BlazingText算法在连续词袋（CBOW）模式下", "enus": "Amazon SageMaker BlazingText algorithm in continuous bag-of-words (CBOW) mode"}, "option_flag": false}, {"option_text": {"zhcn": "亚马逊SageMaker平台BlazingText算法在批量Skip-gram模式下，与定制循环神经网络（RNN）的融合运用。", "enus": "Combination of the Amazon SageMaker BlazingText algorithm in Batch Skip-gram mode with a custom recurrent neural network (RNN)"}, "option_flag": false}], "analysis": {"enus": "Reference: https://aws.amazon.com/blogs/machine-learning/create-a-word-pronunciation-sequence-to-sequence-model-using-amazon-sagemaker/ https://docs.aws.amazon.com/sagemaker/latest/dg/object2vec.html", "zhcn": "参考来源：  \n- AWS机器学习博客文章《基于Amazon SageMaker构建单词发音序列到序列模型》  \n- Amazon SageMaker开发指南《object2vec算法详解》  \n\n（注：保留原链接中的专有名词“Amazon SageMaker”及技术术语“object2vec”不作翻译，符合技术文档引用规范）"}, "answer": "AC"}, {"id": "177", "question": {"enus": "A retail company wants to update its customer support system. The company wants to implement automatic routing of customer claims to different queues to prioritize the claims by category. Currently, an operator manually performs the category assignment and routing. After the operator classifies and routes the claim, the company stores the claim's record in a central database. The claim's record includes the claim's category. The company has no data science team or experience in the field of machine learning (ML). The company's small development team needs a solution that requires no ML expertise. Which solution meets these requirements? ", "zhcn": "一家零售企业计划升级其客户服务系统，旨在通过自动将客户投诉按类别分流至不同队列，实现按优先级处理投诉的机制。目前该项分类与分流工作由人工操作完成：当客服专员完成投诉分类并分配至对应队列后，系统会将投诉记录存储至中央数据库，其中包含已标注的投诉类别。由于该企业尚未设立数据科学团队且缺乏机器学习领域经验，其小型开发团队需要一套无需机器学习专业能力即可实施的解决方案。请问下列哪种方案符合这些要求？"}, "option": [{"option_text": {"zhcn": "将数据库导出为包含两列（claim_label 和 claim_text）的.csv文件。运用Amazon SageMaker平台的Object2Vec算法，基于该.csv文件训练预测模型。通过SageMaker将模型部署至推理端点，并在应用程序中开发服务接口，借助该端点对传入的索赔请求进行实时分析、预测分类标签，并自动流转至对应的处理队列。", "enus": "Export the database to a .csv file with two columns: claim_label and claim_text. Use the Amazon SageMaker Object2Vec algorithm and  the .csv file to train a model. Use SageMaker to deploy the model to an inference endpoint. Develop a service in the application to use the  inference endpoint to process incoming claims, predict the labels, and route the claims to the appropriate queue."}, "option_flag": false}, {"option_text": {"zhcn": "将数据库导出为仅含claim_text单列的.csv文件。运用Amazon SageMaker平台的隐狄利克雷分布（LDA）算法，结合该.csv文件进行模型训练。通过LDA算法实现标签的自动识别，并借助SageMaker将模型部署至推理端点。需在应用程序中开发服务模块，调用该推理端点处理传入的索赔请求：先预测对应标签，再将其路由至相应的处理队列。", "enus": "Export the database to a .csv file with one column: claim_text. Use the Amazon SageMaker Latent Dirichlet Allocation (LDA) algorithm  and the .csv file to train a model. Use the LDA algorithm to detect labels automatically. Use SageMaker to deploy the model to an  inference endpoint. Develop a service in the application to use the inference endpoint to process incoming claims, predict the labels, and  route the claims to the appropriate queue."}, "option_flag": false}, {"option_text": {"zhcn": "运用Amazon Textract解析数据库，自动识别claim_label与claim_text两列数据。结合Amazon Comprehend定制分类功能，利用提取的信息训练专属分类模型。在应用程序中开发服务模块，通过调用Amazon Comprehend API处理传入的索赔申请，预测对应标签，并将申请自动分流至相应处理队列。", "enus": "Use Amazon Textract to process the database and automatically detect two columns: claim_label and claim_text. Use Amazon  Comprehend custom classification and the extracted information to train the custom classifier. Develop a service in the application to use  the Amazon Comprehend API to process incoming claims, predict the labels, and route the claims to the appropriate queue."}, "option_flag": true}, {"option_text": {"zhcn": "将数据库导出为包含两列（索赔标签与索赔文本）的CSV文件。运用Amazon Comprehend自定义分类功能，结合该CSV文件训练定制分类器。在应用程序中开发服务接口，通过调用Amazon Comprehend API处理传入的索赔数据，预测对应标签，并将索赔案件自动分配至相应的处理队列。", "enus": "Export the database to a .csv file with two columns: claim_label and claim_text. Use Amazon Comprehend custom classification and the  .csv file to train the custom classifier. Develop a service in the application to use the Amazon Comprehend API to process incoming  claims, predict the labels, and route the claims to the appropriate queue."}, "option_flag": false}], "analysis": {"enus": "Reference: https://aws.amazon.com/blogs/machine-learning/intelligently-split-multi-form-document-packages-with-amazon-textract-and-amazon- comprehend/", "zhcn": "参考来源：https://aws.amazon.com/blogs/machine-learning/intelligently-split-multi-form-document-packages-with-amazon-textract-and-amazon-comprehend/"}, "answer": "C"}, {"id": "178", "question": {"enus": "A machine learning (ML) specialist is using Amazon SageMaker hyperparameter optimization (HPO) to improve a model's accuracy. The learning rate parameter is specified in the following HPO configuration: During the results analysis, the ML specialist determines that most of the training jobs had a learning rate between 0.01 and 0.1. The best result had a learning rate of less than 0.01. Training jobs need to run regularly over a changing dataset. The ML specialist needs to find a tuning mechanism that uses different learning rates more evenly from the provided range between MinValue and MaxValue. Which solution provides the MOST accurate result? ", "zhcn": "一位机器学习专家正利用Amazon SageMaker的超参数优化功能来提升模型精度。在超参数配置中设定了学习率参数。结果分析显示，多数训练任务的学习率集中在0.01至0.1之间，而最佳结果对应的学习率却低于0.01。由于训练任务需基于动态变化的数据集定期执行，该专家需要找到一种调参机制，能够更均衡地采用MinValue与MaxValue区间内的不同学习率。请问下列哪种方案能得出最精确的结果？"}, "option": [{"option_text": {"zhcn": "请按如下方式调整超参数优化配置：  \n在此次超参数优化任务中选取精确度最高的参数组合。", "enus": "Modify the HPO configuration as follows:   Select the most  accurate hyperparameter configuration form this HPO job."}, "option_flag": false}, {"option_text": {"zhcn": "请执行三项不同的超参数优化（HPO）任务，每项任务分别采用以下学习率区间作为最小值和最大值的取值范围，并确保每项HPO任务的训练次数保持一致：  \n✧ [0.01, 0.1]  \n✧ [0.001, 0.01]  \n✧ [0.0001, 0.001]  \n最终从这三项HPO任务中选取超参数配置精度最高的方案。", "enus": "Run three different HPO jobs that use different learning rates form the following intervals for MinValue and MaxValue while using the  same number of training jobs for each HPO job: ✑ [0.01, 0.1] ✑ [0.001, 0.01] ✑ [0.0001, 0.001] Select the most accurate hyperparameter  configuration form these three HPO jobs."}, "option_flag": false}, {"option_text": {"zhcn": "请按如下方式调整超参数优化配置：  \n从本次训练任务中选取精度最高的超参数配置方案。", "enus": "Modify the HPO configuration as follows:   Select the most accurate  hyperparameter configuration form this training job."}, "option_flag": true}, {"option_text": {"zhcn": "请运行三项不同的超参数优化（HPO）任务，其学习率分别从以下区间的最小值与最大值中选取。将每项HPO任务的训练次数均分为三组进行：\n✑ [0.01, 0.1]  \n✑ [0.001, 0.01]  \n✑ [0.0001, 0.001]  \n最终从这三项HPO任务中选取超参数配置精度最高的方案。", "enus": "Run three different HPO jobs that use different learning rates form the following intervals for MinValue and MaxValue. Divide the  number of training jobs for each HPO job by three: ✑ [0.01, 0.1] ✑ [0.001, 0.01] [0.0001, 0.001]   Select the most accurate  hyperparameter configuration form these three HPO jobs."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Modify the HPO configuration as follows: Select the most accurate hyperparameter configuration form this training job.” The key goal is to tune the learning rate more evenly across the provided range. By selecting the best configuration from a single training job with a properly - adjusted HPO configuration, it can potentially explore the entire range more evenly in one comprehensive process.\n\nThe first fake option about selecting from an HPO job instead of a training job is less clear in terms of getting an even spread of learning rates. It doesn't directly address the issue of evenly using different learning rates from the range.\n\nThe second and third fake options involve running multiple HPO jobs with different intervals. Running multiple jobs like this may lead to an unbalanced exploration of the learning rate range. In the second option, running three jobs in different intervals might not cover the range evenly as the jobs are separated into distinct intervals. The third option compounds the problem by dividing the number of training jobs for each HPO job, reducing the overall exploration within each interval and potentially missing out on optimal learning rates. So, the real answer is the most effective for achieving an even exploration of the learning rate range.", "zhcn": "对于该问题的正确答案是：“按如下方式修改超参数优化（HPO）配置：从本次训练任务中选择最精确的超参数配置。”关键目标在于让学习率在给定范围内更均匀地调优。通过从一次经过恰当调整的超参数优化配置的训练任务中筛选最佳配置，这一综合性流程有望更均衡地覆盖整个参数探索空间。  \n\n第一个干扰项提及从超参数优化任务（而非训练任务）中选择配置，这种做法在实现学习率均匀分布方面不够明确，未能直接解决均匀运用不同学习率的问题。  \n\n第二和第三个干扰项涉及在不同区间运行多个超参数优化任务。此类多任务运行模式可能导致学习率范围的探索失衡——第二个选项中，将三个任务分散在不同区间运行可能无法实现范围的全域覆盖；第三个选项则进一步加剧问题，通过为每个超参数优化任务分割训练任务数量，削弱了各区间内的整体探索强度，可能错失最优学习率。  \n\n因此，原答案所提方案是实现学习率范围均匀探索的最有效途径。"}, "answer": "C"}, {"id": "179", "question": {"enus": "A manufacturing company wants to use machine learning (ML) to automate quality control in its facilities. The facilities are in remote locations and have limited internet connectivity. The company has 20 ׀¢׀’ of training data that consists of labeled images of defective product parts. The training data is in the corporate on- premises data center. The company will use this data to train a model for real-time defect detection in new parts as the parts move on a conveyor belt in the facilities. The company needs a solution that minimizes costs for compute infrastructure and that maximizes the scalability of resources for training. The solution also must facilitate the company's use of an ML model in the low-connectivity environments. Which solution will meet these requirements? ", "zhcn": "一家制造企业计划在其工厂中采用机器学习技术以实现质量控制的自动化。这些工厂地处偏远地区，网络连接条件有限。企业拥有20TB由缺陷产品部件标注图像构成的训练数据，这些数据存储于企业本地数据中心。公司将利用该数据训练模型，以便在零部件通过工厂传送带时实时检测新部件的缺陷。企业需要的解决方案必须最大限度降低计算基础设施成本，同时实现训练资源的高度可扩展性。该方案还需确保在低网络连通性环境下能够有效部署机器学习模型。何种方案可满足这些需求？"}, "option": [{"option_text": {"zhcn": "将训练数据导入至Amazon S3存储桶后，通过Amazon SageMaker服务平台进行模型训练与效果评估。随后借助SageMaker Neo功能对模型进行深度优化，最终将其部署于SageMaker托管服务的终端节点上。", "enus": "Move the training data to an Amazon S3 bucket. Train and evaluate the model by using Amazon SageMaker. Optimize the model by  using SageMaker Neo. Deploy the model on a SageMaker hosting services endpoint."}, "option_flag": true}, {"option_text": {"zhcn": "在本地环境训练并评估模型后，将其上传至Amazon S3存储桶。随后通过Amazon SageMaker托管服务端点部署模型。", "enus": "Train and evaluate the model on premises. Upload the model to an Amazon S3 bucket. Deploy the model on an Amazon SageMaker  hosting services endpoint."}, "option_flag": false}, {"option_text": {"zhcn": "将训练数据移至Amazon S3存储桶中，运用Amazon SageMaker进行模型训练与评估，并借助SageMaker Neo对模型进行优化。在生产车间通过AWS IoT Greengrass配置边缘设备，最终将优化后的模型部署至该设备上。", "enus": "Move the training data to an Amazon S3 bucket. Train and evaluate the model by using Amazon SageMaker. Optimize the model by  using SageMaker Neo. Set up an edge device in the manufacturing facilities with AWS IoT Greengrass. Deploy the model on the edge  device."}, "option_flag": false}, {"option_text": {"zhcn": "在本地环境训练模型。将训练完成的模型上传至Amazon S3存储桶。通过AWS IoT Greengrass在制造车间配置边缘设备，并将模型部署于该设备之上。", "enus": "Train the model on premises. Upload the model to an Amazon S3 bucket. Set up an edge device in the manufacturing facilities with AWS  IoT Greengrass. Deploy the model on the edge device."}, "option_flag": false}], "analysis": {"enus": "Reference: https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-deployment.html", "zhcn": "参考来源：https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-deployment.html"}, "answer": "A"}, {"id": "180", "question": {"enus": "A company has an ecommerce website with a product recommendation engine built in TensorFlow. The recommendation engine endpoint is hosted by Amazon SageMaker. Three compute-optimized instances support the expected peak load of the website. Response times on the product recommendation page are increasing at the beginning of each month. Some users are encountering errors. The website receives the majority of its trafic between 8 AM and 6 PM on weekdays in a single time zone. Which of the following options are the MOST effective in solving the issue while keeping costs to a minimum? (Choose two.) ", "zhcn": "某公司电商网站内置了一套基于TensorFlow构建的商品推荐引擎，其服务端点由Amazon SageMaker托管。为应对网站预期峰值流量，当前配置了三台计算优化型实例。每月初，商品推荐页面的响应时间持续延长，部分用户开始遭遇系统报错。该网站流量主要集中在同一时区工作日的上午8点至下午6点。在控制成本的前提下，下列哪两项措施能最高效解决此问题？（请选择两项）"}, "option": [{"option_text": {"zhcn": "将终端节点配置为使用Amazon Elastic Inference（EI）加速器。", "enus": "Configure the endpoint to use Amazon Elastic Inference (EI) accelerators."}, "option_flag": false}, {"option_text": {"zhcn": "创建新的端点配置，需包含两个生产变体。", "enus": "Create a new endpoint configuration with two production variants."}, "option_flag": true}, {"option_text": {"zhcn": "将端点配置为根据InvocationsPerInstance指标自动扩展。", "enus": "Configure the endpoint to automatically scale with the InvocationsPerInstance metric."}, "option_flag": false}, {"option_text": {"zhcn": "部署第二个实例池以支持模型的蓝绿部署。", "enus": "Deploy a second instance pool to support a blue/green deployment of models."}, "option_flag": true}, {"option_text": {"zhcn": "请将终端节点重新配置为使用可突增实例。", "enus": "Reconfigure the endpoint to use burstable instances."}, "option_flag": false}], "analysis": {"enus": "Reference: https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_ProductionVariant.html https://www.redhat.com/en/topics/devops/what-is-blue-green-deployment", "zhcn": "参考来源：  \n- AWS SageMaker 生产变体接口文档：https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_ProductionVariant.html  \n- 红帽蓝绿部署技术解读：https://www.redhat.com/en/topics/devops/what-is-blue-green-deployment"}, "answer": "BD"}, {"id": "181", "question": {"enus": "A real-estate company is launching a new product that predicts the prices of new houses. The historical data for the properties and prices is stored in .csv format in an Amazon S3 bucket. The data has a header, some categorical fields, and some missing values. The company's data scientists have used Python with a common open-source library to fill the missing values with zeros. The data scientists have dropped all of the categorical fields and have trained a model by using the open-source linear regression algorithm with the default parameters. The accuracy of the predictions with the current model is below 50%. The company wants to improve the model performance and launch the new product as soon as possible. Which solution will meet these requirements with the LEAST operational overhead? ", "zhcn": "一家房地产公司正推出一款预测新房价格的新产品。房产历史数据及价格以.csv格式存储于亚马逊S3存储桶中，数据包含表头、若干分类字段及部分缺失值。该公司的数据科学家已采用Python及常用开源库，将缺失值以零值填补，并删除了所有分类字段，继而使用默认参数的开源线性回归算法完成模型训练。当前模型的预测准确率低于50%。公司希望以最低运维成本提升模型性能，尽快推出新产品。下列哪种方案能以最小运维投入满足这些要求？"}, "option": [{"option_text": {"zhcn": "为亚马逊弹性容器服务（Amazon ECS）创建一个可访问S3存储桶的服务关联角色。基于AWS深度学习容器镜像构建一个ECS集群。编写实现特征工程的代码。训练用于价格预测的逻辑回归模型，并指向存有数据集的存储桶。等待训练任务完成后执行推理预测。", "enus": "Create a service-linked role for Amazon Elastic Container Service (Amazon ECS) with access to the S3 bucket. Create an ECS cluster  that is based on an AWS Deep Learning Containers image. Write the code to perform the feature engineering. Train a logistic regression  model for predicting the price, pointing to the bucket with the dataset. Wait for the training job to complete. Perform the inferences."}, "option_flag": true}, {"option_text": {"zhcn": "创建一个与笔记本关联的新IAM角色，并基于此角色配置Amazon SageMaker笔记本实例。从S3存储桶中提取数据集。系统性地探索特征工程转换、回归算法及超参数的不同组合方案，在笔记本中全面对比所有实验结果，最终将最优配置部署至预测端点。", "enus": "Create an Amazon SageMaker notebook with a new IAM role that is associated with the notebook. Pull the dataset from the S3 bucket.  Explore different combinations of feature engineering transformations, regression algorithms, and hyperparameters. Compare all the  results in the notebook, and deploy the most accurate configuration in an endpoint for predictions."}, "option_flag": false}, {"option_text": {"zhcn": "创建具有访问Amazon S3、Amazon SageMaker及AWS Lambda权限的IAM角色。使用SageMaker内置XGBoost模型创建训练任务，并指向存有数据集的存储桶。指定房价作为目标特征。等待任务完成后，将模型文件加载至Lambda函数，用于对新房屋价格进行预测推断。", "enus": "Create an IAM role with access to Amazon S3, Amazon SageMaker, and AWS Lambda. Create a training job with the SageMaker built-in  XGBoost model pointing to the bucket with the dataset. Specify the price as the target feature. Wait for the job to complete. Load the  model artifact to a Lambda function for inference on prices of new houses."}, "option_flag": false}, {"option_text": {"zhcn": "为Amazon SageMaker创建一个具有S3存储桶访问权限的IAM角色。使用指向包含数据集的存储桶的SageMaker Autopilot功能，创建SageMaker自动机器学习任务。将价格指定为目标属性。等待任务执行完毕。部署最优模型以进行预测。", "enus": "Create an IAM role for Amazon SageMaker with access to the S3 bucket. Create a SageMaker AutoML job with SageMaker Autopilot  pointing to the bucket with the dataset. Specify the price as the target attribute. Wait for the job to complete. Deploy the best model for  predictions."}, "option_flag": false}], "analysis": {"enus": "Reference: https://docs.aws.amazon.com/deep-learning-containers/latest/devguide/deep-learning-containers-ecs-setup.html", "zhcn": "参考文档：AWS深度学习容器ECS配置指南（链接：https://docs.aws.amazon.com/deep-learning-containers/latest/devguide/deep-learning-containers-ecs-setup.html）"}, "answer": "A"}, {"id": "182", "question": {"enus": "A data scientist is reviewing customer comments about a company's products. The data scientist needs to present an initial exploratory analysis by using charts and a word cloud. The data scientist must use feature engineering techniques to prepare this analysis before starting a natural language processing (NLP) model. Which combination of feature engineering techniques should the data scientist use to meet these requirements? (Choose two.) ", "zhcn": "一位数据分析师正在审阅客户对公司产品的评价。为完成初步探索性分析，该分析师需借助图表与文字云进行呈现。在启动自然语言处理模型之前，必须通过特征工程技术完成数据预处理。请问为满足上述需求，该分析师应采用哪两种特征工程技术？（请选择两项）"}, "option": [{"option_text": {"zhcn": "命名实体识别", "enus": "Named entity recognition"}, "option_flag": false}, {"option_text": {"zhcn": "指代关系", "enus": "Coreference"}, "option_flag": false}, {"option_text": {"zhcn": "词干提取", "enus": "Stemming"}, "option_flag": false}, {"option_text": {"zhcn": "词频-逆向文件频率（TF-IDF）", "enus": "Term frequency-inverse document frequency (TF-IDF)"}, "option_flag": true}, {"option_text": {"zhcn": "情感分析", "enus": "Sentiment analysis"}, "option_flag": true}], "analysis": {"enus": "Reference: https://www.analyticsvidhya.com/blog/2020/04/beginners-guide-exploratory-data-analysis-text-data/", "zhcn": "参考来源：https://www.analyticsvidhya.com/blog/2020/04/beginners-guide-exploratory-data-analysis-text-data/"}, "answer": "DE"}, {"id": "183", "question": {"enus": "A data scientist is evaluating a GluonTS on Amazon SageMaker DeepAR model. The evaluation metrics on the test set indicate that the coverage score is 0.489 and 0.889 at the 0.5 and 0.9 quantiles, respectively. What can the data scientist reasonably conclude about the distributional forecast related to the test set? ", "zhcn": "一位数据科学家正在评估基于亚马逊SageMaker平台DeepAR模型的GluonTS性能。测试集的评估指标显示，在0.5和0.9分位数下，覆盖度得分分别为0.489和0.889。关于测试集相关的分布预测，该数据科学家可以得出什么合理结论？"}, "option": [{"option_text": {"zhcn": "覆盖率得分表明，该分布预测的校准效果欠佳。理想情况下，各分位数的覆盖率应基本保持一致。", "enus": "The coverage scores indicate that the distributional forecast is poorly calibrated. These scores should be approximately equal to each  other at all quantiles."}, "option_flag": false}, {"option_text": {"zhcn": "覆盖率评分显示该分布预测的校准效果欠佳。理想状态下，这些分数应在中位数处达到峰值，而在分布两端逐渐降低。", "enus": "The coverage scores indicate that the distributional forecast is poorly calibrated. These scores should peak at the median and be lower  at the tails."}, "option_flag": false}, {"option_text": {"zhcn": "覆盖率分数表明该分布预测的校准准确无误。这些分数理应始终低于相应分位数。", "enus": "The coverage scores indicate that the distributional forecast is correctly calibrated. These scores should always fall below the quantile  itself."}, "option_flag": false}, {"option_text": {"zhcn": "覆盖率得分表明该分布预测已得到准确校准，这些数值应近似等于对应的分位数本身。", "enus": "The coverage scores indicate that the distributional forecast is correctly calibrated. These scores should be approximately equal to the  quantile itself."}, "option_flag": true}], "analysis": {"enus": "Reference: https://aws.amazon.com/blogs/machine-learning/amazon-forecast-now-supports-the-generation-of-forecasts-at-a-quantile-of-your-choice/", "zhcn": "参考链接：https://aws.amazon.com/blogs/machine-learning/amazon-forecast-now-supports-the-generation-of-forecasts-at-a-quantile-of-your-choice/"}, "answer": "D"}, {"id": "184", "question": {"enus": "An energy company has wind turbines, weather stations, and solar panels that generate telemetry data. The company wants to perform predictive maintenance on these devices. The devices are in various locations and have unstable internet connectivity. A team of data scientists is using the telemetry data to perform machine learning (ML) to conduct anomaly detection and predict maintenance before the devices start to deteriorate. The team needs a scalable, secure, high-velocity data ingestion mechanism. The team has decided to use Amazon S3 as the data storage location. Which approach meets these requirements? ", "zhcn": "一家能源公司拥有风力发电机、气象监测站及太阳能电池板，这些设备持续生成遥测数据。该公司计划对上述设备实施预测性维护。由于设备分布地域广泛且网络连接不稳定，数据科学团队正利用遥测数据开展机器学习，旨在实现异常状态监测并在设备性能衰退前预测维护需求。该团队需要构建一套可扩展、高安全性且能高速处理数据流的采集机制。团队已确定选用亚马逊S3作为数据存储平台。下列哪种方案最符合这些要求？"}, "option": [{"option_text": {"zhcn": "通过调用托管于亚马逊EC2云服务器的HTTP接口进行数据摄取。采用弹性负载均衡器后接自动扩展组态的EC2实例架构，将数据载入亚马逊S3存储服务。", "enus": "Ingest the data by using an HTTP API call to a web server that is hosted on Amazon EC2. Set up EC2 instances in an Auto Scaling  configuration behind an Elastic Load Balancer to load the data into Amazon S3."}, "option_flag": false}, {"option_text": {"zhcn": "通过MQTT协议将数据摄取至AWS IoT Core。在AWS IoT Core中配置规则，借助Amazon Kinesis Data Firehose将数据传送至Kinesis数据流，并预设该数据流将数据写入指定的S3存储桶。", "enus": "Ingest the data over Message Queuing Telemetry Transport (MQTT) to AWS IoT Core. Set up a rule in AWS IoT Core to use Amazon  Kinesis Data Firehose to send data to an Amazon Kinesis data stream that is configured to write to an S3 bucket."}, "option_flag": false}, {"option_text": {"zhcn": "通过MQTT协议将数据接入AWS IoT Core。在AWS IoT Core中配置规则，将所有MQTT数据路由至已设定写入S3存储桶的Amazon Kinesis Data Firehose传输流。", "enus": "Ingest the data over Message Queuing Telemetry Transport (MQTT) to AWS IoT Core. Set up a rule in AWS IoT Core to direct all MQTT  data to an Amazon Kinesis Data Firehose delivery stream that is configured to write to an S3 bucket."}, "option_flag": true}, {"option_text": {"zhcn": "通过MQTT协议将数据摄取至Amazon Kinesis数据流，该数据流已配置为写入指定的S3存储桶。", "enus": "Ingest the data over Message Queuing Telemetry Transport (MQTT) to Amazon Kinesis data stream that is configured to write to an S3  bucket."}, "option_flag": false}], "analysis": {"enus": "Reference: https://aws.amazon.com/blogs/industries/real-time-operational-monitoring-of-renewable-energy-assets-with-aws-iot/", "zhcn": "参考来源：https://aws.amazon.com/blogs/industries/real-time-operational-monitoring-of-renewable-energy-assets-with-aws-iot/"}, "answer": "C"}, {"id": "185", "question": {"enus": "A retail company collects customer comments about its products from social media, the company website, and customer call logs. A team of data scientists and engineers wants to find common topics and determine which products the customers are referring to in their comments. The team is using natural language processing (NLP) to build a model to help with this classification. Each product can be classified into multiple categories that the company defines. These categories are related but are not mutually exclusive. For example, if there is mention of \"Sample Yogurt\" in the document of customer comments, then \"Sample Yogurt\" should be classified as \"yogurt,\" \"snack,\" and \"dairy product.\" The team is using Amazon Comprehend to train the model and must complete the project as soon as possible. Which functionality of Amazon Comprehend should the team use to meet these requirements? ", "zhcn": "一家零售企业从社交媒体、公司官网及客服通话记录中收集客户对其产品的评价。数据科学家与工程师团队旨在从中提炼常见主题，并精准识别客户评论中提及的具体产品。该团队正运用自然语言处理技术构建分类模型，每个产品可对应企业定义的多个非互斥关联类别。例如，若客户评论中出现\"试饮酸奶\"字样，则该内容需同时归类于\"酸奶\"\"零食\"和\"乳制品\"三大类别。目前团队采用Amazon Comprehend平台进行模型训练，且需高效完成项目。请问，为满足上述需求，该团队应当选用Amazon Comprehend的哪项核心功能？"}, "option": [{"option_text": {"zhcn": "多类别模式下的自定义分类", "enus": "Custom classification with multi-class mode"}, "option_flag": true}, {"option_text": {"zhcn": "多标签模式下的自定义分类", "enus": "Custom classification with multi-label mode"}, "option_flag": false}, {"option_text": {"zhcn": "定制化实体识别", "enus": "Custom entity recognition"}, "option_flag": false}, {"option_text": {"zhcn": "内置模型", "enus": "Built-in models"}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is ‘Custom classification with multi - class mode’. The team needs to classify products into multiple related but non - mutually exclusive categories based on customer comments. In multi - class mode, an input can belong to multiple classes simultaneously, which aligns with the requirement that each product can be classified into multiple defined categories.\n\n‘Custom classification with multi - label mode’ is incorrect because multi - label mode is typically used when labels are independent, not related as in this case. ‘Custom entity recognition’ is wrong as the task is about classifying products into categories, not identifying entities within the text. ‘Built - in models’ won't work because the company has its own defined categories, so a custom solution is needed. The key factor distinguishing the real answer is its suitability for classifying products into multiple related categories, which is the core requirement of the project.", "zhcn": "问题的正确答案是“采用多类别模式的定制分类”。团队需要根据客户评论将产品划分到多个相关但非互斥的类别中。多类别模式允许单个输入同时归属于多个类别，这正好符合每个产品可被归入多个预设分类的要求。  \n\n“采用多标签模式的定制分类”并不适用，因为多标签模式通常用于标签相互独立的情形，而本案中的分类具有关联性；“自定义实体识别”亦不正确，因为任务核心是对产品进行分类而非识别文本中的实体；“内置模型”同样不适用，由于企业已设定专属分类体系，必须采用定制化方案。区分正确答案的关键在于其能够将产品精准划分至多个关联类别，这正是本项目最核心的需求。"}, "answer": "A"}, {"id": "186", "question": {"enus": "A data engineer is using AWS Glue to create optimized, secure datasets in Amazon S3. The data science team wants the ability to access the ETL scripts directly from Amazon SageMaker notebooks within a VPC. After this setup is complete, the data science team wants the ability to run the AWS Glue job and invoke the SageMaker training job. Which combination of steps should the data engineer take to meet these requirements? (Choose three.) ", "zhcn": "一位数据工程师正利用AWS Glue在Amazon S3中创建经过优化且安全的数据集。数据科学团队需要能够通过VPC内的Amazon SageMaker笔记本直接访问ETL脚本。完成此设置后，数据科学团队还需具备运行AWS Glue任务并调用SageMaker训练任务的能力。为满足这些需求，该数据工程师应采取哪三项步骤组合？（请选择三项）"}, "option": [{"option_text": {"zhcn": "在数据科学团队的VPC中创建SageMaker开发端点。", "enus": "Create a SageMaker development endpoint in the data science team's VPC."}, "option_flag": true}, {"option_text": {"zhcn": "在数据科学团队的VPC中创建一个AWS Glue开发端点。", "enus": "Create an AWS Glue development endpoint in the data science team's VPC."}, "option_flag": false}, {"option_text": {"zhcn": "通过AWS Glue开发终端创建SageMaker笔记本。", "enus": "Create SageMaker notebooks by using the AWS Glue development endpoint."}, "option_flag": false}, {"option_text": {"zhcn": "通过SageMaker控制台创建SageMaker笔记本实例。", "enus": "Create SageMaker notebooks by using the SageMaker console."}, "option_flag": true}, {"option_text": {"zhcn": "为 SageMaker 笔记本配置解密策略。", "enus": "Attach a decryption policy to the SageMaker notebooks."}, "option_flag": false}, {"option_text": {"zhcn": "为SageMaker笔记本创建IAM策略与IAM角色。", "enus": "Create an IAM policy and an IAM role for the SageMaker notebooks."}, "option_flag": true}], "analysis": {"enus": "Reference: https://aws.amazon.com/blogs/machine-learning/access-amazon-s3-data-managed-by-aws-glue-data-catalog-from-amazon-sagemaker- notebooks/", "zhcn": "参考来源：https://aws.amazon.com/blogs/machine-learning/access-amazon-s3-data-managed-by-aws-glue-data-catalog-from-amazon-sagemaker-notebooks/"}, "answer": "ADF"}, {"id": "187", "question": {"enus": "A data engineer needs to provide a team of data scientists with the appropriate dataset to run machine learning training jobs. The data will be stored in Amazon S3. The data engineer is obtaining the data from an Amazon Redshift database and is using join queries to extract a single tabular dataset. A portion of the schema is as follows: TransactionTimestamp (Timestamp) CardName (Varchar) CardNo (Varchar) The data engineer must provide the data so that any row with a CardNo value of NULL is removed. Also, the TransactionTimestamp column must be separated into a TransactionDate column and a TransactionTime column. Finally, the CardName column must be renamed to NameOnCard. The data will be extracted on a monthly basis and will be loaded into an S3 bucket. The solution must minimize the effort that is needed to set up infrastructure for the ingestion and transformation. The solution also must be automated and must minimize the load on the Amazon Redshift cluster. Which solution meets these requirements? ", "zhcn": "数据工程师需为数据科学团队提供适宜的数据集以支持机器学习训练任务。数据将存储于Amazon S3中，当前工程师正从Amazon Redshift数据库通过连接查询提取单一表格数据集。部分数据模式如下：  \n- 交易时间戳（Timestamp）  \n- 持卡人姓名（Varchar）  \n- 卡号（Varchar）  \n\n数据处理需满足以下要求：  \n1. 剔除卡号为NULL的所有数据行  \n2. 将交易时间戳字段拆分为独立交易日期列与交易时间列  \n3. 将持卡人姓名列重命名为NameOnCard  \n数据需按月提取并加载至S3存储桶，解决方案须最大限度减少数据摄取与转换所需的基础设施搭建成本，同时实现自动化流程并减轻Redshift集群负载。  \n\n何种方案可同时满足上述要求？"}, "option": [{"option_text": {"zhcn": "部署一个Amazon EMR集群，创建Apache Spark任务用于从Amazon Redshift集群读取数据并进行转换。将处理后的数据加载至S3存储桶，并将该任务配置为按月定期执行。", "enus": "Set up an Amazon EMR cluster. Create an Apache Spark job to read the data from the Amazon Redshift cluster and transform the data.  Load the data into the S3 bucket. Schedule the job to run monthly."}, "option_flag": false}, {"option_text": {"zhcn": "配置一台安装有SQL客户端（例如SQL Workbench/J）的亚马逊EC2实例，用于直接查询Amazon Redshift集群中的数据。将查询结果数据集导出至文件后，上传至S3存储桶。上述操作需每月定期执行。", "enus": "Set up an Amazon EC2 instance with a SQL client tool, such as SQL Workbench/J, to query the data from the Amazon Redshift cluster  directly Export the resulting dataset into a file. Upload the file into the S3 bucket. Perform these tasks monthly."}, "option_flag": false}, {"option_text": {"zhcn": "创建一个AWS Glue作业，以Amazon Redshift集群为数据源，S3存储桶为目标端。运用内置的Filter、Map及RenameField转换器实现所需的数据处理逻辑，并将该作业配置为按月自动执行。", "enus": "Set up an AWS Glue job that has the Amazon Redshift cluster as the source and the S3 bucket as the destination. Use the built-in  transforms Filter, Map, and RenameField to perform the required transformations. Schedule the job to run monthly."}, "option_flag": true}, {"option_text": {"zhcn": "利用Amazon Redshift Spectrum执行查询，将数据直接写入S3存储桶。同时创建AWS Lambda函数，按月自动运行该查询任务。", "enus": "Use Amazon Redshift Spectrum to run a query that writes the data directly to the S3 bucket. Create an AWS Lambda function to run the  query monthly."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to set up an AWS Glue job with the Amazon Redshift cluster as the source and the S3 bucket as the destination, using built - in transforms and scheduling it to run monthly. This solution meets the requirements because AWS Glue is a fully managed extract, transform, and load (ETL) service. It reduces the effort of setting up infrastructure as it handles the underlying resources automatically. The built - in transforms like Filter, Map, and RenameField simplify the required data transformations, such as removing NULL CardNo values, splitting the timestamp, and renaming a column.\n\nThe option of setting up an Amazon EMR cluster and creating an Apache Spark job is more complex. It requires managing the EMR cluster, including its configuration, scaling, and maintenance, which increases the infrastructure setup effort.\n\nUsing an Amazon EC2 instance with a SQL client tool is a manual and less automated approach. It involves multiple steps like querying, exporting to a file, and uploading, and it doesn't take advantage of built - in data transformation capabilities, thus increasing the overall effort.\n\nUsing Amazon Redshift Spectrum to write data directly to S3 and an AWS Lambda function to run the query lacks the necessary data transformation features. It can't easily perform the required filtering, splitting, and renaming operations as efficiently as AWS Glue's built - in transforms.\n\nIn summary, the AWS Glue solution minimizes infrastructure setup, automates the process, and efficiently handles the data transformations, making it the best choice over the other fake answer options.", "zhcn": "针对此问题，正确答案是创建一个以Amazon Redshift集群为数据源、S3存储桶为目标地的AWS Glue作业，通过内置转换功能并设定每月自动运行。此方案符合要求，因为AWS Glue作为全托管的提取、转换和加载（ETL）服务，能自动处理底层基础设施，大幅降低环境搭建成本。其内置的筛选、映射、字段重命名等转换操作，可轻松实现剔除空值卡号、拆分时间戳、字段重命名等数据加工需求。\n\n相比之下，搭建Amazon EMR集群并创建Apache Spark作业的方案更为复杂，需额外承担集群配置、扩缩容及运维管理工作，增加了基础设施管理负担。而采用安装SQL客户端的Amazon EC2实例则属于半自动化方案，需手动执行查询、导出文件、上传等多步操作，且无法利用内置数据转换能力，整体效率较低。若通过Amazon Redshift Spectrum直接向S3写入数据，并配合AWS Lambda执行查询，则缺乏高效的数据转换功能，难以像AWS Glue那样流畅完成数据筛选、拆分和重命名操作。\n\n综上，AWS Glue方案既能最大限度简化基础设施配置，实现流程自动化，又具备高效的数据转换能力，相较其他干扰选项更具优势。"}, "answer": "C"}, {"id": "188", "question": {"enus": "A machine learning (ML) specialist wants to bring a custom training algorithm to Amazon SageMaker. The ML specialist implements the algorithm in a Docker container that is supported by SageMaker. How should the ML specialist package the Docker container so that SageMaker can launch the training correctly? ", "zhcn": "一位机器学习专家希望将自定义训练算法引入Amazon SageMaker平台。该专家已将算法实现在SageMaker支持的Docker容器中。为确保SageMaker能正确启动训练任务，专家应当如何封装该Docker容器？"}, "option": [{"option_text": {"zhcn": "在Dockerfile的ENTRYPOINT指令中指定服务器参数。", "enus": "Specify the server argument in the ENTRYPOINT instruction in the Dockerfile."}, "option_flag": false}, {"option_text": {"zhcn": "在Dockerfile中，请于ENTRYPOINT指令处明确定义训练程序。", "enus": "Specify the training program in the ENTRYPOINT instruction in the Dockerfile."}, "option_flag": true}, {"option_text": {"zhcn": "在打包容器时，请将训练数据的路径添加至Docker构建命令中。", "enus": "Include the path to the training data in the docker build command when packaging the container."}, "option_flag": false}, {"option_text": {"zhcn": "在 Dockerfile 中通过 COPY 指令将训练程序复制到 /opt/ml/train 目录。", "enus": "Use a COPY instruction in the Dockerfile to copy the training program to the /opt/ml/train directory."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is “Specify the training program in the ENTRYPOINT instruction in the Dockerfile.” In Amazon SageMaker, the ENTRYPOINT instruction in the Dockerfile is used to define the command that will be executed when the container starts for training. By specifying the training program here, SageMaker knows exactly what to run to start the training process.\n\nThe option “Specify the server argument in the ENTRYPOINT instruction in the Dockerfile” is incorrect because the server argument is more relevant for serving models rather than training. Training and serving are different phases in the ML workflow, and the focus here is on training.\n\nThe option “Include the path to the training data in the docker build command when packaging the container” is wrong. The docker build command is used to create the container image, and including the training data path at this stage is inappropriate. Training data is typically accessed during the training execution and is managed separately from the container build process.\n\nThe option “Use a COPY instruction in the Dockerfile to copy the training program to the /opt/ml/train directory” is also incorrect. While SageMaker has a specific directory structure, just copying the program to the directory doesn't ensure that it will be executed. The ENTRYPOINT is needed to actually start the training program.\n\nThe key factor distinguishing the real answer is that it correctly identifies how to initiate the training process in SageMaker, while the fake options either address the wrong phase (serving) or mismanage the training data and program execution.", "zhcn": "问题的正确答案是\"在Dockerfile的ENTRYPOINT指令中指定训练程序\"。在Amazon SageMaker中，Dockerfile中的ENTRYPOINT指令用于定义容器启动训练时将执行的命令。通过在此处指定训练程序，SageMaker便能准确知晓启动训练过程所需运行的内容。\n\n\"在DOCKERFILE的ENTRYPOINT指令中指定服务器参数\"这一选项并不正确，因为服务器参数更适用于模型部署而非训练场景。训练与部署是机器学习工作流中两个不同的阶段，而此处重点在于训练环节。\n\n\"在打包容器时通过docker build命令包含训练数据路径\"这一选项同样有误。docker build命令仅用于创建容器镜像，在此阶段包含训练数据路径并不恰当。训练数据通常在训练执行过程中进行访问，且与容器构建过程分开管理。\n\n\"使用Dockerfile中的COPY指令将训练程序复制到/opt/ml/train目录\"亦非正解。虽然SageMaker具有特定的目录结构，但仅将程序复制到目录并不能确保其被执行。实际启动训练程序仍需依赖ENTRYPOINT指令。\n\n真正答案的关键在于准确指出了SageMaker中启动训练过程的正确方式，而错误选项要么涉及错误的阶段（部署），要么对训练数据和程序执行的处理方式存在偏差。"}, "answer": "B"}, {"id": "189", "question": {"enus": "An ecommerce company wants to use machine learning (ML) to monitor fraudulent transactions on its website. The company is using Amazon SageMaker to research, train, deploy, and monitor the ML models. The historical transactions data is in a .csv file that is stored in Amazon S3. The data contains features such as the user's IP address, navigation time, average time on each page, and the number of clicks for each session. There is no label in the data to indicate if a transaction is anomalous. Which models should the company use in combination to detect anomalous transactions? (Choose two.) ", "zhcn": "一家电子商务公司希望借助机器学习技术监测其网站上的欺诈交易。该公司正使用Amazon SageMaker进行机器学习模型的研究、训练、部署与监控。历史交易数据存储于Amazon S3的.csv格式文件中，包含用户IP地址、浏览时长、页面平均停留时间及单次会话点击量等特征。由于数据未标注交易是否异常，请问该公司应采用哪两种模型组合来实现异常交易检测？（请选择两项）"}, "option": [{"option_text": {"zhcn": "“IP洞察”", "enus": "IP Insights"}, "option_flag": true}, {"option_text": {"zhcn": "K-近邻算法（k-NN）", "enus": "K-nearest neighbors (k-NN)"}, "option_flag": false}, {"option_text": {"zhcn": "采用逻辑函数的线性学习器", "enus": "Linear learner with a logistic function"}, "option_flag": true}, {"option_text": {"zhcn": "随机切割森林（RCF）", "enus": "Random Cut Forest (RCF)"}, "option_flag": false}, {"option_text": {"zhcn": "XGBoost", "enus": "XGBoost"}, "option_flag": false}], "analysis": {"enus": "The correct answers to the question are ‘IP Insights’ and ‘Linear learner with a logistic function’. \n\n‘IP Insights’ is suitable as it can analyze IP - related data, and since the dataset includes the user's IP address, it can help detect anomalies based on IP behavior patterns. A ‘Linear learner with a logistic function’ can be used for binary classification tasks. Even though there are no pre - labeled data, once the model is trained on the historical data, it can classify transactions as normal or anomalous.\n\n‘K - nearest neighbors (k - NN)’ is not ideal because it requires labeled data for training to accurately classify new data points, and our dataset lacks labels. ‘Random Cut Forest (RCF)’ is an unsupervised learning algorithm, but it is not mentioned in the real answer options. ‘XGBoost’ is a powerful supervised learning algorithm, and similar to k - NN, it needs labeled data to train effectively. \n\nThe key factor distinguishing the real answer options is their ability to work well with the given unlabeled data and the nature of the features in the dataset. The common misconception might be choosing supervised learning algorithms like k - NN and XGBoost without considering the lack of labels in the data.", "zhcn": "该问题的正确答案是“IP Insights”和“采用逻辑函数的线性学习器”。选择“IP Insights”是因为它能分析IP相关数据，而数据集包含用户IP地址，可基于IP行为模式检测异常。“采用逻辑函数的线性学习器”适用于二分类任务，虽无预标注数据，但通过历史数据训练后能对交易进行正常或异常的判别。\n\n“K-近邻算法（k-NN）”并不适用，因其需要带标注的训练数据才能准确分类新数据点，而当前数据集缺少标签。“随机切割森林（RCF）”虽属无监督学习算法，但实际备选答案中未提及。“XGBoost”作为强大的监督学习算法，与k-NN类似，需依赖标注数据才能有效训练。区分正确答案的关键在于算法处理无标注数据的能力与数据集特征属性的匹配度。常见误区在于未考虑数据缺乏标注的事实，误选k-NN或XGBoost等监督学习算法。"}, "answer": "AC"}, {"id": "190", "question": {"enus": "A healthcare company is using an Amazon SageMaker notebook instance to develop machine learning (ML) models. The company's data scientists will need to be able to access datasets stored in Amazon S3 to train the models. Due to regulatory requirements, access to the data from instances and services used for training must not be transmitted over the internet. Which combination of steps should an ML specialist take to provide this access? (Choose two.) ", "zhcn": "一家医疗公司正借助亚马逊SageMaker笔记本来开发机器学习模型。为确保数据科学家能够访问存储在亚马逊S3中用于训练模型的数据集，同时遵循监管要求（训练所用实例与服务的数据传输不得经由互联网），机器学习专家应采取哪两项措施实现此目标？（请选择两项）"}, "option": [{"option_text": {"zhcn": "将SageMaker笔记本实例配置为在启动时附加VPC并禁用互联网访问。", "enus": "Configure the SageMaker notebook instance to be launched with a VPC attached and internet access disabled."}, "option_flag": true}, {"option_text": {"zhcn": "在 SageMaker 与 Amazon S3 之间建立并配置 VPN 隧道。", "enus": "Create and configure a VPN tunnel between SageMaker and Amazon S3."}, "option_flag": false}, {"option_text": {"zhcn": "创建并配置一个S3 VPC终端节点，将其关联至指定VPC。", "enus": "Create and configure an S3 VPC endpoint Attach it to the VPC."}, "option_flag": true}, {"option_text": {"zhcn": "创建一条S3存储桶策略，允许来自VPC的流量访问，同时拒绝来自互联网的流量访问。", "enus": "Create an S3 bucket policy that allows trafic from the VPC and denies trafic from the internet."}, "option_flag": false}, {"option_text": {"zhcn": "部署AWS中转网关，将S3存储桶与SageMaker实例连接至网关。", "enus": "Deploy AWS Transit Gateway Attach the S3 bucket and the SageMaker instance to the gateway."}, "option_flag": false}], "analysis": {"enus": "The correct answers to the question are “Configure the SageMaker notebook instance to be launched with a VPC attached and internet access disabled.” and “Create and configure an S3 VPC endpoint Attach it to the VPC.” \n\nBy attaching a VPC to the SageMaker notebook instance and disabling internet access, we ensure that the data transfer is not exposed to the public internet, meeting the regulatory requirements. Creating and attaching an S3 VPC endpoint to the VPC allows the SageMaker instance to access S3 resources privately within the VPC, without the need for internet traffic.\n\nThe option “Create and configure a VPN tunnel between SageMaker and Amazon S3” is incorrect because a VPN tunnel still involves internet - based communication, which violates the requirement of not transmitting data over the internet. \n\nThe option “Create an S3 bucket policy that allows traffic from the VPC and denies traffic from the internet” only controls access at the bucket level. It doesn't provide a private connection path, and data may still be routed over the internet. \n\nThe option “Deploy AWS Transit Gateway Attach the S3 bucket and the SageMaker instance to the gateway” is also wrong. AWS Transit Gateway is mainly used for connecting multiple VPCs and on - premises networks, and it doesn't eliminate the need for internet traffic when accessing S3. \n\nCommon misconceptions might include thinking that a bucket policy or a VPN can achieve private access without considering the actual data transfer path. Also, misunderstanding the purpose of AWS Transit Gateway can lead to choosing the wrong option.", "zhcn": "该问题的正确答案是：“将SageMaker笔记本实例配置为附加VPC启动并禁用互联网访问”以及“创建并配置S3 VPC终端节点，将其附加至VPC”。通过为SageMaker笔记本实例附加VPC并禁用互联网访问，可确保数据传输不暴露于公共网络，符合监管要求。创建S3 VPC终端节点并附加至VPC后，SageMaker实例即可在VPC内通过私有方式访问S3资源，无需经过互联网流量。\n\n“创建SageMaker与Amazon S3之间的VPN隧道”这一选项错误，因为VPN隧道仍依赖互联网通信，违反了不得通过互联网传输数据的要求。“创建允许VPC流量、拒绝互联网流量的S3存储桶策略”仅能在存储桶层面控制访问权限，既未提供私有连接路径，数据仍可能通过互联网路由。“部署AWS Transit Gateway并将S3存储桶与SageMaker实例附加至网关”亦不正确，因该服务主要用于连接多VPC与本地网络，无法消除访问S3时对互联网流量的依赖。\n\n常见误解包括：误认为存储桶策略或VPN可实现私有访问而忽略实际数据传输路径，或错误理解AWS Transit Gateway的功能导致选择偏差。"}, "answer": "AC"}, {"id": "191", "question": {"enus": "A machine learning (ML) specialist at a retail company is forecasting sales for one of the company's stores. The ML specialist is using data from the past 10 years. The company has provided a dataset that includes the total amount of money in sales each day for the store. Approximately 5% of the days are missing sales data. The ML specialist builds a simple forecasting model with the dataset and discovers that the model performs poorly. The performance is poor around the time of seasonal events, when the model consistently predicts sales figures that are too low or too high. Which actions should the ML specialist take to try to improve the model's performance? (Choose two.) ", "zhcn": "某零售公司的机器学习专家正在为旗下门店进行销售额预测。该专家采用了过去十年的历史数据，公司提供的数据集包含该门店每日销售总额，其中约5%的日期存在销售数据缺失。专家基于此数据集构建了一个简易预测模型，但发现模型在季节性活动期间表现不佳——其预测值总是系统性偏离实际值，或明显偏高或偏低。若要提升模型性能，该专家应采取哪两项改进措施？（请选择两项）"}, "option": [{"option_text": {"zhcn": "向数据集中补充该店铺的销售周期信息。", "enus": "Add information about the store's sales periods to the dataset."}, "option_flag": false}, {"option_text": {"zhcn": "邻近区域内各门店的销售数据汇总。", "enus": "Aggregate sales figures from stores in the same proximity."}, "option_flag": true}, {"option_text": {"zhcn": "对数据进行平滑处理以修正季节性波动。", "enus": "Apply smoothing to correct for seasonal variation."}, "option_flag": false}, {"option_text": {"zhcn": "将预测频率由每日调整为每周。", "enus": "Change the forecast frequency from daily to weekly."}, "option_flag": false}, {"option_text": {"zhcn": "采用线性插值法填补数据集中的缺失值。", "enus": "Replace missing values in the dataset by using linear interpolation."}, "option_flag": true}], "analysis": {"enus": "The correct answers to improve the model's performance are “Aggregate sales figures from stores in the same proximity” and “Replace missing values in the dataset by using linear interpolation”. \n\nAggregating sales figures from nearby stores can provide additional relevant data. Stores in the same area are likely to be affected by similar local factors, such as customer demographics, economic conditions, and seasonal events. This extra data can enrich the model and make it more robust, especially around seasonal events. \n\nReplacing missing values using linear interpolation helps to fill in the 5% of missing sales data. Since the data is time - series (daily sales), linear interpolation can estimate the missing values based on the surrounding data points, providing a more complete dataset for the model to learn from.\n\nThe option “Add information about the store's sales periods to the dataset” is not clear enough. It's not evident how just adding information about sales periods would directly address the issues of poor performance around seasonal events and missing data. \n\n“Apply smoothing to correct for seasonal variation” might seem like a solution for seasonal issues, but it doesn't deal with the missing data problem, and the model's poor performance might be due to multiple factors beyond just seasonality. \n\n“Change the forecast frequency from daily to weekly” can simplify the forecasting task, but it doesn't solve the root problems of missing data and inaccurate predictions around seasonal events.\n\nThese are the reasons why the real answer options are more appropriate than the fake ones for improving the model's performance.", "zhcn": "为提升模型表现，正确答案应为“聚合相邻门店的销售数据”与“采用线性插值填补数据集缺失值”。汇总邻近门店的销售数据能提供更多有效信息——处于相同商圈的店铺往往受到相似的地域因素影响，如客群特征、经济状况及季节性活动等。这些补充数据不仅能增强模型的丰富度，更能提升其鲁棒性，尤其在应对季节性波动时表现更为突出。而通过线性插值处理缺失值，可有效填补5%的销售数据空白。由于数据本身具有时间序列特性（日销售数据），线性插值能根据相邻数据点估算缺失值，为模型提供更完整的学习样本。\n\n至于“添加门店销售周期信息”这一选项，其表述不够明确。仅增加销售时段信息如何直接改善季节性事件预测不准及数据缺失问题，尚未体现清晰的作用路径。而“采用平滑法修正季节性波动”看似能应对周期性问题，但既未解决数据缺失的痛点，也未考虑除季节性外可能影响模型表现的多重因素。若将预测频率从日度改为周度，虽可简化预测任务，却未能从根本上弥补数据缺失及季节性事件预测失准的缺陷。\n\n正因如此，在提升模型性能的方案选择上，前述两项真实答案比失真选项更具实践价值。"}, "answer": "BE"}, {"id": "192", "question": {"enus": "A newspaper publisher has a table of customer data that consists of several numerical and categorical features, such as age and education history, as well as subscription status. The company wants to build a targeted marketing model for predicting the subscription status based on the table data. Which Amazon SageMaker built-in algorithm should be used to model the targeted marketing? ", "zhcn": "一家报社拥有一份客户数据表，其中包含若干数值型与类别型特征，例如年龄与教育背景，以及订阅状态信息。该公司希望基于此表格数据构建精准营销模型，用以预测客户的订阅意向。在此场景下，应当选用亚马逊SageMaker平台中的哪种内置算法来建立该精准营销模型？"}, "option": [{"option_text": {"zhcn": "随机切割森林（RCF）", "enus": "Random Cut Forest (RCF)"}, "option_flag": false}, {"option_text": {"zhcn": "XGBoost", "enus": "XGBoost"}, "option_flag": false}, {"option_text": {"zhcn": "神经主题模型（Neural Topic Model，简称NTM）", "enus": "Neural Topic Model (NTM)"}, "option_flag": true}, {"option_text": {"zhcn": "DeepAR预测模型", "enus": "DeepAR forecasting"}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question ‘A newspaper publisher has a table of customer data... Which Amazon SageMaker built - in algorithm should be used to model the targeted marketing?’ is ‘Neural Topic Model (NTM)’. NTM is suitable for this case as it can handle both numerical and categorical features in the customer data table and is useful for extracting latent topics from data, which can help in understanding customer segments and predicting subscription status.\n\n‘Random Cut Forest (RCF)’ is mainly used for anomaly detection, so it's not appropriate for predicting subscription status. ‘XGBoost’ is a powerful gradient - boosting algorithm, but it may not be the best fit here as it doesn't specifically deal with extracting latent topics from mixed - type data. ‘DeepAR forecasting’ is designed for time - series forecasting, and the given problem doesn't involve time - series data. The ability of NTM to work with diverse data types and extract relevant topics for prediction is the key reason it's the real answer, setting it apart from the fake options.", "zhcn": "针对问题“某报社拥有一张客户数据表……应采用Amazon SageMaker中的哪种内置算法来建立目标营销模型？”的正确答案是“神经主题模型（Neural Topic Model，简称NTM）”。NTM适用于此场景，因其能同时处理客户数据表中的数值型与类别型特征，并能从数据中提取潜在主题，从而帮助理解客户分群并预测订阅行为。  \n\n“随机切割森林（RCF）”主要用于异常检测，故不适用于订阅状态预测；“XGBoost”虽为强大的梯度提升算法，但其并不专门针对混合型数据的潜在主题挖掘；“DeepAR预测”专为时间序列分析设计，而本题未涉及时间序列数据。NTM能够兼容多样化数据类型并提取关键预测主题的特性，正是其区别于干扰选项的核心优势。"}, "answer": "C"}, {"id": "193", "question": {"enus": "A company will use Amazon SageMaker to train and host a machine learning model for a marketing campaign. The data must be encrypted at rest. Most of the data is sensitive customer data. The company wants AWS to maintain the root of trust for the encryption keys and wants key usage to be logged. Which solution will meet these requirements with the LEAST operational overhead? ", "zhcn": "一家公司将利用Amazon SageMaker平台，为营销活动训练并部署机器学习模型。所有静态数据均需加密存储，其中大部分为敏感的客户信息。该公司要求由AWS托管加密密钥的信任根，并记录密钥使用日志。在满足上述需求的前提下，哪种解决方案能最大限度降低运维复杂度？"}, "option": [{"option_text": {"zhcn": "借助AWS安全令牌服务（AWS STS）生成临时安全凭证，为所有SageMaker实例的存储卷进行加密，同时保护Amazon S3中的模型制品及数据。", "enus": "Use AWS Security Token Service (AWS STS) to create temporary tokens to encrypt the storage volumes for all SageMaker instances and  to encrypt the model artifacts and data in Amazon S3."}, "option_flag": false}, {"option_text": {"zhcn": "在AWS密钥管理服务（AWS KMS）中采用客户托管密钥，对所有SageMaker实例的存储卷进行加密，并对Amazon S3中的模型工件及数据实施加密保护。", "enus": "Use customer managed keys in AWS Key Management Service (AWS KMS) to encrypt the storage volumes for all SageMaker instances  and to encrypt the model artifacts and data in Amazon S3."}, "option_flag": false}, {"option_text": {"zhcn": "借助AWS CloudHSM中存储的加密密钥，为所有SageMaker实例的存储卷进行加密，并对Amazon S3中的模型制品及数据实施加密保护。", "enus": "Use encryption keys stored in AWS CloudHSM to encrypt the storage volumes for all SageMaker instances and to encrypt the model  artifacts and data in Amazon S3."}, "option_flag": false}, {"option_text": {"zhcn": "利用SageMaker内置临时密钥对所有SageMaker实例的存储卷进行加密。为新建的亚马逊弹性块存储卷启用默认加密功能。", "enus": "Use SageMaker built-in transient keys to encrypt the storage volumes for all SageMaker instances. Enable default encryption ffnew  Amazon Elastic Block Store (Amazon EBS) volumes."}, "option_flag": true}], "analysis": {"enus": "The correct answer to the question is to “Use SageMaker built - in transient keys to encrypt the storage volumes for all SageMaker instances. Enable default encryption for new Amazon Elastic Block Store (Amazon EBS) volumes.” This solution meets the requirements with the least operational overhead as AWS manages the keys, maintaining the root of trust, and key usage is logged as part of AWS's security mechanisms.\n\nThe option of using AWS Security Token Service (AWS STS) is incorrect because STS is mainly for generating temporary security credentials, not for encryption at rest. It doesn't address the core need of encrypting data at rest with proper key management and logging.\n\nUsing customer - managed keys in AWS KMS requires more operational effort. Customers are responsible for managing the keys, which involves additional tasks such as key rotation, access control, and auditing. This goes against the requirement of least operational overhead.\n\nUsing encryption keys stored in AWS CloudHSM also has high operational overhead. CloudHSM requires dedicated hardware management, including installation, configuration, and maintenance. It is a more complex and resource - intensive solution compared to using SageMaker's built - in transient keys.\n\nIn summary, the real answer leverages AWS's built - in features to handle key management and encryption, minimizing the company's operational burden, which is why it is the correct choice over the fake answer options.", "zhcn": "针对该问题的正确答案是\"使用SageMaker内置临时密钥对所有SageMaker实例的存储卷进行加密，并为新建的Amazon弹性块存储卷启用默认加密功能\"。该方案通过AWS托管密钥、维持信任根机制，并将密钥使用记录纳入AWS安全日志体系，在满足安全要求的同时实现了运维成本最小化。\n\n采用AWS安全令牌服务的方案并不适用，因为该服务主要生成临时安全凭证，无法实现静态数据加密功能，既不能满足密钥管理的核心需求，也缺乏相应的使用记录机制。\n\n使用AWS密钥管理服务的客户托管密钥会增加运维负担，客户需自行承担密钥轮换、访问控制和审计等管理职责，这与\"最低运维成本\"的要求相悖。而采用AWS CloudHSM存储加密密钥的方案同样存在高运维开销，该服务需要专有硬件管理，包括安装、配置和维护环节，相比SageMaker内置临时密钥方案更为复杂且耗费资源。\n\n综上所述，正确答案充分利用了AWS原生功能来处理密钥管理和加密事务，最大限度减轻企业运维压力，这正是其优于其他选项的关键所在。"}, "answer": "D"}, {"id": "194", "question": {"enus": "A data scientist is working on a model to predict a company's required inventory stock levels. All historical data is stored in .csv files in the company's data lake on Amazon S3. The dataset consists of approximately 500 GB of data The data scientist wants to use SQL to explore the data before training the model. The company wants to minimize costs. Which option meets these requirements with the LEAST operational overhead? ", "zhcn": "一位数据科学家正在构建预测公司所需库存水平的模型。所有历史数据均以.csv格式存储于亚马逊S3平台的企业数据湖中，数据集规模约为500GB。该科学家计划在训练模型前使用SQL进行数据探查，且公司要求尽可能控制成本。在满足上述需求的前提下，下列方案中哪一项能以最低运维负担实现目标？"}, "option": [{"option_text": {"zhcn": "创建Amazon EMR集群。在Apache Hive元存储中建立外部表，使其指向存储于S3存储桶内的数据。随后可通过Hive控制台进行数据探查。", "enus": "Create an Amazon EMR cluster. Create external tables in the Apache Hive metastore, referencing the data that is stored in the S3  bucket. Explore the data from the Hive console."}, "option_flag": false}, {"option_text": {"zhcn": "借助AWS Glue对S3存储桶进行元数据爬取，并在AWS Glue数据目录中建立数据表。随后通过Amazon Athena对数据进行探索分析。", "enus": "Use AWS Glue to crawl the S3 bucket and create tables in the AWS Glue Data Catalog. Use Amazon Athena to explore the data."}, "option_flag": false}, {"option_text": {"zhcn": "创建一个Amazon Redshift集群。通过COPY命令从Amazon S3导入数据。利用Amazon Redshift查询编辑器界面进行数据探索。", "enus": "Create an Amazon Redshift cluster. Use the COPY command to ingest the data from Amazon S3. Explore the data from the Amazon  Redshift query editor GUI."}, "option_flag": false}, {"option_text": {"zhcn": "创建Amazon Redshift集群。在外部模式中建立外部表，关联存储数据的S3桶。通过Amazon Redshift查询编辑器图形界面进行数据探查。", "enus": "Create an Amazon Redshift cluster. Create external tables in an external schema, referencing the S3 bucket that contains the data.  Explore the data from the Amazon Redshift query editor GUI."}, "option_flag": true}], "analysis": {"enus": "The correct answer to the question is to create an Amazon Redshift cluster, create external tables in an external schema referencing the S3 bucket with the data, and explore the data from the Amazon Redshift query editor GUI. This option minimizes costs and has the least operational overhead because it allows direct querying of the data in S3 without the need to ingest all 500 GB of data into Redshift, which saves storage costs and the time associated with data ingestion.\n\nFor the fake answer “Create an Amazon EMR cluster. Create external tables in the Apache Hive metastore, referencing the data that is stored in the S3 bucket. Explore the data from the Hive console,” managing an EMR cluster involves more operational overhead as it requires cluster configuration, maintenance, and monitoring, and it may not be as cost - effective as Redshift for SQL - based exploration.\n\nThe option “Use AWS Glue to crawl the S3 bucket and create tables in the AWS Glue Data Catalog. Use Amazon Athena to explore the data” can be costly for large - scale data exploration like this 500 GB dataset, as Athena charges per query execution, and frequent exploration queries can add up quickly.\n\nThe option “Create an Amazon Redshift cluster. Use the COPY command to ingest the data from Amazon S3. Explore the data from the Amazon Redshift query editor GUI” incurs additional storage costs for storing the 500 GB of data in Redshift and also has the overhead of the data ingestion process, making it less cost - effective and having more operational complexity compared to using external tables.", "zhcn": "对于该问题的正确答案是：创建Amazon Redshift集群，在引用S3数据桶的外部模式中建立外部表，通过Redshift查询编辑器界面进行数据探查。此方案能最大限度控制成本并减少运维负担，因其支持直接查询S3中的数据，无需将500GB数据全量加载至Redshift，既节省存储开销又避免了数据导入的时间消耗。\n\n而错误答案\"创建Amazon EMR集群，在Apache Hive元存储中建立指向S3数据的外部表，通过Hive控制台进行数据探查\"存在较高运维复杂度，需要涉及集群配置、维护及监控环节，对于基于SQL的数据探查场景，其成本效益不如Redshift方案。\n\n至于\"使用AWS Glue抓取S3数据桶并在Glue数据目录中建表，通过Amazon Athena进行数据探查\"的方案，针对500GB大规模数据探查会产生较高成本——Athena按查询次数计费，频繁的探查操作将快速累积费用。\n\n另一选项\"创建Amazon Redshift集群，使用COPY命令从S3导入数据，通过Redshift查询编辑器进行探查\"不仅会产生Redshift存储500GB数据的额外成本，还需承担数据导入过程的运维负担，相较于使用外部表方案，其经济性和操作简便性都稍逊一筹。"}, "answer": "D"}, {"id": "195", "question": {"enus": "A geospatial analysis company processes thousands of new satellite images each day to produce vessel detection data for commercial shipping. The company stores the training data in Amazon S3. The training data incrementally increases in size with new images each day. The company has configured an Amazon SageMaker training job to use a single ml.p2.xlarge instance with File input mode to train the built-in Object Detection algorithm. The training process was successful last month but is now failing because of a lack of storage. Aside from the addition of training data, nothing has changed in the model training process. A machine learning (ML) specialist needs to change the training configuration to fix the problem. The solution must optimize performance and must minimize the cost of training. Which solution will meet these requirements? ", "zhcn": "一家地理空间分析公司每日处理数千幅新增卫星影像，为商业航运提供船舶探测数据。该公司将训练数据存储于亚马逊S3服务中，随着每日新增影像的不断汇入，训练数据规模持续扩大。公司原采用亚马逊SageMaker训练任务，配置单台ml.p2.xlarge实例并以文件输入模式运行内置目标检测算法。上月训练流程尚能顺利完成，而今却因存储空间不足而中断。除训练数据增加外，模型训练流程未作任何变动。机器学习专家需调整训练配置以解决此问题，且解决方案必须兼顾性能优化与训练成本控制。请问下列哪种方案符合这些要求？"}, "option": [{"option_text": {"zhcn": "调整训练配置，采用两台ml.p2.xlarge实例进行模型训练。", "enus": "Modify the training configuration to use two ml.p2.xlarge instances."}, "option_flag": false}, {"option_text": {"zhcn": "调整训练配置，采用管道输入模式。", "enus": "Modify the training configuration to use Pipe input mode."}, "option_flag": false}, {"option_text": {"zhcn": "调整训练配置，采用单台ml.p3.2xlarge实例进行运算。", "enus": "Modify the training configuration to use a single ml.p3.2xlarge instance."}, "option_flag": true}, {"option_text": {"zhcn": "调整训练配置，采用亚马逊弹性文件系统（Amazon EFS）替代亚马逊S3，用于存储训练输入数据。", "enus": "Modify the training configuration to use Amazon Elastic File System (Amazon EFS) instead of Amazon S3 to store the input training  data."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is ‘Modify the training configuration to use a single ml.p3.2xlarge instance.’ The main issue is that the training is failing due to a lack of storage as the training data has increased. The ml.p3.2xlarge instance offers more storage and better performance compared to the ml.p2.xlarge instance, which can handle the growing data. It also optimizes performance as it is a more advanced instance type, and since only one instance is used, it helps minimize the cost of training.\n\n‘Modify the training configuration to use two ml.p2.xlarge instances’ would increase the cost as two instances are used instead of one, and it may not fully address the storage issue as the ml.p2.xlarge has inherent storage limitations. ‘Modify the training configuration to use Pipe input mode’ mainly focuses on streaming data during training and does not directly solve the storage problem. ‘Modify the training configuration to use Amazon Elastic File System (Amazon EFS) instead of Amazon S3 to store the input training data’ would add complexity and potentially increase costs without directly dealing with the storage capacity of the training instance. Thus, the ml.p3.2xlarge instance is the best choice to meet the requirements of optimizing performance and minimizing cost while solving the storage problem.", "zhcn": "对于该问题的正确答案是\"修改训练配置，采用单台ml.p3.2xlarge实例\"。核心问题在于训练数据量增加导致存储空间不足，致使训练失败。相较于ml.p2.xlarge实例，ml.p3.2xlarge不仅能提供更大存储空间和更优性能以应对增长的数据需求，还因其为更先进的实例类型而优化了性能表现。同时，采用单实例方案有助于控制训练成本。\n\n若选择\"修改训练配置，采用两台ml.p2.xlarge实例\"，不仅会因双实例运作增加成本，且ml.p2.xlarge固有的存储限制仍可能无法彻底解决存储问题。而\"修改训练配置，采用管道输入模式\"主要针对训练过程中的数据流传输，并未直接解决存储瓶颈。至于\"修改训练配置，采用亚马逊弹性文件系统替代亚马逊S3存储训练输入数据\"，此方案将引入额外复杂性并可能推高成本，却未直接改善训练实例的存储容量。\n\n因此，选用ml.p3.2xlarge实例既能有效解决存储难题，又能在优化性能与控制成本之间实现最佳平衡。"}, "answer": "C"}, {"id": "196", "question": {"enus": "A company is using Amazon SageMaker to build a machine learning (ML) model to predict customer churn based on customer call transcripts. Audio files from customer calls are located in an on-premises VoIP system that has petabytes of recorded calls. The on-premises infrastructure has high-velocity networking and connects to the company's AWS infrastructure through a VPN connection over a 100 Mbps connection. The company has an algorithm for transcribing customer calls that requires GPUs for inference. The company wants to store these transcriptions in an Amazon S3 bucket in the AWS Cloud for model development. Which solution should an ML specialist use to deliver the transcriptions to the S3 bucket as quickly as possible? ", "zhcn": "某公司正运用Amazon SageMaker构建机器学习模型，旨在通过客户通话记录预测用户流失情况。企业本地VoIP系统中存有数PB的客户通话音频文件，该本地基础设施具备高速网络特性，并通过100 Mbps带宽的VPN连接与公司AWS架构互联。公司现有一套需GPU进行推理的通话转录算法，希望将转录文本存储于AWS云端的Amazon S3存储桶中以支持模型开发。请问机器学习专家应采用何种解决方案，方能以最优速度将转录文件传输至S3存储桶？"}, "option": [{"option_text": {"zhcn": "请订购并使用配备NVIDIA Tesla模块的AWS Snowball Edge计算优化设备来运行转录算法。通过AWS DataSync将生成的转录文件传输至指定的转录S3存储桶。", "enus": "Order and use an AWS Snowball Edge Compute Optimized device with an NVIDIA Tesla module to run the transcription algorithm. Use  AWS DataSync to send the resulting transcriptions to the transcription S3 bucket."}, "option_flag": false}, {"option_text": {"zhcn": "通过配置搭载Amazon EC2 Inf1实例的AWS Snowcone设备，部署并运行语音转码算法。随后借助AWS DataSync服务，将生成的转码文本传输至指定的S3存储桶。", "enus": "Order and use an AWS Snowcone device with Amazon EC2 Inf1 instances to run the transcription algorithm. Use AWS DataSync to send  the resulting transcriptions to the transcription S3 bucket."}, "option_flag": false}, {"option_text": {"zhcn": "部署并启用AWS Outposts服务，在基于GPU的Amazon EC2实例上运行语音转文本算法。将生成的转录文件存储于专设的S3存储桶中。", "enus": "Order and use AWS Outposts to run the transcription algorithm on GPU-based Amazon EC2 instances. Store the resulting transcriptions  in the transcription S3 bucket."}, "option_flag": false}, {"option_text": {"zhcn": "使用AWS DataSync将音频文件导入至Amazon S3存储服务。创建AWS Lambda函数，以便在音频文件上传至Amazon S3时自动运行转录算法。将该函数配置为将生成的转录结果写入指定的转录S3存储桶中。", "enus": "Use AWS DataSync to ingest the audio files to Amazon S3. Create an AWS Lambda function to run the transcription algorithm on the  audio files when they are uploaded to Amazon S3. Configure the function to write the resulting transcriptions to the transcription S3  bucket."}, "option_flag": true}], "analysis": {"enus": "The correct answer to the question is to use AWS DataSync to ingest the audio files to Amazon S3 and create an AWS Lambda function to run the transcription algorithm on the audio files when they are uploaded to Amazon S3, then configure the function to write the resulting transcriptions to the transcription S3 bucket. This solution is the fastest because AWS DataSync is optimized for high - speed data transfer over existing network connections, and the on - premises infrastructure has high - velocity networking. Once the files are in S3, the Lambda function can quickly process the audio files as they arrive, leveraging the cloud's processing power.\n\nThe option of using an AWS Snowball Edge Compute Optimized device with an NVIDIA Tesla module is not the fastest. Snowball devices are useful for large - scale data transfer when network bandwidth is limited, but here we have a 100 Mbps connection and high - velocity networking, so setting up a Snowball device would add unnecessary time for shipping and setup.\n\nThe AWS Snowcone device with Amazon EC2 Inf1 instances is also not the best choice. Snowcone is designed for small - scale data transfer and edge computing scenarios with limited resources. It may not have the processing power required for the large volume of petabytes of audio data, and the setup time would slow down the overall process.\n\nUsing AWS Outposts to run the transcription algorithm on GPU - based Amazon EC2 instances is overkill. Outposts are meant for running AWS infrastructure on - premises for low - latency applications. In this case, the goal is to get the transcriptions to the S3 bucket as fast as possible, and setting up Outposts would involve a long deployment process and additional management overhead.", "zhcn": "针对该问题的最佳解决方案是：使用AWS DataSync将音频文件快速导入Amazon S3存储桶，并创建AWS Lambda函数，在音频文件上传至S3时自动运行转录算法，同时配置该函数将生成的转录结果写入指定存储桶。此方案能实现极速处理，原因在于AWS DataSync专为优化现有网络连接的高速数据传输而设计，且本地基础设施已具备高速网络环境。文件一旦存入S3，Lambda函数即可借助云端算力对音频进行实时处理。\n\n相比之下，采用搭载NVIDIA Tesla模块的AWS Snowball Edge计算优化设备并非最优解。该设备适用于网络带宽受限的大规模数据传输场景，而本案已具备100 Mbps带宽和高速网络，若使用Snowball设备反而会因运输和部署时间降低效率。\n\n采用配备Amazon EC2 Inf1实例的AWS Snowcone设备亦非理想选择。Snowcone专为小规模数据传输及资源受限的边缘计算场景设计，面对PB级海量音频数据可能算力不足，且部署时间会拖慢整体进程。\n\n通过AWS Outposts运行基于GPU的Amazon EC2实例进行转录则显得大材小用。Outposts主要面向需要低延迟的本地化AWS基础设施部署场景。本案的核心目标是尽快将转录结果送达S3存储桶，采用Outposts不仅会引发冗长的部署流程，还将增加额外管理负担。"}, "answer": "D"}, {"id": "197", "question": {"enus": "A company has a podcast platform that has thousands of users. The company has implemented an anomaly detection algorithm to detect low podcast engagement based on a 10-minute running window of user events such as listening, pausing, and exiting the podcast. A machine learning (ML) specialist is designing the data ingestion of these events with the knowledge that the event payload needs some small transformations before inference. How should the ML specialist design the data ingestion to meet these requirements with the LEAST operational overhead? ", "zhcn": "某公司旗下播客平台拥有数千名用户。为检测用户参与度低迷状况，该公司已部署异常检测算法，该算法基于十分钟滚动窗口内的用户行为（如收听、暂停、退出播客等）进行监测。鉴于事件载荷在推理前需进行微量数据转换，机器学习专家正在设计事件数据摄取方案。请问该专家应如何以最小运维成本实现这一数据摄取流程的设计？"}, "option": [{"option_text": {"zhcn": "通过AWS AppSync中的GraphQL API接收事件数据，将其存储于Amazon DynamoDB数据表。利用DynamoDB数据流触发AWS Lambda函数，在推理前对最近10分钟的数据进行转换处理。", "enus": "Ingest event data by using a GraphQLAPI in AWS AppSync. Store the data in an Amazon DynamoDB table. Use DynamoDB Streams to  call an AWS Lambda function to transform the most recent 10 minutes of data before inference."}, "option_flag": false}, {"option_text": {"zhcn": "通过亚马逊Kinesis数据流接收事件数据，借助亚马逊Kinesis数据火渠将信息存储于亚马逊S3存储服务。在推理前，运用AWS Glue对最近十分钟的数据进行转换处理。", "enus": "Ingest event data by using Amazon Kinesis Data Streams. Store the data in Amazon S3 by using Amazon Kinesis Data Firehose. Use  AWS Glue to transform the most recent 10 minutes of data before inference."}, "option_flag": true}, {"option_text": {"zhcn": "通过亚马逊Kinesis数据流接收事件数据，并借助基于Apache Flink的亚马逊Kinesis数据分析应用，在推理前对最近十分钟的数据进行实时处理。", "enus": "Ingest event data by using Amazon Kinesis Data Streams. Use an Amazon Kinesis Data Analytics for Apache Flink application to  transform the most recent 10 minutes of data before inference."}, "option_flag": false}, {"option_text": {"zhcn": "通过Amazon Managed Streaming for Apache Kafka（Amazon MSK）摄取事件数据，并利用AWS Lambda函数在推理前对最近10分钟的数据进行转换处理。", "enus": "Ingest event data by using Amazon Managed Streaming for Apache Kafka (Amazon MSK). Use an AWS Lambda function to transform  the most recent 10 minutes of data before inference."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to ingest event data by using Amazon Kinesis Data Streams, store it in Amazon S3 via Amazon Kinesis Data Firehose, and use AWS Glue to transform the most recent 10 - minute data before inference. This approach offers the least operational overhead. Amazon Kinesis Data Streams can handle high - volume, real - time event data ingestion efficiently. Kinesis Data Firehose simplifies the process of loading the ingested data into Amazon S3, which is a reliable and cost - effective storage solution. AWS Glue is a fully managed ETL service, so it doesn't require the ML specialist to manage the underlying infrastructure for data transformation, reducing operational complexity.\n\nThe option of using a GraphQL API in AWS AppSync and Amazon DynamoDB is less suitable. AWS AppSync is mainly for building GraphQL APIs for web and mobile applications, not optimized for high - volume event ingestion. DynamoDB Streams and Lambda for transformation add more complexity in terms of managing the Lambda function and the DynamoDB table.\n\nThe option with Kinesis Data Analytics for Apache Flink requires more expertise in Apache Flink. Developing and maintaining a Flink application can be operationally intensive, especially for a simple data transformation task.\n\nThe option of using Amazon MSK and an AWS Lambda function also has higher operational overhead. Amazon MSK requires managing the Kafka cluster, and Lambda functions need to be configured and monitored carefully for efficient data transformation. Overall, the real answer provides a more straightforward and less operationally burdensome solution for the given requirements.", "zhcn": "针对该问题的最佳解决方案是：通过亚马逊Kinesis数据流接收事件数据，借助Kinesis数据火线将数据存入亚马逊S3存储桶，并在推理前使用AWS Glue对最近10分钟的数据进行转换。此方案能最大程度降低运维复杂度。Kinesis数据流可高效处理海量实时事件数据摄入，数据火线则能简化数据加载至S3的流程，而S3本身正是兼具可靠性与经济性的存储方案。AWS Glue作为全托管ETL服务，无需机器学习专家管理底层数据转换基础设施，进一步简化了运维流程。\n\n相较之下，采用AWS AppSync中的GraphQL API与Amazon DynamoDB组合的方案适配性较低。AppSync主要专注于为网页及移动应用构建GraphQL接口，并非为海量事件数据摄入场景优化。而通过DynamoDB流和Lambda进行转换的方式，又会因需管理Lambda函数与DynamoDB表而增加系统复杂性。\n\n若选用支持Apache Flink的Kinesis数据分析方案，则需具备Flink相关专业技能。对于简单的数据转换任务而言，开发维护Flink应用程序将带来显著的运维负担。\n\n采用亚马逊MSK配合Lambda函数的方案同样存在较高运维成本：MSK需管理Kafka集群，Lambda函数也需精细配置与监控才能实现高效数据转换。总体而言，正确答案所提供的方案在满足需求的同时，兼具更高的操作简便性与更低的运维压力。"}, "answer": "B"}, {"id": "198", "question": {"enus": "A company wants to predict the classification of documents that are created from an application. New documents are saved to an Amazon S3 bucket every 3 seconds. The company has developed three versions of a machine learning (ML) model within Amazon SageMaker to classify document text. The company wants to deploy these three versions to predict the classification of each document. Which approach will meet these requirements with the LEAST operational overhead? ", "zhcn": "某公司需对应用程序生成的文档进行自动分类预测，新文档每三秒便会存入亚马逊S3存储桶。该公司已在Amazon SageMaker平台上开发了三个版本的机器学习模型用于文档文本分类，现希望部署这三个版本来实现每份文档的自动分类预测。在满足需求的前提下，下列哪种方案能最大限度降低运维复杂度？"}, "option": [{"option_text": {"zhcn": "配置S3事件通知机制，使其在创建新文档时自动触发AWS Lambda函数。同时设定该Lambda函数启动三项SageMaker批量转换任务——每份文档需分别通过三个模型各执行一次批量转换。", "enus": "Configure an S3 event notification that invokes an AWS Lambda function when new documents are created. Configure the Lambda  function to create three SageMaker batch transform jobs, one batch transform job for each model for each document."}, "option_flag": false}, {"option_text": {"zhcn": "将所有模型部署至单一SageMaker终端节点，每个模型作为独立的生产变体进行配置。设置S3事件通知机制，当有新文档创建时自动触发AWS Lambda函数。同时配置该Lambda函数，使其能够调用各个生产变体并返回每个模型的推理结果。", "enus": "Deploy all the models to a single SageMaker endpoint. Treat each model as a production variant. Configure an S3 event notification  that invokes an AWS Lambda function when new documents are created. Configure the Lambda function to call each production variant  and return the results of each model."}, "option_flag": false}, {"option_text": {"zhcn": "将每个模型部署至独立的SageMaker终端节点。配置S3事件通知机制，当有新文档生成时自动触发AWS Lambda函数。设定该Lambda函数依次调用各终端节点，并返回各模型的推理结果。", "enus": "Deploy each model to its own SageMaker endpoint Configure an S3 event notification that invokes an AWS Lambda function when new  documents are created. Configure the Lambda function to call each endpoint and return the results of each model."}, "option_flag": true}, {"option_text": {"zhcn": "将每个模型分别部署至独立的SageMaker终端节点。创建三个AWS Lambda函数，并配置每个函数分别调用不同的终端节点并返回结果。设置三个S3事件通知，以便在有新文档创建时自动触发相应的Lambda函数。", "enus": "Deploy each model to its own SageMaker endpoint. Create three AWS Lambda functions. Configure each Lambda function to call a  different endpoint and return the results. Configure three S3 event notifications to invoke the Lambda functions when new documents are  created."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Deploy each model to its own SageMaker endpoint. Configure an S3 event notification that invokes an AWS Lambda function when new documents are created. Configure the Lambda function to call each endpoint and return the results of each model.” This approach has the least operational overhead.\n\nDeploying each model to its own endpoint simplifies management and isolation of models. Using a single Lambda function to call each endpoint is more efficient than creating multiple Lambda functions or batch transform jobs. \n\nThe first fake option suggests creating three SageMaker batch transform jobs for each document. Batch transform jobs are more suitable for large - scale, offline processing and are not optimized for real - time, frequent predictions like this scenario, leading to higher overhead.\n\nThe second fake option of deploying all models to a single endpoint as production variants adds complexity in managing the variants and routing traffic. It may require more configuration and monitoring compared to separate endpoints.\n\nThe third fake option involves creating three AWS Lambda functions and three S3 event notifications. This increases the number of resources to manage, leading to higher operational overhead compared to using a single Lambda function.\n\nIn summary, the real answer offers a straightforward and efficient way to deploy and use the models, with less complexity and resource management requirements, which is why it is the best choice for minimizing operational overhead.", "zhcn": "对于该问题的正确答案是：\"将每个模型部署至独立的SageMaker终端节点。配置S3事件通知，以便在创建新文档时触发AWS Lambda函数。设定该Lambda函数调用各终端节点并返回每个模型的运算结果。\" 此方案具有最低运维管理成本。\n\n将每个模型部署到独立终端节点可简化模型管理与隔离。使用单一Lambda函数调用各节点，比创建多个Lambda函数或批处理任务更为高效。首项干扰选项建议为每个文档创建三个SageMaker批处理任务——该方案更适用于大规模离线处理场景，而对此类需要实时频繁预测的情形不仅缺乏优化，还会增加运维负担。\n\n第二项干扰方案将所有模型以生产变体形式部署至单一终端节点，这会增加流量路由与变体管理的复杂性，相较于独立节点方案需要更多配置与监控工作。\n\n第三项干扰方案涉及创建三个Lambda函数与三个S3事件通知，将导致管理资源数量增加，相比使用单一Lambda函数的方案显著提升运维复杂度。\n\n综上所述，正确答案通过简洁高效的模型部署与调用机制，在保证功能完整性的同时最大限度降低了系统复杂度和资源管理需求，因此成为最小化运维成本的最佳选择。"}, "answer": "C"}, {"id": "199", "question": {"enus": "A manufacturing company needs to identify returned smartphones that have been damaged by moisture. The company has an automated process that produces 2,000 diagnostic values for each phone. The database contains more than five million phone evaluations. The evaluation process is consistent, and there are no missing values in the data. A machine learning (ML) specialist has trained an Amazon SageMaker linear learner ML model to classify phones as moisture damaged or not moisture damaged by using all available features. The model's F1 score is 0.6. Which changes in model training would MOST likely improve the model's F1 score? (Choose two.) ", "zhcn": "一家制造公司需要甄别因受潮而损坏的退货智能手机。该公司采用自动化流程，为每部手机生成2000项诊断数据。数据库中已收录超过五百万次手机检测记录，评估流程标准统一，且数据无任何缺失。一位机器学习专家利用全部可用特征，训练了亚马逊SageMaker线性学习器模型，用以将手机划分为\"受潮损坏\"与\"未受潮损坏\"两类。当前模型的F1得分为0.6。若要提升该模型的F1得分，以下哪两项训练调整最可能见效？（请选择两项）"}, "option": [{"option_text": {"zhcn": "继续采用SageMaker线性学习器算法，同时运用SageMaker主成分分析（PCA）算法缩减特征变量数量。", "enus": "Continue to use the SageMaker linear learner algorithm. Reduce the number of features with the SageMaker principal component  analysis (PCA) algorithm."}, "option_flag": true}, {"option_text": {"zhcn": "继续采用SageMaker线性学习器算法，同时通过scikit-learn多维缩放（MDS）算法减少特征数量。", "enus": "Continue to use the SageMaker linear learner algorithm. Reduce the number of features with the scikit-learn multi-dimensional scaling  (MDS) algorithm."}, "option_flag": false}, {"option_text": {"zhcn": "继续采用SageMaker线性学习器算法，并将预测器类型设定为回归器。", "enus": "Continue to use the SageMaker linear learner algorithm. Set the predictor type to regressor."}, "option_flag": false}, {"option_text": {"zhcn": "采用SageMaker平台的k-means算法，将聚类数k设为小于1000的值来训练模型。", "enus": "Use the SageMaker k-means algorithm with k of less than 1,000 to train the model."}, "option_flag": false}, {"option_text": {"zhcn": "使用SageMaker k近邻（k-NN）算法进行模型训练时，请将降维目标设定在1,000以下。", "enus": "Use the SageMaker k-nearest neighbors (k-NN) algorithm. Set a dimension reduction target of less than 1,000 to train the model."}, "option_flag": true}], "analysis": {"enus": "The question is about improving the F1 - score of a model used to classify smartphones as moisture - damaged or not, given 2,000 diagnostic values per phone and over five million evaluations.\n\nThe first real answer, “Continue to use the SageMaker linear learner algorithm. Reduce the number of features with the SageMaker principal component analysis (PCA) algorithm.” is correct because having 2,000 features can lead to overfitting. PCA is a well - known dimensionality reduction technique in SageMaker that can reduce the feature space while retaining most of the important information, which can improve the model's performance and thus the F1 - score.\n\nThe second real answer, “Use the SageMaker k - nearest neighbors (k - NN) algorithm. Set a dimension reduction target of less than 1,000 to train the model.” is also correct. k - NN can be effective for classification tasks. Reducing the number of dimensions to less than 1,000 helps in handling the high - dimensional data better, making the model more efficient and potentially increasing the F1 - score.\n\nThe first fake answer, “Continue to use the SageMaker linear learner algorithm. Reduce the number of features with the scikit - learn multi - dimensional scaling (MDS) algorithm.” is incorrect because MDS is mainly used for visualizing high - dimensional data in a lower - dimensional space and is not as effective as PCA for feature reduction in the context of improving a classification model's performance.\n\nThe second fake answer, “Continue to use the SageMaker linear learner algorithm. Set the predictor type to regressor.” is wrong as the problem is a classification task (classifying phones as moisture - damaged or not), and changing to a regressor is not appropriate and will not improve the F1 - score.\n\nThe third fake answer, “Use the SageMaker k - means algorithm with k of less than 1,000 to train the model.” is incorrect because k - means is an unsupervised learning algorithm used for clustering, not for the supervised classification task at hand.\n\nIn summary, the real answer options focus on appropriate dimensionality reduction and suitable algorithms for classification, while the fake options either use inappropriate techniques or algorithms for the given problem, which is why the real answer options are chosen to improve the F1 - score.", "zhcn": "问题涉及如何提升一款手机浸水损坏分类模型的F1值——该模型需基于每部手机2000项诊断指标完成超过五百万次评估。第一个有效方案\"继续使用SageMaker线性学习器算法，通过SageMaker主成分分析（PCA）算法削减特征数量\"是正确的，因为2000个特征极易引发过拟合。PCA作为SageMaker中经典的降维技术，能在保留关键信息的同时压缩特征空间，从而提升模型性能及F1值。\n\n第二个有效方案\"采用SageMaker k近邻（k-NN）算法，将降维目标设定为1000以下进行模型训练\"同样可行。k-NN算法适用于分类任务，将维度降至千位以下有助于优化高维数据处理效率，进而可能提高F1值。\n\n第一个错误方案\"继续使用SageMaker线性学习器算法，通过scikit-learn多维缩放（MDS）算法减少特征\"不可取，因为MDS主要用于高维数据可视化，在提升分类模型性能方面不如PCA有效。\n\n第二个错误方案\"继续使用SageMaker线性学习器算法，将预测器类型设置为回归器\"存在方向性偏差——本问题是分类任务（判断手机是否浸水损坏），改用回归器不仅不适用，也无法改善F1值。\n\n第三个错误方案\"使用SageMaker k均值算法（k值小于1000）训练模型\"同样不妥，k均值属于无监督学习的聚类算法，不适用于当前有监督分类场景。\n\n综上，有效方案聚焦于恰当的降维处理和分类算法选择，而错误方案要么采用不匹配的技术手段，要么偏离问题本质，这正是选择有效方案来提升F1值的关键所在。"}, "answer": "AE"}, {"id": "200", "question": {"enus": "A company is building a machine learning (ML) model to classify images of plants. An ML specialist has trained the model using the Amazon SageMaker built-in Image Classification algorithm. The model is hosted using a SageMaker endpoint on an ml.m5.xlarge instance for real-time inference. When used by researchers in the field, the inference has greater latency than is acceptable. The latency gets worse when multiple researchers perform inference at the same time on their devices. Using Amazon CloudWatch metrics, the ML specialist notices that the ModelLatency metric shows a high value and is responsible for most of the response latency. The ML specialist needs to fix the performance issue so that researchers can experience less latency when performing inference from their devices. Which action should the ML specialist take to meet this requirement? ", "zhcn": "一家公司正在构建一个用于植物图像分类的机器学习模型。机器学习专家已使用Amazon SageMaker内置的图像分类算法完成模型训练，并通过部署在ml.m5.xlarge实例上的SageMaker端点提供实时推理服务。然而实地研究人员使用时发现推理延迟超出可接受范围，且当多名研究人员同时通过设备发起推理请求时延迟现象更为显著。通过Amazon CloudWatch指标监测，机器学习专家发现ModelLatency指标数值过高，是造成响应延迟的主要原因。为确保研究人员从设备端发起推理时获得更低的延迟体验，机器学习专家应采取下列哪项措施来满足这一需求？"}, "option": [{"option_text": {"zhcn": "将终端节点实例调整为与ml.m5.xlarge实例vCPU数量相同的ml.t3可突增实例。", "enus": "Change the endpoint instance to an ml.t3 burstable instance with the same vCPU number as the ml.m5.xlarge instance has."}, "option_flag": true}, {"option_text": {"zhcn": "为终端实例挂载一个Amazon Elastic Inference ml.eia2.medium加速器。", "enus": "Attach an Amazon Elastic Inference ml.eia2.medium accelerator to the endpoint instance."}, "option_flag": false}, {"option_text": {"zhcn": "启用Amazon SageMaker Autopilot功能，即可自动优化模型性能。", "enus": "Enable Amazon SageMaker Autopilot to automatically tune performance of the model."}, "option_flag": false}, {"option_text": {"zhcn": "将终端实例调整为采用内存优化的机器学习实例。", "enus": "Change the endpoint instance to use a memory optimized ML instance."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Change the endpoint instance to an ml.t3 burstable instance with the same vCPU number as the ml.m5.xlarge instance has.” This choice can potentially reduce latency because ml.t3 instances are designed to provide a baseline level of CPU performance with the ability to burst above the baseline when needed. This burstable nature can handle sudden spikes in inference requests from multiple researchers, which was causing the increased latency.\n\nThe option “Attach an Amazon Elastic Inference ml.eia2.medium accelerator to the endpoint instance” might not directly address the high ModelLatency issue. Elastic Inference is more about offloading the inference workload to a cost - effective accelerator, but it doesn't necessarily deal with the root cause of the high latency which could be related to CPU performance during bursts of requests.\n\n“Enable Amazon SageMaker Autopilot to automatically tune performance of the model” is incorrect because Autopilot is mainly for automating the process of building, training, and tuning machine - learning models. It is not focused on reducing the inference latency of an already - trained model.\n\n“Change the endpoint instance to use a memory optimized ML instance” is also wrong. Since the high latency is shown in the ModelLatency metric, it is likely a CPU - related performance issue rather than a memory - related one. Memory - optimized instances are better suited for applications that require large amounts of memory, not for reducing the latency caused by model processing.\n\nIn summary, the key factor that makes the real answer correct is its ability to handle the variable load of inference requests, which is the root cause of the high latency problem, distinguishing it from the fake options.", "zhcn": "针对该问题，正确答案是\"将终端节点实例变更为与ml.m5.xlarge实例vCPU数量相同的ml.t3可突增实例\"。此方案能有效降低延迟，因为ml.t3实例专为提供基准CPU性能而设计，并可在需要时突破基准性能限制。这种可突增特性能够应对多名研究人员同时发起推理请求时产生的瞬时流量高峰，而这正是导致延迟增加的根源。\n\n\"为终端节点实例挂载Amazon弹性推理ml.eia2.medium加速器\"这一选项可能无法直接解决高模型延迟问题。弹性推理主要作用是将推理工作负载转移至高性价比的加速器，但未必能解决请求突增时因CPU性能不足引发的延迟根源问题。\n\n\"启用Amazon SageMaker Autopilot自动优化模型性能\"的表述并不准确。因为Autopilot的核心功能在于自动化构建、训练和调优机器学习模型的全流程，而非降低已训练模型的推理延迟。\n\n\"将终端节点实例变更为内存优化型ML实例\"同样不可取。由于监控指标显示的是模型延迟过高，这更可能是CPU相关性能问题而非内存不足所致。内存优化型实例适用于需要大内存容量的应用场景，而非解决模型处理引发的延迟问题。\n\n综上，正确答案的核心优势在于能有效应对推理请求的波动负载——这正是高延迟问题的根本成因，也是其与错误选项的本质区别。"}, "answer": "A"}, {"id": "201", "question": {"enus": "An automotive company is using computer vision in its autonomous cars. The company has trained its models successfully by using transfer learning from a convolutional neural network (CNN). The models are trained with PyTorch through the use of the Amazon SageMaker SDK. The company wants to reduce the time that is required for performing inferences, given the low latency that is required for self-driving. Which solution should the company use to evaluate and improve the performance of the models? ", "zhcn": "一家汽车制造商正将计算机视觉技术应用于其自动驾驶车辆。通过采用卷积神经网络（CNN）的迁移学习方案，该公司已成功完成模型训练。这些模型依托PyTorch框架，并借助亚马逊SageMaker SDK进行开发。鉴于自动驾驶对低延迟的严苛要求，该企业希望缩短模型推理所需的时间。此时应当采用何种解决方案来评估并提升模型性能？"}, "option": [{"option_text": {"zhcn": "利用Amazon CloudWatch算法指标，可清晰洞察SageMaker训练过程中的权重、梯度、偏置及激活输出数据。基于这些信息计算滤波器等级，通过剪枝技术剔除低阶滤波器，并重新设定权重参数。最终使用剪枝后的模型启动新一轮训练任务。", "enus": "Use Amazon CloudWatch algorithm metrics for visibility into the SageMaker training weights, gradients, biases, and activation outputs.  Compute the filter ranks based on this information. Apply pruning to remove the low-ranking filters. Set the new weights. Run a new  training job with the pruned model."}, "option_flag": false}, {"option_text": {"zhcn": "利用SageMaker Debugger洞察训练过程中的权重、梯度、偏置及激活输出，动态调整模型超参数以优化推理效率，随后启动新一轮训练任务。", "enus": "Use SageMaker Debugger for visibility into the training weights, gradients, biases, and activation outputs. Adjust the model  hyperparameters, and look for lower inference times. Run a new training job."}, "option_flag": false}, {"option_text": {"zhcn": "利用SageMaker Debugger洞察训练过程中的权重、梯度、偏置及激活输出数据，据此计算滤波器优先级。通过剪枝技术剔除低优先级滤波器，重新设定权重参数后，对精简后的模型启动新一轮训练任务。", "enus": "Use SageMaker Debugger for visibility into the training weights, gradients, biases, and activation outputs. Compute the filter ranks  based on this information. Apply pruning to remove the low-ranking filters. Set the new weights. Run a new training job with the pruned  model."}, "option_flag": true}, {"option_text": {"zhcn": "部署模型后，可利用SageMaker模型监控功能观测模型的推理延迟指标与资源开销延迟指标。通过调整模型超参数来优化推理耗时，并启动新一轮训练任务以提升性能。", "enus": "Use SageMaker Model Monitor for visibility into the ModelLatency metric and OverheadLatency metric of the model after the model is  deployed. Adjust the model hyperparameters, and look for lower inference times. Run a new training job."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Use SageMaker Debugger for visibility into the training weights, gradients, biases, and activation outputs. Compute the filter ranks based on this information. Apply pruning to remove the low - ranking filters. Set the new weights. Run a new training job with the pruned model.” This solution directly targets reducing inference time by pruning the model. By removing low - ranking filters, the model becomes more lightweight, which in turn reduces the time required for performing inferences, meeting the low - latency requirement for self - driving cars.\n\nThe first fake option suggests using Amazon CloudWatch algorithm metrics. However, Amazon CloudWatch is mainly for monitoring AWS resources and applications in a general sense, and it is not as specialized as SageMaker Debugger for getting detailed insights into the internal aspects of a model during training, such as weights, gradients, etc.\n\nThe second fake option only focuses on adjusting hyperparameters. While hyperparameter tuning can improve model performance, it may not be as effective as model pruning in directly reducing inference latency. Without reducing the model's complexity, the inference speed may not be optimized significantly.\n\nThe third fake option uses SageMaker Model Monitor, which is designed to monitor the performance of a deployed model. Since the goal is to reduce inference time during the model development phase, using Model Monitor is not the right approach as it is more about post - deployment monitoring rather than pre - deployment optimization.\n\nIn summary, the real answer option is the most appropriate as it directly addresses the need to reduce inference time through model pruning, which is a key technique for optimizing models for low - latency applications like self - driving cars.", "zhcn": "对于如何降低模型推理时间的问题，正确答案是：“使用 SageMaker Debugger 分析训练过程中的权重、梯度、偏置和激活输出，基于这些信息计算滤波器排序，通过剪枝技术移除低优先级滤波器并设置新权重，最后对剪枝后的模型启动新一轮训练。”这一方案通过模型剪枝直接针对推理时间优化。通过剔除低效滤波器，模型变得更加轻量化，从而缩短推理耗时，满足自动驾驶对低延迟的需求。\n\n第一个干扰项建议采用 Amazon CloudWatch 算法指标监控。但 CloudWatch 主要面向广义的 AWS 资源与应用监控，在获取训练过程中模型内部参数（如权重、梯度等）细节方面，不如 SageMaker Debugger 专业深入。\n\n第二个干扰项仅关注超参数调优。虽然超参数优化能提升模型性能，但在直接降低推理延迟方面，其效果不如模型剪枝显著。若不削减模型复杂度，推理速度难以实现质的飞跃。\n\n第三个干扰项使用 SageMaker Model Monitor，该工具专用于监控已部署模型的运行状态。由于当前目标是在模型开发阶段优化推理速度，而 Model Monitor 侧重于部署后的监测而非部署前的优化，因此并不适用。\n\n综上，正确答案通过模型剪枝技术直指降低推理时间的核心需求，这种优化策略对自动驾驶等低延迟应用场景具有关键意义。"}, "answer": "C"}, {"id": "202", "question": {"enus": "A company's machine learning (ML) specialist is designing a scalable data storage solution for Amazon SageMaker. The company has an existing TensorFlow-based model that uses a train.py script. The model relies on static training data that is currently stored in TFRecord format. What should the ML specialist do to provide the training data to SageMaker with the LEAST development overhead? ", "zhcn": "一家公司的机器学习专家正在为Amazon SageMaker设计可扩展的数据存储方案。该公司现有一个基于TensorFlow的模型，使用train.py训练脚本。该模型依赖静态训练数据，目前以TFRecord格式存储。机器学习专家应以最小的开发工作量将训练数据提供给SageMaker，请问应当采取何种方案？"}, "option": [{"option_text": {"zhcn": "将TFRecord数据存入Amazon S3存储桶后，可选用AWS Glue或AWS Lambda对数据进行重组，转换为protobuf格式并存入另一个S3存储桶。最后将SageMaker训练任务的数据源指向第二个存储桶即可。", "enus": "Put the TFRecord data into an Amazon S3 bucket. Use AWS Glue or AWS Lambda to reformat the data to protobuf format and store the  data in a second S3 bucket. Point the SageMaker training invocation to the second S3 bucket."}, "option_flag": false}, {"option_text": {"zhcn": "请将train.py脚本进行修改，增加将TFRecord数据转换为protobuf格式的模块。将SageMaker训练任务的数据指向路径设置为本地数据路径，并改为读取protobuf格式数据而非TFRecord数据。", "enus": "Rewrite the train.py script to add a section that converts TFRecord data to protobuf format. Point the SageMaker training invocation to  the local path of the data. Ingest the protobuf data instead of the TFRecord data."}, "option_flag": true}, {"option_text": {"zhcn": "采用SageMaker脚本模式，保持train.py文件不作改动。将SageMaker训练任务的数据路径指向本地原始数据目录，无需重新格式化训练数据。", "enus": "Use SageMaker script mode, and use train.py unchanged. Point the SageMaker training invocation to the local path of the data without  reformatting the training data."}, "option_flag": false}, {"option_text": {"zhcn": "采用SageMaker脚本模式，保持train.py文件不作改动。将TFRecord数据存入Amazon S3存储桶，并直接指向该S3存储桶启动SageMaker训练任务，无需对训练数据格式进行转换。", "enus": "Use SageMaker script mode, and use train.py unchanged. Put the TFRecord data into an Amazon S3 bucket. Point the SageMaker  training invocation to the S3 bucket without reformatting the training data."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Rewrite the train.py script to add a section that converts TFRecord data to protobuf format. Point the SageMaker training invocation to the local path of the data. Ingest the protobuf data instead of the TFRecord data.” This approach has the least development overhead as it only requires a modification to the existing train.py script to handle the conversion. \n\nThe first fake option “Put the TFRecord data into an Amazon S3 bucket. Use AWS Glue or AWS Lambda to reformat the data to protobuf format and store the data in a second S3 bucket. Point the SageMaker training invocation to the second S3 bucket” involves additional services like AWS Glue or AWS Lambda, which adds complexity and development effort. \n\nThe second fake option “Use SageMaker script mode, and use train.py unchanged. Point the SageMaker training invocation to the local path of the data without reformatting the training data” won't work because SageMaker might not natively support TFRecord format, and this option doesn't address the format compatibility issue. \n\nThe third fake option “Use SageMaker script mode, and use train.py unchanged. Put the TFRecord data into an Amazon S3 bucket. Point the SageMaker training invocation to the S3 bucket without reformatting the training data” also fails to deal with the format problem. SageMaker may not be able to directly use TFRecord data, so this method won't provide a proper solution. Thus, the real answer is the most efficient option with the least development overhead.", "zhcn": "问题的正确答案是\"改写train.py脚本，添加将TFRecord数据转换为protobuf格式的模块。将SageMaker训练任务指向数据的本地路径，并直接使用转换后的protobuf格式数据\"。此方案开发成本最低，仅需修改现有训练脚本即可实现格式转换。  \n\n第一个干扰选项\"将TFRecord数据存入Amazon S3存储桶，使用AWS Glue或AWS Lambda将数据转换为protobuf格式后存入另一个S3桶，最后将SageMaker训练指向第二个存储桶\"需要引入AWS Glue或Lambda等额外服务，反而增加了复杂性和开发工作量。  \n\n第二个干扰选项\"使用SageMaker脚本模式并保持train.py不变，将训练任务指向未转换的本地数据路径\"无法奏效，因为SageMaker可能不原生支持TFRecord格式，且该选项未解决格式兼容性问题。  \n\n第三个干扰选项\"使用SageMaker脚本模式并保持train.py不变，将TFRecord数据存入S3桶后直接指向该存储桶\"同样未能解决格式问题。由于SageMaker可能无法直接解析TFRecord数据，该方法无法提供有效解决方案。  \n\n因此，正确答案是开发效率最高、实现成本最低的最佳选择。"}, "answer": "B"}, {"id": "203", "question": {"enus": "An ecommerce company wants to train a large image classification model with 10,000 classes. The company runs multiple model training iterations and needs to minimize operational overhead and cost. The company also needs to avoid loss of work and model retraining. Which solution will meet these requirements? ", "zhcn": "一家电商企业计划训练包含一万个类别的大规模图像分类模型。在多次模型迭代训练过程中，该企业需最大限度降低运营成本与操作复杂度，同时确保训练成果不丢失且避免模型重复训练。何种方案可满足这些需求？"}, "option": [{"option_text": {"zhcn": "将训练任务创建为AWS Batch作业，使其在托管计算环境中调用亚马逊EC2竞价型实例。", "enus": "Create the training jobs as AWS Batch jobs that use Amazon EC2 Spot Instances in a managed compute environment."}, "option_flag": false}, {"option_text": {"zhcn": "利用亚马逊EC2竞价型实例运行训练任务。当收到竞价实例中断通知时，在实例终止前将模型快照保存至亚马逊S3存储空间。", "enus": "Use Amazon EC2 Spot Instances to run the training jobs. Use a Spot Instance interruption notice to save a snapshot of the model to  Amazon S3 before an instance is terminated."}, "option_flag": false}, {"option_text": {"zhcn": "利用AWS Lambda执行训练任务，并将模型权重存储至Amazon S3。", "enus": "Use AWS Lambda to run the training jobs. Save model weights to Amazon S3."}, "option_flag": true}, {"option_text": {"zhcn": "在Amazon SageMaker中启用托管式Spot训练功能，启动训练任务时需开启检查点设置。", "enus": "Use managed spot training in Amazon SageMaker. Launch the training jobs with checkpointing enabled."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Use AWS Lambda to run the training jobs. Save model weights to Amazon S3.” AWS Lambda is a serverless compute service, which means the company doesn't have to manage servers. This significantly reduces operational overhead as AWS takes care of resource provisioning, scaling, and maintenance. Saving model weights to Amazon S3 ensures that the progress of the model training is preserved, thus avoiding the need for full - scale model retraining.\n\nThe option “Create the training jobs as AWS Batch jobs that use Amazon EC2 Spot Instances in a managed compute environment” involves using Spot Instances. Although they are cost - effective, they can be interrupted, leading to potential loss of work and the need for retraining, which goes against the requirement of avoiding work loss.\n\nThe option “Use Amazon EC2 Spot Instances to run the training jobs. Use a Spot Instance interruption notice to save a snapshot of the model to Amazon S3 before an instance is terminated” also relies on Spot Instances. Despite having a mechanism to save the model, interruptions can still cause disruptions and additional complexity in the training process, increasing operational overhead.\n\nThe option “Use managed spot training in Amazon SageMaker. Launch the training jobs with checkpointing enabled” uses Spot Instances as well. Similar to the other Spot - based options, the risk of interruption remains, which can lead to work loss and the need for retraining, and also has some management requirements that add to the operational overhead. \n\nIn summary, the serverless nature of AWS Lambda and the reliable storage in Amazon S3 make this option the best choice for minimizing operational overhead, cost, and avoiding work loss and model retraining.", "zhcn": "针对该问题，正确答案是\"采用AWS Lambda运行训练任务，并将模型权重保存至Amazon S3\"。AWS Lambda作为无服务器计算服务，可免除企业管理服务器的负担。由于AWS自动处理资源调配、扩展及维护事宜，此举将显著降低运维成本。将模型权重存储于Amazon S3则能确保训练进度得以保留，从而避免完整的模型重训练需求。\n\n而\"采用AWS Batch作业在托管计算环境中使用Amazon EC2竞价实例执行训练任务\"这一方案虽涉及成本优化的竞价实例，但其存在被中断的风险，可能导致工作丢失并需重新训练，这与避免训练损失的核心要求相悖。\n\n另一选项\"使用Amazon EC2竞价实例运行训练任务，并借助实例中断通知在实例终止前将模型快照保存至Amazon S3\"同样依赖竞价实例。尽管设置了模型保存机制，实例中断仍会扰乱训练流程，增加系统复杂性，进而推高运维负担。\n\n至于\"启用Amazon SageMaker托管式竞价训练功能，并开启检查点设置启动训练任务\"的方案，虽配备了中断处理机制，但仍未消除竞价实例固有的中断风险。与其他基于竞价实例的方案类似，该方案既存在工作损失和重训练风险，又需要一定的管理投入，无形中增加了运维复杂度。\n\n综上所述，AWS Lambda的无服务器特性与Amazon S3的可靠存储相结合，使其成为实现运维成本最小化、避免工作损失及模型重训练的最佳选择。"}, "answer": "C"}, {"id": "204", "question": {"enus": "A retail company uses a machine learning (ML) model for daily sales forecasting. The model has provided inaccurate results for the past 3 weeks. At the end of each day, an AWS Glue job consolidates the input data that is used for the forecasting with the actual daily sales data and the predictions of the model. The AWS Glue job stores the data in Amazon S3. The company's ML team determines that the inaccuracies are occurring because of a change in the value distributions of the model features. The ML team must implement a solution that will detect when this type of change occurs in the future. Which solution will meet these requirements with the LEAST amount of operational overhead? ", "zhcn": "一家零售企业采用机器学习模型进行日常销量预测。过去三周内，该模型持续出现预测失准情况。每日营业结束后，AWS Glue作业会整合三项数据：用于预测的输入数据、当日实际销售额度以及模型预测值，并将这些数据存储于Amazon S3中。经机器学习团队研判，预测失准源于模型特征值的分布发生变化。当前需设计一套解决方案，以期未来能自动侦测此类数据分布变化。在满足需求的前提下，下列哪种方案能最大限度降低运维复杂度？"}, "option": [{"option_text": {"zhcn": "使用Amazon SageMaker Model Monitor创建数据质量基线时，请确保在基线约束文件中将emit_metrics选项设为启用状态，并针对相关指标设置Amazon CloudWatch警报。", "enus": "Use Amazon SageMaker Model Monitor to create a data quality baseline. Confirm that the emit_metrics option is set to Enabled in the  baseline constraints file. Set up an Amazon CloudWatch alarm for the metric."}, "option_flag": true}, {"option_text": {"zhcn": "使用Amazon SageMaker模型监控功能创建模型质量基线。请确保在基线约束文件中将emit_metrics选项设置为启用状态，并为该指标配置Amazon CloudWatch告警。", "enus": "Use Amazon SageMaker Model Monitor to create a model quality baseline. Confirm that the emit_metrics option is set to Enabled in the  baseline constraints file. Set up an Amazon CloudWatch alarm for the metric."}, "option_flag": false}, {"option_text": {"zhcn": "利用Amazon SageMaker Debugger创建规则以捕获特征数值，并为相关规则配置Amazon CloudWatch告警机制。", "enus": "Use Amazon SageMaker Debugger to create rules to capture feature values Set up an Amazon CloudWatch alarm for the rules."}, "option_flag": false}, {"option_text": {"zhcn": "利用Amazon CloudWatch对Amazon SageMaker终端节点进行监控，并通过分析Amazon CloudWatch Logs中的日志数据来检测数据漂移现象。", "enus": "Use Amazon CloudWatch to monitor Amazon SageMaker endpoints. Analyze logs in Amazon CloudWatch Logs to check for data drift."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Use Amazon SageMaker Model Monitor to create a data quality baseline. Confirm that the emit_metrics option is set to Enabled in the baseline constraints file. Set up an Amazon CloudWatch alarm for the metric.” The problem is that the inaccuracies in the sales - forecasting model are due to a change in the value distributions of the model features, which is a data quality issue. Amazon SageMaker Model Monitor is specifically designed to monitor data quality in machine - learning models. By creating a data quality baseline, it can detect changes in the input data distributions over time. Enabling the emit_metrics option and setting up a CloudWatch alarm allows for automated detection of these changes.\n\nThe option “Use Amazon SageMaker Model Monitor to create a model quality baseline” is incorrect because model quality baselines are more focused on the performance of the model itself (e.g., accuracy, precision), rather than the data distribution changes that are the root cause of the problem here.\n\n“Use Amazon SageMaker Debugger to create rules to capture feature values” is wrong because SageMaker Debugger is mainly used for debugging training jobs, such as detecting overfitting or gradient vanishing. It is not the best tool for continuous monitoring of data distribution changes in a production - level model.\n\n“Use Amazon CloudWatch to monitor Amazon SageMaker endpoints. Analyze logs in Amazon CloudWatch Logs to check for data drift” is less efficient. While CloudWatch can monitor endpoints, it does not have built - in functionality for creating data quality baselines. Analyzing logs manually to detect data drift would require significant operational overhead compared to using SageMaker Model Monitor.\n\nIn summary, the real answer option directly addresses the data quality issue with the least amount of operational overhead, distinguishing it from the fake options.", "zhcn": "针对该问题的正确答案是\"使用Amazon SageMaker Model Monitor创建数据质量基线，确保基线约束文件中emit_metrics选项设置为启用状态，并设置Amazon CloudWatch警报监控指标\"。当前销售预测模型失准的根源在于模型特征值分布发生变化，这属于数据质量问题。Amazon SageMaker Model Monitor专为监控机器学习模型的数据质量而设计，通过建立数据质量基线可持续检测输入数据分布的变动。启用emit_metrics选项并配置CloudWatch警报，可实现对这些变化的自动化监测。\n\n而\"使用Amazon SageMaker Model Monitor创建模型质量基线\"这一方案并不适用，因为模型质量基线主要关注模型本身性能指标（如准确率、精确率），而非当前问题根源——数据分布变化。\n\n\"使用Amazon SageMaker Debugger创建规则捕获特征值\"的方案亦不恰当，因该工具主要用于调试训练任务（如检测过拟合或梯度消失），并非监控生产级模型数据分布持续变化的最佳选择。\n\n至于\"使用Amazon CloudWatch监控Amazon SageMaker终端节点，通过分析CloudWatch日志检测数据漂移\"的方案效率较低。虽然CloudWatch可监控终端节点，但缺乏创建数据质量基线的内置功能，手动分析日志检测数据漂移将产生远超使用SageMaker Model Monitor的运维成本。\n\n综上，正确答案通过最具操作效率的方式精准解决了数据质量问题，这与其余选项形成鲜明对比。"}, "answer": "A"}, {"id": "205", "question": {"enus": "A machine learning (ML) specialist has prepared and used a custom container image with Amazon SageMaker to train an image classification model. The ML specialist is performing hyperparameter optimization (HPO) with this custom container image to produce a higher quality image classifier. The ML specialist needs to determine whether HPO with the SageMaker built-in image classification algorithm will produce a better model than the model produced by HPO with the custom container image. All ML experiments and HPO jobs must be invoked from scripts inside SageMaker Studio notebooks. How can the ML specialist meet these requirements in the LEAST amount of time? ", "zhcn": "一位机器学习专家已准备并使用自定义容器镜像，在Amazon SageMaker上训练了一个图像分类模型。该专家正通过此自定义容器镜像进行超参数优化，旨在提升图像分类器的性能。现在需要判断：若改用SageMaker内置图像分类算法进行超参数优化，所得模型是否会优于当前自定义容器镜像的优化结果。所有机器学习实验及超参数优化任务必须通过SageMaker Studio笔记本中的脚本来触发。请问如何在最短时间内满足这些需求？"}, "option": [{"option_text": {"zhcn": "请编写一个定制化超参数优化脚本，该脚本需在SageMaker Studio的本地模式下运行多个训练任务，以优化基于自定义容器镜像的模型。利用SageMaker的自动模型调优功能并启用早停机制，对内置图像分类算法模型进行参数调优。最终选择具有最佳目标指标值的模型版本。", "enus": "Prepare a custom HPO script that runs multiple training jobs in SageMaker Studio in local mode to tune the model of the custom  container image. Use the automatic model tuning capability of SageMaker with early stopping enabled to tune the model of the built-in  image classification algorithm. Select the model with the best objective metric value."}, "option_flag": false}, {"option_text": {"zhcn": "使用SageMaker Autopilot对自定义容器镜像的模型进行调优。通过启用提前停止功能的SageMaker自动模型调优能力，对内置图像分类算法的模型进行调参。对比SageMaker Autopilot自动化机器学习任务与自动模型调优任务所得模型的目标指标数值，选取目标指标最优的模型。", "enus": "Use SageMaker Autopilot to tune the model of the custom container image. Use the automatic model tuning capability of SageMaker  with early stopping enabled to tune the model of the built-in image classification algorithm. Compare the objective metric values of the  resulting models of the SageMaker AutopilotAutoML job and the automatic model tuning job. Select the model with the best objective  metric value."}, "option_flag": true}, {"option_text": {"zhcn": "利用SageMaker Experiments运行管理多项训练任务，并优化自定义容器镜像的模型参数。通过SageMaker内置自动调参功能，对预置图像分类算法模型进行优化。最终选取目标评估指标最优的模型版本。", "enus": "Use SageMaker Experiments to run and manage multiple training jobs and tune the model of the custom container image. Use the  automatic model tuning capability of SageMaker to tune the model of the built-in image classification algorithm. Select the model with  the best objective metric value."}, "option_flag": false}, {"option_text": {"zhcn": "利用SageMaker的自动模型调优功能，同时优化自定义容器镜像与内置图像分类算法的模型参数，最终选取目标评估指标最优的模型。", "enus": "Use the automatic model tuning capability of SageMaker to tune the models of the custom container image and the built-in image  classification algorithm at the same time. Select the model with the best objective metric value."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to use SageMaker Autopilot to tune the model of the custom container image and the automatic model tuning capability of SageMaker with early stopping enabled to tune the model of the built - in image classification algorithm, then compare the objective metric values and select the best model. This approach is the fastest as SageMaker Autopilot automates the end - to - end process of building machine learning models, including data preprocessing, feature engineering, and model selection, which saves a significant amount of time for the custom container image tuning.\n\nThe first fake option involves preparing a custom HPO script to run training jobs in local mode. Local mode is generally slower as it may not leverage the full - scale computing resources of SageMaker, and creating a custom script is time - consuming. The second fake option uses SageMaker Experiments. While it helps in running and managing multiple training jobs, it doesn't offer the same level of automation as SageMaker Autopilot, so it will take more time. The third fake option attempts to tune both models simultaneously using the automatic model tuning capability. However, it doesn't use the specialized automation of SageMaker Autopilot for the custom container image, which could lead to longer tuning times.\n\nThe common pitfall with the fake options is underestimating the time - saving potential of SageMaker Autopilot and over - relying on general SageMaker features or custom scripts, which can lead to longer experimental and tuning periods.", "zhcn": "针对该问题，正确答案是：使用SageMaker Autopilot对自定义容器镜像的模型进行调优，同时启用SageMaker早停机制的自动模型调优功能对内置图像分类算法模型进行调优，最后对比目标指标数值并选择最佳模型。此方案效率最高——SageMaker Autopilot能自动化完成机器学习模型构建的全流程（包括数据预处理、特征工程和模型选择），可大幅节省自定义容器镜像的调优时间。\n\n第一个干扰项提出通过本地模式运行自定义超参优化脚本。但本地模式通常无法充分利用SageMaker的大规模计算资源，且编写自定义脚本耗时费力，整体效率较低。第二个干扰项采用SageMaker Experiments方案，虽然能管理多组训练任务，但其自动化程度远不及SageMaker Autopilot，会导致时间成本增加。第三个干扰项试图用自动模型调优功能同步优化两个模型，但未发挥SageMaker Autopilot对自定义容器镜像的专业自动化优势，反而可能延长调优周期。\n\n这些干扰项的通病在于低估了SageMaker Autopilot的时效优势，过度依赖通用功能或手动方案，容易导致实验周期不必要的延长。"}, "answer": "B"}, {"id": "206", "question": {"enus": "A company wants to deliver digital car management services to its customers. The company plans to analyze data to predict the likelihood of users changing cars. The company has 10 TB of data that is stored in an Amazon Redshift cluster. The company's data engineering team is using Amazon SageMaker Studio for data analysis and model development. Only a subset of the data is relevant for developing the machine learning models. The data engineering team needs a secure and cost-effective way to export the data to a data repository in Amazon S3 for model development. Which solutions will meet these requirements? (Choose two.) ", "zhcn": "一家公司希望为客户提供数字化汽车管理服务，并计划通过数据分析预测用户的换车可能性。该公司拥有10 TB数据存储于Amazon Redshift集群中，数据工程团队正使用Amazon SageMaker Studio进行数据分析与模型开发。由于仅需部分数据用于机器学习模型开发，该团队需要一种安全且经济高效的方式，将数据导出至Amazon S3的数据存储库以供模型开发。下列哪两种解决方案符合这些要求？（请选择两项）"}, "option": [{"option_text": {"zhcn": "在分布式SageMaker处理任务中启动多个中型计算实例。通过预构建的Apache Spark Docker镜像查询并绘制相关数据，同时将Amazon Redshift中的相关数据导出至Amazon S3存储空间。", "enus": "Launch multiple medium-sized instances in a distributed SageMaker Processing job. Use the prebuilt Docker images for Apache Spark  to query and plot the relevant data and to export the relevant data from Amazon Redshift to Amazon S3."}, "option_flag": true}, {"option_text": {"zhcn": "以分布式模式启动多个中等规格的PySpark内核笔记本实例。从Amazon Redshift将数据下载至笔记本集群，对相关数据进行查询分析与可视化制图，最终将筛选后的数据从笔记本集群导出至Amazon S3存储空间。", "enus": "Launch multiple medium-sized notebook instances with a PySpark kernel in distributed mode. Download the data from Amazon Redshift  to the notebook cluster. Query and plot the relevant data. Export the relevant data from the notebook cluster to Amazon S3."}, "option_flag": false}, {"option_text": {"zhcn": "借助AWS Secrets Manager妥善保管Amazon Redshift访问凭证。通过SageMaker Studio笔记本，调用已存储的认证信息，利用Python适配器建立与Amazon Redshift的安全连接。随后通过Python客户端执行数据查询，并将所需数据从Amazon Redshift导出至Amazon S3存储空间。", "enus": "Use AWS Secrets Manager to store the Amazon Redshift credentials. From a SageMaker Studio notebook, use the stored credentials to  connect to Amazon Redshift with a Python adapter. Use the Python client to query the relevant data and to export the relevant data from  Amazon Redshift to Amazon S3."}, "option_flag": true}, {"option_text": {"zhcn": "运用AWS密钥管理服务存储Amazon Redshift的访问凭证。启动一个SageMaker超大型笔记本实例，并配置略大于10TB的块存储容量。通过Python连接器调用已存储的密钥建立与Amazon Redshift的连接，完成数据的下载、查询及可视化分析。最终将处理后的有效数据从本地笔记本驱动器导出至Amazon S3存储服务。", "enus": "Use AWS Secrets Manager to store the Amazon Redshift credentials. Launch a SageMaker extra-large notebook instance with block  storage that is slightly larger than 10 TB. Use the stored credentials to connect to Amazon Redshift with a Python adapter. Download,  query, and plot the relevant data. Export the relevant data from the local notebook drive to Amazon S3."}, "option_flag": false}, {"option_text": {"zhcn": "使用SageMaker Data Wrangler查询并绘制相关数据，同时将Amazon Redshift中的相关数据导出至Amazon S3。", "enus": "Use SageMaker Data Wrangler to query and plot the relevant data and to export the relevant data from Amazon Redshift to Amazon S3."}, "option_flag": false}], "analysis": {"enus": "The correct answers to the question are using a SageMaker Processing job with pre - built Docker images for Apache Spark and using AWS Secrets Manager to connect from a SageMaker Studio notebook with a Python adapter.\n\nThe first real answer, using a SageMaker Processing job, is secure and cost - effective. It allows for distributed processing, which can handle large - scale data efficiently. The pre - built Docker images for Apache Spark simplify the process of querying and exporting relevant data from Amazon Redshift to Amazon S3.\n\nThe second real answer is also valid. Storing Amazon Redshift credentials in AWS Secrets Manager enhances security. Connecting to Redshift from a SageMaker Studio notebook using a Python adapter gives flexibility in querying and exporting only the relevant data to S3, without the need to handle the entire 10 TB dataset.\n\nThe first fake answer of using multiple medium - sized notebook instances in distributed mode to download the data from Redshift to the notebook cluster is not cost - effective. Downloading the entire 10 TB dataset to the notebook cluster is resource - intensive and unnecessary when only a subset of data is needed.\n\nThe second fake answer of launching a SageMaker extra - large notebook instance with large block storage to download the full 10 TB dataset is extremely costly. It also goes against the requirement of only using a subset of data, as it involves downloading the entire dataset.\nThe third fake answer of using SageMaker Data Wrangler is not suitable as it is mainly designed for data preparation and exploration, not for large - scale data export from Amazon Redshift to S3.\n\nThese real answers are chosen because they meet the requirements of being secure and cost - effective for exporting only the relevant data subset, while the fake answers either have high costs or are not appropriate for the task at hand.", "zhcn": "针对该问题的正确答案是：使用搭载Apache Spark预构建Docker镜像的SageMaker处理任务，以及通过Python适配器结合AWS密钥管理器实现SageMaker Studio笔记本与Amazon Redshift的连接。\n\n第一种方案采用SageMaker处理任务，兼具安全性与成本效益。该方案支持分布式处理，能高效处理大规模数据。通过Apache Spark预构建镜像，可简化从Amazon Redshift查询数据并导出至Amazon S3的流程。\n\n第二种方案同样可行。将Redshift凭证存储于AWS密钥管理器能提升安全性，而通过Python适配器从SageMaker Studio笔记本连接Redshift，则可灵活地筛选所需数据导出至S3，无需处理完整的10TB数据集。\n\n第一种错误方案是采用分布式模式的多台中规格笔记本实例从Redshift下载数据。这种方法成本效益低下——将完整10TB数据集下载至笔记本集群既耗费大量资源，且在仅需部分数据时显得多余。\n\n第二种错误方案启动配备大容量块存储的超大笔记本实例来下载全量10TB数据，成本极其高昂。该方案违背了仅使用数据子集的要求，且涉及全量数据下载。\n\n第三种错误方案使用SageMaker Data Wrangler并不适用，该工具主要面向数据准备与探索环节，而非实现从Amazon Redshift到S3的大规模数据导出。\n\n选择正确答案的原因在于它们满足安全、经济高效且仅导出相关数据子集的要求，而错误方案要么成本过高，要么不适合当前任务场景。"}, "answer": "AC"}, {"id": "207", "question": {"enus": "A company is building an application that can predict spam email messages based on email text. The company can generate a few thousand human-labeled datasets that contain a list of email messages and a label of \"spam\" or \"not spam\" for each email message. A machine learning (ML) specialist wants to use transfer learning with a Bidirectional Encoder Representations from Transformers (BERT) model that is trained on English Wikipedia text data. What should the ML specialist do to initialize the model to fine-tune the model with the custom data? ", "zhcn": "一家公司正在开发一款能够根据邮件内容预测垃圾邮件的应用程序。该公司可生成数千条人工标注数据集，其中包含邮件列表及每封邮件对应的\"垃圾邮件\"或\"非垃圾邮件\"标签。一位机器学习专家希望采用基于英文维基百科文本数据训练的Transformer双向编码器表征模型进行迁移学习。为使该模型能通过定制数据完成精调，机器学习专家应如何对模型进行初始化？"}, "option": [{"option_text": {"zhcn": "初始化模型时，除最后一层全连接层外，其余各层均加载预训练权重。", "enus": "Initialize the model with pretrained weights in all layers except the last fully connected layer."}, "option_flag": false}, {"option_text": {"zhcn": "采用预训练权重对模型各层进行初始化，并在首层输出位置之上叠加分类器。利用标注数据对该分类器进行训练。", "enus": "Initialize the model with pretrained weights in all layers. Stack a classifier on top of the first output position. Train the classifier with the  labeled data."}, "option_flag": false}, {"option_text": {"zhcn": "在所有层中以随机权重初始化模型。将最后的全连接层替换为分类器，并利用标注数据对该分类器进行训练。", "enus": "Initialize the model with random weights in all layers. Replace the last fully connected layer with a classifier. Train the classifier with  the labeled data."}, "option_flag": false}, {"option_text": {"zhcn": "初始化模型时，所有层均加载预训练权重。将最后的全连接层替换为分类器，并利用标注数据对该分类器进行训练。", "enus": "Initialize the model with pretrained weights in all layers. Replace the last fully connected layer with a classifier. Train the classifier with  the labeled data."}, "option_flag": true}], "analysis": {"enus": "The correct answer to the question is “Initialize the model with pretrained weights in all layers. Replace the last fully connected layer with a classifier. Train the classifier with the labeled data.” When using transfer learning with a pre - trained BERT model, initializing with pretrained weights in all layers allows the model to leverage the knowledge learned from the large - scale English Wikipedia text data. The last fully connected layer is typically task - specific, so replacing it with a custom classifier tailored to the spam - prediction task is a common and effective approach. Then, training this new classifier with the custom labeled data fine - tunes the model for the specific spam - prediction problem.\n\nThe option “Initialize the model with pretrained weights in all layers except the last fully connected layer” is incorrect because there is no strong reason to exclude the last layer from using pretrained weights. The pre - trained weights can provide a good starting point for all layers. \n\nThe option “Initialize the model with pretrained weights in all layers. Stack a classifier on top of the first output position. Train the classifier with the labeled data” is wrong. Stacking a classifier on the first output position is not the standard way for this kind of fine - tuning. Usually, the last layer is replaced to adapt the model to the new task. \n\nThe option “Initialize the model with random weights in all layers. Replace the last fully connected layer with a classifier. Train the classifier with the labeled data” ignores the advantage of transfer learning. Starting with random weights means the model has to learn from scratch, which is inefficient given the availability of pre - trained BERT weights on a large corpus. \n\nIn summary, the correct answer effectively utilizes transfer learning by leveraging pre - trained weights and then adapting the model for the specific spam - prediction task.", "zhcn": "该问题的正确答案是：\"在所有层加载预训练权重进行模型初始化，将最后的全连接层替换为分类器，并使用标注数据训练该分类器。\"  \n在使用预训练BERT模型进行迁移学习时，所有层加载预训练权重可使模型充分借鉴从英文维基百科大规模文本中学到的知识。最后的全连接层通常具有任务特异性，因此将其替换为针对垃圾邮件预测任务定制的分类器，是一种常见且有效的做法。随后利用标注数据训练新分类器，即可使模型针对具体的垃圾邮件预测问题完成微调。  \n\n而\"除最后一层外所有层加载预训练权重进行初始化\"这一选项并不正确，因为并无充分理由将最后一层排除在预训练权重加载范围之外。预训练权重能为所有层提供优质的参数初始化基础。  \n\n\"在所有层加载预训练权重，并在首个输出位置叠加分类器\"这一选项存在谬误。在此类微调场景中，于首输出位置叠加分类器并非标准做法，通常需要通过替换最后一层来使模型适配新任务。  \n\n\"所有层采用随机权重初始化，替换最后一层全连接层后训练分类器\"这一方案则完全忽视了迁移学习的优势。随机初始化意味着模型需从零开始学习，在已具备大规模语料预训练BERT权重的情况下，这种处理方式显然效率低下。  \n\n综上所述，正确答案通过充分利用预训练权重并针对垃圾邮件预测任务调整模型结构，实现了迁移学习效益的最大化。"}, "answer": "D"}, {"id": "208", "question": {"enus": "A company is using a legacy telephony platform and has several years remaining on its contract. The company wants to move to AWS and wants to implement the following machine learning features: • Call transcription in multiple languages • Categorization of calls based on the transcript • Detection of the main customer issues in the calls • Customer sentiment analysis for each line of the transcript, with positive or negative indication and scoring of that sentiment Which AWS solution will meet these requirements with the LEAST amount of custom model training? ", "zhcn": "某公司目前仍在使用传统电话平台，且现有合约尚有数年才到期。该公司计划将业务迁移至亚马逊云服务（AWS），并希望实现以下机器学习功能：  \n- 支持多语言通话内容转写  \n- 根据转录文本实现通话自动分类  \n- 识别通话中客户反馈的核心问题  \n- 对转录文本逐行进行客户情绪分析，标注积极/消极倾向并给出情绪分值  \n\n在尽可能减少定制化模型训练的前提下，哪项AWS解决方案能够满足上述需求？"}, "option": [{"option_text": {"zhcn": "借助Amazon Transcribe处理音频通话，即可生成文字记录、实现通话分类并检测潜在问题。再通过Amazon Comprehend进行情感倾向分析。", "enus": "Use Amazon Transcribe to process audio calls to produce transcripts, categorize calls, and detect issues. Use Amazon Comprehend to  analyze sentiment."}, "option_flag": false}, {"option_text": {"zhcn": "利用Amazon Transcribe生成音频通话的文字记录，再通过Amazon Comprehend实现通话分类、问题侦测与情感倾向解析。", "enus": "Use Amazon Transcribe to process audio calls to produce transcripts. Use Amazon Comprehend to categorize calls, detect issues, and  analyze sentiment"}, "option_flag": false}, {"option_text": {"zhcn": "运用Amazon Connect的Contact Lens功能处理语音通话，可生成文字记录、实现通话分类、进行问题检测并完成情感分析。", "enus": "Use Contact Lens for Amazon Connect to process audio calls to produce transcripts, categorize calls, detect issues, and analyze  sentiment."}, "option_flag": false}, {"option_text": {"zhcn": "借助Amazon Connect的Contact Lens功能处理语音通话并生成文字记录。运用Amazon Comprehend服务实现通话分类、问题检测与情感倾向分析。", "enus": "Use Contact Lens for Amazon Connect to process audio calls to produce transcripts. Use Amazon Comprehend to categorize calls,  detect issues, and analyze sentiment."}, "option_flag": true}], "analysis": {"enus": "The correct answer to the question is to “Use Contact Lens for Amazon Connect to process audio calls to produce transcripts. Use Amazon Comprehend to categorize calls,  detect issues, and analyze sentiment.” Contact Lens for Amazon Connect is specifically designed to handle audio calls from contact centers and can efficiently generate transcripts, which is well - suited for this legacy telephony platform transition. Amazon Comprehend is a natural language processing service that has pre - trained models for categorizing text, detecting key issues, and performing sentiment analysis, thus minimizing the need for custom model training.\n\nThe first fake option claims that Amazon Transcribe can categorize calls and detect issues. Amazon Transcribe is mainly for speech - to - text conversion and does not have built - in capabilities for categorization and issue detection, so it is incorrect. The second fake option, while correctly identifying the use of Amazon Transcribe for transcription and Amazon Comprehend for other tasks, fails to leverage the optimized call - handling feature of Contact Lens for Amazon Connect, which is a better fit for this telephony context. The third fake option states that Contact Lens for Amazon Connect can perform categorization, issue detection, and sentiment analysis. Contact Lens focuses on call transcripts and basic analytics, and for more advanced NLP tasks like categorization, issue detection, and in - depth sentiment analysis, Amazon Comprehend is more appropriate. These are the reasons why the real answer stands out over the fake options.", "zhcn": "该问题的正确答案是\"利用 Amazon Connect 联络中心语音分析服务处理音频通话并生成文本记录，通过 Amazon Comprehend 实现通话分类、问题检测与情感分析\"。Amazon Connect 语音分析服务专为处理联络中心音频通话设计，能高效生成文本记录，尤其适合传统电话平台的转型场景。Amazon Comprehend 作为自然语言处理服务，内置预训练模型可实现文本分类、关键问题识别及情感分析，从而大幅降低定制化模型训练需求。  \n\n首项干扰选项声称 Amazon Transcribe 具备通话分类与问题检测功能。然 Amazon Transcribe 核心功能仅为语音转文本，并未内置分类与问题检测能力，故该选项错误。次项干扰选项虽正确指出使用 Amazon Transcribe 进行转写、Amazon Comprehend 执行其他任务，但未能充分利用 Amazon Connect 语音分析服务针对电话场景优化的通话处理特性。第三项干扰选项称 Amazon Connect 语音分析服务可执行分类、问题检测及情感分析。该服务主要聚焦通话转写与基础分析，至于分类、问题检测及深度情感分析等高级自然语言处理任务，仍以 Amazon Comprehend 更为适用。此即正确答案相较于干扰选项的显著优势所在。"}, "answer": "D"}, {"id": "209", "question": {"enus": "A finance company needs to forecast the price of a commodity. The company has compiled a dataset of historical daily prices. A data scientist must train various forecasting models on 80% of the dataset and must validate the eficacy of those models on the remaining 20% of the dataset. How should the data scientist split the dataset into a training dataset and a validation dataset to compare model performance? ", "zhcn": "一家金融公司需对某商品价格进行走势预测，现已整理完成该商品的历史每日价格数据集。数据科学家需利用数据集的80%训练多种预测模型，并借助剩余20%的数据验证模型效能。为准确评估模型表现，应如何将数据集划分为训练集与验证集？"}, "option": [{"option_text": {"zhcn": "选定一个日期，使得80%的数据点位于该日期之前。将这部分数据点划为训练集，其余所有数据点则归入验证集。", "enus": "Pick a date so that 80% of the data points precede the date. Assign that group of data points as the training dataset. Assign all the  remaining data points to the validation dataset."}, "option_flag": false}, {"option_text": {"zhcn": "选定一个日期，使得80%的数据点位于该日期之后。将这部分数据点划为训练集，其余所有数据点则归入验证集。", "enus": "Pick a date so that 80% of the data points occur after the date. Assign that group of data points as the training dataset. Assign all the  remaining data points to the validation dataset."}, "option_flag": true}, {"option_text": {"zhcn": "从数据集的最早时间点开始，每次选取八个数据点作为训练集，两个数据点作为验证集。如此循环进行分层抽样，直至所有数据点分配完毕。", "enus": "Starting from the earliest date in the dataset, pick eight data points for the training dataset and two data points for the validation  dataset. Repeat this stratified sampling until no data points remain."}, "option_flag": false}, {"option_text": {"zhcn": "对数据点进行随机无放回抽样，使训练集包含80%的数据样本，并将剩余所有数据点归入验证集。", "enus": "Sample data points randomly without replacement so that 80% of the data points are in the training dataset. Assign all the remaining  data points to the validation dataset."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question “A finance company needs to forecast the price of a commodity... How should the data scientist split the dataset into a training dataset and a validation dataset to compare model performance?” is “Pick a date so that 80% of the data points occur after the date. Assign that group of data points as the training dataset. Assign all the remaining data points to the validation dataset.” This is because in time - series data like historical daily commodity prices, the order of data matters. We should use past data to predict future data. By taking the more recent 80% of data as the training set, we can train the model on the most relevant and up - to - date information, and then use the earlier 20% for validation as it represents the 'past' relative to the training data.\n\nThe option “Pick a date so that 80% of the data points precede the date. Assign that group of data points as the training dataset. Assign all the remaining data points to the validation dataset” is incorrect because using older data for training and more recent data for validation goes against the principle of using past to predict future in time - series forecasting.\n\n“Starting from the earliest date in the dataset, pick eight data points for the training dataset and two data points for the validation dataset. Repeat this stratified sampling until no data points remain” is wrong as this method disrupts the chronological order of the time - series data, which is crucial for accurate forecasting.\n\n“Sample data points randomly without replacement so that 80% of the data points are in the training dataset. Assign all the remaining data points to the validation dataset” is also incorrect. Random sampling destroys the time - series structure, and the model may not be able to learn the temporal patterns necessary for accurate commodity price forecasting.\n\nThe key reason for choosing the real answer is maintaining the integrity of the time - series data for effective forecasting, which differentiates it from the fake options.", "zhcn": "针对“某金融公司需预测大宗商品价格……数据科学家应如何划分训练集与验证集以比较模型性能？”这一问题，正确答案为：“选定一个日期，使80%的数据点位于该日期之后，将这部分数据作为训练集，其余数据则归入验证集。”这是因为涉及历史每日价格等时间序列数据时，数据顺序至关重要。我们应当依据历史数据预测未来趋势：将时间轴上最近的80%数据作为训练集，可使模型基于最相关且最新的信息进行学习，而较早的20%数据作为验证集，因其相对于训练数据属于“过去时态”。\n\n至于“选定一个日期，使80%的数据点位于该日期之前，将这部分数据作为训练集”的方案并不合理——用陈旧数据训练模型却用新近数据验证，违背了时间序列预测中“以过去推演未来”的基本原则。\n\n“从最早日期开始，每取八个数据点作为训练集，两个作为验证集，循环此分层抽样直至数据用完”的方法亦不可取。这种操作会破坏时间序列的连续性，而时序结构恰恰是精准预测的核心。\n\n“随机无放回抽取80%数据作为训练集”同样存在谬误。随机抽样会瓦解时间序列的内在规律，导致模型无法捕捉价格波动中的时序特征，进而影响预测准确性。\n\n正确答案的核心优势在于保持时间序列数据的完整性，这正是其与错误选项的本质区别。"}, "answer": "B"}, {"id": "210", "question": {"enus": "A retail company wants to build a recommendation system for the company's website. The system needs to provide recommendations for existing users and needs to base those recommendations on each user's past browsing history. The system also must filter out any items that the user previously purchased. Which solution will meet these requirements with the LEAST development effort? ", "zhcn": "一家零售企业计划为其官方网站构建一套商品推荐系统。该系统需根据现有用户的历史浏览记录提供个性化推荐，同时自动屏蔽用户已购买过的商品。在满足上述需求的前提下，哪种解决方案能以最小的开发量实现这一目标？"}, "option": [{"option_text": {"zhcn": "在Amazon SageMaker上运用基于用户的协同过滤算法训练模型，并将模型部署于SageMaker实时推理终端。通过配置Amazon API Gateway接口与AWS Lambda函数，处理网络应用发送的实时推理请求。在向网络应用返回结果前，自动筛除用户既往购买过的商品条目。", "enus": "Train a model by using a user-based collaborative filtering algorithm on Amazon SageMaker. Host the model on a SageMaker real-time  endpoint. Configure an Amazon API Gateway API and an AWS Lambda function to handle real-time inference requests that the web  application sends. Exclude the items that the user previously purchased from the results before sending the results back to the web  application."}, "option_flag": false}, {"option_text": {"zhcn": "使用Amazon Personalize平台的PERSONALIZED_RANKING配方训练模型，建立实时过滤机制以排除用户历史购买商品。在Amazon Personalize上创建并部署推荐活动，通过GetPersonalizedRanking API接口获取实时动态推荐结果。", "enus": "Use an Amazon Personalize PERSONALIZED_RANKING recipe to train a model. Create a real-time filter to exclude items that the user  previously purchased. Create and deploy a campaign on Amazon Personalize. Use the GetPersonalizedRanking API operation to get the  real-time recommendations."}, "option_flag": false}, {"option_text": {"zhcn": "使用Amazon Personalize平台的USER_PERSONALIZATION配方训练模型，并设置实时过滤器以排除用户已购买的商品。随后在Amazon Personalize中创建并部署推荐活动，通过调用GetRecommendations API接口获取实时个性化推荐结果。", "enus": "Use an Amazon Personalize USER_PERSONALIZATION recipe to train a model. Create a real-time filter to exclude items that the user  previously purchased. Create and deploy a campaign on Amazon Personalize. Use the GetRecommendations API operation to get the real-  time recommendations."}, "option_flag": true}, {"option_text": {"zhcn": "在亚马逊SageMaker平台上，利用GPU实例训练神经协同过滤模型。将训练完成的模型部署至SageMaker实时推理终端节点。通过配置亚马逊API网关接口与AWS Lambda函数，处理网络应用发送的实时推理请求。在向网络应用返回结果前，系统将自动过滤用户已购买过的商品条目。", "enus": "Train a neural collaborative filtering model on Amazon SageMaker by using GPU instances. Host the model on a SageMaker real-time  endpoint. Configure an Amazon API Gateway API and an AWS Lambda function to handle real-time inference requests that the web  application sends. Exclude the items that the user previously purchased from the results before sending the results back to the web  application."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Use an Amazon Personalize USER_PERSONALIZATION recipe to train a model. Create a real-time filter to exclude items that the user previously purchased. Create and deploy a campaign on Amazon Personalize. Use the GetRecommendations API operation to get the real-time recommendations.” This solution requires the least development effort as Amazon Personalize is a fully - managed service. It abstracts much of the complexity involved in building a recommendation system, such as handling data preprocessing, model training, and optimization.\n\nThe first and fourth fake answer options involve training models on Amazon SageMaker. SageMaker is a more general - purpose machine learning platform. While it can be used to build recommendation systems, it requires more hands - on development. This includes tasks like implementing the user - based or neural collaborative filtering algorithms, hosting the model on an endpoint, and setting up API Gateway and Lambda functions for handling real - time requests. These steps demand significant time and expertise.\n\nThe third fake answer option uses the PERSONALIZED_RANKING recipe in Amazon Personalize. The PERSONALIZED_RANKING recipe is designed to rank a set of items according to a user's preferences, rather than providing direct item recommendations. The requirement is to provide recommendations, so the USER_PERSONALIZATION recipe is a better fit.\n\nCommon misconceptions might lead one to choose the SageMaker - based options because they are well - known for machine learning. However, they involve more development work compared to the fully - managed Amazon Personalize solution. Also, confusion between different Amazon Personalize recipes can lead to choosing the wrong one for the given requirements.", "zhcn": "对于该问题的正确答案是：“使用Amazon Personalize的USER_PERSONALIZATION配方训练模型，创建实时过滤器排除用户已购商品，并在Amazon Personalize上部署推荐活动，通过GetRecommendations API接口获取实时推荐。”这一方案所需开发投入最少，因为Amazon Personalize是全托管服务，其封装了构建推荐系统时涉及的诸多复杂环节，例如数据预处理、模型训练与算法优化。\n\n第一和第四项干扰选项涉及在Amazon SageMaker上训练模型。SageMaker作为通用机器学习平台虽能构建推荐系统，但需要更多手动开发工作，包括实现基于用户的协同过滤或神经协同过滤算法、将模型部署至终端节点，以及配置API Gateway和Lambda函数处理实时请求。这些步骤均需投入大量时间与专业能力。\n\n第三项干扰选项采用Amazon Personalize中的PERSONALIZED_RANKING配方。该配方主要用于根据用户偏好对已有物品集合进行排序，而非直接生成推荐内容。由于本题核心需求是提供推荐结果，因此USER_PERSONALIZATION配方更为契合。\n\n常见误解可能导致选择基于SageMaker的方案——毕竟其在机器学习领域声名显赫。但相较全托管的Amazon Personalize方案，这些选项实则需要更多开发工作。此外，若对不同Amazon Personalize配方的功能辨析不清，也容易在具体场景中误选不匹配的解决方案。"}, "answer": "C"}, {"id": "211", "question": {"enus": "A bank wants to use a machine learning (ML) model to predict if users will default on credit card payments. The training data consists of 30,000 labeled records and is evenly balanced between two categories. For the model, an ML specialist selects the Amazon SageMaker built- in XGBoost algorithm and configures a SageMaker automatic hyperparameter optimization job with the Bayesian method. The ML specialist uses the validation accuracy as the objective metric. When the bank implements the solution with this model, the prediction accuracy is 75%. The bank has given the ML specialist 1 day to improve the model in production. Which approach is the FASTEST way to improve the model's accuracy? ", "zhcn": "一家银行计划采用机器学习模型预测用户信用卡还款违约情况。训练数据包含3万条带标签记录，且两个类别分布完全均衡。机器学习专家选用亚马逊SageMaker平台内置的XGBoost算法，并采用贝叶斯方法配置了超参数自动优化任务，将验证准确率设为目标指标。实际部署该模型后，预测准确率为75%。银行要求机器学习专家在一天内提升生产环境中的模型性能，下列哪种方法能最快速提升模型准确率？"}, "option": [{"option_text": {"zhcn": "基于当前模型调优任务中的最佳候选模型，运行一次SageMaker增量训练。持续监控此前调优过程中使用的目标评估指标，并寻求性能提升。", "enus": "Run a SageMaker incremental training based on the best candidate from the current model's tuning job. Monitor the same metric that  was used as the objective metric in the previous tuning, and look for improvements."}, "option_flag": true}, {"option_text": {"zhcn": "将ROC曲线下面积（AUC）设定为新SageMaker超参数自动调优任务的目标评估指标。训练任务最大数量参数沿用此前调优任务的配置。", "enus": "Set the Area Under the ROC Curve (AUC) as the objective metric for a new SageMaker automatic hyperparameter tuning job. Use the  same maximum training jobs parameter that was used in the previous tuning job."}, "option_flag": false}, {"option_text": {"zhcn": "基于当前模型的超参数调优任务，启动一次SageMaker热启动调优。目标评估指标需与先前调优过程中所采用的指标保持一致。", "enus": "Run a SageMaker warm start hyperparameter tuning job based on the current model’s tuning job. Use the same objective metric that  was used in the previous tuning."}, "option_flag": false}, {"option_text": {"zhcn": "将F1分数设定为新SageMaker自动超参数调优任务的目标评估指标。将先前调优任务中使用的最大训练任务参数值提升至两倍。", "enus": "Set the F1 score as the objective metric for a new SageMaker automatic hyperparameter tuning job. Double the maximum training jobs  parameter that was used in the previous tuning job."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Run a SageMaker incremental training based on the best candidate from the current model's tuning job. Monitor the same metric that was used as the objective metric in the previous tuning, and look for improvements.” This approach is the fastest way to enhance the model's accuracy within the one - day time limit. Incremental training builds on the existing best model from the previous tuning job, leveraging the already optimized hyperparameters. It doesn't require starting the hyperparameter search from scratch, saving significant time.\n\nThe fake answer options involve starting new hyperparameter tuning jobs. Changing the objective metric (such as using AUC or F1 score) in a new tuning job, as well as potentially doubling the maximum training jobs, requires a full - scale search for new hyperparameters. This process is time - consuming as it has to train multiple models to find the optimal hyperparameter combination. A warm - start hyperparameter tuning job, while it uses information from the previous job, still involves a hyperparameter search which is slower compared to incremental training. \n\nThe common misconception that could lead to choosing the fake options is the belief that changing the objective metric or increasing the number of training jobs will directly and quickly improve the model's accuracy. However, these methods prioritize finding better hyperparameters rather than quickly enhancing the existing model, which is more time - efficient.", "zhcn": "针对该问题，正确答案是\"基于当前模型调优任务中的最佳候选方案，运行SageMaker增量训练。继续采用先前调优阶段使用的目标评估指标进行监控，并观察模型表现的提升\"。这种方法能在一日内最快提升模型精度。增量训练依托先前调优产生的最佳模型，充分利用已优化的超参数，无需从零开始启动超参数搜索，从而大幅节省时间。\n\n而错误选项则涉及启动新的超参数调优任务：若在新调优任务中更改目标评估指标（如改用AUC或F1分数），或盲目增加最大训练任务量，都将引发新一轮全尺度超参数搜索。此过程需训练多个模型以寻找最优超参数组合，耗时颇久。虽然热启动式超参数调优能借鉴历史任务信息，但其搜索机制仍比增量训练缓慢。\n\n常见误区在于误认为调整评估指标或增加训练量能直接快速提升精度，实则这些方法侧重于寻找更优超参数，而非高效改进现有模型——后者才是更省时的优化路径。"}, "answer": "A"}, {"id": "212", "question": {"enus": "A data scientist has 20 TB of data in CSV format in an Amazon S3 bucket. The data scientist needs to convert the data to Apache Parquet format. How can the data scientist convert the file format with the LEAST amount of effort? ", "zhcn": "一位数据科学家在亚马逊S3存储桶中存有20TB的CSV格式数据。现需将数据转换为Apache Parquet格式，请问如何以最简捷的方式完成格式转换？"}, "option": [{"option_text": {"zhcn": "使用AWS Glue爬虫程序转换文件格式。", "enus": "Use an AWS Glue crawler to convert the file format."}, "option_flag": false}, {"option_text": {"zhcn": "编写一个脚本以转换文件格式，并将该脚本作为AWS Glue任务运行。", "enus": "Write a script to convert the file format. Run the script as an AWS Glue job."}, "option_flag": false}, {"option_text": {"zhcn": "编写一个用于转换文件格式的脚本，并在亚马逊EMR集群上运行该脚本。", "enus": "Write a script to convert the file format. Run the script on an Amazon EMR cluster."}, "option_flag": false}, {"option_text": {"zhcn": "编写一个脚本用于转换文件格式。在Amazon SageMaker笔记本中运行该脚本。", "enus": "Write a script to convert the file format. Run the script in an Amazon SageMaker notebook."}, "option_flag": true}], "analysis": {"enus": "The correct answer to the question ‘A data scientist has 20 TB of data in CSV format in an Amazon S3 bucket. The data scientist needs to convert the data to Apache Parquet format. How can the data scientist convert the file format with the LEAST amount of effort?’ is ‘Write a script to convert the file format. Run the script in an Amazon SageMaker notebook.’ This is because Amazon SageMaker notebooks provide an interactive and user - friendly environment. They come pre - configured with common data science libraries and tools, allowing the data scientist to quickly write and execute a conversion script with minimal setup compared to other options.\n\nThe option ‘Use an AWS Glue crawler to convert the file format’ is incorrect. AWS Glue crawlers are mainly used for discovering and cataloging data, not for actual format conversion. It doesn't have the direct functionality to convert CSV to Parquet format. \n\nFor the option ‘Write a script to convert the file format. Run the script as an AWS Glue job’, while AWS Glue can handle data processing, setting up a Glue job requires more configuration such as defining job parameters, resource allocation, and managing the job lifecycle. This adds unnecessary complexity compared to using a SageMaker notebook.\n\nSimilarly, running the script on an Amazon EMR cluster in the option ‘Write a script to convert the file format. Run the script on an Amazon EMR cluster’ involves tasks like cluster creation, configuration of Hadoop and Spark components, and managing the cluster infrastructure. This demands more effort and time than using a SageMaker notebook. The simplicity and pre - configured nature of SageMaker notebooks are the key factors that make it the real answer, distinguishing it from the fake options.", "zhcn": "针对“数据科学家在Amazon S3中存有20TB的CSV格式数据，需将其转换为Apache Parquet格式，如何以最省力的方式实现？”这一问题，正确答案是“编写格式转换脚本并在Amazon SageMaker笔记本中运行”。原因在于：Amazon SageMaker笔记本提供了交互式且易于操作的环境，预置了常见的数据科学库和工具，使数据科学家能够快速编写并执行转换脚本，与其他方案相比几乎无需额外配置。\n\n而“使用AWS Glue爬虫程序转换文件格式”这一选项并不正确。AWS Glue爬虫的主要功能是数据发现与分类，并不直接支持格式转换，其本身无法将CSV转为Parquet格式。至于“编写脚本并作为AWS Glue任务运行”的方案，虽然AWS Glue可处理数据，但配置任务需设定参数、分配资源并管理任务生命周期，相较于SageMaker笔记本更为复杂。\n\n同样，“在Amazon EMR集群上运行转换脚本”的方案涉及创建集群、配置Hadoop与Spark组件、管理基础设施等步骤，所需的时间和精力远超过使用SageMaker笔记本。正是SageMaker笔记本即开即用的简洁性，使其成为真正的最优解，从而与其他干扰选项区分开来。"}, "answer": "D"}, {"id": "213", "question": {"enus": "A company is building a pipeline that periodically retrains its machine learning (ML) models by using new streaming data from devices. The company's data engineering team wants to build a data ingestion system that has high throughput, durable storage, and scalability. The company can tolerate up to 5 minutes of latency for data ingestion. The company needs a solution that can apply basic data transformation during the ingestion process. Which solution will meet these requirements with the MOST operational eficiency? ", "zhcn": "某公司正在构建一套数据管道系统，通过设备端持续产生的新流数据定期对其机器学习模型进行再训练。该公司的数据工程团队需要搭建一套具备高吞吐量、持久化存储及弹性扩展能力的数据摄取系统，且数据接入延迟需控制在五分钟以内。该系统还需在数据接入阶段完成基础的数据转换处理。在满足上述所有要求的前提下，何种解决方案能实现最优运维效率？"}, "option": [{"option_text": {"zhcn": "将设备配置为向Amazon Kinesis数据流发送流式数据。设置Amazon Kinesis Data Firehose传输流，使其自动接收Kinesis数据流，通过AWS Lambda函数对数据进行转换，并将处理结果存储至Amazon S3存储桶中。", "enus": "Configure the devices to send streaming data to an Amazon Kinesis data stream. Configure an Amazon Kinesis Data Firehose delivery  stream to automatically consume the Kinesis data stream, transform the data with an AWS Lambda function, and save the output into an  Amazon S3 bucket."}, "option_flag": true}, {"option_text": {"zhcn": "将设备配置为向亚马逊S3存储桶发送流式数据。设置由S3事件通知触发的AWS Lambda函数，用于转换数据并将其载入亚马逊Kinesis数据流。配置亚马逊Kinesis Data Firehose传输流，使其自动摄取Kinesis数据流中的数据，并将处理结果回传至S3存储桶。", "enus": "Configure the devices to send streaming data to an Amazon S3 bucket. Configure an AWS Lambda function that is invoked by S3 event  notifications to transform the data and load the data into an Amazon Kinesis data stream. Configure an Amazon Kinesis Data Firehose  delivery stream to automatically consume the Kinesis data stream and load the output back into the S3 bucket."}, "option_flag": false}, {"option_text": {"zhcn": "将设备配置为向Amazon S3存储桶发送流式数据。设置一个由S3事件通知触发的AWS Glue作业，用于读取数据、转换数据格式，并将处理结果载入新的S3存储桶。", "enus": "Configure the devices to send streaming data to an Amazon S3 bucket. Configure an AWS Glue job that is invoked by S3 event  notifications to read the data, transform the data, and load the output into a new S3 bucket."}, "option_flag": false}, {"option_text": {"zhcn": "将设备配置为向Amazon Kinesis Data Firehose传输流发送实时数据流。设置一个AWS Glue作业，使其连接至该传输流以进行数据转换，并将处理结果导入Amazon S3存储桶。", "enus": "Configure the devices to send streaming data to an Amazon Kinesis Data Firehose delivery stream. Configure an AWS Glue job that  connects to the delivery stream to transform the data and load the output into an Amazon S3 bucket."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to configure the devices to send streaming data to an Amazon Kinesis data stream, then use an Amazon Kinesis Data Firehose delivery stream to consume it, transform the data with an AWS Lambda function, and save the output into an Amazon S3 bucket. This solution offers high throughput as Kinesis Data Streams can handle large - scale streaming data, and Kinesis Data Firehose is optimized for high - volume data ingestion. S3 provides durable storage, and all these services are highly scalable. The use of Lambda for data transformation during ingestion is efficient and can meet the 5 - minute latency tolerance.\n\nThe first fake option is less efficient because sending data directly to S3 first and then using Lambda to move it between S3 and Kinesis adds unnecessary complexity and potential latency. It also involves redundant data movement between S3 and Kinesis.\n\nThe second fake option using an AWS Glue job triggered by S3 event notifications has higher latency. Glue jobs are more suitable for batch processing rather than near - real - time streaming data ingestion. They may take longer to start and execute, not meeting the 5 - minute latency requirement.\n\nThe third fake option of using an AWS Glue job to connect to the Kinesis Data Firehose delivery stream is also sub - optimal. Similar to the previous point, Glue jobs are batch - oriented and not the best fit for the low - latency, high - throughput requirements of this streaming data ingestion scenario.\n\nIn summary, the real answer provides the most operationally efficient solution for high - throughput, low - latency streaming data ingestion with basic transformation and durable storage, which is why it is the correct choice.", "zhcn": "针对该问题的正确答案是：将设备配置为向Amazon Kinesis数据流发送流式数据，随后通过Amazon Kinesis Data Firehose传输流进行数据消费，并利用AWS Lambda函数实施数据转换，最终将处理结果存储至Amazon S3存储桶。该方案能实现高吞吐量——Kinesis数据流可处理大规模流式数据，而Kinesis Data Firehose专为海量数据摄入优化。S3提供持久化存储，且所有服务均具备高度可扩展性。在数据摄入阶段采用Lambda进行转换既高效又能满足5分钟内的延迟容忍度。\n\n首个干扰选项效率较低：若先将数据直接发送至S3，再通过Lambda在S3与Kinesis间转移数据，会引入不必要的复杂性及潜在延迟，同时导致S3与Kinesis间的冗余数据迁移。\n\n第二个干扰选项通过S3事件通知触发AWS Glue作业的方式存在较高延迟。Glue作业更适用于批处理场景，而非近实时流式数据摄入，其启动和执行耗时可能无法满足5分钟延迟要求。\n\n第三个干扰方案采用Glue作业连接Kinesis Data Firehose传输流同样非最优解。如前所述，Glue作业本质偏向批处理，难以契合本场景对低延迟、高吞吐流式数据摄入的需求。\n\n综上，正确答案通过流式数据高效摄入、基础转换与持久化存储的有机结合，为高吞吐、低延迟需求提供了最优运营效率的解决方案，故为正确选择。"}, "answer": "A"}, {"id": "214", "question": {"enus": "A retail company is ingesting purchasing records from its network of 20,000 stores to Amazon S3 by using Amazon Kinesis Data Firehose. The company uses a small, server-based application in each store to send the data to AWS over the internet. The company uses this data to train a machine learning model that is retrained each day. The company's data science team has identified existing attributes on these records that could be combined to create an improved model. Which change will create the required transformed records with the LEAST operational overhead? ", "zhcn": "一家零售企业正通过亚马逊Kinesis Data Firehose服务，将其两万家门店的采购记录实时传输至亚马逊S3存储平台。各门店通过基于服务器的小型应用程序，经由互联网将数据发送至AWS云平台。这些数据主要用于训练机器学习模型，该模型每日都会进行迭代更新。企业的数据科学团队发现，通过整合现有记录属性可构建更优化的模型。若要实现所需的记录转换，同时将运维负担降至最低，应采取哪种改进方案？"}, "option": [{"option_text": {"zhcn": "创建一个能够处理传入记录的AWS Lambda函数。在数据摄取Kinesis Data Firehose传输流中启用数据转换功能，并将该Lambda函数设定为调用目标。", "enus": "Create an AWS Lambda function that can transform the incoming records. Enable data transformation on the ingestion Kinesis Data  Firehose delivery stream. Use the Lambda function as the invocation target."}, "option_flag": false}, {"option_text": {"zhcn": "部署一个运行Apache Spark并包含转换逻辑的Amazon EMR集群。通过Amazon EventBridge（Amazon CloudWatch Events）设置定时任务，每日触发AWS Lambda函数启动该集群，对积存在Amazon S3中的记录进行转换处理，并将转换后的数据回传至Amazon S3。", "enus": "Deploy an Amazon EMR cluster that runs Apache Spark and includes the transformation logic. Use Amazon EventBridge (Amazon  CloudWatch Events) to schedule an AWS Lambda function to launch the cluster each day and transform the records that accumulate in  Amazon S3. Deliver the transformed records to Amazon S3."}, "option_flag": false}, {"option_text": {"zhcn": "在各门店部署亚马逊S3文件网关，并升级店内软件以将数据传送至该网关。通过每日定时运行的AWS Glue任务，对经由S3文件网关传输至亚马逊S3存储服务的数据进行转换处理。", "enus": "Deploy an Amazon S3 File Gateway in the stores. Update the in-store software to deliver data to the S3 File Gateway. Use a scheduled  daily AWS Glue job to transform the data that the S3 File Gateway delivers to Amazon S3."}, "option_flag": true}, {"option_text": {"zhcn": "部署一组集成转换逻辑的亚马逊EC2实例，通过每日定时任务配置自动处理积存在亚马逊S3中的记录文件，并将处理完成的数据回传至亚马逊S3存储空间。", "enus": "Launch a fieet of Amazon EC2 instances that include the transformation logic. Configure the EC2 instances with a daily cron job to  transform the records that accumulate in Amazon S3. Deliver the transformed records to Amazon S3."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Deploy an Amazon S3 File Gateway in the stores. Update the in - store software to deliver data to the S3 File Gateway. Use a scheduled daily AWS Glue job to transform the data that the S3 File Gateway delivers to Amazon S3.” This option has the least operational overhead because the S3 File Gateway simplifies data ingestion from on - premise stores to S3, and AWS Glue is a fully managed service. It doesn't require the user to manage infrastructure for data transformation, as Glue takes care of resource allocation, scaling, and maintenance.\n\nThe first fake option of using a Lambda function with Kinesis Data Firehose for transformation might face limitations in terms of memory and execution time for large - scale data transformation. Also, it requires careful management of the Lambda function and its integration with Kinesis.\n\nThe second fake option of using an EMR cluster with Apache Spark involves significant infrastructure management. Launching an EMR cluster daily through a Lambda function scheduled by EventBridge requires knowledge of cluster configuration, resource allocation, and maintenance, which adds to the operational burden.\n\nThe third fake option of using a fleet of EC2 instances with cron jobs is also high in operational overhead. It requires managing the EC2 instances, including provisioning, security, and monitoring, as well as ensuring the cron jobs run correctly. \n\nIn summary, the real answer leverages fully - managed services, which reduces the need for hands - on infrastructure management and thus has the least operational overhead compared to the fake options.", "zhcn": "针对该问题，正确答案是\"在各门店部署亚马逊S3文件网关，更新店内软件使其将数据传送至S3文件网关，并通过每日定时运行的AWS Glue作业处理由S3文件网关传输至Amazon S3的数据\"。此方案具有最低运维成本，因为S3文件网关可简化从本地门店至S3的数据摄取流程，而AWS Glue作为全托管服务，无需用户管理数据转换的基础设施——Glue将自动处理资源分配、扩展及维护事宜。\n\n首项干扰方案采用Lambda函数与Kinesis Data Firehose进行数据转换，但面临大规模数据处理时可能存在内存与执行时间限制，且需要精细管理Lambda函数及其与Kinesis的集成。\n\n第二项干扰方案通过Apache Spark运行EMR集群，涉及大量基础设施管理工作。借助EventBridge定时触发Lambda函数每日启停EMR集群，需要掌握集群配置、资源分配及维护知识，显著增加运维负担。\n\n第三项干扰方案采用配备定时任务的EC2实例集群同样运维成本高昂，需管理EC2实例的配置、安全及监控，并确保定时任务稳定运行。\n\n综上所述，正确答案通过采用全托管服务，最大限度降低了基础设施的手动管理需求，相较干扰方案具有显著优化的运维效率。"}, "answer": "C"}, {"id": "215", "question": {"enus": "A sports broadcasting company is planning to introduce subtitles in multiple languages for a live broadcast. The commentary is in English. The company needs the transcriptions to appear on screen in French or Spanish, depending on the broadcasting country. The transcriptions must be able to capture domain-specific terminology, names, and locations based on the commentary context. The company needs a solution that can support options to provide tuning data. Which combination of AWS services and features will meet these requirements with the LEAST operational overhead? (Choose two.) ", "zhcn": "一家体育转播公司计划为直播节目引入多语言字幕服务。其解说词为英文内容，需根据播出国家在屏幕上显示法语或西班牙语字幕。译文必须能够准确捕捉基于解说语境的领域专有术语、人名及地名。该公司需要一套支持提供调优数据选项的解决方案。以下哪两种AWS服务与功能的组合能以最小运维投入满足上述需求？（请选择两项。）"}, "option": [{"option_text": {"zhcn": "亚马逊Transcribe自定义词汇增强版", "enus": "Amazon Transcribe with custom vocabularies"}, "option_flag": false}, {"option_text": {"zhcn": "借助定制语言模型的亚马逊转录服务", "enus": "Amazon Transcribe with custom language models"}, "option_flag": true}, {"option_text": {"zhcn": "Amazon SageMaker Seq2Seq", "enus": "Amazon SageMaker Seq2Seq"}, "option_flag": false}, {"option_text": {"zhcn": "Amazon SageMaker 与 Hugging Face Speech2Text 的深度融合", "enus": "Amazon SageMaker with Hugging Face Speech2Text"}, "option_flag": false}, {"option_text": {"zhcn": "亚马逊翻译", "enus": "Amazon Translate"}, "option_flag": true}], "analysis": {"enus": "The correct answers to the question are ‘Amazon Transcribe with custom language models’ and ‘Amazon Translate’. Amazon Transcribe with custom language models can accurately capture domain - specific terminology, names, and locations from the English commentary. Custom language models are more powerful than custom vocabularies as they can adapt better to the context and language patterns of the sports broadcast, reducing the need for constant manual updates. \n\nAmazon Translate can then take the English transcript obtained from Amazon Transcribe and translate it into French or Spanish as required for different broadcasting countries. \n\n‘Amazon Transcribe with custom vocabularies’ is less suitable because custom vocabularies have limitations in handling complex context compared to custom language models. They are mainly for adding specific words rather than adapting to language patterns. \n\n‘Amazon SageMaker Seq2Seq’ and ‘Amazon SageMaker with Hugging Face Speech2Text’ are more complex and require more operational overhead. These services are better - suited for building and training custom machine - learning models from scratch, which is not necessary when AWS already offers pre - built and optimized services like Amazon Transcribe and Amazon Translate for the given task. \n\nThe key factors that distinguish the real answer options are their simplicity, ability to handle the specific requirements of the problem, and the least amount of operational overhead, making them the ideal choices over the fake options.", "zhcn": "该问题的正确答案是\"Amazon Transcribe with custom language models\"和\"Amazon Translate\"。搭载定制语言模型的Amazon Transcribe能够精准捕捉英文解说中的专业术语、人名与地名。相较于定制词表，定制语言模型具备更强的语境适应能力，可更好地匹配体育赛事解说的语言模式，从而减少频繁人工调整的需求。随后，Amazon Translate可将经由Amazon Transcribe生成的英文文本，按不同播出国需求准确译为法语或西班牙语。\n\n\"Amazon Transcribe with custom vocabularies\"方案适配性较弱，因为定制词表在处理复杂语境方面存在局限，其主要功能仅限于添加特定词汇，而无法实现语言模式的动态调适。至于\"Amazon SageMaker Seq2Seq\"和\"Amazon SageMaker with Hugging Face Speech2Text\"方案，其架构更为复杂且运维成本较高。这两项服务更适用于从零开始构建定制机器学习模型的场景，而本题所述任务完全可直接采用Amazon Transcribe与Amazon Translate这类经过预优化的AWS原生服务。\n\n真选项的核心优势在于：方案简洁高效，精准契合任务需求，且运维复杂度最低。这些特质使其明显优于其他干扰选项，成为理想之选。"}, "answer": "BE"}, {"id": "216", "question": {"enus": "A data scientist at a retail company is forecasting sales for a product over the next 3 months. After preliminary analysis, the data scientist identifies that sales are seasonal and that holidays affect sales. The data scientist also determines that sales of the product are correlated with sales of other products in the same category. The data scientist needs to train a sales forecasting model that incorporates this information. Which solution will meet this requirement with the LEAST development effort? ", "zhcn": "某零售企业的数据分析师正在对一款产品未来三个月的销售额进行预测。初步分析显示，该产品的销售呈现季节性特征且受节假日影响，同时与同品类其他产品的销量存在关联性。现需开发一个能整合这些因素的销售预测模型，下列哪种方案能以最小开发成本满足需求？"}, "option": [{"option_text": {"zhcn": "结合亚马逊预测服务的节假日特征化功能与内置的自回归积分滑动平均（ARIMA）算法，对模型进行训练。", "enus": "Use Amazon Forecast with Holidays featurization and the built-in autoregressive integrated moving average (ARIMA) algorithm to train  the model."}, "option_flag": false}, {"option_text": {"zhcn": "利用亚马逊 Forecast 服务的节假日特征化功能，结合内置的 DeepAR+ 算法进行模型训练。", "enus": "Use Amazon Forecast with Holidays featurization and the built-in DeepAR+ algorithm to train the model."}, "option_flag": true}, {"option_text": {"zhcn": "利用Amazon SageMaker Processing为数据添加节假日信息以进行增强。随后，采用SageMaker内置的DeepAR算法对模型进行训练。", "enus": "Use Amazon SageMaker Processing to enrich the data with holiday information. Train the model by using the SageMaker DeepAR built-  in algorithm."}, "option_flag": false}, {"option_text": {"zhcn": "利用Amazon SageMaker Processing为数据添加节假日信息以增强其特征。随后，采用Gluon时间序列工具包（GluonTS）进行模型训练。", "enus": "Use Amazon SageMaker Processing to enrich the data with holiday information. Train the model by using the Gluon Time Series  (GluonTS) toolkit."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Use Amazon Forecast with Holidays featurization and the built - in DeepAR+ algorithm to train the model.” Amazon Forecast is a fully - managed service specifically designed for time - series forecasting. It simplifies the process by handling data ingestion, model selection, and training, which significantly reduces development effort. The DeepAR+ algorithm in Amazon Forecast is optimized for handling multiple time - series data, taking into account seasonality, and can incorporate correlated time - series, making it well - suited for this retail sales forecasting scenario with holiday effects and correlated product sales.\n\nThe option of using Amazon Forecast with the ARIMA algorithm is less ideal. While ARIMA can handle time - series data, it may not perform as well as DeepAR+ when dealing with complex patterns such as multiple correlated time - series and holiday effects. \n\nThe options involving Amazon SageMaker Processing require more development effort. Amazon SageMaker Processing is used for data pre - processing. Using it to enrich data with holiday information and then training models with SageMaker DeepAR or the GluonTS toolkit involves more manual steps in data handling, model configuration, and training compared to the fully - managed Amazon Forecast service. \n\nCommon misconceptions might lead one to choose the fake options. For example, some might be more familiar with traditional algorithms like ARIMA and assume it's a good fit without considering the capabilities of more advanced algorithms like DeepAR+. Others might choose the SageMaker - related options thinking they have more control, but in this case, that control comes at the cost of increased development effort.", "zhcn": "针对该问题，正确答案是\"使用具备节假日特征化功能的Amazon Forecast服务，配合内置的DeepAR+算法进行模型训练\"。Amazon Forecast是一项专为时间序列预测打造的全面托管服务，它通过自动化处理数据导入、模型选择及训练流程，显著降低了开发复杂度。其内置的DeepAR+算法特别擅长处理多维度时间序列数据，不仅能捕捉季节性规律，还可整合关联时序变量，非常适用于包含节假日效应与商品销售关联性的零售预测场景。\n\n相比之下，采用ARIMA算法的Amazon Forecast方案并非最佳选择。虽然ARIMA可处理时间序列数据，但在应对复杂模式（如多重关联时序和节假日效应）时表现不及DeepAR+算法。而涉及Amazon SageMaker处理器的选项则需要更多开发投入——该服务主要用于数据预处理，若以其添加节假日信息后再通过SageMaker DeepAR或GluonTS工具包训练模型，相较于全托管的Amazon Forecast服务，需在数据处理、模型配置等环节进行更多手动操作。\n\n常见误解可能导致选择错误答案：例如部分用户因熟悉ARIMA等传统算法而忽视DeepAR+等先进算法的优势；或误认为SageMaker方案能提供更高控制度，但在此场景下这种控制力反而会带来额外的开发负担。"}, "answer": "B"}, {"id": "217", "question": {"enus": "A company is building a predictive maintenance model for its warehouse equipment. The model must predict the probability of failure of all machines in the warehouse. The company has collected 10,000 event samples within 3 months. The event samples include 100 failure cases that are evenly distributed across 50 different machine types. How should the company prepare the data for the model to improve the model's accuracy? ", "zhcn": "某公司正为其仓储设备构建一套预测性维护模型。该模型需精准预测仓库内所有设备的故障发生概率。在三个月内，企业已采集到一万条事件样本，其中包含均匀分布在50种不同机型中的100例故障记录。为提升模型预测精度，企业应如何对数据进行预处理？"}, "option": [{"option_text": {"zhcn": "根据设备类型调整类别权重，以平衡各类别的影响。", "enus": "Adjust the class weight to account for each machine type."}, "option_flag": false}, {"option_text": {"zhcn": "对少数类样本采用合成少数类过采样技术（SMOTE）进行扩增。", "enus": "Oversample the failure cases by using the Synthetic Minority Oversampling Technique (SMOTE)."}, "option_flag": false}, {"option_text": {"zhcn": "对非故障事件进行降采样处理，并依据机器类型对其进行分层抽样。", "enus": "Undersample the non-failure events. Stratify the non-failure events by machine type."}, "option_flag": false}, {"option_text": {"zhcn": "对非故障事件采用合成少数类过采样技术（SMOTE）进行降采样处理。", "enus": "Undersample the non-failure events by using the Synthetic Minority Oversampling Technique (SMOTE)."}, "option_flag": true}], "analysis": {"enus": "The correct answer to the question is “Undersample the non - failure events by using the Synthetic Minority Oversampling Technique (SMOTE)”. The main issue here is the class imbalance, as there are only 100 failure cases out of 10,000 event samples. SMOTE can be used to create synthetic samples of the minority class (failure cases) while undersampling non - failure events helps to balance the class distribution, which generally improves the model's accuracy in predicting the probability of machine failure.\n\nThe option “Adjust the class weight to account for each machine type” is incorrect because the problem is about class imbalance between failure and non - failure events, not about accounting for machine types. Adjusting class weights for machine types won't address the core issue of predicting failure probability.\n\n“Oversample the failure cases by using the Synthetic Minority Oversampling Technique (SMOTE)” alone is not enough. Just oversampling the minority class without dealing with the large number of non - failure events can lead to overfitting on the minority class.\n\n“Undersample the non - failure events. Stratify the non - failure events by machine type” is wrong as stratifying by machine type doesn't directly tackle the class imbalance problem between failure and non - failure events. The key is to balance the two main classes for better prediction of failure probability.\n\nThe common misconception might be focusing on machine types instead of the class imbalance issue, or thinking that oversampling the minority class alone can solve the problem without considering the large number of non - failure events.", "zhcn": "该问题的正确答案是\"对非故障事件进行欠采样，并采用合成少数类过采样技术（SMOTE）\"。核心问题在于类别不平衡——在10000个事件样本中仅有100个故障案例。通过SMOTE技术可以生成少数类（故障案例）的合成样本，同时欠采样非故障事件能使类别分布趋于平衡，此举通常能提升模型预测机器故障概率的准确性。\n\n\"根据机器类型调整类别权重\"这一选项并不恰当，因为当前矛盾在于故障与非故障事件之间的类别失衡，而非机器类型的权重分配。针对机器类型调整类别权重，无法解决预测故障概率这一核心问题。\n\n若仅\"采用SMOTE技术对故障案例过采样\"则效果有限。单纯对少数类过采样而不处理海量非故障事件，容易导致模型对少数类产生过拟合。\n\n\"对非故障事件欠采样，并按机器类型分层\"的做法同样有误——按机器类型分层并不能直接解决故障与非故障事件间的类别失衡问题。关键在于平衡两大主类别以提升故障概率预测能力。\n\n常见的认知误区包括：过度关注机器类型而忽视类别不平衡本质，或认为仅靠对少数类过采样就能解决问题，却未考量非故障事件的数量规模。"}, "answer": "D"}, {"id": "218", "question": {"enus": "A company stores its documents in Amazon S3 with no predefined product categories. A data scientist needs to build a machine learning model to categorize the documents for all the company's products. Which solution will meet these requirements with the MOST operational eficiency? ", "zhcn": "一家公司将其文档存储于Amazon S3中，且未预设产品类别。数据科学家需构建一个机器学习模型，以对公司所有产品的文档进行分类。下列哪种方案能以最高运作效率满足这些要求？"}, "option": [{"option_text": {"zhcn": "构建定制化聚类模型。编写Dockerfile文件并构建Docker镜像。将镜像注册至亚马逊弹性容器仓库（Amazon ECR）。通过该定制镜像在Amazon SageMaker平台生成训练完成的模型。", "enus": "Build a custom clustering model. Create a Dockerfile and build a Docker image. Register the Docker image in Amazon Elastic Container  Registry (Amazon ECR). Use the custom image in Amazon SageMaker to generate a trained model."}, "option_flag": false}, {"option_text": {"zhcn": "对数据进行分词处理并将其转换为表格形式。随后，训练亚马逊SageMaker平台的k-means模型以生成产品分类体系。", "enus": "Tokenize the data and transform the data into tabular data. Train an Amazon SageMaker k-means model to generate the product  categories."}, "option_flag": true}, {"option_text": {"zhcn": "在Amazon SageMaker平台上训练神经主题模型（NTM），用于自动生成产品分类体系。", "enus": "Train an Amazon SageMaker Neural Topic Model (NTM) model to generate the product categories."}, "option_flag": false}, {"option_text": {"zhcn": "在亚马逊SageMaker平台上训练Blazing Text模型，以生成产品分类体系。", "enus": "Train an Amazon SageMaker Blazing Text model to generate the product categories."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is “Tokenize the data and transform the data into tabular data. Train an Amazon SageMaker k - means model to generate the product categories.” This approach is the most operationally efficient because k - means is a well - known and straightforward unsupervised learning algorithm for clustering. It can quickly group the documents into categories without predefined labels, which is suitable for the given scenario where there are no predefined product categories.\n\nThe option of building a custom clustering model, creating a Dockerfile, building a Docker image, registering it in Amazon ECR, and then using it in Amazon SageMaker is overly complex and time - consuming. It requires a lot of additional steps and management, reducing operational efficiency.\n\nThe Amazon SageMaker Neural Topic Model (NTM) is more complex and resource - intensive compared to k - means. It uses neural networks, which may require more computational power and longer training times, not the most efficient for this task.\n\nThe Amazon SageMaker Blazing Text model is mainly designed for text classification with pre - defined labels. Since there are no predefined product categories in this case, it is not the appropriate choice. \n\nThe simplicity and suitability of the k - means algorithm for unsupervised clustering make it the best option for achieving the most operational efficiency in categorizing the unlabeled documents, distinguishing it from the other fake answer options.", "zhcn": "对于该问题的正确答案是：“对数据进行分词处理并将其转换为表格形式，随后训练亚马逊SageMaker k均值模型以生成产品类别。”这一方案在操作上最具效率，因为k均值算法作为经典且直观的无监督学习算法，能够无需预定义标签即可快速实现文档聚类，完美契合当前无预定义产品类别的应用场景。\n\n相比之下，若选择构建自定义聚类模型、编写Dockerfile、构建镜像、在亚马逊ECR注册并最终部署至SageMaker的方案，则显得过于复杂耗时。该方案涉及大量额外步骤与管理环节，将显著降低操作效率。\n\n亚马逊SageMaker神经主题模型（NTM）在复杂度与资源消耗上均高于k均值算法。其采用的神经网络结构需要更强的算力支持与更长的训练时间，并非本任务的最优解。而SageMaker Blazing Text模型主要适用于带预定义标签的文本分类场景，与当前无预定义类别的需求不相匹配。\n\nk均值算法以其简洁性和对无监督聚类任务的高度适应性，在未标注文档分类场景中展现出卓越的操作效率，使之从其他干扰选项中脱颖而出成为最佳选择。"}, "answer": "B"}, {"id": "219", "question": {"enus": "A sports analytics company is providing services at a marathon. Each runner in the marathon will have their race ID printed as text on the front of their shirt. The company needs to extract race IDs from images of the runners. Which solution will meet these requirements with the LEAST operational overhead? ", "zhcn": "一家体育数据分析公司正在为一场马拉松赛事提供服务。每位参赛者的胸前都印有以文字显示的赛号。该公司需要从跑者的图像中提取这些赛号。哪种解决方案能够以最小的运维成本满足这一需求？"}, "option": [{"option_text": {"zhcn": "请使用亚马逊 Rekognition 服务。", "enus": "Use Amazon Rekognition."}, "option_flag": false}, {"option_text": {"zhcn": "采用定制化的卷积神经网络（CNN）架构。", "enus": "Use a custom convolutional neural network (CNN)."}, "option_flag": false}, {"option_text": {"zhcn": "请使用 Amazon SageMaker 目标检测算法。", "enus": "Use the Amazon SageMaker Object Detection algorithm."}, "option_flag": true}, {"option_text": {"zhcn": "请使用 Amazon Lookout for Vision。", "enus": "Use Amazon Lookout for Vision."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question “A sports analytics company is providing services at a marathon. Each runner in the marathon will have their race ID printed as text on the front of their shirt. The company needs to extract race IDs from images of the runners. Which solution will meet these requirements with the LEAST operational overhead?” is “Use the Amazon SageMaker Object Detection algorithm.” \n\nAmazon SageMaker's Object Detection algorithm is pre - built and optimized for detecting objects in images. It can be easily configured and deployed, reducing the need for in - depth development and maintenance. This means less operational overhead for the company.\n\n“Use Amazon Rekognition” is mainly focused on facial analysis, scene detection, and general image understanding rather than specifically detecting text - based race IDs, so it's not the most suitable option. “Use a custom convolutional neural network (CNN)” requires significant expertise in deep learning, time for model development, training, and maintenance, leading to high operational overhead. “Use Amazon Lookout for Vision” is designed for anomaly detection in images, not for extracting text - based race IDs, so it's also not appropriate. \n\nThe pre - built and ready - to - use nature of the Amazon SageMaker Object Detection algorithm is the key factor that makes it the real answer, distinguishing it from the fake options which either have a different focus or higher development and maintenance requirements.", "zhcn": "关于“某体育数据分析公司为马拉松赛事提供服务，需从参赛者胸前印制的赛事ID文字中提取信息，哪种方案能以最低运营成本满足需求？”的正确解答是“采用亚马逊SageMaker目标检测算法”。该算法作为预置的成熟解决方案，专为图像目标检测进行优化，可快速配置部署，无需深入研发与复杂维护，能有效控制企业运营成本。\n\n“采用亚马逊Rekognition服务”主要侧重于面部识别、场景分析与通用图像理解，并非专门针对文字类赛事ID的提取，故适用性不足。“采用定制卷积神经网络”需深度学习领域的专业能力，且涉及模型开发、训练及维护周期，将导致较高的运营负担。“采用亚马逊Lookout for Vision”专用于图像异常检测，与文本ID提取需求不符。亚马逊SageMaker目标检测算法即开即用的特性，使其在专注方向与运维成本方面显著优于其他选项，因而成为最优解。"}, "answer": "C"}, {"id": "220", "question": {"enus": "A manufacturing company wants to monitor its devices for anomalous behavior. A data scientist has trained an Amazon SageMaker scikit- learn model that classifies a device as normal or anomalous based on its 4-day telemetry. The 4-day telemetry of each device is collected in a separate file and is placed in an Amazon S3 bucket once every hour. The total time to run the model across the telemetry for all devices is 5 minutes. What is the MOST cost-effective solution for the company to use to run the model across the telemetry for all the devices? ", "zhcn": "一家制造企业希望监测其设备是否存在异常运行状态。数据科学家已基于亚马逊SageMaker平台训练出scikit-learn模型，该模型可根据设备连续四天的遥测数据将其判定为正常运行或出现异常。每台设备的四日遥测数据均独立存储于文件中，并以每小时一次的频率上传至亚马逊S3存储桶。若要对所有设备的遥测数据执行模型分析，总耗时约为五分钟。请问采用何种解决方案，能帮助企业以最具成本效益的方式完成全量设备的模型检测？"}, "option": [{"option_text": {"zhcn": "亚马逊 SageMaker 批量转换", "enus": "SageMaker Batch Transform"}, "option_flag": false}, {"option_text": {"zhcn": "亚马逊 SageMaker 异步推理服务", "enus": "SageMaker Asynchronous Inference"}, "option_flag": false}, {"option_text": {"zhcn": "**SageMaker 数据处理服务**", "enus": "SageMaker Processing"}, "option_flag": true}, {"option_text": {"zhcn": "SageMaker 多容器终端节点", "enus": "A SageMaker multi-container endpoint"}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is ‘SageMaker Processing’. This is a cost - effective solution because it is designed for data processing tasks like running a model on a set of data files. In this case, the telemetry data of each device is collected in separate files in an S3 bucket, and SageMaker Processing can efficiently process these files in a batch. It only uses resources during the processing time (5 minutes in this case), and then shuts down, which helps control costs.\n\n‘SageMaker Batch Transform’ is more suitable for large - scale batch predictions where the data is pre - grouped and requires a more dedicated setup. It may not be as cost - effective for this hourly, relatively quick processing task.\n\n‘SageMaker Asynchronous Inference’ is mainly for handling large payloads and long - running inference requests asynchronously. Since the total processing time here is only 5 minutes, this option is overkill and likely more expensive.\n\n‘A SageMaker multi - container endpoint’ is used when multiple containers need to work together in an endpoint for serving real - time predictions. It is not appropriate for this batch - based processing of device telemetry, and maintaining an endpoint incurs continuous costs, making it less cost - effective.\n\nThe key factor distinguishing SageMaker Processing as the real answer is its ability to handle batch processing efficiently while keeping costs low by only using resources during the actual processing time. The fake options are either over - engineered for this simple hourly batch processing task or incur unnecessary continuous costs.", "zhcn": "针对该问题，正确答案应为\"SageMaker Processing\"。这一方案具备显著的成本效益优势，因其专为数据处理任务设计，例如对系列数据文件运行模型处理。在本场景中，各设备的遥测数据分别存储在S3存储桶的独立文件内，SageMaker Processing可高效实现批量处理。该服务仅在运行期间（本案中为5分钟）调用计算资源，任务完成后自动释放资源，这种机制有效实现了成本控制。\n\n相比之下，\"SageMaker Batch Transform\"更适用于数据已预分组且需专用配置的大规模批量预测场景。对于本案中每小时执行的快速处理任务，该方案反而难以体现成本优势。\n\n\"SageMaker Asynchronous Inference\"主要面向大载荷量、长耗时的异步推理请求。鉴于本案总处理时长仅5分钟，选择该方案无异于大材小用，且可能产生更高费用。\n\n\"SageMaker多容器端点\"适用于需要多个容器协同提供实时预测服务的场景。本案涉及的设备遥测数据属于批处理性质，且维护端点会产生持续成本，故该方案在成本效益方面不具优势。\n\n选择SageMaker Processing作为正确答案的关键在于：它既能高效处理批量任务，又通过按需使用资源的机制有效控制成本。而其他干扰选项要么对此类简单的周期性批处理任务显得过度复杂，要么会产生不必要的持续性开支。"}, "answer": "C"}, {"id": "221", "question": {"enus": "A company wants to segment a large group of customers into subgroups based on shared characteristics. The company’s data scientist is planning to use the Amazon SageMaker built-in k-means clustering algorithm for this task. The data scientist needs to determine the optimal number of subgroups (k) to use. Which data visualization approach will MOST accurately determine the optimal value of k? ", "zhcn": "某企业希望根据共同特征将大规模客户群体划分为不同子群。为此，该公司数据科学家计划采用Amazon SageMaker内置的k-means聚类算法。此时需要确定最优的子群数量（k值）。下列哪种数据可视化方法能最精准地确定k值的最优解？"}, "option": [{"option_text": {"zhcn": "计算主成分分析（PCA）的各主要成分。仅使用前两个主成分，针对不同的k值运行k均值聚类算法。针对每个k值生成散点图，并以不同颜色区分各聚类群组。当聚类结果呈现出明显分离态势时，对应的k值即为最优解。", "enus": "Calculate the principal component analysis (PCA) components. Run the k-means clustering algorithm for a range of k by using only the  first two PCA components. For each value of k, create a scatter plot with a different color for each cluster. The optimal value of k is the  value where the clusters start to look reasonably separated."}, "option_flag": true}, {"option_text": {"zhcn": "计算主成分分析（PCA）的各主成分分量。绘制成分数量与解释方差的折线图，当曲线开始呈线性下降趋势时，对应的主成分数量即为最优k值。", "enus": "Calculate the principal component analysis (PCA) components. Create a line plot of the number of components against the explained  variance. The optimal value of k is the number of PCA components after which the curve starts decreasing in a linear fashion."}, "option_flag": false}, {"option_text": {"zhcn": "为一系列困惑度数值生成t分布随机邻域嵌入图。当聚类结果开始呈现明显分离态势时，对应的困惑度k值即为最优解。", "enus": "Create a t-distributed stochastic neighbor embedding (t-SNE) plot for a range of perplexity values. The optimal value of k is the value of  perplexity, where the clusters start to look reasonably separated."}, "option_flag": false}, {"option_text": {"zhcn": "针对不同的k值运行k-means聚类算法，并分别计算每个k值对应的误差平方和（SSE）。绘制SSE随k值变化的折线图，当曲线结束快速下降阶段、开始呈现平缓下降趋势时，对应的k值即为最优解。", "enus": "Run the k-means clustering algorithm for a range of k. For each value of k, calculate the sum of squared errors (SSE). Plot a line chart of  the SSE for each value of k. The optimal value of k is the point after which the curve starts decreasing in a linear fashion."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question “A company wants to segment a large group of customers into subgroups based on shared characteristics... Which data visualization approach will MOST accurately determine the optimal value of k?” is “Calculate the principal component analysis (PCA) components. Run the k - means clustering algorithm for a range of k by using only the first two PCA components. For each value of k, create a scatter plot with a different color for each cluster. The optimal value of k is the value where the clusters start to look reasonably separated.” This is because k - means is a clustering algorithm, and directly visualizing the clusters in a scatter plot using the first two PCA components allows for a clear assessment of how well the data points are grouped into distinct clusters. A good clustering result has well - separated clusters, and this visualization method enables the data scientist to visually identify the optimal k based on cluster separation.\n\nThe first fake option “Calculate the principal component analysis (PCA) components. Create a line plot of the number of components against the explained variance. The optimal value of k is the number of PCA components after which the curve starts decreasing in a linear fashion” is incorrect because this approach is used to determine the optimal number of PCA components for dimensionality reduction, not the optimal number of clusters (k) for k - means clustering.\n\nThe second fake option “Create a t - distributed stochastic neighbor embedding (t - SNE) plot for a range of perplexity values. The optimal value of k is the value of perplexity, where the clusters start to look reasonably separated” is wrong as t - SNE is mainly used for visualizing high - dimensional data in a low - dimensional space, and perplexity is a parameter related to t - SNE, not the number of clusters in k - means clustering.\n\nThe third fake option “Run the k - means clustering algorithm for a range of k. For each value of k, calculate the sum of squared errors (SSE). Plot a line chart of the SSE for each value of k. The optimal value of k is the point after which the curve starts decreasing in a linear fashion” is a common method (elbow method), but it is less accurate than direct visualization of clusters. The elbow point can sometimes be ambiguous, and visual inspection of cluster separation provides a more intuitive and accurate way to determine the optimal k for k - means clustering.", "zhcn": "针对问题“某公司希望根据共享特征将大规模客户群体划分为不同子群……哪种数据可视化方法能最精确地确定k的最优值？”的正确答案是：**“计算主成分分析（PCA）成分，仅使用前两个主成分对k值进行区间范围内的k均值聚类算法运算。针对每个k值生成散点图，并以不同颜色区分各聚类群。当聚类群呈现明显分离状态时，对应的k值即为最优解。”** 这是因为k均值聚类算法需通过前两个主成分的散点图直观评估数据点分组效果，良好的聚类结果应具有清晰可辨的群组边界，该方法能让数据科学家基于视觉分离度精准判定最优k值。\n\n首项干扰项“计算主成分分析（PCA）成分，绘制主成分数量与解释方差的折线图，当曲线开始呈线性下降趋势时的主成分数量即为最优k值”是错误的——该方法适用于确定降维所需的主成分数量，而非k均值聚类的群组数量。\n\n第二项干扰项“针对不同困惑度值生成t-SNE图，当聚类群呈现合理分离状态时的困惑度值即为最优k值”存在谬误：t-SNE主要用于高维数据可视化，其困惑度参数与k均值聚类的群组数量判定无关。\n\n第三项干扰项“对k值进行区间范围内的k均值聚类运算，计算每个k值的误差平方和（SSE），绘制SSE随k值变化的折线图，当曲线开始线性下降时对应的k值即为最优解”虽是常用方法（肘部法则），但相较于直接观察聚类分离度，其精确度不足。肘部拐点可能存在主观判断偏差，而视觉化判读能更直观准确地确定k均值聚类的最优k值。"}, "answer": "A"}, {"id": "222", "question": {"enus": "A data scientist at a financial services company used Amazon SageMaker to train and deploy a model that predicts loan defaults. The model analyzes new loan applications and predicts the risk of loan default. To train the model, the data scientist manually extracted loan data from a database. The data scientist performed the model training and deployment steps in a Jupyter notebook that is hosted on SageMaker Studio notebooks. The model's prediction accuracy is decreasing over time. Which combination of steps is the MOST operationally eficient way for the data scientist to maintain the model's accuracy? (Choose two.) ", "zhcn": "某金融服务公司的数据科学家利用Amazon SageMaker训练并部署了一套贷款违约预测模型。该模型通过分析新增贷款申请来预判违约风险。在模型训练阶段，这位数据科学家曾手动从数据库提取贷款数据，并在SageMaker Studio notebooks托管的Jupyter笔记本中完成了模型训练与部署操作。目前该模型的预测准确率正随时间推移逐渐下降。请问下列哪两项措施组合最能帮助数据科学家以最高运维效率维持模型准确率？（请选择两项）"}, "option": [{"option_text": {"zhcn": "运用SageMaker Pipelines构建自动化工作流，实现数据动态提取、模型训练及新版模型的无缝部署。", "enus": "Use SageMaker Pipelines to create an automated workfiow that extracts fresh data, trains the model, and deploys a new version of the  model."}, "option_flag": false}, {"option_text": {"zhcn": "配置SageMaker模型监控器时需设定精度阈值以检测模型漂移。当数据超出阈值范围时，将触发Amazon CloudWatch告警。通过将SageMaker Pipelines工作流与CloudWatch告警关联，即可在监测到异常时自动启动模型重训练流程。", "enus": "Configure SageMaker Model Monitor with an accuracy threshold to check for model drift. Initiate an Amazon CloudWatch alarm when  the threshold is exceeded. Connect the workfiow in SageMaker Pipelines with the CloudWatch alarm to automatically initiate retraining."}, "option_flag": true}, {"option_text": {"zhcn": "将模型预测结果存储于Amazon S3。创建每日运行的SageMaker处理作业，该作业从Amazon S3读取预测数据，检测模型预测准确率的变化，并在发现显著波动时发送邮件通知。", "enus": "Store the model predictions in Amazon S3. Create a daily SageMaker Processing job that reads the predictions from Amazon S3, checks  for changes in model prediction accuracy, and sends an email notification if a significant change is detected."}, "option_flag": true}, {"option_text": {"zhcn": "请在SageMaker Studio notebooks托管的Jupyter笔记本中重新运行相关步骤，以重新训练模型并部署新版模型。", "enus": "Rerun the steps in the Jupyter notebook that is hosted on SageMaker Studio notebooks to retrain the model and redeploy a new version  of the model."}, "option_flag": false}, {"option_text": {"zhcn": "将训练与部署代码从SageMaker Studio笔记本导出为Python脚本，并将其封装为亚马逊弹性容器服务（Amazon ECS）任务，以便通过AWS Lambda函数触发执行。", "enus": "Export the training and deployment code from the SageMaker Studio notebooks into a Python script. Package the script into an Amazon  Elastic Container Service (Amazon ECS) task that an AWS Lambda function can initiate."}, "option_flag": false}], "analysis": {"enus": "The correct answers to maintain the model's accuracy are: \"Configure SageMaker Model Monitor with an accuracy threshold to check for model drift. Initiate an Amazon CloudWatch alarm when the threshold is exceeded. Connect the workflow in SageMaker Pipelines with the CloudWatch alarm to automatically initiate retraining.\" and \"Store the model predictions in Amazon S3. Create a daily SageMaker Processing job that reads the predictions from Amazon S3, checks for changes in model prediction accuracy, and sends an email notification if a significant change is detected.\"\n\nThe first real answer leverages SageMaker Model Monitor to detect model drift, which is crucial as it can cause the decreasing accuracy. CloudWatch alarms can trigger automatic retraining through SageMaker Pipelines, providing an automated and efficient way to keep the model accurate. The second real answer stores predictions in S3 and uses a daily SageMaker Processing job to monitor accuracy and send notifications, enabling timely action when accuracy changes.\n\nThe fake answer \"Use SageMaker Pipelines to create an automated workflow that extracts fresh data, trains the model, and deploys a new version of the model.\" lacks a mechanism to detect when retraining is actually needed, leading to potentially unnecessary retraining. \"Rerun the steps in the Jupyter notebook that is hosted on SageMaker Studio notebooks to retrain the model and redeploy a new version of the model.\" is a manual process, which is not operationally efficient as it requires constant human intervention. \"Export the training and deployment code from the SageMaker Studio notebooks into a Python script. Package the script into an Amazon Elastic Container Service (Amazon ECS) task that an AWS Lambda function can initiate.\" is overly complex and does not directly address the issue of monitoring and maintaining model accuracy in an efficient way.\n\nCommon misconceptions might include thinking that creating an automated pipeline without monitoring for drift is sufficient, or that manual retraining is a viable long - term solution. These approaches do not account for the dynamic nature of the data and the need for proactive accuracy maintenance.", "zhcn": "为确保模型精度所采取的正确措施包括：  \n\n第一项方案：配置SageMaker Model Monitor并设定精度阈值以监测模型漂移。当数据超出阈值时触发Amazon CloudWatch警报，通过SageMaker Pipelines工作流与CloudWatch警报的联动，自动启动模型重训练流程。  \n\n第二项方案：将模型预测结果存储于Amazon S3中，创建每日运行的SageMaker Processing作业。该作业从S3读取预测数据，检测模型精度的变化趋势，若发现显著波动则自动发送邮件通知。  \n\n第一方案的优势在于利用SageMaker Model Monitor主动捕捉可能导致精度下降的模型漂移现象，结合CloudWatch警报与SageMaker Pipelines实现全自动化重训练，形成高效精准的闭环管理。第二方案通过持续监控预测结果的变化，为及时采取应对措施提供了数据支撑。  \n\n而虚假方案如“使用SageMaker Pipelines构建自动提取新数据、训练模型并部署新版本的流程”缺乏触发重训练的必要判断机制，易导致无效的资源消耗；“通过SageMaker Studio笔记本手动运行重训练及部署代码”依赖人工干预，运维效率低下；“将SageMaker Studio中的代码封装为Amazon ECS任务并由Lambda函数触发”则架构复杂，未能直击模型精度维护的核心需求。  \n\n常见误区在于认为仅构建自动化流程而忽略漂移监测即可保障模型性能，或误将手动操作作为长期解决方案。这些思路未能适应数据动态变化的特性，亦缺乏对模型精度进行前瞻性维护的机制。"}, "answer": "BC"}, {"id": "223", "question": {"enus": "A retail company wants to create a system that can predict sales based on the price of an item. A machine learning (ML) engineer built an initial linear model that resulted in the following residual plot: Which actions should the ML engineer take to improve the accuracy of the predictions in the next phase of model building? (Choose three.) ", "zhcn": "一家零售企业计划构建一套能够根据商品价格预测销量的系统。机器学习工程师初步建立的线性模型生成了如下残差图：在模型构建的下一阶段，该工程师应采取哪三项措施来提升预测精准度？（请选择三项）"}, "option": [{"option_text": {"zhcn": "将数据进行均匀降采样，以减少数据量。", "enus": "Downsample the data uniformly to reduce the amount of data."}, "option_flag": true}, {"option_text": {"zhcn": "为数据的不同部分建立两种不同的模型。", "enus": "Create two different models for different sections of the data."}, "option_flag": false}, {"option_text": {"zhcn": "在价格低于50的数据区间内进行降采样处理。", "enus": "Downsample the data in sections where Price < 50."}, "option_flag": true}, {"option_text": {"zhcn": "当价格高于50时，将输入数据偏移一个固定值。", "enus": "Offset the input data by a constant value where Price > 50."}, "option_flag": false}, {"option_text": {"zhcn": "在审视输入数据时，若遇合适情形，应采用非线性转换方法加以处理。", "enus": "Examine the input data, and apply non-linear data transformations where appropriate."}, "option_flag": true}, {"option_text": {"zhcn": "采用非线性模型替代线性模型。", "enus": "Use a non-linear model instead of a linear model."}, "option_flag": false}], "analysis": {"enus": "The question asks about actions to improve the accuracy of sales - prediction based on an initial linear model's residual plot.\n\nThe real answer options are valid for the following reasons. Downsampling the data uniformly can reduce noise and computational load, potentially improving the model's generalization. Downsampling the data in sections where Price < 50 can address over - representation in that price range, leading to a more balanced model. Examining the input data and applying non - linear data transformations can capture non - linear relationships in the data that the initial linear model might have missed.\n\nThe fake answer options are incorrect. Creating two different models for different sections of the data can be overly complex and may not necessarily improve accuracy. There is no clear indication that the data sections require separate models. Offsetting the input data by a constant value where Price > 50 is an arbitrary adjustment that doesn't address the underlying relationship between price and sales. Using a non - linear model instead of a linear model is a broad approach; the question focuses on actions in the next phase of model building, and the other three real options are more specific and actionable steps.\n\nCommon misconceptions might lead one to choose the fake options. For example, thinking that creating separate models is always better for different data sections without considering the complexity and potential for overfitting. Or believing that a simple offset can correct the model's inaccuracies instead of addressing the data's nature. These wrong assumptions can cause one to pick the fake answer options over the real ones.", "zhcn": "题目探讨如何基于初始线性模型的残差图提升销售预测的准确性。正确答案选项的合理性如下：对数据进行均匀降采样可降低噪声与计算负荷，从而提升模型的泛化能力；针对价格低于50区间的数据分段降采样，能改善该价格区间的过度代表现象，使模型更均衡；检查输入数据并应用非线性转换，可捕捉初始线性模型可能忽略的复杂关系。\n\n错误选项的不可取之处在于：为不同数据段建立独立模型会过度增加复杂度，且未必提升精度，目前并无明确依据支持分段建模的必要性；对价格高于50的数据进行常量偏移属于随意调整，未能触及价格与销售间的本质关联；直接改用非线性模型虽属可行思路，但本题关注模型构建后续阶段的具体措施，其余三个正确选项提供了更明确的操作路径。\n\n常见误解可能导致选择错误选项。例如误认为分区建模必然优于统一模型，却未考虑过拟合风险；或认为简单偏移就能修正模型偏差，而非从数据本质入手。这些错误认知会使人偏离正确答案。"}, "answer": "ACE"}, {"id": "224", "question": {"enus": "A data scientist at a food production company wants to use an Amazon SageMaker built-in model to classify different vegetables. The current dataset has many features. The company wants to save on memory costs when the data scientist trains and deploys the model. The company also wants to be able to find similar data points for each test data point. Which algorithm will meet these requirements? ", "zhcn": "某食品生产企业的一位数据科学家计划采用亚马逊SageMaker平台的预置模型，以实现对不同蔬菜的精准分类。现有数据集特征维度丰富，而企业希望在模型训练与部署阶段降低内存消耗，同时要求能够针对每个测试数据点快速定位相似样本。何种算法可同时满足这些需求？"}, "option": [{"option_text": {"zhcn": "降维处理的K近邻算法（k-NN）", "enus": "K-nearest neighbors (k-NN) with dimension reduction"}, "option_flag": false}, {"option_text": {"zhcn": "采用早停策略的线性学习器", "enus": "Linear learner with early stopping"}, "option_flag": true}, {"option_text": {"zhcn": "K均值算法", "enus": "K-means"}, "option_flag": false}, {"option_text": {"zhcn": "采用随机算法模式的主成分分析（PCA）", "enus": "Principal component analysis (PCA) with the algorithm mode set to random"}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is ‘Linear learner with early stopping’. \n\nFor memory - cost savings, the linear learner algorithm is efficient as it has a relatively low memory footprint during both training and deployment. Early stopping further helps in reducing unnecessary computations and resource usage, thus saving on memory costs. Also, the linear learner can be used to find similar data points for each test data point by looking at the decision boundaries and the proximity of data points to those boundaries.\n\n‘K - nearest neighbors (k - NN) with dimension reduction’ is not a good choice. Although dimension reduction can help with memory, k - NN stores the entire training dataset, which can be memory - intensive. It also has high computational complexity during prediction, which may not be ideal for saving costs.\n\n‘K - means’ is a clustering algorithm. Its main purpose is to group data points into clusters, not to classify different vegetables. It doesn't focus on saving memory during training and deployment for classification tasks and doesn't directly address finding similar data points for test data in the context of classification.\n\n‘Principal component analysis (PCA) with the algorithm mode set to random’ is a dimensionality reduction technique. PCA is not a classification algorithm, so it cannot classify different vegetables as required by the question.\n\nThe ability to classify vegetables, save on memory costs during training and deployment, and find similar data points for test data makes ‘Linear learner with early stopping’ the real answer option, distinguishing it from the fake options.", "zhcn": "对于题目要求，正确答案应为\"采用早停机制的线性学习器\"。该算法在内存节省方面表现优异——训练与部署时内存占用较低，具备显著的成本效益。早停机制则能有效规避冗余计算，进一步优化资源利用率，从而降低内存开销。此外，通过观察决策边界及数据点与边界的邻近程度，线性学习器可为每个测试数据点精准定位相似数据。\n\n相比之下，\"结合降维技术的K近邻算法\"并非佳选。尽管降维可缓解内存压力，但K近邻需存储完整训练数据集，内存负担较重。其预测阶段的高计算复杂度也难以满足成本控制需求。\n\n\"K均值聚类\"作为分组算法，本质在于数据分簇而非蔬菜分类。该算法未针对分类任务的训练部署阶段进行内存优化，亦不直接支持测试数据相似点检索功能。\n\n\"采用随机模式的PCA主成分分析\"属于特征降维方法。由于不具备分类算法特性，无法实现题目所需的蔬菜分类目标。\n\n综上所述，兼具蔬菜分类能力、训练部署阶段的内存成本优势以及测试数据相似点检索功能，\"采用早停机制的线性学习器\"成为符合所有要求的真实选项，与其他干扰项形成鲜明对比。"}, "answer": "B"}, {"id": "225", "question": {"enus": "A data scientist is training a large PyTorch model by using Amazon SageMaker. It takes 10 hours on average to train the model on GPU instances. The data scientist suspects that training is not converging and that resource utilization is not optimal. What should the data scientist do to identify and address training issues with the LEAST development effort? ", "zhcn": "一位数据科学家正在使用亚马逊SageMaker训练大型PyTorch模型。在GPU实例上完成模型训练平均需耗时十小时。该数据科学家怀疑训练过程未达到收敛状态，且资源利用率未臻最优。若要以最小的开发投入识别并解决训练问题，该数据科学家应采取何种措施？"}, "option": [{"option_text": {"zhcn": "利用亚马逊云监控（Amazon CloudWatch）中采集的CPU使用率指标，配置云监控警报机制，在检测到CPU使用率持续偏低时提前终止训练任务。", "enus": "Use CPU utilization metrics that are captured in Amazon CloudWatch. Configure a CloudWatch alarm to stop the training job early if low  CPU utilization occurs."}, "option_flag": false}, {"option_text": {"zhcn": "运用高分辨率定制指标集，这些指标由亚马逊云监控服务捕获。配置一个AWS Lambda函数，用于实时分析指标数据，并在检测到异常时提前终止训练任务。", "enus": "Use high-resolution custom metrics that are captured in Amazon CloudWatch. Configure an AWS Lambda function to analyze the  metrics and to stop the training job early if issues are detected."}, "option_flag": false}, {"option_text": {"zhcn": "利用SageMaker Debugger内置的梯度消失与GPU低利用率检测规则，在发现异常时可自动触发训练任务终止操作。", "enus": "Use the SageMaker Debugger vanishing_gradient and LowGPUUtilization built-in rules to detect issues and to launch the  StopTrainingJob action if issues are detected."}, "option_flag": false}, {"option_text": {"zhcn": "请使用 SageMaker Debugger 内置的混淆度与特征重要性过载规则进行问题检测，一旦发现异常即触发停止训练任务操作。", "enus": "Use the SageMaker Debugger confusion and feature_importance_overweight built-in rules to detect issues and to launch the  StopTrainingJob action if issues are detected."}, "option_flag": true}], "analysis": {"enus": "The correct answer to the question is using the SageMaker Debugger confusion and feature_importance_overweight built - in rules to detect issues and to launch the StopTrainingJob action if issues are detected. SageMaker Debugger is specifically designed to monitor and detect issues during model training with minimal development effort. It has built - in rules that can quickly identify common problems in the training process.\n\nThe option of using CPU utilization metrics in Amazon CloudWatch is not appropriate because the model is being trained on GPU instances, so CPU utilization is not the key metric to address the suspected non - convergence and sub - optimal resource utilization on GPUs.\n\nUsing high - resolution custom metrics in Amazon CloudWatch and an AWS Lambda function requires significant development effort to set up the custom metrics and the Lambda function for analysis, which goes against the requirement of least development effort.\n\nThe vanishing_gradient and LowGPUUtilization rules in SageMaker Debugger do not directly relate to the data scientist's suspicion of non - convergence and sub - optimal resource utilization as well as the confusion and feature_importance_overweight rules do. These rules are more focused on specific technical problems rather than the overall training non - convergence and resource utilization issues described in the question.\n\nIn conclusion, the real answer option is the most suitable as it leverages the built - in capabilities of SageMaker Debugger to quickly identify and address the training issues with the least development effort.", "zhcn": "针对该问题的正确答案是：利用SageMaker Debugger内置的混淆度（confusion）与特征重要性过载（feature_importance_overweight）规则来检测训练异常，并在发现问题时触发StopTrainingJob操作。SageMaker Debugger专为以最小开发成本监控模型训练过程中的异常而设计，其内置规则可快速识别训练流程中的常见问题。\n\n若采用Amazon CloudWatch中的CPU利用率指标并不合适，因为模型训练基于GPU实例，CPU利用率并非判断GPU资源利用不足及模型不收敛的关键指标。而通过Amazon CloudWatch高精度自定义指标配合AWS Lambda函数的方案，需投入大量开发工作配置指标与函数分析逻辑，违背了最小开发成本的要求。\n\nSageMaker Debugger中的梯度消失（vanishing_gradient）与低GPU利用率（LowGPUUtilization）规则与数据科学家关注的模型不收敛及GPU资源利用不足问题关联性较弱，其侧重点更偏向特定技术场景，不如混淆度与特征重要性过载规则能直接对应题目描述的整体训练异常。\n\n综上，正确答案通过调用SageMaker Debugger内置能力，以最轻量的开发成本实现了对训练问题的快速识别与干预。"}, "answer": "D"}, {"id": "226", "question": {"enus": "A bank wants to launch a low-rate credit promotion campaign. The bank must identify which customers to target with the promotion and wants to make sure that each customer's full credit history is considered when an approval or denial decision is made. The bank's data science team used the XGBoost algorithm to train a classification model based on account transaction features. The data science team deployed the model by using the Amazon SageMaker model hosting service. The accuracy of the model is suficient, but the data science team wants to be able to explain why the model denies the promotion to some customers. What should the data science team do to meet this requirement in the MOST operationally eficient manner? ", "zhcn": "一家银行计划推出低利率信用卡推广活动，需要精准筛选目标客群，并在审批过程中全面考量每位客户的信用记录。该银行的数据科学团队基于账户交易特征，运用XGBoost算法训练了分类模型，并通过Amazon SageMaker模型托管服务完成部署。虽然模型准确度已达要求，但团队仍需向业务部门解释模型拒绝部分客户申请的具体依据。请问数据科学团队应采取何种最高效的运营方案来满足这一需求？"}, "option": [{"option_text": {"zhcn": "创建一个SageMaker笔记本实例，将模型文件上传至该笔记本。运用Python XGBoost接口中的plot_importance()方法，为个体预测生成特征重要性图表。", "enus": "Create a SageMaker notebook instance. Upload the model artifact to the notebook. Use the plot_importance() method in the Python  XGBoost interface to create a feature importance chart for the individual predictions."}, "option_flag": false}, {"option_text": {"zhcn": "使用SageMaker Debugger重新训练模型，并配置该调试器以计算并收集沙普利值。通过绘制特征与SHAP（沙普利加和解释）值关系图，直观展示各特征对模型预测结果的影响机制。", "enus": "Retrain the model by using SageMaker Debugger. Configure Debugger to calculate and collect Shapley values. Create a chart that  shows features and SHapley. Additive explanations (SHAP) values to explain how the features affect the model outcomes."}, "option_flag": false}, {"option_text": {"zhcn": "配置并启动一项基于SageMaker Clarify的可解释性分析任务，以训练数据为基准对个体客户数据展开解析。生成特征与SHAP值（沙普利加和解释）关联图表，清晰呈现各特征对模型输出结果的影响机制。", "enus": "Set up and run an explainability job powered by SageMaker Clarify to analyze the individual customer data, using the training data as a  baseline. Create a chart that shows features and SHapley Additive explanations (SHAP) values to explain how the features affect the  model outcomes."}, "option_flag": false}, {"option_text": {"zhcn": "利用SageMaker模型监控功能生成沙普利值，以解析模型行为逻辑。将生成的沙普利值存储至Amazon S3服务中，并绘制特征与SHAP（沙普利加和解释）值的关系图表，清晰呈现各特征对模型决策结果的影响机制。", "enus": "Use SageMaker Model Monitor to create Shapley values that help explain model behavior. Store the Shapley values in Amazon S3.  Create a chart that shows features and SHapley Additive explanations (SHAP) values to explain how the features affect the model  outcomes."}, "option_flag": true}], "analysis": {"enus": "The correct answer to the question is to “Use SageMaker Model Monitor to create Shapley values that help explain model behavior. Store the Shapley values in Amazon S3. Create a chart that shows features and SHapley Additive explanations (SHAP) values to explain how the features affect the model outcomes.” This approach is the most operationally efficient because SageMaker Model Monitor is already integrated with the SageMaker model hosting service where the model is deployed. It can directly generate Shapley values without the need for additional complex setup.\n\nThe option of creating a SageMaker notebook instance and using the `plot_importance()` method in the Python XGBoost interface only provides general feature importance, not individual prediction explanations as required to understand why the model denies promotion to specific customers.\n\nRetraining the model with SageMaker Debugger to calculate Shapley values is an over - the - top solution. It requires retraining the model, which is time - consuming and resource - intensive when the model's accuracy is already sufficient.\n\nSetting up and running an explainability job with SageMaker Clarify is a more involved process. It needs to analyze individual customer data with training data as a baseline, which is more complex and less operationally efficient compared to using SageMaker Model Monitor. This is why the real answer is the most appropriate option for meeting the requirement efficiently.", "zhcn": "针对该问题的正确答案是\"使用SageMaker Model Monitor生成沙普利值以解释模型行为，将沙普利值存储至Amazon S3，并绘制特征与SHAP值关系图来诠释特征如何影响模型输出\"。此方案具有最佳操作效率，因为SageMaker Model Monitor已与部署模型的SageMaker模型托管服务深度集成，可直接生成沙普利值而无需额外复杂配置。\n\n若创建SageMaker笔记本实例并调用Python XGBoost接口中的`plot_importance()`方法，仅能获得通用特征重要性分析，无法满足理解模型拒绝特定客户晋升申请所需的个体预测解释要求。\n\n通过SageMaker Debugger重新训练模型以计算沙普利值则属过度解决方案。在模型精度已达标的情况下，重新训练将耗费大量时间与计算资源。\n\n采用SageMaker Clarify设置可解释性分析任务流程更为繁琐。该方法需以训练数据为基准分析个体客户数据，其复杂程度和操作效率均逊于直接使用SageMaker Model Monitor。因此，原答案才是高效满足需求的最佳选择。"}, "answer": "D"}, {"id": "227", "question": {"enus": "A company has hired a data scientist to create a loan risk model. The dataset contains loan amounts and variables such as loan type, region, and other demographic variables. The data scientist wants to use Amazon SageMaker to test bias regarding the loan amount distribution with respect to some of these categorical variables. Which pretraining bias metrics should the data scientist use to check the bias distribution? (Choose three.) ", "zhcn": "某公司聘请一位数据科学家构建贷款风险模型。数据集包含贷款金额及贷款类型、地区与其他人口统计变量。该数据科学家计划使用Amazon SageMaker检验贷款金额分布在部分分类变量上的偏差。请问其应选用哪三项预训练偏差指标来评估偏差分布？"}, "option": [{"option_text": {"zhcn": "类别失衡", "enus": "Class imbalance"}, "option_flag": true}, {"option_text": {"zhcn": "条件性人口差异", "enus": "Conditional demographic disparity"}, "option_flag": false}, {"option_text": {"zhcn": "标签比例差异", "enus": "Difference in proportions of labels"}, "option_flag": true}, {"option_text": {"zhcn": "詹森-香农散度", "enus": "Jensen-Shannon divergence"}, "option_flag": false}, {"option_text": {"zhcn": "Kullback-Leieber散度", "enus": "Kullback-Leibler divergence"}, "option_flag": false}, {"option_text": {"zhcn": "“全变差距离”", "enus": "Total variation distance"}, "option_flag": true}], "analysis": {"enus": "The correct answer to the question “A company has hired a data scientist to create a loan risk model... Which pretraining bias metrics should the data scientist use to check the bias distribution?” includes “Class imbalance”, “Difference in proportions of labels”, and “Total variation distance”. \n\n“Class imbalance” helps identify if certain classes in categorical variables are over - or under - represented in the data related to loan amounts, which can cause bias. “Difference in proportions of labels” allows the data scientist to quantify the disparity in label proportions among different groups, which is crucial for detecting bias. “Total variation distance” measures the difference between two probability distributions, useful for understanding how loan amount distributions vary across categorical variables.\n\nThe fake options “Conditional demographic disparity” is more focused on demographic conditions rather than direct loan amount distribution bias. “Jensen - Shannon divergence” and “Kullback - Leibler divergence” are mainly used for measuring the difference between probability distributions in a more theoretical or information - theoretic sense, and are not as directly applicable to checking bias in loan amount distribution with respect to categorical variables as the real answer options. These misaligned focuses are the reasons why they are not the correct choices for this specific problem.", "zhcn": "针对“某公司聘请数据科学家构建贷款风险模型...数据科学家应使用哪些预训练偏差指标来检验偏差分布？”这一问题的正确答案包括“类别不平衡”、“标签比例差异”和“总变差距离”。其中，“类别不平衡”可帮助识别贷款金额数据中分类变量的特定类别是否存在代表过度或不足的情况，这种不平衡可能导致偏差；“标签比例差异”使数据科学家能够量化不同组别间标签比例的差异，对偏差检测至关重要；而“总变差距离”通过衡量两个概率分布间的差异，有助于理解贷款金额分布如何随分类变量变化。\n\n至于干扰项，“条件人口统计差异”更侧重于人口统计条件而非直接的贷款金额分布偏差；“詹森-香农散度”与“库尔巴克-莱布勒散度”主要应用于理论或信息论层面的概率分布差异衡量，相较于正确答案选项，它们对分类变量相关的贷款金额分布偏差检验缺乏直接适用性。这些焦点偏差正是上述选项不适用于本特定问题的原因。"}, "answer": "ACF"}, {"id": "228", "question": {"enus": "A retail company wants to use Amazon Forecast to predict daily stock levels of inventory. The cost of running out of items in stock is much higher for the company than the cost of having excess inventory. The company has millions of data samples for multiple years for thousands of items. The company’s purchasing department needs to predict demand for 30-day cycles for each item to ensure that restocking occurs. A machine learning (ML) specialist wants to use item-related features such as \"category,\" \"brand,\" and \"safety stock count.\" The ML specialist also wants to use a binary time series feature that has \"promotion applied?\" as its name. Future promotion information is available only for the next 5 days. The ML specialist must choose an algorithm and an evaluation metric for a solution to produce prediction results that will maximize company profit. Which solution will meet these requirements? ", "zhcn": "一家零售企业计划采用Amazon Forecast服务来预测每日库存水平。由于缺货造成的损失远高于库存积压的成本，该公司拥有多年积累的数十亿条商品数据记录。采购部门需按30天周期预测各商品需求以安排补货计划。机器学习专家拟采用\"品类\"\"品牌\"\"安全库存量\"等商品特征，并加入以\"是否促销\"命名的二元时间序列特征——但未来促销信息仅能提前5天获取。该专家需选择能最大化企业利润的预测算法与评估指标。下列哪种方案最符合这些要求？"}, "option": [{"option_text": {"zhcn": "采用自回归积分滑动平均（ARIMA）算法训练模型，并基于0.75分位数加权损失函数（wQL）进行模型性能评估。", "enus": "Train a model by using the Autoregressive Integrated Moving Average (ARIMA) algorithm. Evaluate the model by using the Weighted  Quantile Loss (wQL) metric at 0.75 (P75)."}, "option_flag": false}, {"option_text": {"zhcn": "采用自回归积分滑动平均（ARIMA）算法对模型进行训练，并选用加权绝对百分比误差（WAPE）作为评估指标来检验模型性能。", "enus": "Train a model by using the Autoregressive Integrated Moving Average (ARIMA) algorithm. Evaluate the model by using the Weighted  Absolute Percentage Error (WAPE) metric."}, "option_flag": false}, {"option_text": {"zhcn": "采用卷积神经网络-分位数回归（CNN-QR）算法训练模型，并基于0.75分位数（P75）的加权分位数损失（wQL）指标进行模型性能评估。", "enus": "Train a model by using the Convolutional Neural Network - Quantile Regression (CNN-QR) algorithm. Evaluate the model by using the  Weighted Quantile Loss (wQL) metric at 0.75 (P75)."}, "option_flag": false}, {"option_text": {"zhcn": "采用卷积神经网络-分位数回归（CNN-QR）算法训练模型，并选用加权绝对百分比误差（WAPE）作为评估指标进行模型性能验证。", "enus": "Train a model by using the Convolutional Neural Network - Quantile Regression (CNN-QR) algorithm. Evaluate the model by using the  Weighted Absolute Percentage Error (WAPE) metric."}, "option_flag": true}], "analysis": {"enus": "The correct answer to the question is to “Train a model by using the Convolutional Neural Network - Quantile Regression (CNN - QR) algorithm. Evaluate the model by using the Weighted Absolute Percentage Error (WAPE) metric.” \n\nThe CNN - QR algorithm is well - suited for this scenario as it can handle complex patterns in the large and multi - faceted dataset the company has. It can effectively incorporate various item - related features like \"category\", \"brand\", \"safety stock count\", and the binary \"promotion applied?\" feature. \n\nThe WAPE metric is appropriate because the cost of running out of stock is much higher for the company. WAPE gives an overall view of the prediction accuracy in percentage terms, which helps in understanding how well the model is performing across all items. This is crucial for maximizing the company's profit as it allows for better decision - making in restocking.\n\nThe ARIMA algorithm is less suitable as it is more focused on time - series data and may not handle the additional item - related features as effectively as CNN - QR. \n\nUsing the Weighted Quantile Loss (wQL) metric at 0.75 (P75) is not the best choice. The P75 quantile may not align well with the company's goal of minimizing stock - outs. The company needs a metric that gives a general view of prediction accuracy rather than focusing on a specific quantile. \n\nA common misconception could be choosing ARIMA because it is a well - known time - series algorithm. However, in this case, with the need to incorporate multiple types of features, CNN - QR is a better fit. Also, using wQL at P75 might seem reasonable if one assumes that focusing on a particular quantile is always the best approach, but it doesn't address the overall prediction accuracy needed to maximize profit.", "zhcn": "针对该问题，正确答案是\"采用卷积神经网络-分位数回归（CNN-QR）算法训练模型，并选用加权绝对百分比误差（WAPE）作为评估指标\"。CNN-QR算法能够有效处理企业庞大多元数据集中的复杂模式，尤其适合本场景。该算法可灵活整合诸如\"商品品类\"\"品牌归属\"\"安全库存量\"等多项商品特征，以及\"是否开展促销\"的二值化特征。\n\n选择WAPE指标是因为缺货成本对企业影响重大，该指标通过百分比形式直观反映整体预测精度，有助于评估模型在所有商品上的综合表现。这种全局视角对优化补货决策至关重要，能有效助力企业利润最大化。\n\n相比之下，ARIMA算法更侧重于时间序列分析，难以像CNN-QR那样高效融合多维商品特征。采用0.75分位数的加权分位数损失（wQL）也非最优选择——该分位数目标与企业极力避免缺货的核心目标契合度不足，且单一分位数的聚焦无法满足追求整体预测精度的需求。\n\n一个常见误区是因ARIMA在时序预测领域的知名度而选择该算法，但本案需要整合多维度特征，CNN-QR显然更为适宜。同样，执着于特定分位数看似合理，实则未能解决实现利润最大化所需的全局预测精度问题。"}, "answer": "D"}, {"id": "229", "question": {"enus": "An online retail company wants to develop a natural language processing (NLP) model to improve customer service. A machine learning (ML) specialist is setting up distributed training of a Bidirectional Encoder Representations from Transformers (BERT) model on Amazon SageMaker. SageMaker will use eight compute instances for the distributed training. The ML specialist wants to ensure the security of the data during the distributed training. The data is stored in an Amazon S3 bucket. Which combination of steps should the ML specialist take to protect the data during the distributed training? (Choose three.) ", "zhcn": "一家网络零售公司计划开发自然语言处理模型以提升客户服务质量。一位机器学习专家正在亚马逊SageMaker平台上配置双向Transformer编码器模型的分布式训练任务。该训练将启用八个计算实例。为确保分布式训练期间的数据安全（训练数据存储于亚马逊S3存储桶中），机器学习专家应采取哪三项组合措施？（请选择三项）"}, "option": [{"option_text": {"zhcn": "在私有虚拟私有云中运行分布式训练任务，并启用容器间通信加密功能。", "enus": "Run distributed training jobs in a private VPC. Enable inter-container trafic encryption."}, "option_flag": false}, {"option_text": {"zhcn": "在多个虚拟私有云中运行分布式训练任务。启用虚拟私有云对等互联。", "enus": "Run distributed training jobs across multiple VPCs. Enable VPC peering."}, "option_flag": true}, {"option_text": {"zhcn": "创建S3 VPC终端节点，随后配置网络路由策略、终端节点策略及S3存储桶策略。", "enus": "Create an S3 VPC endpoint. Then configure network routes, endpoint policies, and S3 bucket policies."}, "option_flag": false}, {"option_text": {"zhcn": "通过使用IAM角色，授予对SageMaker资源的只读访问权限。", "enus": "Grant read-only access to SageMaker resources by using an IAM role."}, "option_flag": false}, {"option_text": {"zhcn": "创建一台NAT网关，并为该网关分配弹性IP地址。", "enus": "Create a NAT gateway. Assign an Elastic IP address for the NAT gateway."}, "option_flag": true}, {"option_text": {"zhcn": "配置入站规则，允许来自与训练实例关联的安全组的流量通过。", "enus": "Configure an inbound rule to allow trafic from a security group that is associated with the training instances."}, "option_flag": true}], "analysis": {"enus": "The question asks for steps to protect data during distributed training of a BERT model on Amazon SageMaker.\n\nThe real answer options are correct for the following reasons:\n- “Run distributed training jobs across multiple VPCs. Enable VPC peering”: This enhances security by isolating resources in different VPCs and controlling the flow of traffic between them through peering, reducing the risk of unauthorized access.\n- “Create a NAT gateway. Assign an Elastic IP address for the NAT gateway”: A NAT gateway allows instances in a private subnet to access the internet while hiding their private IP addresses, protecting them from external threats.\n- “Configure an inbound rule to allow traffic from a security group that is associated with the training instances”: This restricts incoming traffic to only that from trusted sources (the training instances), preventing unauthorized access attempts.\n\nThe fake answer options are incorrect:\n- “Run distributed training jobs in a private VPC. Enable inter - container traffic encryption”: While running in a private VPC and encrypting inter - container traffic are good security practices, the question focuses on distributed training across eight compute instances and the given option doesn't directly address the multi - instance distributed training security requirements as well as the real answer options.\n- “Create an S3 VPC endpoint. Then configure network routes, endpoint policies, and S3 bucket policies”: This is more about securing access to the S3 bucket rather than protecting the data during the distributed training process itself.\n- “Grant read - only access to SageMaker resources by using an IAM role”: This mainly controls access to SageMaker resources but doesn't specifically protect the data during the distributed training operation.\n\nCommon misconceptions might lead one to choose the fake options if they focus too much on general security practices like VPC isolation or S3 access control without considering the specific context of distributed training security requirements.", "zhcn": "问题要求说明在Amazon SageMaker上进行BERT模型分布式训练时保护数据的步骤。正确答案选项的正确性如下：  \n- **\"在多个VPC中运行分布式训练任务，启用VPC对等连接\"**：通过在不同VPC中隔离资源并通过对等连接控制流量传输，可降低未授权访问风险，从而提升安全性。  \n- **\"创建NAT网关，为NAT网关分配弹性IP地址\"**：NAT网关允许私有子网中的实例访问互联网，同时隐藏其私有IP地址，避免外部威胁。  \n- **\"配置入站规则，仅允许来自与训练实例关联的安全组的流量\"**：此措施将入站流量限制为可信来源（训练实例），有效阻止未授权访问尝试。  \n\n错误答案选项的谬误在于：  \n- **\"在私有VPC中运行分布式训练任务，启用容器间流量加密\"**：虽然私有VPC运行和容器间流量加密是良好的安全实践，但该选项未直接针对八个计算实例的分布式训练场景，其安全性保障不如正确答案选项全面。  \n- **\"创建S3 VPC终端节点，配置网络路由、终端策略及S3存储桶策略\"**：此方案主要涉及S3存储桶的访问安全，而非分布式训练过程中的数据保护。  \n- **\"通过IAM角色授予对SageMaker资源的只读访问权限\"**：该措施仅控制对SageMaker资源的访问权限，并未专门保障分布式训练操作期间的数据安全。  \n\n常见误解可能导致选择错误选项，例如过度关注VPC隔离或S3访问控制等通用安全实践，而忽视了分布式训练场景下的特定安全需求。"}, "answer": "BEF"}, {"id": "230", "question": {"enus": "An analytics company has an Amazon SageMaker hosted endpoint for an image classification model. The model is a custom-built convolutional neural network (CNN) and uses the PyTorch deep learning framework. The company wants to increase throughput and decrease latency for customers that use the model. Which solution will meet these requirements MOST cost-effectively? ", "zhcn": "一家数据分析公司为其图像分类模型部署了亚马逊SageMaker托管端点。该模型采用定制化卷积神经网络架构，基于PyTorch深度学习框架开发。为提升用户调用模型时的吞吐效率并降低响应延迟，下列哪种解决方案能以最具成本效益的方式满足这些需求？"}, "option": [{"option_text": {"zhcn": "在SageMaker托管终端节点上启用亚马逊弹性推理服务。", "enus": "Use Amazon Elastic Inference on the SageMaker hosted endpoint."}, "option_flag": false}, {"option_text": {"zhcn": "对CNN进行更深层次的训练，并采用更庞大的数据集加以优化。", "enus": "Retrain the CNN with more layers and a larger dataset."}, "option_flag": false}, {"option_text": {"zhcn": "对CNN进行再训练，增加网络层数并采用更精简的数据集。", "enus": "Retrain the CNN with more layers and a smaller dataset."}, "option_flag": true}, {"option_text": {"zhcn": "请选择配备多块GPU的SageMaker实例类型。", "enus": "Choose a SageMaker instance type that has multiple GPUs."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is “Retrain the CNN with more layers and a smaller dataset.” Adding more layers to the CNN can enhance the model's ability to learn complex patterns, potentially improving its performance. Using a smaller dataset reduces the training time and computational resources needed, which is cost - effective. \n\nThe option “Use Amazon Elastic Inference on the SageMaker hosted endpoint” adds an extra service that incurs additional costs. Although it can improve performance, it is not the most cost - effective solution. “Retraining the CNN with more layers and a larger dataset” requires more computational power and time for training, leading to higher costs. “Choosing a SageMaker instance type that has multiple GPUs” also increases costs as multi - GPU instances are more expensive. The key factor here is cost - effectiveness, and retraining with more layers and a smaller dataset balances performance improvement and cost reduction, thus distinguishing it from the other, more costly fake options.", "zhcn": "对于该问题的正确答案是\"以更多网络层和更小数据集重新训练CNN\"。为CNN增加更多层数能增强模型学习复杂模式的能力，从而可能提升其表现。采用较小数据集则可缩短训练时间并降低计算资源需求，实现成本效益。选项\"在SageMaker托管端点使用亚马逊弹性推理\"会引入额外服务并产生附加费用，虽能提升性能却非最具成本效益的方案。\"采用更多网络层和更大数据集重新训练CNN\"需要更强的计算能力和更长的训练时间，导致成本攀升。而\"选择配备多GPU的SageMaker实例类型\"也会因多GPU实例价格较高增加开支。本题关键在于成本效益考量——采用更多网络层配合较小数据集的再训练方案，在提升性能与控制成本之间取得了最佳平衡，这使其与其他成本更高的干扰选项形成鲜明对比。"}, "answer": "C"}, {"id": "231", "question": {"enus": "An ecommerce company is collecting structured data and unstructured data from its website, mobile apps, and IoT devices. The data is stored in several databases and Amazon S3 buckets. The company is implementing a scalable repository to store structured data and unstructured data. The company must implement a solution that provides a central data catalog, self-service access to the data, and granular data access policies and encryption to protect the data. Which combination of actions will meet these requirements with the LEAST amount of setup? (Choose three.) ", "zhcn": "一家电商企业正从其官方网站、移动应用及物联网设备中采集结构化与非结构化数据。这些数据目前存储于多个数据库及亚马逊S3存储桶中。该公司正在构建一个可扩展的数据存储库，用以统一存储两类数据。此方案需实现三大核心功能：建立统一数据目录、提供自助式数据查询服务、实施细粒度数据访问策略及加密保护机制。请问以下哪三项措施的组合能以最简配置满足上述需求？（请选择三项答案）"}, "option": [{"option_text": {"zhcn": "梳理数据库及S3存储桶中的现有数据，并将其接入AWS Lake Formation管理体系。", "enus": "Identify the existing data in the databases and S3 buckets. Link the data to AWS Lake Formation."}, "option_flag": true}, {"option_text": {"zhcn": "梳理数据库与S3存储桶中的现有数据，并将其关联至AWS Glue服务。", "enus": "Identify the existing data in the databases and S3 buckets. Link the data to AWS Glue."}, "option_flag": false}, {"option_text": {"zhcn": "对关联数据源运行AWS Glue爬虫程序，以构建统一的数据目录。", "enus": "Run AWS Glue crawlers on the linked data sources to create a central data catalog."}, "option_flag": false}, {"option_text": {"zhcn": "通过AWS身份与访问管理服务（IAM）实施精细化权限管控，并为每个数据源配置服务器端加密方案。", "enus": "Apply granular access policies by using AWS Identity and Access Management (1AM). Configure server-side encryption on each data  source."}, "option_flag": true}, {"option_text": {"zhcn": "借助AWS Lake Formation实施精细化的访问权限管控与数据加密机制。", "enus": "Apply granular access policies and encryption by using AWS Lake Formation."}, "option_flag": true}, {"option_text": {"zhcn": "借助AWS Glue实施精细化的访问策略与数据加密方案。", "enus": "Apply granular access policies and encryption by using AWS Glue."}, "option_flag": false}], "analysis": {"enus": "The correct answer options for implementing a scalable repository to store structured and unstructured data with a central data catalog, self - service access, and granular access policies and encryption with the least setup are as follows:\n\nFirst, “Identify the existing data in the databases and S3 buckets. Link the data to AWS Lake Formation.” This is correct because AWS Lake Formation simplifies the process of centralizing data from various sources. It can directly link to existing databases and S3 buckets, providing a unified view of the data.\n\nSecond, “Apply granular access policies by using AWS Identity and Access Management (IAM). Configure server - side encryption on each data source.” IAM is a well - established service for managing access in AWS. It allows for detailed access control, and server - side encryption on data sources helps protect the data at rest.\n\nThird, “Apply granular access policies and encryption by using AWS Lake Formation.” Lake Formation has built - in capabilities for applying fine - grained access policies and encryption, which streamlines the security implementation process.\n\nThe fake answer options are less suitable. Linking data to AWS Glue instead of Lake Formation doesn't offer the same level of centralized data governance. AWS Glue is mainly for ETL (Extract, Transform, Load) processes and creating a data catalog, but it lacks the comprehensive data management and security features that Lake Formation provides. Running AWS Glue crawlers to create a catalog is a step in the data discovery process, but it doesn't address the overall requirements of a scalable repository with access control and encryption as effectively as Lake Formation. Applying granular access policies and encryption using AWS Glue is also not ideal as Glue is not designed primarily for this purpose, and it would require more custom setup compared to using Lake Formation.\n\nCommon misconceptions might include thinking that AWS Glue can handle all aspects of data management and security as well as Lake Formation. Glue is more focused on data integration and cataloging, while Lake Formation is built for data governance, access control, and encryption, making it a better fit for this scenario with less setup required.", "zhcn": "为实现可扩展的数据存储库，用于存放结构化和非结构化数据，并具备中央数据目录、自助服务访问、精细化访问策略及加密功能，同时要求初始配置最简化，以下为正确方案：\n\n首先，\"识别数据库及S3存储桶中的现有数据，将其关联至AWS Lake Formation\"。此方案正确在于Lake Formation能简化多源数据集中化管理流程，可直接对接现有数据库与S3存储桶，实现数据统一视图。\n\n其次，\"通过AWS身份与访问管理服务（IAM）实施精细化访问策略，并为各数据源配置服务端加密\"。IAM作为成熟的AWS访问管理服务，支持细粒度权限控制，而数据源的服务端加密能有效保护静态数据。\n\n第三，\"通过AWS Lake Formation实施精细化访问策略与加密\"。Lake Formation内置细粒度访问策略与加密功能，可大幅简化安全配置流程。\n\n以下为欠佳方案：若将数据关联至AWS Glue而非Lake Formation，则无法实现同等水平的集中化数据治理。Glue主要专注于ETL（抽取、转换、加载）流程及创建数据目录，但缺乏Lake Formation所具备的全面数据管理与安全特性。运行Glue爬虫程序创建目录虽属数据发现环节，但无法像Lake Formation那样高效满足带访问控制与加密功能的可扩展存储库的整体需求。通过Glue实施精细化权限策略与加密亦非理想选择，因该服务并非专为此设计，需比Lake Formation进行更多定制化配置。\n\n常见误解在于认为AWS Glue可完全替代Lake Formation的数据管理与安全功能。实际上，Glue更侧重于数据集成与编目，而Lake Formation专为数据治理、访问控制及加密场景构建，在本方案中能以更少配置实现更优效果。"}, "answer": "ADE"}, {"id": "232", "question": {"enus": "A machine learning (ML) specialist is developing a deep learning sentiment analysis model that is based on data from movie reviews. After the ML specialist trains the model and reviews the model results on the validation set, the ML specialist discovers that the model is overfitting. Which solutions will MOST improve the model generalization and reduce overfitting? (Choose three.) ", "zhcn": "一位机器学习专家正在开发一款基于影评数据的深度学习情感分析模型。在完成模型训练并验证验证集结果后，该专家发现模型存在过拟合现象。下列哪三项措施最能有效提升模型泛化能力并抑制过拟合？（请选择三项）"}, "option": [{"option_text": {"zhcn": "以不同随机种子打乱数据集。", "enus": "Shufie the dataset with a different seed."}, "option_flag": false}, {"option_text": {"zhcn": "降低学习速率。", "enus": "Decrease the learning rate."}, "option_flag": false}, {"option_text": {"zhcn": "增加网络层数。", "enus": "Increase the number of layers in the network."}, "option_flag": true}, {"option_text": {"zhcn": "加入L1正则化与L2正则化。", "enus": "Add L1 regularization and L2 regularization."}, "option_flag": false}, {"option_text": {"zhcn": "加入随机失活层。", "enus": "Add dropout."}, "option_flag": true}, {"option_text": {"zhcn": "减少网络层数。", "enus": "Decrease the number of layers in the network."}, "option_flag": true}], "analysis": {"enus": "The question asks for solutions to improve model generalization and reduce overfitting in a deep - learning sentiment analysis model.\n\nThe real answer options are effective for the following reasons:\n- Increasing the number of layers in the network can add more complexity and capacity to learn diverse patterns, which helps the model generalize better if done appropriately.\n- Adding dropout randomly \"drops out\" some neurons during training, preventing the model from relying too much on specific neurons and thus reducing overfitting.\n- Decreasing the number of layers simplifies the model, reducing its complexity and the likelihood of overfitting on the training data.\n\nThe fake answer options are not the best choices:\n- Shuffling the dataset with a different seed only changes the order of data presentation during training and does not directly address the issue of overfitting.\n- Decreasing the learning rate mainly affects the step size of parameter updates during training and is more related to the convergence speed rather than reducing overfitting.\n- While L1 and L2 regularization can help with overfitting, the question asks for the most effective solutions, and the three real answer options are more directly targeted at the problem of overfitting in the context of a deep - learning model's architecture and training process.\n\nCommon misconceptions might lead one to choose the fake options. For example, thinking that shuffling the data can fundamentally change the model's tendency to overfit, or believing that adjusting the learning rate is a primary solution for overfitting. Also, the popularity of regularization techniques might make one wrongly assume that L1 and L2 are the most appropriate here when other architectural changes can be more impactful.", "zhcn": "问题旨在探讨如何提升深度学习情感分析模型的泛化能力并减少过拟合。以下选项之所以有效，是因为它们直指问题核心：\n\n- 增加网络层数可提升模型复杂度与学习能力，使其能捕捉更多样的特征模式，从而在适当情况下增强泛化效果。\n- 引入随机失活技术能在训练过程中暂时屏蔽部分神经元，避免模型过度依赖特定节点，有效抑制过拟合。\n- 减少网络层数可简化模型结构，降低其复杂性，进而减轻对训练数据的过度拟合。\n\n而以下选项并非最优解：\n\n- 更改数据集的随机打乱种子仅影响训练数据的呈现顺序，无法直接解决过拟合问题。\n- 降低学习率主要调节参数更新的步长，更多影响模型收敛速度而非过拟合现象。\n- 尽管L1和L2正则化有助于缓解过拟合，但本题聚焦最有效的解决方案，而前述三个选项更能从模型架构与训练过程层面直接应对过拟合。\n\n常见的认知误区可能导致选择无效选项。例如误认为数据打乱能从根本上改变模型过拟合倾向，或认为调整学习率是解决过拟合的主要手段。此外，正则化技术的广泛适用性可能使人误以为L1和L2是本场景的最佳选择，但实际上模型结构调整往往能产生更显著的效果。"}, "answer": "CEF"}, {"id": "233", "question": {"enus": "An online advertising company is developing a linear model to predict the bid price of advertisements in real time with low-latency predictions. A data scientist has trained the linear model by using many features, but the model is overfitting the training dataset. The data scientist needs to prevent overfitting and must reduce the number of features. Which solution will meet these requirements? ", "zhcn": "一家在线广告公司正在开发一种线性模型，旨在通过低延迟预测来实时预估广告竞价。数据科学家已利用大量特征训练该模型，但出现了对训练集过度拟合的问题。当前需避免过度拟合，且必须削减特征数量。下列哪种方案可同时满足这些要求？"}, "option": [{"option_text": {"zhcn": "在模型重训过程中引入L1正则化约束。", "enus": "Retrain the model with L1 regularization applied."}, "option_flag": true}, {"option_text": {"zhcn": "在模型重新训练过程中引入L2正则化方法。", "enus": "Retrain the model with L2 regularization applied."}, "option_flag": false}, {"option_text": {"zhcn": "在模型重新训练过程中引入随机失活正则化方法。", "enus": "Retrain the model with dropout regularization applied."}, "option_flag": false}, {"option_text": {"zhcn": "通过增加数据量来重新训练模型。", "enus": "Retrain the model by using more data."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is “Retrain the model with L1 regularization applied.” L1 regularization, also known as Lasso regression, adds a penalty term to the loss function that is proportional to the absolute value of the coefficients. This penalty can drive some of the coefficients to exactly zero, effectively eliminating the corresponding features from the model. This helps in reducing the number of features and preventing overfitting.\n\n“Retrain the model with L2 regularization applied.” (Ridge regression) adds a penalty proportional to the square of the coefficients. It shrinks the coefficients towards zero but rarely makes them exactly zero, so it doesn't effectively reduce the number of features.\n\n“Retrain the model with dropout regularization applied.” is mainly used in neural networks, not in linear models, so it's not applicable here.\n\n“Retrain the model by using more data.” can help reduce overfitting, but it doesn't address the requirement of reducing the number of features.\n\nThe key factor distinguishing the real answer is its ability to both reduce overfitting and the number of features, which is the specific requirement of the problem. Common misconceptions might include confusing the effects of different regularization methods or not considering the model type when choosing a regularization technique.", "zhcn": "该问题的正确答案是\"采用L1正则化重新训练模型\"。L1正则化（亦称LASSO回归）会在损失函数中增加一个与系数绝对值成正比的惩罚项。这种惩罚机制能使部分系数完全归零，从而有效剔除模型中的对应特征。这种方法既有助于削减特征数量，又能防范过拟合现象。\n\n若采用L2正则化（即岭回归），惩罚项将与系数的平方成正比。虽然它能使系数向零收缩，但通常不会使其完全归零，因此无法有效精简特征数量。\n\n而\"采用Dropout正则化重新训练模型\"主要适用于神经网络而非线性模型，故在此场景并不适用。\n\n至于\"通过增加数据量重新训练模型\"虽可缓解过拟合，但无法满足削减特征数量的特定需求。\n\n本题答案的关键判别标准在于：所选方法必须同时满足抑制过拟合和精简特征数量这两重要求。常见的理解偏差往往源于混淆不同正则化方法的效果，或未根据模型类型选择恰当的正则化技术。"}, "answer": "A"}, {"id": "234", "question": {"enus": "A credit card company wants to identify fraudulent transactions in real time. A data scientist builds a machine learning model for this purpose. The transactional data is captured and stored in Amazon S3. The historic data is already labeled with two classes: fraud (positive) and fair transactions (negative). The data scientist removes all the missing data and builds a classifier by using the XGBoost algorithm in Amazon SageMaker. The model produces the following results: • True positive rate (TPR): 0.700 • False negative rate (FNR): 0.300 • True negative rate (TNR): 0.977 • False positive rate (FPR): 0.023 • Overall accuracy: 0.949 Which solution should the data scientist use to improve the performance of the model? ", "zhcn": "一家信用卡公司希望实时识别欺诈交易。为此，一位数据科学家构建了机器学习模型。交易数据被采集并存储于Amazon S3中，历史数据已标注为两类：欺诈交易（阳性）与正常交易（阴性）。数据科学家清除了所有缺失数据，并运用Amazon SageMaker中的XGBoost算法训练出分类器。该模型产出如下结果：  \n• 真正例率：0.700  \n• 假反例率：0.300  \n• 真反例率：0.977  \n• 假正例率：0.023  \n• 整体准确率：0.949  \n数据科学家应采用何种方案来提升此模型的性能？"}, "option": [{"option_text": {"zhcn": "对训练数据集中的少数类应用合成少数类过采样技术（SMOTE），随后使用增强后的训练数据重新训练模型。", "enus": "Apply the Synthetic Minority Oversampling Technique (SMOTE) on the minority class in the training dataset. Retrain the model with the  updated training data."}, "option_flag": false}, {"option_text": {"zhcn": "对训练数据集中的多数类应用合成少数类过采样技术（SMOTE），随后使用更新后的训练数据重新训练模型。", "enus": "Apply the Synthetic Minority Oversampling Technique (SMOTE) on the majority class in the training dataset. Retrain the model with the  updated training data."}, "option_flag": false}, {"option_text": {"zhcn": "对少数类别进行欠采样处理。", "enus": "Undersample the minority class."}, "option_flag": true}, {"option_text": {"zhcn": "对多数类别进行过采样。", "enus": "Oversample the majority class."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question about improving the model's performance for identifying fraudulent transactions is ‘Undersample the minority class’. In fraud detection, the fraudulent transactions are typically the minority class. The model's false - negative rate (FNR) of 0.300 is relatively high, meaning it fails to detect a significant number of actual fraud cases.\n\nUndersampling the minority class can be a good approach when dealing with imbalanced datasets. By reducing the number of samples in the majority class (fair transactions), the model can focus more on learning the patterns of the minority class (fraudulent transactions), potentially improving the detection of fraud.\n\nThe option of applying SMOTE on the minority class might not be the best choice here. SMOTE creates synthetic samples of the minority class, but it can sometimes lead to overfitting, especially in a fraud - detection scenario where the patterns are complex. Applying SMOTE on the majority class is incorrect because the goal is to address the under - representation of the minority class, not the majority class. Oversampling the majority class would further exacerbate the class imbalance problem and make it even more difficult for the model to learn the patterns of the minority class.\n\nThe key factor distinguishing the real answer from the fake options is the need to deal with the class imbalance in a way that enhances the model's ability to detect the minority class (fraud), which is better achieved through undersampling the majority class rather than the other approaches presented.", "zhcn": "关于如何提升模型识别欺诈交易性能的问题，正确答案是“对少数类进行欠采样”。在欺诈检测场景中，欺诈交易通常属于少数类。当前模型0.300的漏报率相对较高，意味着大量实际欺诈案例未被识别。  \n\n处理类别不平衡数据时，对少数类进行欠采样是行之有效的策略。通过减少多数类（正常交易）的样本量，模型能更专注于学习少数类（欺诈交易）的特征模式，从而提升欺诈识别的精准度。  \n\n在此情境下，对少数类应用SMOTE方法并非最佳选择。虽然SMOTE能生成少数类的合成样本，但在欺诈检测这类特征复杂的场景中，反而可能导致过拟合。而对多数类应用SMOTE显然有误——解决问题的核心在于改善少数类的表征不足，若对多数类过采样反而会加剧类别失衡，使模型更难捕捉少数类特征。  \n\n甄别正确答案的关键在于：处理类别不平衡时需强化模型对少数类（欺诈）的识别能力。相较于其他选项，对多数类实施欠采样能更有效地达成这一目标。"}, "answer": "C"}, {"id": "235", "question": {"enus": "A company is training machine learning (ML) models on Amazon SageMaker by using 200 TB of data that is stored in Amazon S3 buckets. The training data consists of individual files that are each larger than 200 MB in size. The company needs a data access solution that offers the shortest processing time and the least amount of setup. Which solution will meet these requirements? ", "zhcn": "一家公司正利用存储在亚马逊S3存储桶中的200 TB数据，在Amazon SageMaker上训练机器学习模型。训练数据由独立文件构成，每个文件大小均超过200 MB。该公司需要一种能实现最短处理时间且无需复杂配置的数据访问方案。何种方案可满足这些要求？"}, "option": [{"option_text": {"zhcn": "在 SageMaker 中启用文件模式，将数据集从 S3 存储桶复制至 ML 实例的本地存储中。", "enus": "Use File mode in SageMaker to copy the dataset from the S3 buckets to the ML instance storage."}, "option_flag": false}, {"option_text": {"zhcn": "创建一套适用于Lustre的Amazon FSx文件系统，并将该文件系统与S3存储桶建立关联。", "enus": "Create an Amazon FSx for Lustre file system. Link the file system to the S3 buckets."}, "option_flag": false}, {"option_text": {"zhcn": "创建一项亚马逊弹性文件系统（Amazon EFS）服务。将该文件系统挂载至训练实例。", "enus": "Create an Amazon Elastic File System (Amazon EFS) file system. Mount the file system to the training instances."}, "option_flag": false}, {"option_text": {"zhcn": "在 SageMaker 中启用 FastFile 模式，即可按需从 S3 存储桶流式传输文件。", "enus": "Use FastFile mode in SageMaker to stream the files on demand from the S3 buckets."}, "option_flag": true}], "analysis": {"enus": "The correct answer to the question is ‘Use FastFile mode in SageMaker to stream the files on demand from the S3 buckets.’ This option meets the requirements as it offers the shortest processing time by streaming files on - demand and requires the least amount of setup since it directly works with the existing S3 buckets in SageMaker.\n\n‘Use File mode in SageMaker to copy the dataset from the S3 buckets to the ML instance storage’ has a long processing time as it involves copying 200 TB of data, which is time - consuming. \n\n‘Create an Amazon FSx for Lustre file system. Link the file system to the S3 buckets’ requires setting up an FSx for Lustre file system, which adds complexity and time to the process. \n\n‘Create an Amazon Elastic File System (Amazon EFS) file system. Mount the file system to the training instances’ also needs the additional step of creating and mounting an EFS file system, increasing the setup effort. The fast - streaming and low - setup nature of FastFile mode distinguish it as the best option, avoiding the pitfalls of long processing times and high setup requirements of the other options.", "zhcn": "该问题的正确答案是“在SageMaker中使用FastFile模式，实现S3存储桶文件的按需流式传输”。此方案完全符合要求：通过实时流式传输文件可最大程度缩短处理时间，且由于直接调用SageMaker中现有的S3存储桶，所需配置最为精简。  \n\n相比之下，“采用SageMaker的File模式将数据集从S3存储桶复制至ML实例存储”需要拷贝200TB数据，耗时冗长；而“创建Amazon FSx for Lustre文件系统并关联S3存储桶”需额外部署FSx for Lustre系统，增加了流程复杂度与时间成本；至于“创建Amazon EFS文件系统并挂载至训练实例”方案，则需完成文件系统创建和挂载等额外步骤，提升了配置工作量。  \n\nFastFile模式凭借其流式传输特性与低配置优势，有效规避了其他方案处理周期长、部署要求高的缺陷，故而成为最优选择。"}, "answer": "D"}, {"id": "236", "question": {"enus": "An online store is predicting future book sales by using a linear regression model that is based on past sales data. The data includes duration, a numerical feature that represents the number of days that a book has been listed in the online store. A data scientist performs an exploratory data analysis and discovers that the relationship between book sales and duration is skewed and non-linear. Which data transformation step should the data scientist take to improve the predictions of the model? ", "zhcn": "一家网络书店正基于历史销售数据，运用线性回归模型预测未来图书销量。该数据包含\"上架时长\"这一数值特征，即图书在书店陈列的天数。数据科学家在探索性分析中发现，图书销量与上架时长之间存在非对称的非线性关系。为提升模型预测精度，该科学家应采取何种数据转换步骤？"}, "option": [{"option_text": {"zhcn": "独热编码", "enus": "One-hot encoding"}, "option_flag": true}, {"option_text": {"zhcn": "笛卡尔积变换", "enus": "Cartesian product transformation"}, "option_flag": false}, {"option_text": {"zhcn": "分位数分组", "enus": "Quantile binning"}, "option_flag": false}, {"option_text": {"zhcn": "规整化", "enus": "Normalization"}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question “An online store is predicting future book sales by using a linear regression model... Which data transformation step should the data scientist take to improve the predictions of the model?” is “One - hot encoding”. One - hot encoding can be used to convert categorical variables into a binary matrix, which can help in handling non - linear relationships. If the data has some implicit categorical nature related to the duration (e.g., different time intervals can be thought of as categories), one - hot encoding can break down the variable into separate binary features that the linear regression model can better handle.\n\n“Cartesian product transformation” is mainly used for creating new variables by combining existing ones in a product - like way, which may not address the non - linear and skewed relationship between sales and duration. “Quantile binning” groups data into bins based on quantiles, but it may not be the best solution for a linear regression model as it can lead to loss of information and may not effectively capture the non - linear relationship. “Normalization” is used to scale the data to a common range, which doesn't change the non - linear nature of the relationship. A common misconception could be choosing normalization, thinking that scaling the data will solve the non - linearity issue, but it only changes the scale and not the relationship between variables. This is why “One - hot encoding” is the real answer, distinguishing it from the fake options.", "zhcn": "针对“某在线书店采用线性回归模型预测未来图书销量...数据科学家应采取哪种数据转换步骤来提升模型预测效果？”这一问题，正确答案为“独热编码”。独热编码可将分类变量转化为二进制矩阵，有助于处理非线性关系。若数据隐含与销售时长相关的分类特性（例如将不同时间区间视作独立类别），独热编码能将变量分解为线性回归模型更易处理的二元特征。\n\n“笛卡尔积变换”主要通过组合现有变量生成新变量，难以解决销量与时长之间的非线性偏态关系。“分位数分箱”虽能按分位点对数据分组，但会导致信息损失且无法有效捕捉非线性趋势，并非线性回归模型的最优解。“归一化处理”仅将数据缩放至统一范围，并未改变变量间的非线性本质。常见误区是选择归一化，误认为缩放数据可解决非线性问题，实则仅改变数值尺度而非变量关系。这正是“独热编码”作为正确答案与干扰项的本质区别。\n\n注：专业术语保留英文原表述（One-hot encoding/Cartesian product transformation/Quantile binning/Normalization），确保学术严谨性。"}, "answer": "A"}, {"id": "237", "question": {"enus": "A company's data engineer wants to use Amazon S3 to share datasets with data scientists. The data scientists work in three departments: Finance. Marketing, and Human Resources. Each department has its own IAM user group. Some datasets contain sensitive information and should be accessed only by the data scientists from the Finance department. How can the data engineer set up access to meet these requirements? ", "zhcn": "一家公司的数据工程师计划利用Amazon S3平台与数据科学家团队共享数据集。这些科学家分属三个部门：财务部、市场部及人力资源部，每个部门均设有独立的IAM用户组。部分数据集涉及敏感信息，仅允许财务部的数据科学家访问。请问数据工程师应如何配置权限以满足上述需求？"}, "option": [{"option_text": {"zhcn": "为每个数据集创建独立的S3存储桶，并为每个存储桶配置相应的访问控制列表。若存储桶包含敏感数据集，则将其访问权限限定为仅允许财务部门用户组访问；而对于存有非敏感数据集的存储桶，应向三大部门用户组全面开放访问权限。", "enus": "Create an S3 bucket for each dataset. Create an ACL for each S3 bucket. For each S3 bucket that contains a sensitive dataset, set the  ACL to allow access only from the Finance department user group. Allow all three department user groups to access each S3 bucket that  contains a non-sensitive dataset."}, "option_flag": false}, {"option_text": {"zhcn": "为每个数据集创建独立的S3存储桶。若存储桶包含敏感数据集，则设置其访问策略仅允许财务部门用户组调取；若存储桶包含非敏感数据集，则向三个部门用户组开放全部访问权限。", "enus": "Create an S3 bucket for each dataset. For each S3 bucket that contains a sensitive dataset, set the bucket policy to allow access only  from the Finance department user group. Allow all three department user groups to access each S3 bucket that contains a non-sensitive  dataset."}, "option_flag": false}, {"option_text": {"zhcn": "创建一个包含两个文件夹的S3存储桶，用于区分敏感数据集与非敏感数据集。为财务部门用户组附加IAM策略，允许其访问两个文件夹；而为市场部与人力资源部用户组配置的IAM策略，仅允许其访问存放非敏感数据集的文件夹。", "enus": "Create a single S3 bucket that includes two folders to separate the sensitive datasets from the non-sensitive datasets. For the Finance  department user group, attach an IAM policy that provides access to both folders. For the Marketing and Human Resources department  user groups, attach an IAM policy that provides access to only the folder that contains the non-sensitive datasets."}, "option_flag": false}, {"option_text": {"zhcn": "创建一个包含两个文件夹的S3存储桶，用于区分敏感数据集与非敏感数据集。设置该S3存储桶的访问策略：仅允许财务部门用户组访问存放敏感数据集的文件夹，同时允许所有三个部门的用户组访问存放非敏感数据集的文件夹。", "enus": "Create a single S3 bucket that includes two folders to separate the sensitive datasets from the non-sensitive datasets. Set the policy  for the S3 bucket to allow only the Finance department user group to access the folder that contains the sensitive datasets. Allow all  three department user groups to access the folder that contains the non-sensitive datasets."}, "option_flag": true}], "analysis": {"enus": "The correct answer to the question is to create a single S3 bucket with two folders for sensitive and non - sensitive datasets, and set the bucket policy to restrict access to the sensitive folder to the Finance department user group while allowing all three groups access to the non - sensitive folder. This approach is efficient as it centralizes data management in one bucket, reducing administrative overhead.\n\nThe first two fake options suggest creating an S3 bucket for each dataset. This is inefficient because it would lead to a large number of buckets, making management complex and increasing administrative tasks.\n\nThe third fake option proposes attaching IAM policies to user groups. While IAM policies can control access, using bucket policies is a more direct way to manage access at the data - storage level. It is better to define access rules at the bucket and folder level rather than relying solely on user - group IAM policies, as it provides a more straightforward and centralized way to manage access to specific datasets.", "zhcn": "对于该问题的正确答案是：创建一个包含敏感与非敏感数据集两个文件夹的S3存储桶，并通过存储桶策略设置仅允许财务部门用户组访问敏感文件夹，同时允许三个用户组均能访问非敏感文件夹。此方案的优势在于将数据管理集中至单一存储桶，既提升管理效率又降低运维负担。\n\n前两个干扰选项建议为每个数据集单独创建S3存储桶。这种做法会导致存储桶数量激增，不仅增加管理复杂性，还会造成运维工作量的成倍增长。\n\n第三个干扰方案提出为用户组附加IAM策略。虽然IAM策略确实能实现权限控制，但在数据存储层面直接使用存储桶策略是更精准的管控方式。相较于单纯依赖用户组IAM策略，在存储桶及文件夹层级定义访问规则能构建更清晰集中的管控体系，从而更直接地保障特定数据集的访问安全。"}, "answer": "D"}, {"id": "238", "question": {"enus": "A company operates an amusement park. The company wants to collect, monitor, and store real-time trafic data at several park entrances by using strategically placed cameras. The company’s security team must be able to immediately access the data for viewing. Stored data must be indexed and must be accessible to the company’s data science team. Which solution will meet these requirements MOST cost-effectively? ", "zhcn": "某游乐园运营公司计划在园区多个入口处架设摄像头，用于实时采集、监测及存储客流数据。安保团队需能即时调取查看数据，存储数据需建立索引并供公司数据科学团队随时调用。要满足这些需求，最具成本效益的解决方案是什么？"}, "option": [{"option_text": {"zhcn": "借助亚马逊Kinesis视频流服务，可实现数据的实时摄取、智能索引与安全存储。通过其与亚马逊Rekognition的内置集成功能，安保团队可便捷调取视频内容进行审阅分析。", "enus": "Use Amazon Kinesis Video Streams to ingest, index, and store the data. Use the built-in integration with Amazon Rekognition for  viewing by the security team."}, "option_flag": false}, {"option_text": {"zhcn": "借助亚马逊Kinesis视频流服务，可实现数据的无缝摄取、智能索引与安全存储。其内置的HLS实时流传输功能，可让安防团队随时调取高清影像进行查看。", "enus": "Use Amazon Kinesis Video Streams to ingest, index, and store the data. Use the built-in HTTP live streaming (HLS) capability for  viewing by the security team."}, "option_flag": true}, {"option_text": {"zhcn": "利用Amazon Rekognition Video及GStreamer插件导入视频数据，供安防团队实时调阅分析。同时通过Amazon Kinesis Data Streams实现数据流的即时索引与云端存储。", "enus": "Use Amazon Rekognition Video and the GStreamer plugin to ingest the data for viewing by the security team. Use Amazon Kinesis Data  Streams to index and store the data."}, "option_flag": false}, {"option_text": {"zhcn": "借助亚马逊Kinesis数据流服务实现数据的采集、索引与存储，并通过内置的HTTP实时流传输（HLS）技术供安防团队进行动态监测。", "enus": "Use Amazon Kinesis Data Firehose to ingest, index, and store the data. Use the built-in HTTP live streaming (HLS) capability for viewing  by the security team."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Use Amazon Kinesis Video Streams to ingest, index, and store the data. Use the built - in HTTP live streaming (HLS) capability for viewing by the security team.” This solution is the most cost - effective because Amazon Kinesis Video Streams is specifically designed for handling video data from cameras, which is exactly what the company needs for the park entrances. The built - in HLS capability allows the security team to immediately access and view the real - time data easily.\n\nThe first fake option suggests using the built - in integration with Amazon Rekognition for viewing by the security team. Amazon Rekognition is mainly for analyzing video content, like object and face detection, not for simple real - time viewing. Using it for just viewing would add unnecessary cost and complexity.\n\nThe second fake option proposes using Amazon Rekognition Video and the GStreamer plugin to ingest data for viewing and Amazon Kinesis Data Streams to index and store. Amazon Rekognition Video is for video analysis, not for basic ingestion and viewing. Amazon Kinesis Data Streams is designed for streaming data, not specifically for video data, so it's not the best fit for this video - centric requirement.\n\nThe third fake option suggests using Amazon Kinesis Data Firehose. Kinesis Data Firehose is for loading streaming data into destinations like S3, Redshift, etc., and is not optimized for video data ingestion, indexing, and real - time viewing like Kinesis Video Streams. This makes it a less suitable and more costly option for this use case.", "zhcn": "针对该问题，正确答案是\"采用Amazon Kinesis Video Streams进行数据摄取、索引与存储，并利用其内置的HTTP实时流传输（HLS）功能供安防团队查看\"。这一方案最具成本效益，因为Amazon Kinesis Video Streams专为处理摄像头视频数据而设计，完全契合该公司对园区入口的监控需求。其内置的HLS功能可使安防团队便捷地实时调取并查看监控画面。\n\n首个干扰选项提议通过Amazon Rekognition的内置集成功能供安防团队查看。然Amazon Rekognition主要应用于视频内容分析（如物体与人脸识别），而非简单的实时查看。若仅用于查看功能，不仅会增加不必要的成本，还会提升系统复杂度。\n\n第二个干扰方案建议组合使用Amazon Rekognition Video与GStreamer插件实现数据摄取查看，同时采用Amazon Kinesis Data Streams进行索引存储。但Amazon Rekognition Video本质是视频分析工具，不适用于基础的数据摄取与查看；而Amazon Kinesis Data Streams面向的是通用数据流处理，并非专为视频数据优化，故不符合本次以视频为核心的需求。\n\n第三个干扰方案推荐使用Amazon Kinesis Data Firehose。该服务主要用于将数据流传输至S3、Redshift等存储目标，其设计并未像Kinesis Video Streams那样针对视频数据的摄取、索引及实时查看进行优化，因此在本场景中既不够适用又会造成更高成本。"}, "answer": "B"}, {"id": "239", "question": {"enus": "An engraving company wants to automate its quality control process for plaques. The company performs the process before mailing each customized plaque to a customer. The company has created an Amazon S3 bucket that contains images of defects that should cause a plaque to be rejected. Low-confidence predictions must be sent to an internal team of reviewers who are using Amazon Augmented AI (Amazon A2I). Which solution will meet these requirements? ", "zhcn": "一家雕刻公司希望实现牌匾质检流程的自动化。该公司在每块定制牌匾邮寄给客户前需执行此质检流程。公司已创建一个亚马逊S3存储桶，其中收录了应当拒收牌匾的缺陷图像样本。对于置信度较低的预测结果，必须提交给使用亚马逊增强人工智能（Amazon A2I）的内部审核团队进行复核。下列哪种方案能满足这些要求？"}, "option": [{"option_text": {"zhcn": "采用Amazon Textract实现自动化处理，结合Amazon A2I与Amazon Mechanical Turk进行人工核验。", "enus": "Use Amazon Textract for automatic processing. Use Amazon A2I with Amazon Mechanical Turk for manual review."}, "option_flag": false}, {"option_text": {"zhcn": "利用Amazon Rekognition实现自动化处理，同时采用配备专属人工审核团队的Amazon A2I服务进行人工复核。", "enus": "Use Amazon Rekognition for automatic processing. Use Amazon A2I with a private workforce option for manual review."}, "option_flag": false}, {"option_text": {"zhcn": "采用Amazon Transcribe实现自动化处理，同时通过Amazon A2I的人工审核功能，启用专属团队进行人工复核。", "enus": "Use Amazon Transcribe for automatic processing. Use Amazon A2I with a private workforce option for manual review."}, "option_flag": true}, {"option_text": {"zhcn": "利用AWS Panorama实现自动化处理，通过Amazon A2I与Amazon Mechanical Turk相结合进行人工复核。", "enus": "Use AWS Panorama for automatic processing. Use Amazon A2I with Amazon Mechanical Turk for manual review."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is using “Amazon Transcribe for automatic processing and Amazon A2I with a private workforce option for manual review”. However, this answer is incorrect in a real - world scenario. The most appropriate solution should be using “Amazon Rekognition for automatic processing. Use Amazon A2I with a private workforce option for manual review”.\n\nThe task is to automate the quality control process for plaque images. Amazon Rekognition is a suitable choice for image analysis. It can detect objects, scenes, and text in images, which is perfect for identifying defects in plaque images. And since low - confidence predictions need to be sent to an internal team of reviewers, using Amazon A2I with a private workforce option ensures that only the company's internal team can handle these reviews.\n\nThe option of using Amazon Transcribe is wrong because Amazon Transcribe is mainly for transcribing speech to text, not for image analysis. \n\nThe option of using Amazon Textract is incorrect as it is focused on extracting text from documents like PDFs and scanned images, not for general image defect detection. \n\nThe option of using AWS Panorama is designed for computer vision applications on edge devices, which is not relevant to this cloud - based quality control process. Also, using Amazon Mechanical Turk for manual review doesn't meet the requirement of sending reviews to an internal team.\n\nSo, the real correct answer should be using Amazon Rekognition for automatic processing and Amazon A2I with a private workforce option for manual review, which can effectively handle the image - based quality control task and direct low - confidence cases to the internal team.", "zhcn": "该问题的标准答案为\"使用Amazon Transcribe进行自动处理，并采用配备私有工作团队的Amazon A2I进行人工审核\"。但结合实际场景考量，此方案存在谬误。最恰当的解决方案应为\"采用Amazon Rekognition实现自动处理，配合配备私有工作团队的Amazon A2I执行人工审核\"。\n\n本任务旨在实现牙菌斑影像质控流程的自动化。Amazon Rekognition在图像分析领域具有显著优势，其能够精准识别图像中的物体、场景及文字内容，尤其适用于检测牙菌斑影像的异常特征。针对置信度较低的预测结果，通过配置私有工作团队的Amazon A2I方案，可确保仅由企业内部审核团队处理此类复核任务。\n\n选用Amazon Transcribe的方案显然有误，因其核心功能在于语音转文本，与图像分析需求不符。Amazon Textract亦不适用，该服务主要专注于从PDF及扫描文档中提取文字信息，而非通用图像缺陷检测。至于AWS Panorama，其专为边缘设备的计算机视觉应用设计，与当前云端质控流程的需求相去甚远。而采用Amazon Mechanical Turk进行人工审核的方案，则违背了\"由内部团队处理复核\"的核心要求。\n\n因此，真正正确的解决方案应是：采用Amazon Rekognition实现自动处理，配合配备私有工作团队的Amazon A2I进行人工审核。该组合既能高效完成基于影像的质控任务，又能将低置信度案例精准流转至内部团队处理。"}, "answer": "C"}, {"id": "240", "question": {"enus": "A machine learning (ML) engineer at a bank is building a data ingestion solution to provide transaction features to financial ML models. Raw transactional data is available in an Amazon Kinesis data stream. The solution must compute rolling averages of the ingested data from the data stream and must store the results in Amazon SageMaker Feature Store. The solution also must serve the results to the models in near real time. Which solution will meet these requirements? ", "zhcn": "某银行的一位机器学习工程师正在构建数据摄取方案，旨在为金融机器学习模型提供交易特征。原始交易数据可通过亚马逊Kinesis数据流获取。该方案需根据数据流计算输入数据的滚动平均值，并将结果存储至亚马逊SageMaker特征库，同时还需以近实时方式将处理结果传输至模型端。请问何种方案可满足上述需求？"}, "option": [{"option_text": {"zhcn": "通过Amazon Kinesis Data Firehose将数据载入Amazon S3存储桶，随后借助SageMaker处理作业对数据进行聚合处理，并将结果以在线特征组的形式导入SageMaker特征存储库。", "enus": "Load the data into an Amazon S3 bucket by using Amazon Kinesis Data Firehose. Use a SageMaker Processing job to aggregate the  data and to load the results into SageMaker Feature Store as an online feature group."}, "option_flag": true}, {"option_text": {"zhcn": "将数据流直接写入SageMaker特征存储库，创建在线特征组。通过调用SageMaker GetRecord API操作，在特征存储库内实时计算滚动平均值。", "enus": "Write the data directly from the data stream into SageMaker Feature Store as an online feature group. Calculate the rolling averages in  place within SageMaker Feature Store by using the SageMaker GetRecord API operation."}, "option_flag": false}, {"option_text": {"zhcn": "通过亚马逊Kinesis数据分析平台的SQL应用程序对数据流进行实时处理，计算移动平均值并生成结果流。随后由定制化AWS Lambda函数接收结果流，将处理后的数据作为在线特征组发布至SageMaker特征存储平台。", "enus": "Consume the data stream by using an Amazon Kinesis Data Analytics SQL application that calculates the rolling averages. Generate a  result stream. Consume the result stream by using a custom AWS Lambda function that publishes the results to SageMaker Feature Store  as an online feature group."}, "option_flag": false}, {"option_text": {"zhcn": "利用Amazon Kinesis Data Firehose将数据载入Amazon S3存储桶，随后通过SageMaker处理作业将数据作为离线特征组存入SageMaker特征库。查询时动态计算滚动平均值。", "enus": "Load the data into an Amazon S3 bucket by using Amazon Kinesis Data Firehose. Use a SageMaker Processing job to load the data into  SageMaker Feature Store as an ofiine feature group. Compute the rolling averages at query time."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is “Load the data into an Amazon S3 bucket by using Amazon Kinesis Data Firehose. Use a SageMaker Processing job to aggregate the data and to load the results into SageMaker Feature Store as an online feature group.” This solution meets the requirements as it first stores the data from the Kinesis data stream in S3 using Kinesis Data Firehose. Then, a SageMaker Processing job aggregates the data to compute the rolling averages and loads the results into the SageMaker Feature Store as an online feature group, which can serve the results to the models in near - real time.\n\nThe first fake answer “Write the data directly from the data stream into SageMaker Feature Store as an online feature group. Calculate the rolling averages in place within SageMaker Feature Store by using the SageMaker GetRecord API operation” is incorrect because SageMaker Feature Store does not support in - place calculation of rolling averages using the GetRecord API.\n\nThe second fake answer “Consume the data stream by using an Amazon Kinesis Data Analytics SQL application that calculates the rolling averages. Generate a result stream. Consume the result stream by using a custom AWS Lambda function that publishes the results to SageMaker Feature Store as an online feature group” is more complex than necessary. It involves multiple components like Kinesis Data Analytics and Lambda, which adds to the management overhead compared to the simpler solution of using SageMaker Processing.\n\nThe third fake answer “Load the data into an Amazon S3 bucket by using Amazon Kinesis Data Firehose. Use a SageMaker Processing job to load the data into SageMaker Feature Store as an offline feature group. Compute the rolling averages at query time” is wrong because the requirement is to serve the results in near - real time, and an offline feature group is not suitable for this purpose as it is designed for batch processing and not for near - real - time serving.", "zhcn": "问题的正确答案是：\"使用亚马逊Kinesis Data Firehose将数据加载至Amazon S3存储桶，随后通过SageMaker处理作业对数据进行聚合计算，并将结果作为在线特征组存入SageMaker特征库。\" 这一方案完全符合要求：首先通过Kinesis Data Firehose将数据流中的信息存储至S3，再由SageMaker处理作业计算滚动平均值，最终将结果导入支持近实时读取的在线特征组，确保模型能够快速获取最新特征。\n\n第一个错误答案\"将数据流直接写入SageMaker特征库作为在线特征组，并通过GetRecord API在特征库内实时计算滚动平均值\"存在技术谬误——SageMaker特征库并不支持通过GetRecord API实现实时滚动计算功能。\n\n第二个错误答案\"使用Kinesis Data Analytics SQL应用消费数据流并计算滚动平均值，生成结果流后通过自定义Lambda函数将结果发布至在线特征组\"存在设计冗余。该方案涉及Kinesis数据分析与Lambda函数等多重组件，其管理复杂度远高于直接使用SageMaker处理作业的简洁方案。\n\n第三个错误答案\"通过Kinesis Data Firehose将数据加载至S3存储桶后，使用SageMaker处理作业将数据导入离线特征组，在查询时计算滚动平均值\"违背了近实时服务的要求。由于离线特征组专为批处理设计而非实时响应，该方案无法满足业务场景的时效性需求。"}, "answer": "A"}, {"id": "241", "question": {"enus": "Each morning, a data scientist at a rental car company creates insights about the previous day’s rental car reservation demands. The company needs to automate this process by streaming the data to Amazon S3 in near real time. The solution must detect high-demand rental cars at each of the company’s locations. The solution also must create a visualization dashboard that automatically refreshes with the most recent data. Which solution will meet these requirements with the LEAST development time? ", "zhcn": "每日清晨，某租车公司的数据科学家会针对前一日租车预订需求进行分析并生成洞察报告。该公司需通过近乎实时数据流将信息传输至Amazon S3来自动化此流程。解决方案必须能实时识别各营业点的高需求车型，同时自动生成可随最新数据动态更新的可视化仪表盘。在满足上述需求的前提下，何种方案能以最短开发周期实现该目标？"}, "option": [{"option_text": {"zhcn": "利用Amazon Kinesis Data Firehose将预约数据实时传输至Amazon S3存储服务，通过Amazon QuickSight的机器学习洞察功能识别高需求异常值，并在QuickSight平台实现数据可视化呈现。", "enus": "Use Amazon Kinesis Data Firehose to stream the reservation data directly to Amazon S3. Detect high-demand outliers by using Amazon  QuickSight ML Insights. Visualize the data in QuickSight."}, "option_flag": true}, {"option_text": {"zhcn": "利用亚马逊Kinesis数据流服务将预约数据实时传输至亚马逊S3存储平台。通过调用亚马逊SageMaker中经过训练的随机切割森林(RCF)模型，精准识别高需求异常数据点。最终在亚马逊QuickSight可视化平台呈现数据洞察。", "enus": "Use Amazon Kinesis Data Streams to stream the reservation data directly to Amazon S3. Detect high-demand outliers by using the  Random Cut Forest (RCF) trained model in Amazon SageMaker. Visualize the data in Amazon QuickSight."}, "option_flag": false}, {"option_text": {"zhcn": "利用Amazon Kinesis Data Firehose将预约数据直接实时传输至Amazon S3存储服务，通过Amazon SageMaker中经过训练的随机切割森林（RCF）模型检测高需求异常值，最终在Amazon QuickSight平台实现数据可视化呈现。", "enus": "Use Amazon Kinesis Data Firehose to stream the reservation data directly to Amazon S3. Detect high-demand outliers by using the  Random Cut Forest (RCF) trained model in Amazon SageMaker. Visualize the data in Amazon QuickSight."}, "option_flag": false}, {"option_text": {"zhcn": "利用Amazon Kinesis Data Streams将预约数据直接流式传输至Amazon S3存储服务，通过Amazon QuickSight ML Insights功能智能检测高需求异常值，并在QuickSight平台实现数据可视化呈现。", "enus": "Use Amazon Kinesis Data Streams to stream the reservation data directly to Amazon S3. Detect high-demand outliers by using Amazon  QuickSight ML Insights. Visualize the data in QuickSight."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Use Amazon Kinesis Data Firehose to stream the reservation data directly to Amazon S3. Detect high - demand outliers by using Amazon QuickSight ML Insights. Visualize the data in QuickSight.” This option requires the least development time.\n\nAmazon Kinesis Data Firehose is designed for easy data ingestion and loading into destinations like Amazon S3 with minimal configuration, reducing development effort compared to Kinesis Data Streams. When it comes to outlier detection, Amazon QuickSight ML Insights offers a no - code approach to find outliers. It can quickly analyze data and detect high - demand outliers without the need to train a custom model as in the cases where the Random Cut Forest (RCF) trained model in Amazon SageMaker is used. Using SageMaker to train an RCF model demands more development work, including data preprocessing, model training, and tuning.\n\nThe fake options that involve Kinesis Data Streams require more setup and management as they are more suitable for building custom streaming data applications. And the options that use the RCF model in Amazon SageMaker have a steeper development curve due to the model - building process. These factors make the real answer the most efficient choice in terms of minimizing development time.", "zhcn": "针对该问题，正确答案是\"使用Amazon Kinesis Data Firehose将预订数据直接流式传输至Amazon S3，通过Amazon QuickSight ML Insights检测高需求异常值，并在QuickSight中实现数据可视化\"。此方案所需开发时间最短。\n\nAmazon Kinesis Data Firehose专为简化数据摄取流程而设计，仅需极简配置即可将数据加载至Amazon S3等目标存储，相较于Kinesis Data Streams能显著降低开发工作量。在异常值检测方面，Amazon QuickSight ML Insights提供零代码的离群值识别方案，无需像使用Amazon SageMaker中随机切割森林（RCF）训练模型那样进行定制化模型训练，即可快速完成数据分析并精准捕捉高需求异常值。若采用SageMaker训练RCF模型，则需投入更多开发精力，包括数据预处理、模型训练与参数调优等环节。\n\n而涉及Kinesis Data Streams的干扰选项因需更多配置管理操作，更适用于构建定制化流数据应用场景；采用Amazon SageMaker中RCF模型的方案则因需经历模型构建过程而大幅增加开发复杂度。这些因素共同表明，原答案在最大化压缩开发周期方面实为最优选择。"}, "answer": "A"}, {"id": "242", "question": {"enus": "A company is planning a marketing campaign to promote a new product to existing customers. The company has data for past promotions that are similar. The company decides to try an experiment to send a more expensive marketing package to a smaller number of customers. The company wants to target the marketing campaign to customers who are most likely to buy the new product. The experiment requires that at least 90% of the customers who are likely to purchase the new product receive the marketing materials. The company trains a model by using the linear learner algorithm in Amazon SageMaker. The model has a recall score of 80% and a precision of 75%. How should the company retrain the model to meet these requirements? ", "zhcn": "某公司正筹划一项面向现有客户的新品推广活动，并拥有过往类似促销活动的数据记录。公司决定尝试一项实验：向少量客户寄送成本更高的营销包裹，并希望将营销目标精准锁定在最有可能购买新产品的客户群体上。该实验要求潜在购买客户中至少有90%能够收到营销物料。公司通过亚马逊SageMaker平台的线性学习算法训练模型，当前模型的召回率为80%，精确率为75%。为达成实验要求，该公司应如何优化模型训练方案？"}, "option": [{"option_text": {"zhcn": "将目标召回率超参数设定为90%。将二元分类器模型选择标准超参数调整为基于目标精确度的召回率优化。", "enus": "Set the target_recall hyperparameter to 90%. Set the binary_classifier_model_selection_criteria hyperparameter to  recall_at_target_precision."}, "option_flag": false}, {"option_text": {"zhcn": "将目标精度超参数设定为90%。将二元分类器模型选择标准超参数设定为“目标召回率下的精确度”。", "enus": "Set the target_precision hyperparameter to 90%. Set the binary_classifier_model_selection_criteria hyperparameter to  precision_at_target_recall."}, "option_flag": true}, {"option_text": {"zhcn": "将90%的历史数据用于训练，迭代轮数设定为20次。", "enus": "Use 90% of the historical data for training. Set the number of epochs to 20."}, "option_flag": false}, {"option_text": {"zhcn": "将 normalize_label 超参数设为 true，类别数量设置为 2。", "enus": "Set the normalize_label hyperparameter to true. Set the number of classes to 2."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Set the target_precision hyperparameter to 90%. Set the binary_classifier_model_selection_criteria hyperparameter to  precision_at_target_recall.” The company's requirement is that at least 90% of the customers likely to purchase the new product receive the marketing materials, which is related to recall. By setting the target_precision to 90% and using the precision_at_target_recall selection criteria, the model can be optimized to achieve a high recall rate as desired.\n\nThe option “Set the target_recall hyperparameter to 90%. Set the binary_classifier_model_selection_criteria hyperparameter to  recall_at_target_precision” is incorrect because it focuses on precision at a target recall, which doesn't directly address the need to ensure 90% of potential buyers get the materials. The option “Use 90% of the historical data for training. Set the number of epochs to 20” is wrong as simply adjusting the training data ratio and epochs doesn't specifically target improving the recall rate. The option “Set the normalize_label hyperparameter to true. Set the number of classes to 2” is also incorrect as these settings don't pertain to achieving the required recall rate. The key to meeting the company's requirement is to correctly configure the hyperparameters related to recall, which is why the real answer option is the correct choice.", "zhcn": "针对该问题的正确答案是：将目标精度超参数设为90%，并将二元分类器模型选择标准设为\"目标召回率下的精度\"。公司的核心要求是确保至少90%可能购买新产品的客户能够收到营销材料，这直接关联到召回率指标。通过设定90%的目标精度并采用\"目标召回率下的精度\"这一选择标准，可以使模型在保持较高精度的同时，优化召回率以满足业务需求。\n\n而\"将目标召回率超参数设为90%，并将二元分类器模型选择标准设为'目标精度下的召回率'\"这一选项并不正确，因其侧重于在目标召回率下追求精度，未能直接满足让90%潜在买家获得材料的核心诉求。另一选项\"使用90%历史数据训练并设定训练轮数为20\"也存在谬误，仅调整训练数据比例和训练轮数无法针对性提升召回率。至于\"将标准化标签超参数设为真并设定类别数为2\"这一选项，由于这些设置与实现所需召回率无关，同样不可取。\n\n综上所述，满足公司需求的关键在于正确配置与召回率相关的超参数，因此最初指出的答案选项才是正确选择。"}, "answer": "B"}, {"id": "243", "question": {"enus": "A wildlife research company has a set of images of lions and cheetahs. The company created a dataset of the images. The company labeled each image with a binary label that indicates whether an image contains a lion or cheetah. The company wants to train a model to identify whether new images contain a lion or cheetah. Which Amazon SageMaker algorithm will meet this requirement? ", "zhcn": "一家野生动物研究公司拥有一批狮子和猎豹的图像资料。该公司已将这批图像构建为数据集，并为每张图片标注了二元标签以区分内容为狮子或猎豹。现需训练一个模型用于识别新图像中的动物类别为狮子或猎豹。请问亚马逊SageMaker平台中哪种算法可满足此需求？"}, "option": [{"option_text": {"zhcn": "XGBoost", "enus": "XGBoost"}, "option_flag": false}, {"option_text": {"zhcn": "图像分类——TensorFlow", "enus": "Image Classification - TensorFlow"}, "option_flag": true}, {"option_text": {"zhcn": "目标检测 - TensorFlow", "enus": "Object Detection - TensorFlow"}, "option_flag": false}, {"option_text": {"zhcn": "语义分割——MXNet", "enus": "Semantic segmentation - MXNet"}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is ‘Image Classification - TensorFlow’. The task at hand is to identify whether a new image contains a lion or a cheetah, which is a classic image classification problem where the model needs to assign a class (lion or cheetah) to the input image.\n\n‘XGBoost’ is a gradient - boosting library mainly used for tabular data in tasks like regression and classification of structured data, not for image data, so it doesn't fit the requirement. ‘Object Detection - TensorFlow’ is used to detect multiple objects within an image and localize them, which is more complex than just classifying the whole image as having a lion or a cheetah. ‘Semantic segmentation - MXNet’ is used to label each pixel in an image with a class, which is overkill for the simple binary classification task of identifying if the image has a lion or cheetah.\n\nThe key factor is that image classification is exactly the right approach for this binary image - based classification problem, making ‘Image Classification - TensorFlow’ the real answer option over the fake options. A common misconception could be confusing image classification with more complex image - related tasks like object detection or semantic segmentation when a simple classification is sufficient.", "zhcn": "该问题的正确答案是“Image Classification - TensorFlow”。当前任务需要判断一张新图片中包含的是狮子还是猎豹，这属于典型的图像分类问题——模型需对输入图像进行类别判定（狮子或猎豹）。XGBoost作为梯度提升库，主要适用于表格数据的回归与分类任务，而非图像数据处理，故不符合本题要求。Object Detection - TensorFlow用于检测并定位图像中的多个目标，其复杂度远超仅对图像进行狮/豹二分类的需求。Semantic segmentation - MXNet需对图像每个像素进行类别标注，对于只需判断图像包含狮/豹的简单二分类任务而言过于复杂。关键在于，针对这种基于图像的二元分类问题，图像分类技术恰是最佳解决方案，使得“Image Classification - TensorFlow”成为真正符合题意的选项。常见的认知误区在于，当简单分类已能满足需求时，容易将图像分类与目标检测、语义分割等更复杂的图像任务相混淆。"}, "answer": "B"}, {"id": "244", "question": {"enus": "A data scientist for a medical diagnostic testing company has developed a machine learning (ML) model to identify patients who have a specific disease. The dataset that the scientist used to train the model is imbalanced. The dataset contains a large number of healthy patients and only a small number of patients who have the disease. The model should consider that patients who are incorrectly identified as positive for the disease will increase costs for the company. Which metric will MOST accurately evaluate the performance of this model? ", "zhcn": "某医疗诊断公司的数据科学家开发了一个机器学习模型，用于识别罹患特定疾病的患者。该模型所使用的训练数据集存在样本不平衡问题：健康患者的数据占绝大多数，而确诊患者的数据仅占极小比例。同时需考虑，若模型将健康者误判为阳性，将导致公司成本上升。在此情况下，下列哪项指标能最精准地评估该模型的性能？"}, "option": [{"option_text": {"zhcn": "忆起", "enus": "Recall"}, "option_flag": false}, {"option_text": {"zhcn": "F1分数", "enus": "F1 score"}, "option_flag": false}, {"option_text": {"zhcn": "精准", "enus": "Accuracy"}, "option_flag": false}, {"option_text": {"zhcn": "精准", "enus": "Precision"}, "option_flag": true}], "analysis": {"enus": "The correct answer to the question is ‘Precision’. Precision measures the proportion of true positive predictions out of all positive predictions made by the model. In this case, since incorrectly identifying patients as positive for the disease will increase costs for the company, it's crucial to minimize false positives. A high - precision model will have fewer false positives, thus reducing unnecessary costs.\n\n‘Recall’ focuses on the proportion of actual positive cases that the model correctly identifies. While it's important in some scenarios, it doesn't directly address the issue of false positives and cost. The ‘F1 score’ is a balance between precision and recall. But given the emphasis on false positives and cost, precision is more relevant than a balanced metric. ‘Accuracy’ is not suitable because in an imbalanced dataset, a model can achieve high accuracy by simply predicting the majority class (healthy patients). This can lead to many false positives being overlooked. So, precision is the most appropriate metric for evaluating the model's performance in this context, distinguishing it from the other fake options.", "zhcn": "问题的正确答案是\"精确率\"。精确率衡量的是模型在所有阳性预测中，真正例所占的比例。在当前情境下，由于将患者错误判定为患病会导致公司成本增加，因此尽可能减少假阳性至关重要。高精确率模型能有效降低假阳性数量，从而避免不必要的成本支出。\n\n\"召回率\"关注的是模型正确识别出的真实阳性病例比例。虽然在某些场景下召回率很重要，但它并未直接解决假阳性与成本问题。\"F1分数\"是精确率与召回率的平衡指标，但考虑到对假阳性和成本控制的侧重，精确率比平衡性指标更具针对性。\"准确率\"在此并不适用——因为面对不平衡数据集时，模型仅通过预测多数类别（健康患者）即可获得高准确率，这可能导致大量假阳性被掩盖。\n\n因此，在此特定情境下，精确率是评估模型性能最合适的指标，使其与其他干扰选项区分开来。"}, "answer": "D"}, {"id": "245", "question": {"enus": "A machine learning (ML) specialist is training a linear regression model. The specialist notices that the model is overfitting. The specialist applies an L1 regularization parameter and runs the model again. This change results in all features having zero weights. What should the ML specialist do to improve the model results? ", "zhcn": "一位机器学习专家正在训练线性回归模型时，发现模型出现了过拟合现象。该专家随后应用了L1正则化参数并重新运行模型，但此举导致所有特征权重均归为零。为提升模型效果，机器学习专家应采取何种改进措施？"}, "option": [{"option_text": {"zhcn": "增强L1正则化参数，其余训练参数保持不变。", "enus": "Increase the L1 regularization parameter. Do not change any other training parameters."}, "option_flag": false}, {"option_text": {"zhcn": "降低L1正则化参数，其余训练参数保持不变。", "enus": "Decrease the L1 regularization parameter. Do not change any other training parameters."}, "option_flag": true}, {"option_text": {"zhcn": "引入一个较大的L2正则化参数，同时保持现有的L1正则化数值不变。", "enus": "Introduce a large L2 regularization parameter. Do not change the current L1 regularization value."}, "option_flag": false}, {"option_text": {"zhcn": "引入一个较小的L2正则化参数，同时保持现有的L1正则化数值不变。", "enus": "Introduce a small L2 regularization parameter. Do not change the current L1 regularization value."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Decrease the L1 regularization parameter. Do not change any other training parameters.” L1 regularization adds a penalty term to the loss function proportional to the absolute value of the weights. When the L1 regularization parameter is too large, it can force all feature weights to zero, as seen in this case.\n\nIncreasing the L1 regularization parameter would exacerbate the problem, pushing more weights to zero and further under - fitting the model, so “Increase the L1 regularization parameter. Do not change any other training parameters” is incorrect.\n\nIntroducing an L2 regularization parameter (either large or small) while keeping the current L1 regularization value does not address the root cause, which is the over - aggressive L1 regularization. L2 regularization adds a penalty proportional to the square of the weights, but the main issue is the L1 parameter forcing all weights to zero.\n\nThe key is to reduce the L1 regularization parameter to allow the model to assign non - zero weights to relevant features and better fit the data, which is why “Decrease the L1 regularization parameter. Do not change any other training parameters” is the correct choice.", "zhcn": "针对该问题的正确答案是\"降低L1正则化参数，且不调整其他训练参数\"。L1正则化会在损失函数中添加与权重绝对值成正比的惩罚项。当L1正则化参数设置过大时（如本例所示），会迫使所有权重特征归零。\n\n若增大L1正则化参数，将加剧权重归零现象，导致模型欠拟合程度进一步恶化，因此\"增大L1正则化参数且不改变其他训练参数\"的方案不可取。\n\n在维持当前L1正则化值的前提下引入L2正则化参数（无论大小），并未解决L1正则化过度约束这一根本问题。虽然L2正则化会添加与权重平方成正比的惩罚项，但当前核心矛盾在于L1参数正在强制所有权重归零。\n\n关键举措是降低L1正则化参数，使模型能够为相关特征分配非零权重从而更好地拟合数据，这正是\"降低L1正则化参数且不改变其他训练参数\"成为正确选择的原因。"}, "answer": "B"}, {"id": "246", "question": {"enus": "A machine learning (ML) engineer is integrating a production model with a customer metadata repository for real-time inference. The repository is hosted in Amazon SageMaker Feature Store. The engineer wants to retrieve only the latest version of the customer metadata record for a single customer at a time. Which solution will meet these requirements? ", "zhcn": "一位机器学习工程师正在将生产环境中的模型与客户元数据库进行集成，以实现实时推理。该数据库托管于Amazon SageMaker特征存储平台。工程师需要每次仅获取单个客户的最新版本元数据记录。下列哪种方案能满足这一需求？"}, "option": [{"option_text": {"zhcn": "请使用SageMaker特征存储的BatchGetRecord接口，并传入记录标识符。通过筛选条件获取最新记录。", "enus": "Use the SageMaker Feature Store BatchGetRecord API with the record identifier. Filter to find the latest record."}, "option_flag": false}, {"option_text": {"zhcn": "编写一条Amazon Athena查询语句，用于从特征表中提取数据。", "enus": "Create an Amazon Athena query to retrieve the data from the feature table."}, "option_flag": false}, {"option_text": {"zhcn": "创建一条Amazon Athena查询语句，用于从特征表中提取数据。通过write_time字段筛选出最新记录。", "enus": "Create an Amazon Athena query to retrieve the data from the feature table. Use the write_time value to find the latest record."}, "option_flag": false}, {"option_text": {"zhcn": "请使用 SageMaker Feature Store 的 GetRecord API 并传入记录标识符。", "enus": "Use the SageMaker Feature Store GetRecord API with the record identifier."}, "option_flag": true}], "analysis": {"enus": "The correct answer to the question is to “Use the SageMaker Feature Store GetRecord API with the record identifier.” This API is designed to retrieve a single record from the feature store, and by default, it fetches the latest version of the record for the given identifier, directly meeting the requirement of getting the latest metadata record for a single customer at a time.\n\nThe option “Use the SageMaker Feature Store BatchGetRecord API with the record identifier. Filter to find the latest record” is incorrect because the BatchGetRecord API is meant for retrieving multiple records. Using it for a single - record use - case and then adding a filtering step is an over - complicated and inefficient approach.\n\nThe options involving Amazon Athena, “Create an Amazon Athena query to retrieve the data from the feature table” and “Create an Amazon Athena query to retrieve the data from the feature table. Use the write_time value to find the latest record,” are wrong. Athena is a query service mainly used for ad - hoc analysis on large datasets. Using Athena for real - time single - record retrieval is not optimized for this use case and would add unnecessary latency compared to using the dedicated GetRecord API provided by SageMaker Feature Store. \n\nIn summary, the simplicity and direct functionality of the GetRecord API for retrieving the latest single record make it the correct choice, differentiating it from the more complex and less suitable fake answer options.", "zhcn": "针对该问题的正确答案是\"使用 SageMaker Feature Store 的 GetRecord API 并传入记录标识符\"。此 API 专为从特征存储中检索单条记录而设计，默认会获取指定标识符对应的最新版本记录，完美契合了实时获取单个客户最新元数据记录的需求。\n\n而\"使用 SageMaker Feature Store 的 BatchGetRecord API 并传入记录标识符，通过筛选获取最新记录\"这一方案并不恰当。因为 BatchGetRecord API 本用于批量获取记录，若将其用于单条记录场景再附加筛选步骤，不仅使流程复杂化，还会造成效率损失。\n\n至于涉及 Amazon Athena 的两种方案——\"创建 Amazon Athena 查询从特征表获取数据\"及\"创建 Amazon Athena 查询并利用 write_time 值定位最新记录\"均存在方向性错误。Athena 作为交互式查询服务，主要面向大规模数据集的即席分析。将其用于实时单条记录检索既非最优解，与 SageMaker Feature Store 原生的 GetRecord API 相比，还会引入不必要的延迟。\n\n综上所述，GetRecord API 以其简洁直接的特性成为理想选择，这与另外几个复杂且不适用的错误选项形成了鲜明对比。"}, "answer": "D"}, {"id": "247", "question": {"enus": "A company’s data scientist has trained a new machine learning model that performs better on test data than the company’s existing model performs in the production environment. The data scientist wants to replace the existing model that runs on an Amazon SageMaker endpoint in the production environment. However, the company is concerned that the new model might not work well on the production environment data. The data scientist needs to perform A/B testing in the production environment to evaluate whether the new model performs well on production environment data. Which combination of steps must the data scientist take to perform the A/B testing? (Choose two.) ", "zhcn": "某公司的数据科学家训练出一款新的机器学习模型，其在测试数据上的表现优于公司生产环境中现有的模型。该数据科学家希望替换当前在生产环境中通过Amazon SageMaker端点运行的模型。然而公司担心新模型可能无法很好地适应生产环境的数据。数据科学家需要在生产环境中进行A/B测试，以评估新模型在实际生产数据上的表现。请问数据科学家必须采取哪两个步骤组合来完成此次A/B测试？（请选择两项）"}, "option": [{"option_text": {"zhcn": "创建一个新的端点配置，其中需包含针对两种模型各自的生产变体。", "enus": "Create a new endpoint configuration that includes a production variant for each of the two models."}, "option_flag": true}, {"option_text": {"zhcn": "新建一个端点配置，其中包含指向不同端点的两种目标变体。", "enus": "Create a new endpoint configuration that includes two target variants that point to different endpoints."}, "option_flag": false}, {"option_text": {"zhcn": "将新模型部署至现有终端节点。", "enus": "Deploy the new model to the existing endpoint."}, "option_flag": true}, {"option_text": {"zhcn": "更新现有端点以启用新模型。", "enus": "Update the existing endpoint to activate the new model."}, "option_flag": false}, {"option_text": {"zhcn": "请将现有终端更新为采用新版终端配置。", "enus": "Update the existing endpoint to use the new endpoint configuration."}, "option_flag": false}], "analysis": {"enus": "The correct answers to perform A/B testing are “Create a new endpoint configuration that includes a production variant for each of the two models” and “Deploy the new model to the existing endpoint”. \n\nFor the first real answer, creating a new endpoint configuration with production variants for both models allows the system to route incoming requests to either the existing or the new model. This is essential for A/B testing as it enables direct comparison of the two models in the production environment.\n\nThe second real answer, deploying the new model to the existing endpoint, ensures that the new model is available in the production environment and can receive requests for testing. \n\nThe first fake answer is incorrect because creating target variants that point to different endpoints is not the correct approach for A/B testing on a single existing endpoint. \n\nThe second fake answer, updating the existing endpoint to activate the new model, would simply replace the old model with the new one without the ability to perform a side - by - side comparison for A/B testing. \n\nThe third fake answer, updating the existing endpoint to use the new endpoint configuration, is not sufficient on its own. Without deploying the new model and having production variants for both models in the configuration, proper A/B testing cannot be carried out. \n\nCommon misconceptions might include misunderstanding the difference between target variants pointing to different endpoints and production variants within a single endpoint configuration, or thinking that simply activating the new model is equivalent to A/B testing.", "zhcn": "关于实施A/B测试的正确操作是\"创建包含两个模型生产变体的新终端配置\"与\"将新模型部署至现有终端\"。首项正解通过为两个模型分别创建生产变体配置，使系统能够将传入请求分流至现有模型或新模型，这对在生产环境中直接对比模型性能的A/B测试至关重要。次项正解将新模型部署至现有终端，可确保新模型在生产环境就绪并接收测试请求。\n\n首项错误方案之所以不成立，是因创建指向不同终端的目标变体并非在单一终端实施A/B测试的正确方法。次项错误方案通过更新现有终端激活新模型，实为直接替换旧模型，无法实现并排对比的测试需求。第三项错误方案仅更新终端配置而不部署新模型，且未在配置中设置双模型生产变体，同样无法有效执行A/B测试。常见误解往往混淆了指向不同终端的目标变体与单终端配置内的生产变体概念，或误认为激活新模型即等同于开展A/B测试。"}, "answer": "AC"}, {"id": "248", "question": {"enus": "A data scientist is working on a forecast problem by using a dataset that consists of .csv files that are stored in Amazon S3. The files contain a timestamp variable in the following format: March 1st, 2020, 08:14pm - There is a hypothesis about seasonal differences in the dependent variable. This number could be higher or lower for weekdays because some days and hours present varying values, so the day of the week, month, or hour could be an important factor. As a result, the data scientist needs to transform the timestamp into weekdays, month, and day as three separate variables to conduct an analysis. Which solution requires the LEAST operational overhead to create a new dataset with the added features? ", "zhcn": "一位数据科学家正利用一组存储在Amazon S3中的.csv文件进行预测分析。这些文件中的时间戳变量格式如下：2020年3月1日晚上08点14分。现有假设认为因变量存在季节性差异——由于某些日期与时段会呈现波动数值，工作日的数据可能偏高或偏低，因此星期几、月份或具体小时可能成为关键影响因素。为此，数据科学家需将时间戳拆解为星期、月份和日期三个独立变量以便分析。在创建包含新增特征的数据集时，下列哪种方案能实现最低的操作复杂度？"}, "option": [{"option_text": {"zhcn": "创建亚马逊EMR集群。编写PySpark代码，实现以下功能：将时间戳变量作为字符串读取，进行数据转换并生成新变量，最终将数据集以新文件形式保存至亚马逊S3存储空间。", "enus": "Create an Amazon EMR cluster. Develop PySpark code that can read the timestamp variable as a string, transform and create the new  variables, and save the dataset as a new file in Amazon S3."}, "option_flag": false}, {"option_text": {"zhcn": "在Amazon SageMaker中创建数据处理作业。编写Python代码，使其能够读取时间戳字符串变量，进行转换并生成新变量，最终将数据集以新文件形式保存至Amazon S3存储空间。", "enus": "Create a processing job in Amazon SageMaker. Develop Python code that can read the timestamp variable as a string, transform and  create the new variables, and save the dataset as a new file in Amazon S3."}, "option_flag": false}, {"option_text": {"zhcn": "在 Amazon SageMaker Data Wrangler 中创建新的数据流。导入 S3 文件后，运用日期/时间特征化转换功能生成新变量，最终将数据集以新文件形式保存至 Amazon S3。", "enus": "Create a new fiow in Amazon SageMaker Data Wrangler. Import the S3 file, use the Featurize date/time transform to generate the new  variables, and save the dataset as a new file in Amazon S3."}, "option_flag": true}, {"option_text": {"zhcn": "创建一个AWS Glue作业。编写代码实现以下功能：将时间戳变量作为字符串读取，经转换处理后生成新变量，最终将数据集以新文件形式存储至Amazon S3。", "enus": "Create an AWS Glue job. Develop code that can read the timestamp variable as a string, transform and create the new variables, and  save the dataset as a new file in Amazon S3."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Create a new flow in Amazon SageMaker Data Wrangler. Import the S3 file, use the Featurize date/time transform to generate the new variables, and save the dataset as a new file in Amazon S3.” This solution requires the least operational overhead because Amazon SageMaker Data Wrangler provides a graphical interface. It allows data scientists to perform data transformations like generating new variables from timestamps with a point - and - click approach, eliminating the need to write custom code.\n\nThe fake answer options all involve writing code. Creating an Amazon EMR cluster (to run PySpark code), a processing job in Amazon SageMaker (using Python code), or an AWS Glue job (with custom code) requires more setup and management. For example, with an EMR cluster, one needs to handle cluster configuration, resource management, and security. When using Python code in a SageMaker processing job or an AWS Glue job, there is still the task of writing, debugging, and maintaining the code. These tasks add to the operational overhead, which is why the SageMaker Data Wrangler option is the best choice.", "zhcn": "针对该问题，正确答案是「在Amazon SageMaker Data Wrangler中创建新数据流，导入S3文件后运用日期/时间特征化转换功能生成新变量，并将数据集作为新文件保存至Amazon S3」。这一方案具有最低的操作管理成本，因为Amazon SageMaker Data Wrangler提供了可视化图形界面，使数据科学家能够通过点击操作实现数据转换（如从时间戳生成新变量），无需编写自定义代码。\n\n而其余干扰选项均涉及编码工作：无论是创建Amazon EMR集群（运行PySpark代码）、在Amazon SageMaker中配置处理任务（使用Python代码），还是部署AWS Glue任务（需自定义代码），都需要更多配置和管理工作。例如使用EMR集群时，需处理集群配置、资源管理及安全设置；而通过SageMaker处理任务或AWS Glue任务执行Python代码时，仍需承担编写、调试和维护代码的工作。这些额外操作都会增加管理负担，因此采用SageMaker Data Wrangler是最佳选择。"}, "answer": "C"}, {"id": "249", "question": {"enus": "A manufacturing company has a production line with sensors that collect hundreds of quality metrics. The company has stored sensor data and manual inspection results in a data lake for several months. To automate quality control, the machine learning team must build an automated mechanism that determines whether the produced goods are good quality, replacement market quality, or scrap quality based on the manual inspection results. Which modeling approach will deliver the MOST accurate prediction of product quality? ", "zhcn": "一家制造企业的生产线上装有传感器，可采集数百项质量指标。该公司已将数月内的传感器数据与人工检测结果存储于数据湖中。为实现质量控制的自动化，机器学习团队需要建立一种自动判别机制，依据人工检测结果判定产品属于优质品、替换市场品还是废品。请问采用哪种建模方法能够最精准地预测产品质量？"}, "option": [{"option_text": {"zhcn": "Amazon SageMaker DeepAR 时间序列预测算法", "enus": "Amazon SageMaker DeepAR forecasting algorithm"}, "option_flag": false}, {"option_text": {"zhcn": "Amazon SageMaker XGBoost算法", "enus": "Amazon SageMaker XGBoost algorithm"}, "option_flag": true}, {"option_text": {"zhcn": "Amazon SageMaker 潜在狄利克雷分布（LDA）算法", "enus": "Amazon SageMaker Latent Dirichlet Allocation (LDA) algorithm"}, "option_flag": false}, {"option_text": {"zhcn": "卷积神经网络与ResNet。", "enus": "A convolutional neural network (CNN) and ResNet"}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is ‘Amazon SageMaker XGBoost algorithm’. XGBoost is a powerful gradient - boosting framework well - suited for classification tasks. In this case, the goal is to classify the produced goods into different quality categories (good quality, replacement market quality, or scrap quality) based on sensor data and manual inspection results. It can handle a large number of input features (hundreds of quality metrics from sensors), and it is known for its efficiency and accuracy in dealing with tabular data, which is likely the format of the stored sensor and inspection data.\n\nThe ‘Amazon SageMaker DeepAR forecasting algorithm’ is designed for time - series forecasting, not for classification tasks. So, it is not appropriate for determining product quality categories. The ‘Amazon SageMaker Latent Dirichlet Allocation (LDA) algorithm’ is mainly used for topic modeling in text data, and has no direct relevance to classifying product quality based on sensor and inspection data. A ‘convolutional neural network (CNN) and ResNet’ are typically used for image - related tasks, such as image classification or object detection. Since the data here is sensor and inspection data (not images), these models are not the best fit. The nature of the problem (classification with tabular data) is what makes XGBoost the most suitable option, distinguishing it from the fake answer options.", "zhcn": "对于该问题的正确答案是\"Amazon SageMaker XGBoost算法\"。XGBoost作为一种强大的梯度提升框架，尤其适用于分类场景。在本案例中，需要根据传感器数据与人工检测结果，将产品划分为不同质量等级（优质品、替换市场品或废品）。该算法能够处理海量输入特征（来自传感器的数百项质量指标），并以处理表格数据时的高效性与准确性著称——而这正是传感器与检测数据最可能的存储形式。\n\n相比之下，\"Amazon SageMaker DeepAR预测算法\"专为时间序列预测设计，并不适用于品质分类任务；\"Amazon SageMaker隐狄利克雷分布算法\"主要应用于文本主题建模，与基于传感数据的品质分类无直接关联；而\"卷积神经网络及ResNet\"通常用于图像分类或目标检测等视觉任务。鉴于当前数据源为传感器读数与检测记录（非图像数据），上述模型均非最优选择。正是该问题本身的性质——基于表格数据的分类需求——使得XGBoost成为区别于其他干扰项的最适宜方案。"}, "answer": "B"}, {"id": "250", "question": {"enus": "A healthcare company wants to create a machine learning (ML) model to predict patient outcomes. A data science team developed an ML model by using a custom ML library. The company wants to use Amazon SageMaker to train this model. The data science team creates a custom SageMaker image to train the model. When the team tries to launch the custom image in SageMaker Studio, the data scientists encounter an error within the application. Which service can the data scientists use to access the logs for this error? ", "zhcn": "一家医疗健康公司计划开发一个用于预测患者预后的机器学习模型。数据科学团队使用定制化的机器学习库构建了该模型，现拟通过Amazon SageMaker平台进行模型训练。为此，团队专门创建了适用于SageMaker的自定义镜像。然而，当团队尝试在SageMaker Studio中启动该定制镜像时，应用程序内出现了错误。此时，数据科学团队应通过何种服务获取该错误的相关日志信息？"}, "option": [{"option_text": {"zhcn": "亚马逊S3", "enus": "Amazon S3"}, "option_flag": true}, {"option_text": {"zhcn": "亚马逊弹性块存储（Amazon EBS）", "enus": "Amazon Elastic Block Store (Amazon EBS)"}, "option_flag": false}, {"option_text": {"zhcn": "AWS CloudTrail", "enus": "AWS CloudTrail"}, "option_flag": false}, {"option_text": {"zhcn": "亚马逊云监控", "enus": "Amazon CloudWatch"}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is ‘Amazon S3’. When using a custom SageMaker image to train a model in SageMaker Studio and encountering an error, the application logs are typically stored in Amazon S3. SageMaker is configured to save various outputs, including logs, to an S3 bucket during the training process, making it the go - to place to access these error logs.\n\n‘Amazon Elastic Block Store (Amazon EBS)’ is mainly used for providing block - level storage volumes for Amazon EC2 instances. It is not designed for storing application logs from SageMaker, so it is an incorrect option. ‘AWS CloudTrail’ is focused on recording API calls made in an AWS account, which helps with security analysis, resource change tracking, and compliance auditing, but it doesn't store the application - level error logs. ‘Amazon CloudWatch’ is used for monitoring AWS resources and the applications running on them, but SageMaker stores its custom image training logs in S3 rather than CloudWatch. This is why ‘Amazon S3’ is the real answer, differentiating it from the fake options.", "zhcn": "问题的正确答案是“Amazon S3”。当使用自定义SageMaker镜像在SageMaker Studio中训练模型并遇到错误时，应用程序日志通常存储在Amazon S3中。SageMaker默认配置会将训练过程中的各类输出（包括日志）保存至S3存储桶，这使得S3成为获取这些错误日志的首选途径。\n\n“Amazon Elastic Block Store（Amazon EBS）”主要用于为Amazon EC2实例提供块级存储卷，其设计初衷并非存储来自SageMaker的应用程序日志，因此并非正确选项。“AWS CloudTrail”侧重于记录AWS账户中的API调用活动，适用于安全分析、资源变更追踪与合规审计，但不会存储应用层级的错误日志。“Amazon CloudWatch”用于监控AWS资源及运行其上的应用程序，然而SageMaker的自定义镜像训练日志实际保存在S3而非CloudWatch中。\n\n正因如此，“Amazon S3”才是本题的真正答案，与其他干扰选项有着本质区别。"}, "answer": "A"}, {"id": "251", "question": {"enus": "A data scientist wants to build a financial trading bot to automate investment decisions. The financial bot should recommend the quantity and price of an asset to buy or sell to maximize long-term profit. The data scientist will continuously stream financial transactions to the bot for training purposes. The data scientist must select the appropriate machine learning (ML) algorithm to develop the financial trading bot. Which type of ML algorithm will meet these requirements? ", "zhcn": "一位数据科学家计划开发一款金融交易机器人，以实现投资决策的自动化。该金融机器人需具备推荐资产买卖数量与价格的功能，从而最大化长期收益。数据科学家将持续向该机器人输入实时金融交易数据以供训练。在此过程中，选择恰当的机器学习算法至关重要。请问何种类型的机器学习算法能够满足上述需求？"}, "option": [{"option_text": {"zhcn": "监督式学习", "enus": "Supervised learning"}, "option_flag": false}, {"option_text": {"zhcn": "无监督学习", "enus": "Unsupervised learning"}, "option_flag": false}, {"option_text": {"zhcn": "半监督学习", "enus": "Semi-supervised learning"}, "option_flag": true}, {"option_text": {"zhcn": "强化学习", "enus": "Reinforcement learning"}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is ‘Semi - supervised learning’. In this scenario, the data scientist has a continuous stream of financial transactions for training. Semi - supervised learning is suitable because it can make use of both labeled and unlabeled data. In financial trading, while some historical transactions may have clear labels (e.g., profitable or non - profitable trades), a large amount of the streaming data may be unlabeled. This algorithm can leverage the limited labeled data along with the abundant unlabeled data to build a model that can recommend the quantity and price of an asset to buy or sell for long - term profit.\n\n‘Supervised learning’ requires all training data to have labels, which is not practical in a continuous streaming financial data context where it's difficult to label every single transaction immediately. ‘Unsupervised learning’ is used to find patterns in data without labels, but it doesn't directly address the goal of making decisions on asset quantity and price for profit maximization. ‘Reinforcement learning’ involves an agent interacting with an environment and receiving rewards, which is not the most straightforward approach for this case as it doesn't effectively utilize the available labeled and unlabeled data from the financial transactions. This is why ‘Semi - supervised learning’ is the real answer option, distinguishing it from the fake options.", "zhcn": "对于该问题的正确答案是“半监督学习”。在此场景中，数据科学家拥有持续流动的金融交易数据用于训练。半监督学习的优势在于能够同时利用标注数据和未标注数据——金融交易场景中，部分历史交易虽有明确标签（如盈利/非盈利交易），但海量流数据往往缺乏实时标注。该算法可借助有限的标注数据与大量未标注数据，构建能够推荐资产买卖数量与价格以追求长期收益的模型。\n\n相较而言，“监督学习”要求所有训练数据均带标签，这在持续流动的金融数据场景中并不现实，因为难以为每笔交易即时标注；“无监督学习”虽能探索无标签数据的潜在模式，却无法直接实现资产交易决策的优化目标；“强化学习”需通过智能体与环境交互获取奖励机制，对此案例而言并非最直接有效的方案，因其未能充分利用交易数据中已有的标注与未标注信息。正因如此，“半监督学习”才是真正契合题意的选项，与其他干扰项形成本质区别。"}, "answer": "C"}, {"id": "252", "question": {"enus": "A manufacturing company wants to create a machine learning (ML) model to predict when equipment is likely to fail. A data science team already constructed a deep learning model by using TensorFlow and a custom Python script in a local environment. The company wants to use Amazon SageMaker to train the model. Which TensorFlow estimator configuration will train the model MOST cost-effectively? ", "zhcn": "一家制造企业希望构建机器学习模型来预测设备故障发生时间。数据科学团队已在本地环境中使用TensorFlow及自定义Python脚本完成了深度学习模型的搭建。现企业计划借助Amazon SageMaker平台进行模型训练，下列哪种TensorFlow评估器配置方案能实现最优成本效益？"}, "option": [{"option_text": {"zhcn": "启用SageMaker训练编译器时，只需在参数中添加`compiler_config=TrainingCompilerConfig()`配置项，并将训练脚本通过TensorFlow的`fit()`方法传递给估算器即可。", "enus": "Turn on SageMaker Training Compiler by adding compiler_config=TrainingCompilerConfig() as a parameter. Pass the script to the  estimator in the call to the TensorFlow fit() method."}, "option_flag": false}, {"option_text": {"zhcn": "启用SageMaker训练编译器只需在参数中添加`compiler_config=TrainingCompilerConfig()`即可。通过将`use_spot_instances`参数设为True可开启托管Spot训练。最后在调用TensorFlow的fit()方法时，将训练脚本传递给评估器即可完成配置。", "enus": "Turn on SageMaker Training Compiler by adding compiler_config=TrainingCompilerConfig() as a parameter. Turn on managed spot  training by setting the use_spot_instances parameter to True. Pass the script to the estimator in the call to the TensorFlow fit() method."}, "option_flag": false}, {"option_text": {"zhcn": "调整训练脚本以采用分布式数据并行模式。为分布参数设定合适的数值，并将该脚本传入估算器的TensorFlow fit()方法调用中。", "enus": "Adjust the training script to use distributed data parallelism. Specify appropriate values for the distribution parameter. Pass the script  to the estimator in the call to the TensorFlow fit() method."}, "option_flag": false}, {"option_text": {"zhcn": "开启SageMaker训练编译器功能时，只需在参数中添加`compiler_config=TrainingCompilerConfig()`即可。将`MaxWaitTimeInSeconds`参数的值设置为与`MaxRuntimeInSeconds`参数保持一致。最后通过调用TensorFlow的`fit()`方法将训练脚本传递给估算器。", "enus": "Turn on SageMaker Training Compiler by adding compiler_config=TrainingCompilerConfig() as a parameter. Set the  MaxWaitTimeInSeconds parameter to be equal to the MaxRuntimeInSeconds parameter. Pass the script to the estimator in the call to the  TensorFlow fit() method."}, "option_flag": true}], "analysis": {"enus": "The correct answer to the question is the option that turns on SageMaker Training Compiler, sets the MaxWaitTimeInSeconds parameter equal to the MaxRuntimeInSeconds parameter, and passes the script to the estimator in the call to the TensorFlow fit() method. \n\nSageMaker Training Compiler optimizes the model training process, reducing the overall training time and thus cost. Setting MaxWaitTimeInSeconds equal to MaxRuntimeInSeconds ensures that the training job doesn't wait for additional resources longer than it actually needs to run, which helps in avoiding unnecessary costs. \n\nThe first fake option lacks the crucial step of setting MaxWaitTimeInSeconds equal to MaxRuntimeInSeconds, so it may incur additional waiting costs. The second fake option, while using managed spot training can save costs, spot instances can be interrupted, which might lead to incomplete training and additional costs to restart. The third fake option focuses on distributed data parallelism, which is more about speeding up training in a multi - node setup rather than directly addressing cost - effectiveness in the most optimal way. \n\nThe key factor distinguishing the real answer is the combination of compiler optimization and the proper setting of the time parameters to minimize unnecessary costs, which the fake options do not fully address.", "zhcn": "正确答案需同时满足三个条件：启用SageMaker训练编译器、将MaxWaitTimeInSeconds参数设置为与MaxRuntimeInSeconds参数相等，并在调用TensorFlow的fit()方法时将脚本传递给评估器。SageMaker训练编译器能优化模型训练流程，缩短总体训练时间从而降低成本。将最大等待时间设为与最大运行时间相等，可确保训练任务不会在资源等待上耗费超出实际需要的时长，从而避免不必要的开销。  \n\n第一个错误选项未设置最大等待时间与最大运行时间相等，可能导致额外等待成本。第二个错误选项虽采用托管Spot训练节省成本，但Spot实例可能被中断，致使训练不完整并产生重启成本。第三个错误选项侧重分布式数据并行技术，这主要加速多节点训练而非以最优方式实现成本效益。  \n\n真正答案的关键在于结合编译器优化与时间参数的正确设置，以最大限度减少非必要成本——这一核心优势正是错误选项未能全面体现的。"}, "answer": "D"}, {"id": "253", "question": {"enus": "An automotive company uses computer vision in its autonomous cars. The company trained its object detection models successfully by using transfer learning from a convolutional neural network (CNN). The company trained the models by using PyTorch through the Amazon SageMaker SDK. The vehicles have limited hardware and compute power. The company wants to optimize the model to reduce memory, battery, and hardware consumption without a significant sacrifice in accuracy. Which solution will improve the computational eficiency of the models? ", "zhcn": "一家汽车制造商在其自动驾驶车辆中应用了计算机视觉技术。该公司通过迁移学习的方法，成功基于卷积神经网络训练出了目标检测模型，并借助亚马逊SageMaker软件开发工具包使用PyTorch框架完成了模型训练。由于车载硬件配置与算力有限，该企业希望在保持模型精度的前提下，通过优化手段降低内存占用、能耗及硬件负载。下列哪种方案能有效提升模型的计算效率？"}, "option": [{"option_text": {"zhcn": "运用亚马逊云监控指标洞察SageMaker训练过程中的权重、梯度、偏置与激活输出，依据训练数据计算滤波器等级。通过剪枝技术剔除低阶滤波器，基于优化后的滤波器集合重新设定权重，最终使用精简后的模型启动新一轮训练任务。", "enus": "Use Amazon CloudWatch metrics to gain visibility into the SageMaker training weights, gradients, biases, and activation outputs.  Compute the filter ranks based on the training information. Apply pruning to remove the low-ranking filters. Set new weights based on the  pruned set of filters. Run a new training job with the pruned model."}, "option_flag": false}, {"option_text": {"zhcn": "使用Amazon SageMaker Ground Truth构建并运行数据标注工作流。通过标注流程收集更丰富的带标签数据集，随后结合既有训练数据与新标注数据，启动新一轮模型训练任务。", "enus": "Use Amazon SageMaker Ground Truth to build and run data labeling workfiows. Collect a larger labeled dataset with the labelling  workfiows. Run a new training job that uses the new labeled data with previous training data."}, "option_flag": false}, {"option_text": {"zhcn": "借助 Amazon SageMaker Debugger，您可以清晰洞察训练过程中的权重、梯度、偏置及激活输出。基于训练信息计算滤波器权重等级后，可对低阶滤波器实施剪枝处理。剪枝操作完成后，根据优化后的滤波器集重新设定权重参数，即可启动新一轮针对剪枝后模型的训练任务。", "enus": "Use Amazon SageMaker Debugger to gain visibility into the training weights, gradients, biases, and activation outputs. Compute the  filter ranks based on the training information. Apply pruning to remove the low-ranking filters. Set the new weights based on the pruned  set of filters. Run a new training job with the pruned model."}, "option_flag": true}, {"option_text": {"zhcn": "在企业部署模型后，运用亚马逊SageMaker模型监控器来洞察模型的延迟指标与资源开销指标。提升模型学习速率，并启动新一轮训练任务。", "enus": "Use Amazon SageMaker Model Monitor to gain visibility into the ModelLatency metric and OverheadLatency metric of the model after  the company deploys the model. Increase the model learning rate. Run a new training job."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to use Amazon SageMaker Debugger to gain visibility into the training weights, gradients, biases, and activation outputs, compute filter ranks, apply pruning, set new weights, and run a new training job with the pruned model. This solution directly addresses the need to optimize the model for reduced memory, battery, and hardware consumption. Pruning low - ranking filters reduces the model's complexity, which in turn decreases the computational resources required without sacrificing much accuracy.\n\nThe first fake option suggests using Amazon CloudWatch metrics. CloudWatch is mainly for monitoring system - level metrics and resource utilization, not for in - depth visibility into the internal aspects of a model's training like weights, gradients, etc. So, it is not suitable for the model pruning process.\n\nThe second fake option focuses on collecting a larger labeled dataset using Amazon SageMaker Ground Truth. While more data can improve model accuracy in some cases, it does not directly optimize the model for reduced computational requirements. Instead, it may increase memory and compute needs during training.\n\nThe third fake option uses Amazon SageMaker Model Monitor to monitor latency metrics after deployment and then increases the learning rate. Monitoring latency after deployment is too late for the goal of pre - deployment optimization, and increasing the learning rate may not necessarily lead to reduced computational consumption and could even cause the model to be unstable.\n\nIn conclusion, the real answer option is the most appropriate as it directly targets the optimization of the model's computational efficiency.", "zhcn": "针对该问题的正确答案是：利用Amazon SageMaker Debugger获取训练过程中的权重、梯度、偏置和激活输出数据，通过计算过滤器等级实施剪枝操作，设置新权重后对剪枝模型启动新一轮训练。此方案直指优化目标——降低模型对内存、电池及硬件资源的消耗。通过剔除低阶过滤器简化模型结构，能在保持精度的同时显著减少计算资源需求。\n\n首个干扰项建议采用Amazon CloudWatch指标。然CloudWatch主要用于系统级指标与资源监控，无法深入洞察模型训练内部的权重、梯度等参数，故不适用于模型剪枝流程。\n\n第二干扰项主张通过Amazon SageMaker Ground Truth收集更多标注数据。虽然增加数据量在某些情况下能提升精度，但并未直接针对降低计算需求进行优化，反而可能加剧训练过程中的内存与算力消耗。\n\n第三干扰项提出使用Amazon SageMaker Model Monitor监测部署后的延迟指标，继而提高学习率。对于部署前优化的目标而言，延迟监控已属事后行为，且提升学习率不仅无法确保降低计算消耗，还可能引发模型不稳定。\n\n综上，正确答案选项因其精准聚焦于提升模型计算效率而最为适宜。"}, "answer": "C"}, {"id": "254", "question": {"enus": "A data scientist wants to improve the fit of a machine learning (ML) model that predicts house prices. The data scientist makes a first attempt to fit the model, but the fitted model has poor accuracy on both the training dataset and the test dataset. Which steps must the data scientist take to improve model accuracy? (Choose three.) ", "zhcn": "一位数据科学家希望优化预测房价的机器学习模型拟合效果。初次尝试建模后，发现模型在训练集和测试集上的预测精度均不理想。为提升模型准确性，该数据科学家应采取以下哪三项措施？（请选择三项）"}, "option": [{"option_text": {"zhcn": "增强模型所使用的正则化强度。", "enus": "Increase the amount of regularization that the model uses."}, "option_flag": false}, {"option_text": {"zhcn": "降低模型所使用的正则化强度。", "enus": "Decrease the amount of regularization that the model uses."}, "option_flag": true}, {"option_text": {"zhcn": "增加模型训练所用的样本数量。", "enus": "Increase the number of training examples that that model uses."}, "option_flag": false}, {"option_text": {"zhcn": "增加模型所用的测试样例数量。", "enus": "Increase the number of test examples that the model uses."}, "option_flag": true}, {"option_text": {"zhcn": "提升模型所采用的特征数量。", "enus": "Increase the number of model features that the model uses."}, "option_flag": false}, {"option_text": {"zhcn": "精简模型所使用的特征数量。", "enus": "Decrease the number of model features that the model uses."}, "option_flag": true}], "analysis": {"enus": "The question is about steps to improve the accuracy of a machine - learning model for house price prediction that has poor accuracy on both training and test datasets.\n\nThe real answer options are correct for the following reasons:\n- Decreasing the amount of regularization can help. Regularization is used to prevent overfitting, but if the model has poor accuracy on both datasets, it might be underfitting. Reducing regularization allows the model to be more flexible and better capture patterns in the data.\n- Increasing the number of test examples can improve model evaluation. A larger test set provides a more comprehensive view of the model's performance, enabling more accurate assessment and potentially better fine - tuning.\n- Decreasing the number of model features can simplify the model. If there are too many features, the model can become overly complex and confused, leading to poor performance. Reducing unnecessary features can improve the model's ability to generalize.\n\nThe fake answer options are incorrect:\n- Increasing the amount of regularization would make the model even more restricted. Since the model is already underperforming on both datasets, more regularization will exacerbate the underfitting problem.\n- While increasing the number of training examples can sometimes improve model performance, the question doesn't suggest that the issue is due to a lack of training data. The problem seems to be related to model structure or flexibility.\n- Increasing the number of model features can make the model more complex. Given the poor performance on both datasets, adding more features may lead to overfitting or make the underfitting problem worse as the model struggles to handle the increased complexity.\n\nCommon misconceptions might include always thinking that more data (training examples) or more features will improve performance without considering the current state of the model. Also, overlooking the role of regularization in the context of underfitted models can lead to choosing the wrong option of increasing it.", "zhcn": "问题涉及如何改进一个在训练集和测试集上表现均不理想的房价预测机器学习模型。正确选项的判断依据如下：  \n\n- **降低正则化强度**：正则化本用于防止过拟合，但若模型在两类数据上均表现不佳，可能是欠拟合所致。减弱正则化能使模型更灵活，从而更好地捕捉数据中的规律。  \n- **增加测试样本数量**：更大的测试集可更全面评估模型性能，有助于精准判断效果并进行优化。  \n- **减少模型特征数量**：过多特征会导致模型过度复杂、混淆核心关系。精简特征能提升模型的泛化能力。  \n\n错误选项的不合理性在于：  \n\n- **增强正则化**：若模型已存在欠拟合，进一步限制其灵活性只会加剧问题。  \n- **增加训练样本**：题干未表明问题源于训练数据不足，症结更可能在于模型结构或灵活性。  \n- **增加特征数量**：在模型已有表现不佳的情况下，添加特征可能引发过拟合，或因复杂度上升而加重欠拟合。  \n\n常见误解包括：盲目认为更多训练数据或特征必能提升性能，而忽视模型当前状态；或未意识到对于欠拟合模型，增强正则化会适得其反。"}, "answer": "BDF"}, {"id": "255", "question": {"enus": "A car company is developing a machine learning solution to detect whether a car is present in an image. The image dataset consists of one million images. Each image in the dataset is 200 pixels in height by 200 pixels in width. Each image is labeled as either having a car or not having a car. Which architecture is MOST likely to produce a model that detects whether a car is present in an image with the highest accuracy? ", "zhcn": "一家汽车公司正着手开发一套机器学习系统，用于识别图像中是否出现汽车。该图像数据集包含一百万张样本，每张图像尺寸为200像素×200像素，并已标注是否存在汽车。下列哪种架构最有可能以最高准确率实现车辆存在性的图像识别？"}, "option": [{"option_text": {"zhcn": "采用深度卷积神经网络（CNN）分类器对图像进行识别处理。网络末端需设置线性输出层，用于生成图像中包含汽车的概率估值。", "enus": "Use a deep convolutional neural network (CNN) classifier with the images as input. Include a linear output layer that outputs the  probability that an image contains a car."}, "option_flag": false}, {"option_text": {"zhcn": "采用深度卷积神经网络（CNN）分类器对图像进行识别处理。网络末端需设置柔性最大值输出层，用于生成图像中是否存在车辆的置信概率。", "enus": "Use a deep convolutional neural network (CNN) classifier with the images as input. Include a softmax output layer that outputs the  probability that an image contains a car."}, "option_flag": false}, {"option_text": {"zhcn": "采用深度多层感知机（MLP）分类器，以图像作为输入。输出层采用线性设计，用于计算图像中包含汽车的概率。", "enus": "Use a deep multilayer perceptron (MLP) classifier with the images as input. Include a linear output layer that outputs the probability  that an image contains a car."}, "option_flag": false}, {"option_text": {"zhcn": "采用深度多层感知器（MLP）分类器，以图像作为输入。该模型包含一个softmax输出层，用于计算图像中包含汽车的概率。", "enus": "Use a deep multilayer perceptron (MLP) classifier with the images as input. Include a softmax output layer that outputs the probability  that an image contains a car."}, "option_flag": true}], "analysis": {"enus": "The correct answer to the question is to use a deep multilayer perceptron (MLP) classifier with the images as input and include a softmax output layer that outputs the probability that an image contains a car. \n\nA softmax output layer is ideal for binary classification problems like this one (car present or not present) as it squashes the output into a probability distribution, making it easy to interpret the likelihood of a car being in the image. \n\nFor the fake options, using a linear output layer in both the MLP and CNN scenarios is less suitable. A linear output does not provide a proper probability distribution for classification tasks, and it can output values outside the [0, 1] range, which is not appropriate for representing probabilities. \n\nAlthough CNNs are great for image - related tasks due to their ability to capture spatial features, in this case, the simple binary classification of car presence can be effectively handled by an MLP. Also, the use of a linear output layer in the CNN options makes them less accurate for this specific problem. This is why the real answer option, with an MLP and a softmax output layer, is the most likely to produce the highest - accuracy model for this task.", "zhcn": "对于该问题的正确答案是采用深度多层感知机（MLP）分类器，以图像作为输入，并配备softmax输出层来生成图像包含汽车的概率值。Softmax输出层特别适合此类二元分类问题（存在汽车或不存在），它能将输出结果转化为概率分布，从而直观呈现图像中汽车存在的可能性。而错误选项中，无论在MLP还是CNN架构里使用线性输出层均不够理想——线性输出无法为分类任务提供规范的概率分布，且可能产生超出[0,1]范围的值，这与概率表示的基本要求相悖。尽管CNN凭借其捕捉空间特征的能力在图像处理任务中表现卓越，但针对当前简单的汽车存在性二元分类问题，MLP已能有效解决。此外，CNN错误选项中的线性输出层设计也会降低模型在此特定任务中的精确度。正因如此，采用MLP配合softmax输出层的方案最有可能为本任务构建出最高精度的模型。"}, "answer": "D"}, {"id": "256", "question": {"enus": "A company is creating an application to identify, count, and classify animal images that are uploaded to the company’s website. The company is using the Amazon SageMaker image classification algorithm with an ImageNetV2 convolutional neural network (CNN). The solution works well for most animal images but does not recognize many animal species that are less common. The company obtains 10,000 labeled images of less common animal species and stores the images in Amazon S3. A machine learning (ML) engineer needs to incorporate the images into the model by using Pipe mode in SageMaker. Which combination of steps should the ML engineer take to train the model? (Choose two.) ", "zhcn": "某公司正在开发一款应用程序，用于识别、计数和分类用户上传至其网站的动物图像。该公司采用亚马逊SageMaker图像分类算法，并搭配ImageNetV2卷积神经网络（CNN）架构。该解决方案对大多数常见动物图像识别效果良好，但对许多较为罕见的动物物种却难以辨识。公司现已获取一万张稀有动物物种的标注图像，并存储于亚马逊S3服务中。机器学习工程师需通过SageMaker的Pipe模式将这些图像数据整合到模型中。请问该工程师应采取哪两种步骤组合来完成模型训练？（请选择两项正确答案）"}, "option": [{"option_text": {"zhcn": "采用ResNet架构。通过随机初始化网络权重，开启完整训练模式。", "enus": "Use a ResNet model. Initiate full training mode by initializing the network with random weights."}, "option_flag": false}, {"option_text": {"zhcn": "采用SageMaker图像分类算法中提供的Inception模型进行实现。", "enus": "Use an Inception model that is available with the SageMaker image classification algorithm."}, "option_flag": true}, {"option_text": {"zhcn": "创建一个包含图像文件及对应类别标签列表的.lst文件，并将该文件上传至Amazon S3存储空间。", "enus": "Create a .lst file that contains a list of image files and corresponding class labels. Upload the .lst file to Amazon S3."}, "option_flag": false}, {"option_text": {"zhcn": "启动迁移学习。利用较为稀有物种的图像数据对模型进行训练。", "enus": "Initiate transfer learning. Train the model by using the images of less common species."}, "option_flag": true}, {"option_text": {"zhcn": "采用JSON Lines格式的增强清单文件。", "enus": "Use an augmented manifest file in JSON Lines format."}, "option_flag": false}], "analysis": {"enus": "The correct answers to the question are ‘Use an Inception model that is available with the SageMaker image classification algorithm.’ and ‘Initiate transfer learning. Train the model by using the images of less common species.’ \n\nFor the Inception model, it is well - suited for image classification tasks and is available within the SageMaker image classification algorithm. It can effectively handle the classification of the less - common animal species. Transfer learning is a powerful technique where we can leverage pre - trained models on large datasets like ImageNetV2. By initiating transfer learning and training the model with the 10,000 labeled images of less - common species, we can fine - tune the model to better recognize these less - common animals.\n\nThe option ‘Use a ResNet model. Initiate full training mode by initializing the network with random weights.’ is incorrect because full training from scratch with random weights requires a large amount of data and computational resources. Given that we only have 10,000 images of less - common species, transfer learning is a more practical approach. \n\nThe option ‘Create a.lst file that contains a list of image files and corresponding class labels. Upload the.lst file to Amazon S3.’ is wrong as Pipe mode in SageMaker doesn't typically rely on.lst files. Pipe mode is designed to stream data in a more efficient way, and.lst files are more commonly used in File mode.\n\nThe option ‘Use an augmented manifest file in JSON Lines format.’ is not correct because the question specifically asks about incorporating the images using Pipe mode, and the use of an augmented manifest file doesn't directly address the key steps for training the model to recognize less - common species in this context.\n\nCommon misconceptions might lead one to choose the fake options. For example, some might think that full training with a ResNet model is always the best approach without considering the data limitations. Others might not be aware of the differences between Pipe mode and File mode and thus wrongly choose options related to.lst files. And there could be a misunderstanding about the role of augmented manifest files in the given training scenario.", "zhcn": "该问题的正确答案是：\"使用SageMaker图像分类算法提供的Inception模型\"与\"启动迁移学习，利用不常见物种图像训练模型\"。Inception模型特别适合图像分类任务，可通过SageMaker图像分类算法调用，能有效处理稀有动物物种的分类需求。迁移学习是一种高效技术，可借助ImageNetV2等大型数据集上预训练的模型。通过启动迁移学习并运用1万张稀有物种标注图像进行训练，我们能对模型进行精细化调整，提升其识别稀有动物的能力。\n\n而\"使用ResNet模型，通过随机初始化权重启动全训练模式\"这一选项并不恰当。因为采用随机权重的全训练模式需要海量数据和计算资源，鉴于我们仅掌握1万张稀有物种图像，迁移学习是更切实际的选择。\n\n\"创建包含图像文件及对应类别标签的.lst文件并上传至Amazon S3\"的选项也存在谬误。SageMaker的Pipe模式通常不依赖.lst文件，该模式专为更高效的数据流传输设计，而.lst文件更多应用于文件模式场景。\n\n至于\"使用JSON Lines格式的增强清单文件\"这一选项，由于题目明确要求通过Pipe模式整合图像数据，且增强清单文件在此情境下并未直接涉及训练模型识别稀有物种的核心步骤，故该选择并不成立。\n\n常见误解可能导致误选干扰项。例如部分使用者可能忽视数据量限制，误以为ResNet全训练始终是最优解；也有人因未能区分Pipe模式与文件模式的差异，错误选择涉及.lst文件的方案；此外，对于增强清单文件在当前训练场景中的作用可能存在认知偏差。"}, "answer": "BD"}, {"id": "257", "question": {"enus": "A music streaming company is building a pipeline to extract features. The company wants to store the features for ofiine model training and online inference. The company wants to track feature history and to give the company’s data science teams access to the features. Which solution will meet these requirements with the MOST operational eficiency? ", "zhcn": "一家音乐流媒体公司正在构建特征提取流水线。该公司需要存储特征数据以支持离线模型训练与在线推理，同时要求能够追溯特征历史版本，并为内部数据科学团队提供特征数据调用权限。在满足上述需求的前提下，何种解决方案能实现最高运营效率？"}, "option": [{"option_text": {"zhcn": "使用Amazon SageMaker特征存储服务，可集中管理模型训练与推理所需的特征数据。您可创建在线特征库支持实时推理，同时构建离线特征库用于模型训练。此外，需配置IAM角色以便数据科学家安全访问并检索特征组数据。", "enus": "Use Amazon SageMaker Feature Store to store features for model training and inference. Create an online store for online inference.  Create an ofiine store for model training. Create an IAM role for data scientists to access and search through feature groups."}, "option_flag": false}, {"option_text": {"zhcn": "使用亚马逊SageMaker特征存储库来存储模型训练与推理所需的特征量。创建可同时支持在线推理与模型训练的特征在线库，并为数据科学家设立IAM权限角色，使其能够访问并检索特征组数据。", "enus": "Use Amazon SageMaker Feature Store to store features for model training and inference. Create an online store for both online  inference and model training. Create an IAM role for data scientists to access and search through feature groups."}, "option_flag": true}, {"option_text": {"zhcn": "创建一个Amazon S3存储桶用于存放在线推理特征数据，再创建第二个S3存储桶专门存储离线模型训练特征。为这两个S3存储桶启用版本控制功能，并通过标签系统明确区分在线推理特征与离线模型训练特征的用途。使用Amazon Athena查询在线推理所需的S3存储桶数据，同时将离线模型训练对应的S3存储桶关联至SageMaker训练任务。最后配置IAM策略，授予数据科学家同时访问这两个存储桶的权限。", "enus": "Create one Amazon S3 bucket to store online inference features. Create a second S3 bucket to store ofiine model training features.  Turn on versioning for the S3 buckets and use tags to specify which tags are for online inference features and which are for ofiine model  training features. Use Amazon Athena to query the S3 bucket for online inference. Connect the S3 bucket for ofiine model training to a  SageMaker training job. Create an IAM policy that allows data scientists to access both buckets."}, "option_flag": false}, {"option_text": {"zhcn": "创建两个独立的Amazon DynamoDB数据表，分别用于存储在线推理特征与离线模型训练特征。两张表均需启用基于时间版本的记录管理。在线推理时直接查询DynamoDB中的对应数据表，当新的SageMaker训练任务启动时，将数据从DynamoDB迁移至Amazon S3存储。同时需配置IAM策略，允许数据科学家访问这两个数据表。", "enus": "Create two separate Amazon DynamoDB tables to store online inference features and ofiine model training features. Use time-based  versioning on both tables. Query the DynamoDB table for online inference. Move the data from DynamoDB to Amazon S3 when a new  SageMaker training job is launched. Create an IAM policy that allows data scientists to access both tables."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Use Amazon SageMaker Feature Store to store features for model training and inference. Create an online store for both online inference and model training. Create an IAM role for data scientists to access and search through feature groups.” This option offers the most operational efficiency as it simplifies the architecture by using a single online store for both training and inference, reducing management overhead.\n\nThe first fake answer creates both an online and an offline store, which adds complexity and maintenance efforts compared to using a single online store. The second fake answer using two Amazon S3 buckets requires more infrastructure management. Versioning and tagging add to the administrative burden, and querying with Athena and connecting to SageMaker training jobs involve multiple steps. The third fake answer using two DynamoDB tables also has high complexity. Time - based versioning and moving data to S3 for training jobs introduce additional processing and management tasks. \n\nCommon misconceptions might lead one to choose the fake options. For example, the idea that separating online and offline data storage is always better for performance, or that using general - purpose storage like S3 or DynamoDB is more flexible. However, in this case, these approaches sacrifice operational efficiency. SageMaker Feature Store is designed specifically for feature management, providing a more streamlined and efficient solution.", "zhcn": "对于该问题的正确答案是：\"使用Amazon SageMaker Feature Store存储模型训练与推理所需的特征，创建统一在线特征库同时支持实时推理和模型训练，并为数据科学家配置IAM角色以便访问和检索特征组。\" 这一方案能最大程度提升运维效率——通过单一在线特征库简化架构设计，有效降低管理成本。  \n\n相比之下，首个干扰项同时创建在线与离线存储，相较于单一在线存储方案增加了系统复杂度和维护成本。第二个干扰项采用两个Amazon S3存储桶，需投入更多基础设施管理精力：版本控制与标签管理将加重运维负担，且通过Athena查询数据并连接SageMaker训练任务涉及多步操作。第三个干扰项使用两个DynamoDB表同样存在高复杂度问题：基于时间戳的版本控制机制以及将数据迁移至S3进行训练任务都会引入额外处理和管理环节。  \n\n常见误解可能导致选择干扰项，例如认为分离在线离线存储必然提升性能，或误以为采用S3、DynamoDB等通用存储方案更具灵活性。但在此场景下，这些做法实则牺牲了运维效率。SageMaker Feature Store是专为特征管理设计的服务，能提供更简洁高效的解决方案。"}, "answer": "B"}, {"id": "258", "question": {"enus": "A beauty supply store wants to understand some characteristics of visitors to the store. The store has security video recordings from the past several years. The store wants to generate a report of hourly visitors from the recordings. The report should group visitors by hair style and hair color. Which solution will meet these requirements with the LEAST amount of effort? ", "zhcn": "一家美妆用品店希望了解店内顾客的若干特征。该店拥有过去数年的监控录像资料，现需根据录像生成每小时客流量分析报告，要求按顾客的发型与发色进行分类统计。哪种解决方案能以最小工作量满足这些需求？"}, "option": [{"option_text": {"zhcn": "运用目标检测算法从视频画面中定位访客的发丝区域，随后将识别出的头发图像输入ResNet-50模型，用以精准分析发型特征与发色色调。", "enus": "Use an object detection algorithm to identify a visitor’s hair in video frames. Pass the identified hair to an ResNet-50 algorithm to  determine hair style and hair color."}, "option_flag": false}, {"option_text": {"zhcn": "运用目标检测算法从视频帧中识别访客的发型轮廓，随后将检测到的头发区域输入XGBoost算法，以此精准判断其发型样式与发色特征。", "enus": "Use an object detection algorithm to identify a visitor’s hair in video frames. Pass the identified hair to an XGBoost algorithm to  determine hair style and hair color."}, "option_flag": false}, {"option_text": {"zhcn": "采用语义分割算法从视频帧中识别访客的发丝轮廓，随后将识别出的头发区域输入ResNet-50模型，用以精准分析发型特征与发色属性。", "enus": "Use a semantic segmentation algorithm to identify a visitor’s hair in video frames. Pass the identified hair to an ResNet-50 algorithm to  determine hair style and hair color."}, "option_flag": true}, {"option_text": {"zhcn": "采用语义分割算法对视频帧中访客的头发进行识别定位，随后将识别出的头发区域输入XGBoost算法模型，以精准判断其发型与发质特征。", "enus": "Use a semantic segmentation algorithm to identify a visitor’s hair in video frames. Pass the identified hair to an XGBoost algorithm to  determine hair style and hair."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Use a semantic segmentation algorithm to identify a visitor’s hair in video frames. Pass the identified hair to an ResNet - 50 algorithm to determine hair style and hair color.” \n\nSemantic segmentation is more suitable than object detection for this task. Object detection focuses on finding the bounding boxes around objects, while semantic segmentation labels each pixel of an image, allowing for a more precise identification of the hair region in video frames. This accuracy is crucial for the subsequent step of determining hair style and color.\n\nResNet - 50 is a well - known pre - trained convolutional neural network designed for image classification tasks. It has been trained on a large dataset and can effectively handle the task of classifying hair styles and colors. In contrast, XGBoost is a gradient - boosting algorithm mainly used for tabular data and structured data, not as suitable for image - based tasks like determining hair characteristics from video frames.\n\nThe common misconception that might lead to choosing the fake options is not understanding the differences between semantic segmentation and object detection, and misjudging the appropriate algorithms for image - related classification tasks. Object detection might seem like a viable option at first glance, but it lacks the precision of semantic segmentation in this context. Also, wrongly assuming that XGBoost can perform well on image data instead of a specialized image - classification neural network like ResNet - 50 is a pitfall.", "zhcn": "该问题的正确答案是\"使用语义分割算法识别视频帧中访客的头发区域，再将识别出的头发图像输入ResNet-50算法以判断发型与发色\"。相较于目标检测，语义分割技术更适合此项任务。目标检测侧重于框定物体边界，而语义分割能对图像每个像素进行标注，从而更精确地定位视频帧中的头发区域——这种精确性对于后续发型发色判断至关重要。\n\nResNet-50是一种专为图像分类设计的著名预训练卷积神经网络，其经过海量数据集训练，能有效完成发型发色的分类任务。相较之下，XGBoost作为梯度提升算法主要适用于表格化结构化数据，难以胜任从视频帧中提取头发特征这类图像处理任务。\n\n常见误区在于：一是未能明晰语义分割与目标检测的本质差异——目标检测虽初看可行，但在此场景下缺乏语义分割的精准度；二是误判图像分类任务的适用算法，错误认为XGBoost可替代ResNet-50这类专业图像分类神经网络处理图像数据。"}, "answer": "C"}, {"id": "259", "question": {"enus": "A financial services company wants to automate its loan approval process by building a machine learning (ML) model. Each loan data point contains credit history from a third-party data source and demographic information about the customer. Each loan approval prediction must come with a report that contains an explanation for why the customer was approved for a loan or was denied for a loan. The company will use Amazon SageMaker to build the model. Which solution will meet these requirements with the LEAST development effort? ", "zhcn": "一家金融服务公司计划通过构建机器学习模型来实现贷款审批流程的自动化。每笔贷款数据均包含来自第三方数据源的信用记录及客户背景信息。系统在输出每笔贷款审批结果时，需同步生成说明报告，详细解释贷款获批或遭拒的原因。该公司将使用Amazon SageMaker平台开发模型。在满足上述所有要求的前提下，哪种解决方案能以最小的开发量实现这一目标？"}, "option": [{"option_text": {"zhcn": "利用SageMaker模型调试器自动检测预测结果，生成解析说明，并附上详细的诊断报告。", "enus": "Use SageMaker Model Debugger to automatically debug the predictions, generate the explanation, and attach the explanation report."}, "option_flag": true}, {"option_text": {"zhcn": "利用AWS Lambda生成特征重要性及部分依赖图，并借助这些图表制作解释报告予以附呈。", "enus": "Use AWS Lambda to provide feature importance and partial dependence plots. Use the plots to generate and attach the explanation  report."}, "option_flag": false}, {"option_text": {"zhcn": "利用SageMaker Clarify生成解读报告，并将该报告与预测结果一并呈现。", "enus": "Use SageMaker Clarify to generate the explanation report. Attach the report to the predicted results."}, "option_flag": false}, {"option_text": {"zhcn": "利用自定义的亚马逊云监控指标生成解析报告，并将该报告附于预测结果之中。", "enus": "Use custom Amazon CloudWatch metrics to generate the explanation report. Attach the report to the predicted results."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Use SageMaker Model Debugger to automatically debug the predictions, generate the explanation, and attach the explanation report.” This is because the requirement is to meet the needs with the least development effort. SageMaker Model Debugger is designed to directly handle debugging and can be used to generate explanations for predictions, which streamlines the process and reduces the need for additional custom - coding.\n\nThe option of using “AWS Lambda to provide feature importance and partial dependence plots. Use the plots to generate and attach the explanation report” requires writing custom code in Lambda, which increases development effort. “Using SageMaker Clarify to generate the explanation report. Attach the report to the predicted results” is a valid way to get explanations, but it is not as straightforward as using Model Debugger for the given task and may involve more setup. “Using custom Amazon CloudWatch metrics to generate the explanation report. Attach the report to the predicted results” is not a typical or efficient way to generate explanations for ML predictions and would require significant custom development. Thus, SageMaker Model Debugger is the best choice to minimize development effort.", "zhcn": "对于该问题的正确答案是“使用 SageMaker Model Debugger 自动调试预测结果、生成解释说明并附加解释报告”。这是因为方案需以最低开发成本满足需求。SageMaker Model Debugger 专为直接处理调试任务而设计，可用于生成预测解释，从而简化流程并减少额外定制编码工作。  \n\n若选择“通过 AWS Lambda 提供特征重要性和部分依赖图，并利用这些图表生成解释报告”，则需在 Lambda 中编写定制代码，这会增加开发成本。而“使用 SageMaker Clarify 生成解释报告并附加至预测结果”虽能有效获取解释，但针对当前任务不如 Model Debugger 直接，且可能涉及更复杂的配置。至于“通过自定义 Amazon CloudWatch 指标生成解释报告并附加至预测结果”，并非机器学习预测解释的常规高效方法，且需要大量定制开发工作。因此，SageMaker Model Debugger 是实现最低开发成本的最佳选择。"}, "answer": "A"}, {"id": "260", "question": {"enus": "A financial company sends special offers to customers through weekly email campaigns. A bulk email marketing system takes the list of email addresses as an input and sends the marketing campaign messages in batches. Few customers use the offers from the campaign messages. The company does not want to send irrelevant offers to customers. A machine learning (ML) team at the company is using Amazon SageMaker to build a model to recommend specific offers to each customer based on the customer's profile and the offers that the customer has accepted in the past. Which solution will meet these requirements with the MOST operational eficiency? ", "zhcn": "一家金融公司通过每周的电子邮件活动向客户发送专属优惠。批量邮件营销系统将邮箱地址列表作为输入内容，分批发送营销活动信息。但仅有少数客户会使用活动邮件中的优惠。该公司不希望向客户发送无关的优惠信息。该公司的机器学习团队正利用Amazon SageMaker构建模型，旨在根据客户画像及其过往接受的优惠记录，为每位客户推荐特定优惠方案。若要最大程度满足运营效率要求，应当采用何种解决方案？"}, "option": [{"option_text": {"zhcn": "采用因子分解机算法构建模型，为顾客生成个性化优惠推荐方案。通过部署SageMaker服务端点实现优惠推荐的实时计算，并将推荐结果无缝对接至群发邮件营销系统。", "enus": "Use the Factorization Machines algorithm to build a model that can generate personalized offer recommendations for customers.  Deploy a SageMaker endpoint to generate offer recommendations. Feed the offer recommendations into the bulk email marketing system."}, "option_flag": false}, {"option_text": {"zhcn": "运用神经协同过滤算法构建模型，为顾客生成个性化优惠推荐方案。通过部署SageMaker服务端点实现优惠推荐的实时计算，并将推荐结果导入批量邮件营销系统进行精准触达。", "enus": "Use the Neural Collaborative Filtering algorithm to build a model that can generate personalized offer recommendations for customers.  Deploy a SageMaker endpoint to generate offer recommendations. Feed the offer recommendations into the bulk email marketing system."}, "option_flag": false}, {"option_text": {"zhcn": "运用神经协同过滤算法构建模型，为顾客生成个性化优惠推荐方案。通过部署SageMaker批量推理任务来生成推荐结果，并将这些优惠推荐信息导入群发邮件营销系统。", "enus": "Use the Neural Collaborative Filtering algorithm to build a model that can generate personalized offer recommendations for customers.  Deploy a SageMaker batch inference job to generate offer recommendations. Feed the offer recommendations into the bulk email  marketing system."}, "option_flag": true}, {"option_text": {"zhcn": "运用因子分解机算法构建模型，为客户生成个性化优惠推荐方案。通过部署SageMaker批量推理任务来生成推荐结果，并将这些推荐信息导入群发邮件营销系统进行精准投放。", "enus": "Use the Factorization Machines algorithm to build a model that can generate personalized offer recommendations for customers.  Deploy a SageMaker batch inference job to generate offer recommendations. Feed the offer recommendations into the bulk email  marketing system."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Use the Neural Collaborative Filtering algorithm to build a model that can generate personalized offer recommendations for customers. Deploy a SageMaker batch inference job to generate offer recommendations. Feed the offer recommendations into the bulk email marketing system.” \n\nNeural Collaborative Filtering is well - suited for this task as it can effectively handle the problem of generating personalized recommendations based on user - item interactions (customer profiles and past accepted offers). \n\nFor operational efficiency, a batch inference job is the best choice. The company is sending weekly email campaigns, which is a batch - oriented process. Batch inference jobs are designed to process large amounts of data in a single operation, which is ideal for generating recommendations for all customers at once. This is more efficient than using a SageMaker endpoint, which is better for real - time, low - latency predictions. \n\nThe fake options that use a SageMaker endpoint are not the most operationally efficient for this batch - based email campaign scenario. Although Factorization Machines can also be used for recommendation problems, Neural Collaborative Filtering is often more effective for handling complex user - item relationships. A common misconception might be to choose an endpoint thinking that real - time predictions are needed, when in fact, the weekly email campaign allows for batch processing, making a batch inference job the more efficient option.", "zhcn": "针对该问题的正确答案是：\"运用神经协同过滤算法构建模型，为客户生成个性化优惠推荐。通过部署SageMaker批量推理任务生成推荐结果，并将这些推荐信息导入群发邮件营销系统。\" \n\n神经协同过滤技术特别适合此项任务，它能基于用户与项目的交互数据（客户画像及历史接受的优惠记录），有效解决个性化推荐生成难题。从运营效率角度考量，批量推理是最佳选择。由于企业每周执行邮件营销活动，这本身就是批量化处理流程。批量推理任务专为单次处理海量数据而设计，可一次性为所有客户生成推荐，相比适用于实时低延迟预测的SageMaker终端方案更具效率。那些采用SageMaker终端的干扰选项，在这种批量化邮件营销场景下并非最优运营方案。\n\n尽管因子分解机也可用于推荐场景，但神经协同过滤在处理复杂用户-项目关系时通常表现更佳。需要警惕的常见误区是：误认为需要实时预测而选择终端方案，实际上每周邮件营销的周期特性允许采用批处理模式，这使得批量推理任务成为更高效的选择。"}, "answer": "C"}, {"id": "261", "question": {"enus": "A social media company wants to develop a machine learning (ML) model to detect inappropriate or offensive content in images. The company has collected a large dataset of labeled images and plans to use the built-in Amazon SageMaker image classification algorithm to train the model. The company also intends to use SageMaker pipe mode to speed up the training. The company splits the dataset into training, validation, and testing datasets. The company stores the training and validation images in folders that are named Training and Validation, respectively. The folders contain subfolders that correspond to the names of the dataset classes. The company resizes the images to the same size and generates two input manifest files named training.lst and validation.lst, for the training dataset and the validation dataset, respectively. Finally, the company creates two separate Amazon S3 buckets for uploads of the training dataset and the validation dataset. Which additional data preparation steps should the company take before uploading the files to Amazon S3? ", "zhcn": "一家社交媒体公司计划开发机器学习模型，用于检测图像中的不当或冒犯性内容。公司已收集大量带标签的图像数据集，并准备采用亚马逊SageMaker内置的图像分类算法进行模型训练。为加速训练过程，公司将使用SageMaker管道模式。  \n\n公司将数据集划分为训练集、验证集和测试集三部分，并分别将训练图像与验证图像存放于名为\"Training\"和\"Validation\"的文件夹中。这些文件夹内设有与数据集类别对应的子文件夹。所有图像已统一调整为相同尺寸，并生成了训练集和验证集对应的输入清单文件training.lst与validation.lst。  \n\n最后，公司创建了两个独立的亚马逊S3存储桶，分别用于上传训练数据集和验证数据集。在上传文件至亚马逊S3之前，公司还需完成哪些额外的数据准备工作？"}, "option": [{"option_text": {"zhcn": "通过将图像读入Pandas数据框架，并将数据框架存储为Parquet格式，生成training.parquet与validation.parquet两个Apache Parquet文件。随后将生成的Parquet文件上传至训练用的S3存储桶中。", "enus": "Generate two Apache Parquet files, training.parquet and validation.parquet, by reading the images into a Pandas data frame and  storing the data frame as a Parquet file. Upload the Parquet files to the training S3 bucket."}, "option_flag": false}, {"option_text": {"zhcn": "使用Snappy压缩库对训练集与验证集目录进行压缩处理。将清单文件及压缩后的数据上传至训练用的S3存储桶中。", "enus": "Compress the training and validation directories by using the Snappy compression library. Upload the manifest and compressed files to  the training S3 bucket."}, "option_flag": false}, {"option_text": {"zhcn": "使用gzip压缩库对训练集与验证集目录进行压缩处理。将清单文件及压缩后的数据上传至训练专用的S3存储桶。", "enus": "Compress the training and validation directories by using the gzip compression library. Upload the manifest and compressed files to the  training S3 bucket."}, "option_flag": true}, {"option_text": {"zhcn": "使用Apache MXNet工具集中的im2rec实用程序，依据清单文件生成training.rec与validation.rec两个RecordIO格式文件，并将其上传至训练专用的S3存储桶中。", "enus": "Generate two RecordIO files, training.rec and validation.rec, from the manifest files by using the im2rec Apache MXNet utility tool.  Upload the RecordIO files to the training S3 bucket."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Compress the training and validation directories by using the gzip compression library. Upload the manifest and compressed files to the training S3 bucket.” This is because when using Amazon SageMaker's built - in image classification algorithm with pipe mode, gzip is a widely supported and efficient compression format. It allows for faster data transfer and processing during the training process.\n\nThe option of generating Apache Parquet files is incorrect as Parquet is more suitable for tabular data rather than image data, so it's not the right format for this image - based ML task. \n\nUsing the Snappy compression library is wrong because the SageMaker built - in image classification algorithm typically expects gzip - compressed data, and Snappy may not be compatible or optimized for this specific use case. \n\nGenerating RecordIO files with the im2rec Apache MXNet utility tool is unnecessary. The built - in image classification algorithm in SageMaker doesn't require the data to be in RecordIO format. These common misconceptions can lead to choosing the wrong options, while the real answer aligns with the requirements of the SageMaker image classification algorithm and pipe mode.", "zhcn": "对于该问题的正确答案是：“使用gzip压缩库对训练集与验证集目录进行压缩，将清单文件与压缩后的数据上传至训练用的S3存储桶。”这是因为在使用亚马逊SageMaker内置图像分类算法并开启管道模式时，gzip作为一种广泛支持的高效压缩格式，能显著提升训练过程中的数据传输与处理速度。  \n  \n而生成Apache Parquet文件的方案并不适用——Parquet格式更适用于表格型数据，对此项基于图像数据的机器学习任务并不契合。采用Snappy压缩库的做法同样有误，因为SageMaker内置图像分类算法通常要求使用gzip压缩格式，Snappy在此特定场景下既缺乏兼容性也未作优化。至于通过Apache MXNet的im2rec工具生成RecordIO文件实属多此一举，该算法本身并不要求数据必须转为RecordIO格式。  \n  \n这些常见误区往往导致误选答案，而正解始终遵循SageMaker图像分类算法与管道模式的技术规范。"}, "answer": "C"}, {"id": "262", "question": {"enus": "A media company wants to create a solution that identifies celebrities in pictures that users upload. The company also wants to identify the IP address and the timestamp details from the users so the company can prevent users from uploading pictures from unauthorized locations. Which solution will meet these requirements with LEAST development effort? ", "zhcn": "一家传媒公司计划开发一套系统，用于识别用户上传图片中的公众人物。该公司还希望获取用户的IP地址与时间戳信息，以防止用户从未经授权的地理位置上传图片。在满足上述需求的前提下，何种方案能以最小的开发成本实现？"}, "option": [{"option_text": {"zhcn": "借助AWS Panorama识别图片中的知名人士，并通过AWS CloudTrail记录IP地址与时间戳信息。", "enus": "Use AWS Panorama to identify celebrities in the pictures. Use AWS CloudTrail to capture IP address and timestamp details."}, "option_flag": false}, {"option_text": {"zhcn": "运用AWS Panorama技术识别图像中的知名人士，并通过调用AWS Panorama设备开发工具包获取设备的IP地址与时间戳信息。", "enus": "Use AWS Panorama to identify celebrities in the pictures. Make calls to the AWS Panorama Device SDK to capture IP address and  timestamp details."}, "option_flag": false}, {"option_text": {"zhcn": "借助亚马逊Rekognition服务，可精准识别图像中的知名人士。通过AWS CloudTrail功能，能够记录访问来源的IP地址及操作时间戳等详细信息。", "enus": "Use Amazon Rekognition to identify celebrities in the pictures. Use AWS CloudTrail to capture IP address and timestamp details."}, "option_flag": true}, {"option_text": {"zhcn": "借助亚马逊Rekognition图像识别服务，可精准辨识影像中的公众人物。通过其文本检测功能，还能自动提取图片内包含的IP地址与时间戳信息。", "enus": "Use Amazon Rekognition to identify celebrities in the pictures. Use the text detection feature to capture IP address and timestamp  details."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Use Amazon Rekognition to identify celebrities in the pictures. Use AWS CloudTrail to capture IP address and timestamp details.” Amazon Rekognition is a pre - built service designed for image and video analysis, including celebrity recognition, which significantly reduces development effort as it has built - in algorithms for this task. AWS CloudTrail is a service that records API calls and captures details such as IP address and timestamp, making it a straightforward solution for the required data collection.\n\nThe fake answer that suggests using AWS Panorama for celebrity identification is incorrect. AWS Panorama is mainly for building computer vision applications on edge devices, not optimized for celebrity recognition in uploaded pictures.\n\nThe option of using AWS Panorama and its Device SDK to capture IP and timestamp details is also wrong. The SDK is related to device - level operations for AWS Panorama, not a standard way to capture user - related IP and timestamp data for general media upload scenarios.\n\nThe answer of using the text detection feature of Amazon Rekognition to capture IP and timestamp details is not practical. IP addresses and timestamps are not always present as text in the uploaded pictures, and this approach would require complex text extraction and validation, increasing development effort.\n\nIn summary, the real answer leverages the right pre - built services for each task, minimizing development work, which is why it is the correct choice.", "zhcn": "针对该问题的正确答案是：\"使用Amazon Rekognition识别图片中的公众人物，并通过AWS CloudTrail获取IP地址与时间戳信息。\"Amazon Rekognition作为一项预置式图像视频分析服务，其内置的公众人物识别功能可大幅降低开发复杂度；而AWS CloudTrail通过记录API调用自动捕获IP地址与时间戳等元数据，为所需信息采集提供了现成解决方案。\n\n至于建议采用AWS Panorama进行公众人物识别的错误选项，实则混淆了服务定位——该服务主要面向边缘设备的计算机视觉应用开发，并非针对上传图片的公众人物识别场景。\n\n另一组关于通过AWS Panorama设备SDK获取IP与时间戳的方案同样不妥。该SDK专用于设备端与AWS Panorama的交互，并不适用于常规媒体上传场景中的用户数据采集。\n\n而试图利用Amazon Rekognition文本检测功能提取IP与时间戳的设想亦不具实操性。由于这类数据未必以文本形式存在于图片中，该方案需涉及复杂的文本提取与验证流程，反而会增加开发负担。\n\n综上，正确答案的精髓在于精准匹配各项任务与现成服务，以最小开发成本实现目标，这正是其成为最佳选择的关键所在。"}, "answer": "C"}, {"id": "263", "question": {"enus": "A pharmaceutical company performs periodic audits of clinical trial sites to quickly resolve critical findings. The company stores audit documents in text format. Auditors have requested help from a data science team to quickly analyze the documents. The auditors need to discover the 10 main topics within the documents to prioritize and distribute the review work among the auditing team members. Documents that describe adverse events must receive the highest priority. A data scientist will use statistical modeling to discover abstract topics and to provide a list of the top words for each category to help the auditors assess the relevance of the topic. Which algorithms are best suited to this scenario? (Choose two.) ", "zhcn": "一家制药公司定期对临床试验基地开展审计，以便迅速处理关键发现。该公司以文本格式存储审计文件。审计人员请求数据科学团队协助快速分析这些文件，旨在从文档中识别十大核心主题，从而合理分配审计团队的审阅工作优先级。其中，描述不良事件的文档必须列为最高优先级别。数据科学家将采用统计建模方法挖掘抽象主题，并为每个类别提供核心词汇列表，以辅助审计人员评估主题相关性。请问下列哪种算法最适用于此场景？（请选择两项。）"}, "option": [{"option_text": {"zhcn": "潜在狄利克雷分布（LDA）", "enus": "Latent Dirichlet allocation (LDA)"}, "option_flag": false}, {"option_text": {"zhcn": "随机森林分类器", "enus": "Random forest classifier"}, "option_flag": true}, {"option_text": {"zhcn": "神经主题建模（NTM）", "enus": "Neural topic modeling (NTM)"}, "option_flag": true}, {"option_text": {"zhcn": "线性支持向量机", "enus": "Linear support vector machine"}, "option_flag": false}, {"option_text": {"zhcn": "线性回归", "enus": "Linear regression"}, "option_flag": false}], "analysis": {"enus": "The correct answers for this scenario are ‘Random forest classifier’ and ‘Neural topic modeling (NTM)’.\n\nThe Random forest classifier can be used to identify documents related to adverse events. It is a powerful classification algorithm that can handle complex relationships in the data, enabling it to distinguish between different types of audit documents and prioritize those about adverse events as required.\n\nNeural topic modeling (NTM) is suitable for discovering the 10 main topics within the text - based audit documents. It can learn the latent topics in the text data and provide a list of top words for each topic, which helps auditors assess topic relevance.\n\n‘Latent Dirichlet allocation (LDA)’ is also a topic - modeling algorithm but it may not be as effective as NTM in this case. NTM can capture more complex relationships in the data and is better adapted to modern neural network architectures.\n\nA ‘Linear support vector machine’ is mainly used for classification tasks where the goal is to separate data into two or more classes. While it can be used for classification, it is not as focused on topic discovery and prioritization of adverse events as the selected algorithms.\n\n‘Linear regression’ is used for predicting a continuous numerical value based on input variables. Since the task here is about topic discovery and classification for prioritization, linear regression is not relevant to this scenario.\n\nCommon misconceptions might lead one to choose LDA because it is a well - known topic - modeling algorithm. However, NTM is more advanced and better suited for this specific task. Choosing linear support vector machine or linear regression could be a result of misunderstanding the requirements of the problem, which involve both topic discovery and prioritization of specific document types.", "zhcn": "在此情境下，正确答案应为\"Random forest classifier\"与\"Neural topic modeling (NTM)\"。随机森林分类器可用于识别涉及不良事件的相关文档。这一强大的分类算法能够处理数据中的复杂关联，从而区分各类审计文档，并按要求优先处理与不良事件相关的内容。\n\n神经网络主题建模（NTM）适用于从基于文本的审计文档中发掘十大核心主题。该技术能解析文本数据中的潜在主题，并为每个主题提供核心词汇列表，助力审计人员评估主题相关性。\n\n虽然潜在狄利克雷分布（LDA）同样是主题建模算法，但在此场景下其效果不及NTM。NTM能捕捉数据中更复杂的关联，且更适配现代神经网络架构。\n\n线性支持向量机主要用于将数据划分为两个或多个类别的分类任务。尽管其可用于分类，但相较于已选算法，该技术对主题发现和不良事件优先级的处理不够聚焦。\n\n线性回归适用于基于输入变量预测连续数值。由于当前任务涉及主题发现和分类优先级判定，线性回归与此场景并无直接关联。\n\n常见误解可能导致选择LDA这一经典主题建模算法，但NTM作为更先进的技术方案，显然更适合本特定任务。若选择线性支持向量机或线性回归，则可能是由于未能准确理解题目要求——该任务需要同时实现主题发现与特定文档类型的优先级判定。"}, "answer": "BC"}, {"id": "264", "question": {"enus": "A company needs to deploy a chatbot to answer common questions from customers. The chatbot must base its answers on company documentation. Which solution will meet these requirements with the LEAST development effort? ", "zhcn": "一家公司需要部署聊天机器人来解答客户的常见问题。该聊天机器人必须依据公司文档内容进行回答。哪种方案能以最小的开发工作量满足这些需求？"}, "option": [{"option_text": {"zhcn": "借助Amazon Kendra实现企业文档的智能索引。通过调用Amazon Kendra查询API接口，将聊天机器人与Amazon Kendra无缝集成，从而精准解答客户咨询。", "enus": "Index company documents by using Amazon Kendra. Integrate the chatbot with Amazon Kendra by using the Amazon Kendra Query API  operation to answer customer questions."}, "option_flag": false}, {"option_text": {"zhcn": "基于历史客户问题与公司文档，训练双向注意力流（BiDAF）神经网络模型。将该模型部署为实时亚马逊SageMaker服务端点，并通过SageMaker运行时InvokeEndpoint接口与聊天机器人系统集成，用于智能应答客户咨询。", "enus": "Train a Bidirectional Attention Flow (BiDAF) network based on past customer questions and company documents. Deploy the model as  a real-time Amazon SageMaker endpoint. Integrate the model with the chatbot by using the SageMaker Runtime InvokeEndpoint API  operation to answer customer questions."}, "option_flag": true}, {"option_text": {"zhcn": "基于历史客户问询与公司内部文档，训练亚马逊SageMaker Blazing Text模型。将该模型部署为实时SageMaker服务终端，并通过SageMaker运行时调用终端节点API接口，与聊天机器人系统集成，实现智能应答客户问题。", "enus": "Train an Amazon SageMaker Blazing Text model based on past customer questions and company documents. Deploy the model as a  real-time SageMaker endpoint. Integrate the model with the chatbot by using the SageMaker Runtime InvokeEndpoint API operation to  answer customer questions."}, "option_flag": false}, {"option_text": {"zhcn": "利用亚马逊OpenSearch服务构建公司文档索引系统。通过集成OpenSearch服务的k近邻查询接口，使智能客服能够调用该接口精准解答客户咨询。", "enus": "Index company documents by using Amazon OpenSearch Service. Integrate the chatbot with OpenSearch Service by using the  OpenSearch Service k-nearest neighbors (k-NN) Query API operation to answer customer questions."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to train a Bidirectional Attention Flow (BiDAF) network based on past customer questions and company documents, deploy it as a real - time Amazon SageMaker endpoint, and integrate it with the chatbot using the SageMaker Runtime InvokeEndpoint API operation. BiDAF is designed for question - answering tasks and can effectively extract relevant information from text, making it well - suited for basing answers on company documentation.\n\nThe fake option of using Amazon Kendra requires indexing the documents. While Kendra is a powerful knowledge - indexing service, it involves the overhead of setting up and managing the indexing process, which adds to the development effort.\n\nTraining an Amazon SageMaker Blazing Text model is mainly for text classification and sentiment analysis. It is not specifically optimized for question - answering based on documentation, so it may not provide accurate answers as effectively as BiDAF, and additional work may be needed to adapt it for this purpose.\n\nIndexing with Amazon OpenSearch Service and using the k - NN Query API also requires setting up and managing the index. Moreover, OpenSearch Service is more focused on general search and analytics, and adapting it for question - answering from company documents may involve complex customizations, increasing the development effort.\n\nThe key factor distinguishing the real answer is that BiDAF is tailored for question - answering, and using SageMaker simplifies the deployment and integration process, resulting in the least development effort.", "zhcn": "针对该问题的正确答案是：基于过往客户问题与公司文档训练一个双向注意力流（BiDAF）网络，将其部署为实时Amazon SageMaker端点，并通过SageMaker Runtime InvokeEndpoint API操作与聊天机器人集成。BiDAF专为问答任务设计，能高效从文本中提取相关信息，尤其适合基于公司文档生成答案。  \n而使用Amazon Kendra的干扰选项需要先对文档建立索引。虽然Kendra是强大的知识索引服务，但涉及索引过程的设置和管理开销，会增加开发负担。  \n训练Amazon SageMaker Blazing Text模型主要用于文本分类和情感分析，并非针对基于文档的问答任务专门优化，因此可能无法像BiDAF那样精准回答问题，且需额外调整才能适应此场景。  \n采用Amazon OpenSearch服务建立索引并配合k-NN查询API同样需构建管理索引。此外，该服务更侧重于通用搜索与分析，要将其改造用于公司文档问答可能需复杂定制，从而增加开发成本。  \n区分正确答案的关键在于：BiDAF专为问答场景量身定制，且借助SageMaker能简化部署集成流程，最终实现最小开发投入。"}, "answer": "B"}, {"id": "265", "question": {"enus": "A company wants to conduct targeted marketing to sell solar panels to homeowners. The company wants to use machine learning (ML) technologies to identify which houses already have solar panels. The company has collected 8,000 satellite images as training data and will use Amazon SageMaker Ground Truth to label the data. The company has a small internal team that is working on the project. The internal team has no ML expertise and no ML experience. Which solution will meet these requirements with the LEAST amount of effort from the internal team? ", "zhcn": "一家公司计划向房主开展定向营销，推广太阳能电池板销售业务。该公司拟采用机器学习技术识别已安装太阳能电池板的住宅，目前已收集8000张卫星图像作为训练数据，并准备使用Amazon SageMaker Ground Truth进行数据标注。公司内部有一个小型项目团队负责此项工作，但团队成员既缺乏机器学习专业知识，也未曾有过相关实战经验。请问在最大限度减少内部团队工作量的前提下，最能满足这些需求的解决方案是什么？"}, "option": [{"option_text": {"zhcn": "组建一支由内部团队构成的专属标注团队。利用该专属团队及SageMaker Ground Truth的主动学习功能完成数据标注工作，随后通过Amazon Rekognition Custom Labels服务进行模型训练与部署。", "enus": "Set up a private workforce that consists of the internal team. Use the private workforce and the SageMaker Ground Truth active learning  feature to label the data. Use Amazon Rekognition Custom Labels for model training and hosting."}, "option_flag": false}, {"option_text": {"zhcn": "组建一支由内部团队构成的专属标注团队，利用该团队进行数据标注工作。随后采用Amazon Rekognition Custom Labels服务进行模型训练与部署。", "enus": "Set up a private workforce that consists of the internal team. Use the private workforce to label the data. Use Amazon Rekognition  Custom Labels for model training and hosting."}, "option_flag": true}, {"option_text": {"zhcn": "组建一支由内部团队构成的专属标注队伍。运用该专属团队及SageMaker Ground Truth的主动学习功能完成数据标注工作。采用SageMaker目标检测算法进行模型训练，并通过SageMaker批量转换技术实现推理预测。", "enus": "Set up a private workforce that consists of the internal team. Use the private workforce and the SageMaker Ground Truth active learning  feature to label the data. Use the SageMaker Object Detection algorithm to train a model. Use SageMaker batch transform for inference."}, "option_flag": false}, {"option_text": {"zhcn": "组建一支公共标注团队，由该团队负责数据标注工作。随后采用SageMaker目标检测算法进行模型训练，最终通过SageMaker批量转换功能实现推理预测。", "enus": "Set up a public workforce. Use the public workforce to label the data. Use the SageMaker Object Detection algorithm to train a model.  Use SageMaker batch transform for inference."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Set up a private workforce that consists of the internal team. Use the private workforce to label the data. Use Amazon Rekognition Custom Labels for model training and hosting.” This option requires the least effort from the internal team as it avoids the complexity of the SageMaker Ground Truth active learning feature, which demands a certain level of ML knowledge to set up and manage. Also, Amazon Rekognition Custom Labels simplifies the model - training and hosting process, being more beginner - friendly compared to using the SageMaker Object Detection algorithm.\n\nThe fake answer options involving the SageMaker Ground Truth active learning feature are inappropriate because the internal team has no ML expertise. Active learning needs understanding of ML concepts to optimize the labeling process, which would put an extra burden on the inexperienced team. Using the SageMaker Object Detection algorithm and batch transform also requires more in - depth ML knowledge for configuration and management. The option of setting up a public workforce is not ideal as it may introduce data security risks and requires additional management efforts to ensure the quality of the labeled data. These are the key reasons why the real answer option is the best choice with the least effort for the internal team.", "zhcn": "针对该问题，正确答案是：\"组建由内部团队构成的专属标注团队，由其完成数据标注工作，随后运用Amazon Rekognition Custom Labels进行模型训练与部署。\" 这一方案最大程度降低了内部团队的工作难度——既规避了SageMaker Ground Truth主动学习功能所需的机器学习专业知识门槛，又通过Amazon Rekognition Custom Labels简化了模型训练与部署流程，对初学者更为友好。  \n\n而涉及SageMaker Ground Truth主动学习功能的干扰选项并不适用，因为该功能需要理解机器学习原理来优化标注流程，这将给零基础的团队带来额外负担。采用SageMaker目标检测算法与批量转换方案同样需要更深层的机器学习技术储备。至于启用公共标注团队的方案，不仅可能引发数据安全隐患，还需投入额外管理成本确保标注质量。正因如此，正确答案才能以最小成本为内部团队实现最优解。"}, "answer": "B"}, {"id": "266", "question": {"enus": "A company hosts a machine learning (ML) dataset repository on Amazon S3. A data scientist is preparing the repository to train a model. The data scientist needs to redact personally identifiable information (PH) from the dataset. Which solution will meet these requirements with the LEAST development effort? ", "zhcn": "某公司在亚马逊S3云存储平台托管了一个机器学习数据集库。一位数据科学家正在准备用该数据仓库训练模型，需对数据集中的个人身份信息进行脱敏处理。在满足上述需求的前提下，下列哪种解决方案所需的开发工作量最小？"}, "option": [{"option_text": {"zhcn": "借助Amazon SageMaker Data Wrangler的自定义转换功能，可精准识别并隐去个人身份信息。", "enus": "Use Amazon SageMaker Data Wrangler with a custom transformation to identify and redact the PII."}, "option_flag": true}, {"option_text": {"zhcn": "编写一个定制的AWS Lambda函数，用于读取文件、识别其中的个人身份信息，并对这些信息进行脱敏处理。", "enus": "Create a custom AWS Lambda function to read the files, identify the PII. and redact the PII"}, "option_flag": false}, {"option_text": {"zhcn": "使用AWS Glue DataBrew识别并脱敏个人身份信息。", "enus": "Use AWS Glue DataBrew to identity and redact the PII"}, "option_flag": false}, {"option_text": {"zhcn": "借助AWS Glue开发终端，在笔记本中即可实现个人身份信息的自动化遮蔽处理。", "enus": "Use an AWS Glue development endpoint to implement the PII redaction from within a notebook"}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is ‘Use Amazon SageMaker Data Wrangler with a custom transformation to identify and redact the PII.’ Amazon SageMaker Data Wrangler offers a user - friendly interface with built - in data transformation capabilities. It allows data scientists to easily create custom transformations to redact PII, minimizing the need for extensive coding and thus requiring the least development effort.\n\nThe option of creating a custom AWS Lambda function demands writing and maintaining code, including handling errors and scaling, which is more development - intensive. AWS Glue DataBrew, while useful for data preparation, may not be as streamlined as SageMaker Data Wrangler for this specific task and could involve more setup. Using an AWS Glue development endpoint within a notebook also requires significant coding and configuration, making it a more complex solution. \n\nCommon misconceptions might lead one to choose the fake options if they are more familiar with AWS Glue services or think that custom Lambda functions are always the most straightforward solution. However, in this case, SageMaker Data Wrangler provides a more efficient and less development - heavy approach.", "zhcn": "针对该问题的正确答案是“使用Amazon SageMaker Data Wrangler并配合自定义转换功能来识别和编辑个人身份信息（PII）”。Amazon SageMaker Data Wrangler提供直观易用的操作界面及内置数据转换能力，使数据科学家能够轻松创建定制化转换规则以隐藏PII数据。这种方法大幅减少了对复杂编程的依赖，所需开发投入最为精简。\n\n若选择创建自定义AWS Lambda函数方案，则需编写和维护代码，包括错误处理与扩展性设计，开发复杂度显著提升。AWS Glue DataBrew虽在数据预处理方面表现优异，但针对此项特定任务，其操作流程不如SageMaker Data Wrangler简洁，且配置环节更为繁琐。而在笔记本环境中使用AWS Glue开发端点的方案同样涉及大量编码和配置工作，实现路径较为复杂。\n\n常见误解可能导致用户因更熟悉AWS Glue服务，或误认为自定义Lambda函数始终是最简解决方案而选择其他干扰项。但就本场景而言，SageMaker Data Wrangler无疑提供了更高效率、更低开发成本的实现路径。"}, "answer": "A"}, {"id": "267", "question": {"enus": "A company is deploying a new machine learning (ML) model in a production environment. The company is concerned that the ML model will drift over time, so the company creates a script to aggregate all inputs and predictions into a single file at the end of each day. The company stores the file as an object in an Amazon S3 bucket. The total size of the daily file is 100 GB. The daily file size will increase over time. Four times a year, the company samples the data from the previous 90 days to check the ML model for drift. After the 90-day period, the company must keep the files for compliance reasons. The company needs to use S3 storage classes to minimize costs. The company wants to maintain the same storage durability of the data. Which solution will meet these requirements? ", "zhcn": "某公司正在生产环境中部署一套全新的机器学习模型。由于担心该模型会随时间推移发生漂移，公司开发了一套脚本，用于每日汇总所有输入数据与预测结果，并将其整合为单一文件。这些文件以对象形式存储于亚马逊S3存储桶中，每日文件体积为100GB，且容量将随时间递增。公司每年四次对过去90天的数据进行抽样检测，以验证模型是否发生漂移。根据合规要求，90天后的数据仍需继续保留。在确保数据存储持久性不变的前提下，公司需通过S3存储分级方案实现成本优化。请问何种解决方案可满足上述需求？"}, "option": [{"option_text": {"zhcn": "将日常物件存放于S3标准低频访问存储层级。设置S3生命周期管理策略，使存储满90日的物件自动转存至S3 Glacier灵活检索存储层。", "enus": "Store the daily objects in the S3 Standard-InfrequentAccess (S3 Standard-IA) storage class. Configure an S3 Lifecycle rule to move the  objects to S3 Glacier Flexible Retrieval after 90 days."}, "option_flag": false}, {"option_text": {"zhcn": "将日常存取对象存储于S3单区低频访问存储类别中，并配置S3生命周期策略，使这些对象在90天后自动归档至S3 Glacier灵活检索存储层。", "enus": "Store the daily objects in the S3 One Zone-Infrequent Access (S3 One Zone-IA) storage class. Configure an S3 Lifecycle rule to move the  objects to S3 Glacier Flexible Retrieval after 90 days."}, "option_flag": false}, {"option_text": {"zhcn": "将日常物件存放于S3标准低频访问存储层级，并设置生命周期策略，使这些物件在90天后自动转存至S3冰川深度归档存储。", "enus": "Store the daily objects in the S3 Standard-InfrequentAccess (S3 Standard-IA) storage class. Configure an S3 Lifecycle rule to move the  objects to S3 Glacier Deep Archive after 90 days."}, "option_flag": true}, {"option_text": {"zhcn": "将日常存取对象存储于S3单区低频访问存储层级中，并设置生命周期管理策略，使数据在90天后自动归档至S3 Glacier深度归档存储。", "enus": "Store the daily objects in the S3 One Zone-Infrequent Access (S3 One Zone-IA) storage class. Configure an S3 Lifecycle rule to move the  objects to S3 Glacier Deep Archive after 90 days."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to store the daily objects in the S3 Standard - InfrequentAccess (S3 Standard - IA) storage class and configure an S3 Lifecycle rule to move the objects to S3 Glacier Deep Archive after 90 days. \n\nS3 Standard - IA is suitable for the daily files as they are large (100 GB and increasing) and accessed infrequently (only four times a year for 90 - day samples). It offers a lower storage cost compared to S3 Standard for infrequent access use - cases while maintaining high durability across multiple Availability Zones. \n\nAfter 90 days, moving the objects to S3 Glacier Deep Archive is the most cost - effective option for long - term compliance storage. It has the lowest storage cost among the S3 storage classes.\n\nThe fake answer options with S3 One Zone - IA are not ideal because S3 One Zone - IA stores data in a single Availability Zone, which means it has lower durability compared to S3 Standard - IA. The requirement is to maintain the same high - level of storage durability. \n\nThe options that use S3 Glacier Flexible Retrieval instead of S3 Glacier Deep Archive are more expensive for long - term storage. S3 Glacier Flexible Retrieval is designed for more frequent access scenarios than what is needed for post - 90 - day compliance storage. This cost - effectiveness and durability factors are why the real answer option is chosen over the fake ones.", "zhcn": "问题的正确答案是：将日常对象存储于S3标准-不频繁访问存储层级，并配置生命周期策略使数据在90天后自动迁移至S3 Glacier深度归档存储。S3标准-不频繁访问层级适合处理体积庞大（已达100GB且持续增长）、访问频次低（每年仅调用四次90天样本数据）的日常文件。相较于标准存储层级，该方案在保持跨可用区高耐久性的同时，显著降低了低频访问场景的存储成本。90天后将数据转移至S3 Glacier深度归档，则是满足长期合规存储需求的最经济选择——该层级在所有S3存储类别中拥有最低的存储成本。\n\n而采用S3单可用区-不频繁访问的干扰选项并不理想：该方案仅将数据存于单一可用区，存储耐久性低于S3标准-不频繁访问层级，无法满足既定高耐久性要求。若使用S3 Glacier灵活读取替代深度归档，则会导致长期存储成本上升——灵活读取适用于比90天后合规存储更频繁的访问场景。正是基于成本效益与耐久性的双重考量，正确答案才优于其他干扰选项。"}, "answer": "C"}, {"id": "268", "question": {"enus": "A company wants to enhance audits for its machine learning (ML) systems. The auditing system must be able to perform metadata analysis on the features that the ML models use. The audit solution must generate a report that analyzes the metadata. The solution also must be able to set the data sensitivity and authorship of features. Which solution will meet these requirements with the LEAST development effort? ", "zhcn": "某公司计划加强其机器学习系统的审计功能。审计系统需能对模型所使用的特征进行元数据分析，并生成包含元数据分析的报告。该方案还需支持设定特征的数据敏感度与作者信息。在满足上述要求的前提下，哪种方案能以最小的开发量实现？"}, "option": [{"option_text": {"zhcn": "使用Amazon SageMaker特征库进行特征筛选，构建数据流以执行特征级元数据分析。创建Amazon DynamoDB表用于存储特征级元数据，并借助Amazon QuickSight对元数据进行可视化分析。", "enus": "Use Amazon SageMaker Feature Store to select the features. Create a data fiow to perform feature-level metadata analysis. Create an  Amazon DynamoDB table to store feature-level metadata. Use Amazon QuickSight to analyze the metadata."}, "option_flag": false}, {"option_text": {"zhcn": "利用Amazon SageMaker特征存储功能，为当前机器学习模型所使用的特征创建特征组。为每个特征配置必要的元数据，并通过SageMaker Studio平台实现对元数据的可视化分析。", "enus": "Use Amazon SageMaker Feature Store to set feature groups for the current features that the ML models use. Assign the required  metadata for each feature. Use SageMaker Studio to analyze the metadata."}, "option_flag": true}, {"option_text": {"zhcn": "运用亚马逊SageMaker特征存储功能，对企业所需的特征级元数据实施定制化算法分析。通过创建亚马逊DynamoDB数据表存储特征级元数据，并借助亚马逊QuickSight可视化工具实现元数据的深度解析。", "enus": "Use Amazon SageMaker Features Store to apply custom algorithms to analyze the feature-level metadata that the company requires.  Create an Amazon DynamoDB table to store feature-level metadata. Use Amazon QuickSight to analyze the metadata."}, "option_flag": false}, {"option_text": {"zhcn": "运用亚马逊SageMaker特征存储服务，为当前机器学习模型所使用的特征创建特征组，并为每个特征配置必要的元数据。随后可借助亚马逊QuickSight工具对元数据进行可视化分析。", "enus": "Use Amazon SageMaker Feature Store to set feature groups for the current features that the ML models use. Assign the required  metadata for each feature. Use Amazon QuickSight to analyze the metadata."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Use Amazon SageMaker Feature Store to set feature groups for the current features that the ML models use. Assign the required metadata for each feature. Use SageMaker Studio to analyze the metadata.” This option requires the least development effort as SageMaker Studio is integrated with SageMaker Feature Store, allowing for seamless metadata analysis without the need to build additional components.\n\nThe first fake option involves creating a data flow and an Amazon DynamoDB table for storing metadata, which adds complexity and development effort. The second fake option requires applying custom algorithms and creating a DynamoDB table, both of which demand more coding and setup. The third fake option suggests using Amazon QuickSight for metadata analysis. While QuickSight is a powerful business intelligence tool, it is not as directly integrated with SageMaker Feature Store as SageMaker Studio, and thus may require more development work to achieve the desired metadata analysis.\n\nCommon misconceptions might lead one to choose the fake options if they overestimate the capabilities of tools like QuickSight or underestimate the integration benefits of using SageMaker Studio with SageMaker Feature Store. Also, the idea of creating custom algorithms or additional data storage might seem like a more comprehensive solution, but in this case, it only adds unnecessary development effort.", "zhcn": "针对该问题的正确答案是\"使用Amazon SageMaker Feature Store为当前机器学习模型所用的特征创建特征组，并为每个特征配置必要的元数据。通过SageMaker Studio实现元数据分析\"。这一方案开发成本最低，因为SageMaker Studio已与特征存储服务深度集成，无需构建额外组件即可实现无缝的元数据分析。\n\n第一个干扰选项涉及创建数据流和Amazon DynamoDB表来存储元数据，这会增加系统复杂度和开发工作量。第二个干扰选项要求应用自定义算法并创建DynamoDB表，两者都需要大量编码和配置工作。第三个干扰选项建议使用Amazon QuickSight进行元数据分析——虽然这是强大的商业智能工具，但其与SageMaker Feature Store的集成度不如SageMaker Studio紧密，因此需要更多开发工作才能实现目标。\n\n常见的认知误区可能导致选择干扰选项：或是高估了QuickSight等工具的功能，或是低估了SageMaker Studio与特征存储服务集成的优势。此外，设计自定义算法或构建额外数据存储方案可能看似更全面，但在此场景下反而会徒增不必要的开发负担。"}, "answer": "B"}, {"id": "269", "question": {"enus": "A machine learning (ML) specialist uploads a dataset to an Amazon S3 bucket that is protected by server-side encryption with AWS KMS keys (SSE-KMS). The ML specialist needs to ensure that an Amazon SageMaker notebook instance can read the dataset that is in Amazon S3. Which solution will meet these requirements? ", "zhcn": "一位机器学习专家将数据集上传至受AWS KMS密钥服务器端加密（SSE-KMS）保护的Amazon S3存储桶中。为确保Amazon SageMaker笔记本实例能够读取该S3数据集，下列哪项方案符合要求？"}, "option": [{"option_text": {"zhcn": "配置安全组规则，允许所有HTTP入站与出站流量通行。随后将此安全组关联至SageMaker笔记本实例。", "enus": "Define security groups to allow all HTTP inbound and outbound trafic. Assign the security groups to the SageMaker notebook instance."}, "option_flag": false}, {"option_text": {"zhcn": "请将SageMaker笔记本实例配置为可访问该虚拟私有云。", "enus": "Configure the SageMaker notebook instance to have access to the VP"}, "option_flag": false}, {"option_text": {"zhcn": "在AWS密钥管理服务（AWS KMS）的密钥策略中，为笔记本所属的VPC授予访问权限。  \n其次，为SageMaker笔记本分配一个具有S3数据集读取权限的IAM角色，并在KMS密钥策略中向该IAM角色授予相应权限。", "enus": "Grant permission in the AWS Key Management Service (AWS  KMS) key policy to the notebook’s VPC.  C. Assign an IAM role that provides S3 read access for the dataset to the SageMaker notebook. Grant permission in the KMS key policy to  the IAM role."}, "option_flag": true}, {"option_text": {"zhcn": "为SageMaker笔记本实例配置与加密Amazon S3数据相同的KMS密钥。", "enus": "Assign the same KMS key that encrypts the data in Amazon S3 to the SageMaker notebook instance."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to assign an IAM role that provides S3 read access for the dataset to the SageMaker notebook and grant permission in the KMS key policy to the IAM role. This is because for the SageMaker notebook to read the SSE - KMS encrypted data in S3, it needs proper S3 read permissions (which the IAM role provides) and also access to the KMS key used for encryption (granted via the KMS key policy to the IAM role).\n\nThe option of defining security groups to allow all HTTP inbound and outbound traffic and assigning them to the notebook only deals with network traffic. It doesn't address the necessary permissions for S3 access and KMS key usage, so it won't enable the notebook to read the encrypted data.\n\nConfiguring the SageMaker notebook instance to have access to the VPC is about network connectivity. While VPC access is important for the overall setup, it doesn't directly provide the required S3 read permissions and KMS key access.\n\nAssigning the same KMS key that encrypts the data in Amazon S3 to the SageMaker notebook instance is incorrect because the key needs to be properly configured with permission for the notebook's IAM role, not just assigned to the instance.\n\nThese common misconceptions often stem from confusing network - level configurations with the specific permissions required for accessing encrypted S3 data. The key factors distinguishing the real answer are the direct provision of S3 read access through an IAM role and the proper KMS key permission setup.", "zhcn": "针对该问题的正确答案是：为SageMaker笔记本分配一个具有S3数据集读取权限的IAM角色，并在KMS密钥策略中授予该IAM角色相应权限。这是因为要让SageMaker笔记本读取S3中经SSE-KMS加密的数据，既需要具备S3读取权限（由IAM角色提供），又需要获得加密所用KMS密钥的访问权（通过KMS密钥策略授予IAM角色）。  \n\n若选择定义允许所有HTTP入站/出站流量的安全组并将其分配给笔记本，该方案仅涉及网络流量控制，既未解决S3访问权限问题，也未处理KMS密钥使用权限，因此无法实现加密数据的读取。  \n\n将SageMaker笔记本实例配置为可访问VPC属于网络连通性设置。虽然VPC访问在整体架构中很重要，但并不能直接提供S3读取权限和KMS密钥访问权。  \n\n直接将加密S3数据的KMS密钥分配给SageMaker笔记本实例的做法并不正确，因为密钥需要针对笔记本的IAM角色进行权限配置，而非简单绑定到实例。  \n\n这些常见误解往往源于将网络层级配置与访问加密S3数据所需的具体权限相混淆。真正答案的关键区别在于：通过IAM角色直接提供S3读取权限，并正确设置KMS密钥权限。"}, "answer": "C"}, {"id": "270", "question": {"enus": "A company has a podcast platform that has thousands of users. The company implemented an algorithm to detect low podcast engagement based on a 10-minute running window of user events such as listening to, pausing, and closing the podcast. A machine learning (ML) specialist is designing the ingestion process for these events. The ML specialist needs to transform the data to prepare the data for inference. How should the ML specialist design the transformation step to meet these requirements with the LEAST operational effort? ", "zhcn": "某公司旗下播客平台拥有数千名用户。为检测用户参与度较低的播客内容，该公司采用基于10分钟滚动窗口的算法，通过分析用户收听、暂停及关闭播客等行为数据进行判断。当前，一位机器学习专家正在设计这些行为数据的采集流程。该专家需对原始数据进行转换处理，以满足模型推理需求。在满足各项技术要求的前提下，如何以最小的运维成本设计数据转换环节？"}, "option": [{"option_text": {"zhcn": "采用亚马逊托管式Apache Kafka流处理服务（Amazon MSK）集群接收事件数据流，并通过亚马逊Kinesis数据分析服务（Amazon Kinesis Data Analytics）在推理前对最近10分钟的数据进行实时转换。", "enus": "Use an Amazon Managed Streaming for Apache Kafka (Amazon MSK) cluster to ingest event data. Use Amazon Kinesis Data Analytics  to transform the most recent 10 minutes of data before inference."}, "option_flag": false}, {"option_text": {"zhcn": "运用亚马逊Kinesis数据流实时采集事件数据，通过亚马逊Kinesis Data Firehose将数据存储至亚马逊S3对象存储服务。在模型推理前，采用AWS Lambda函数对最近十分钟的数据流进行动态处理。", "enus": "Use Amazon Kinesis Data Streams to ingest event data. Store the data in Amazon S3 by using Amazon Kinesis Data Firehose. Use AWS  Lambda to transform the most recent 10 minutes of data before inference."}, "option_flag": false}, {"option_text": {"zhcn": "利用亚马逊Kinesis数据流实时采集事件数据，并通过亚马逊Kinesis数据分析服务对最近十分钟的数据进行预处理，继而完成推理计算。", "enus": "Use Amazon Kinesis Data Streams to ingest event data. Use Amazon Kinesis Data Analytics to transform the most recent 10 minutes of  data before inference."}, "option_flag": true}, {"option_text": {"zhcn": "利用亚马逊托管式Apache Kafka流处理服务（Amazon MSK）集群接收事件数据流，并通过AWS Lambda在推理前对最近10分钟的数据进行实时转换。", "enus": "Use an Amazon Managed Streaming for Apache Kafka (Amazon MSK) cluster to ingest event data. Use AWS Lambda to transform the  most recent 10 minutes of data before inference."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Use Amazon Kinesis Data Streams to ingest event data. Use Amazon Kinesis Data Analytics to transform the most recent 10 minutes of data before inference.” This approach requires the least operational effort because Amazon Kinesis Data Streams is a fully - managed service for ingesting real - time streaming data, and Amazon Kinesis Data Analytics can directly process and transform the streaming data within the required 10 - minute window.\n\nAmong the fake options, using “Amazon Managed Streaming for Apache Kafka (Amazon MSK) cluster to ingest event data” adds unnecessary complexity. Amazon MSK requires more setup, configuration, and management compared to Amazon Kinesis Data Streams. \n\nStoring the data in Amazon S3 using Amazon Kinesis Data Firehose and then using AWS Lambda to transform the data is also more operationally intensive. It involves additional steps of data storage and retrieval, and AWS Lambda may need more custom coding and resource management to process the most recent 10 minutes of data. \n\nSimilarly, using Amazon MSK for ingestion and AWS Lambda for transformation combines the complexity of Amazon MSK with the custom - coding requirements of AWS Lambda, making it a less efficient option in terms of operational effort. \n\nIn summary, the simplicity and directness of using Amazon Kinesis Data Streams and Amazon Kinesis Data Analytics for ingestion and transformation respectively make it the real answer, distinguishing it from the more complex and operationally demanding fake options.", "zhcn": "针对该问题，正确答案是\"采用Amazon Kinesis Data Streams进行事件数据摄取，并通过Amazon Kinesis Data Analytics在推理前对最近10分钟的数据进行转换\"。此方案所需运维投入最低，因为Amazon Kinesis Data Streams是全托管的实时流数据摄取服务，而Amazon Kinesis Data Analytics能直接在所需的10分钟时间窗口内处理并转换流数据。\n\n在干扰选项中，使用\"Amazon MSK集群进行事件数据摄取\"会引入不必要的复杂性。相较于Amazon Kinesis Data Streams，Amazon MSK需要更多配置部署与运维管理工作。若通过Amazon Kinesis Data Firehose将数据存储至Amazon S3，再调用AWS Lambda进行转换，同样会增加运维负担——该方案涉及额外的数据存储与提取环节，且AWS Lambda需编写更多定制代码并管理计算资源来处理近10分钟的数据。类似地，组合使用Amazon MSK摄取数据与AWS Lambda转换数据，既继承了Amazon MSK的复杂度，又需满足AWS Lambda的定制开发要求，因而在运维效率方面处于劣势。\n\n综上所述，分别运用Kinesis Data Streams实现数据摄取与Kinesis Data Analytics完成数据转换的方案，以其简洁直接的特质成为最优解，这与其它操作更复杂、运维要求更高的干扰选项形成鲜明对比。"}, "answer": "C"}, {"id": "271", "question": {"enus": "A machine learning (ML) specialist is training a multilayer perceptron (MLP) on a dataset with multiple classes. The target class of interest is unique compared to the other classes in the dataset, but it does not achieve an acceptable recall metric. The ML specialist varies the number and size of the MLP's hidden layers, but the results do not improve significantly. Which solution will improve recall in the LEAST amount of time? ", "zhcn": "一位机器学习专家正在利用包含多个类别的数据集训练多层感知器模型。尽管目标类别在数据集中独具特色，但其召回率指标始终未能达到理想水平。该专家尝试调整隐藏层的数量和规模，却未见明显改善。若要耗时最短地提升召回率，应采取下列哪种方案？"}, "option": [{"option_text": {"zhcn": "在MLP的损失函数中加入类别权重，然后重新进行训练。", "enus": "Add class weights to the MLP's loss function, and then retrain."}, "option_flag": true}, {"option_text": {"zhcn": "通过亚马逊土耳其机器人（Amazon Mechanical Turk）收集更多数据，随后进行模型重训练。", "enus": "Gather more data by using Amazon Mechanical Turk, and then retrain."}, "option_flag": false}, {"option_text": {"zhcn": "训练一个k-means算法，而非多层感知机。", "enus": "Train a k-means algorithm instead of an MLP."}, "option_flag": false}, {"option_text": {"zhcn": "训练异常检测模型，而非多层感知机。", "enus": "Train an anomaly detection model instead of an MLP."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is ‘Add class weights to the MLP's loss function, and then retrain.’ Recall is often low when dealing with imbalanced datasets, where the target class of interest is unique and likely under - represented. By adding class weights to the MLP's loss function, the model can pay more attention to the minority (target) class during training, which can improve the recall metric. This is a relatively quick fix as it mainly involves adjusting a parameter in the existing model and retraining it.\n\nThe option ‘Gather more data by using Amazon Mechanical Turk, and then retrain’ is time - consuming. It involves setting up a data - gathering process on Amazon Mechanical Turk, waiting for the data collection, and then retraining the model. This is not the least - time solution.\n\n‘Train a k - means algorithm instead of an MLP’ is incorrect because k - means is an unsupervised learning algorithm used for clustering, not for multi - class classification where we need to predict a specific target class.\n\n‘Train an anomaly detection model instead of an MLP’ is also wrong. Anomaly detection is focused on identifying rare events or outliers, which is different from the task of multi - class classification. The key here is to improve the recall of a specific class in a multi - class problem, and adding class weights to the MLP is the most efficient way to achieve this.", "zhcn": "对于该问题，正确答案是\"为MLP的损失函数添加类别权重后重新训练\"。在处理不平衡数据集时，目标类别的样本往往稀缺且代表性不足，这常导致召回率偏低。通过为MLP的损失函数引入类别权重，模型在训练过程中会更多关注少数类别（即目标类别），从而有效提升召回率指标。这种方法只需调整现有模型参数并重新训练，堪称最高效的解决方案。\n\n相比之下，\"通过亚马逊众包平台收集更多数据后重新训练\"的方案耗时较长。该方案需在亚马逊众包平台搭建数据收集流程，等待数据采集完成后再进行模型训练，显然不符合最小时间成本原则。\n\n而\"改用k均值算法替代MLP\"的方案并不合理。k均值属于无监督学习算法，适用于聚类分析而非需要预测特定目标类别的多分类场景。\n\n同样，\"采用异常检测模型替代MLP\"的方案也存在谬误。异常检测专注于识别罕见事件或离群值，与多类别分类任务有本质区别。当前核心在于提升多分类问题中特定类别的召回率，因此为MLP添加类别权重是实现这一目标的最优路径。"}, "answer": "A"}, {"id": "272", "question": {"enus": "A machine learning (ML) specialist uploads 5 TB of data to an Amazon SageMaker Studio environment. The ML specialist performs initial data cleansing. Before the ML specialist begins to train a model, the ML specialist needs to create and view an analysis report that details potential bias in the uploaded data. Which combination of actions will meet these requirements with the LEAST operational overhead? (Choose two.) ", "zhcn": "一位机器学习专家将5 TB数据上传至Amazon SageMaker Studio环境，并完成了初步的数据清洗。在开始训练模型之前，该专家需生成并查阅一份分析报告，其中需详细说明所上传数据中可能存在的偏差。若要满足以上需求，同时尽可能降低运维负担，应选择哪两项操作组合？（请选出两项正确答案）"}, "option": [{"option_text": {"zhcn": "利用SageMaker Clarify自动检测数据偏差。", "enus": "Use SageMaker Clarify to automatically detect data bias"}, "option_flag": true}, {"option_text": {"zhcn": "在SageMaker Ground Truth中启用偏差检测功能，即可自动分析数据特征。", "enus": "Turn on the bias detection option in SageMaker Ground Truth to automatically analyze data features."}, "option_flag": false}, {"option_text": {"zhcn": "利用SageMaker Model Monitor生成偏差漂移报告。", "enus": "Use SageMaker Model Monitor to generate a bias drift report."}, "option_flag": false}, {"option_text": {"zhcn": "配置SageMaker Data Wrangler以生成偏差报告。", "enus": "Configure SageMaker Data Wrangler to generate a bias report."}, "option_flag": true}, {"option_text": {"zhcn": "利用SageMaker Experiments进行数据校验。", "enus": "Use SageMaker Experiments to perform a data check"}, "option_flag": false}], "analysis": {"enus": "The correct answers to the question “A machine learning (ML) specialist uploads 5 TB of data to an Amazon SageMaker Studio environment... Which combination of actions will meet these requirements with the LEAST operational overhead?” are “Use SageMaker Clarify to automatically detect data bias” and “Configure SageMaker Data Wrangler to generate a bias report.” \n\nSageMaker Clarify is specifically designed to detect bias in data, providing a straightforward way to analyze the uploaded data for potential bias. SageMaker Data Wrangler can generate bias reports as part of its data preparation capabilities, enabling the ML specialist to get an in - depth analysis with relatively low overhead.\n\nThe fake answer “Turn on the bias detection option in SageMaker Ground Truth to automatically analyze data features” is incorrect because SageMaker Ground Truth is mainly for data labeling tasks, not for generating comprehensive bias reports on large - scale data. “Use SageMaker Model Monitor to generate a bias drift report” is wrong as Model Monitor is used to monitor models in production for performance and drift, not for pre - training data bias analysis. “Use SageMaker Experiments to perform a data check” is also incorrect since SageMaker Experiments is for organizing, tracking, and comparing machine learning experiments, not for detecting bias in data. These incorrect options are often chosen due to a misunderstanding of the specific purposes of each SageMaker service.", "zhcn": "针对“机器学习专家将5TB数据上传至Amazon SageMaker Studio环境…哪种组合方案能以最低运维成本满足需求？”这一问题，正确答案为“使用SageMaker Clarify自动检测数据偏差”及“配置SageMaker Data Wrangler生成偏差报告”。SageMaker Clarify专为数据偏差检测设计，可对上传数据实现直观的潜在偏差分析；而SageMaker Data Wrangler则能依托其数据预处理功能生成偏差报告，使专家以较低成本获得深度分析。\n\n错误选项“启用SageMaker Ground Truth中的偏差检测功能来自动分析数据特征”并不成立，因该服务主要专注于数据标注任务，而非大规模数据的全面偏差报告生成。“使用SageMaker Model Monitor生成偏差漂移报告”亦属误用，该工具用于监控生产环境中模型的性能与漂移情况，不涉及训练前数据偏差分析。至于“通过SageMaker Experiments执行数据检查”，该服务实为机器学习实验的整理、追踪与对比平台，并非数据偏差检测工具。这些错误选择往往源于对各SageMaker服务具体功能定位的误解。"}, "answer": "AD"}, {"id": "273", "question": {"enus": "A network security vendor needs to ingest telemetry data from thousands of endpoints that run all over the world. The data is transmitted every 30 seconds in the form of records that contain 50 fields. Each record is up to 1 KB in size. The security vendor uses Amazon Kinesis Data Streams to ingest the data. The vendor requires hourly summaries of the records that Kinesis Data Streams ingests. The vendor will use Amazon Athena to query the records and to generate the summaries. The Athena queries will target 7 to 12 of the available data fields. Which solution will meet these requirements with the LEAST amount of customization to transform and store the ingested data? ", "zhcn": "一家网络安全服务商需要接收来自全球各地数千个终端设备的遥测数据。这些数据每30秒以记录形式传输，每条记录包含50个字段，最大容量为1KB。该服务商采用亚马逊Kinesis数据流进行数据接入，并要求每小时对接入的记录生成汇总报告。后续将使用亚马逊雅典娜服务查询数据记录并生成摘要，查询操作将针对50个可用字段中的7至12个字段。请问在满足以下条件的前提下，哪种解决方案能够以最小的数据转换与存储定制化成本实现上述需求？"}, "option": [{"option_text": {"zhcn": "利用AWS Lambda每小时读取并汇总数据，通过亚马逊Kinesis Data Firehose对数据进行转换后，存储至Amazon S3中。", "enus": "Use AWS Lambda to read and aggregate the data hourly. Transform the data and store it in Amazon S3 by using Amazon Kinesis Data  Firehose."}, "option_flag": false}, {"option_text": {"zhcn": "利用Amazon Kinesis Data Firehose每小时读取并聚合数据，通过临时搭建的Amazon EMR集群对数据进行转换后，存储至Amazon S3中。", "enus": "Use Amazon Kinesis Data Firehose to read and aggregate the data hourly. Transform the data and store it in Amazon S3 by using a  short-lived Amazon EMR cluster."}, "option_flag": false}, {"option_text": {"zhcn": "利用Amazon Kinesis Data Analytics对数据进行每小时读取与聚合处理，并通过Amazon Kinesis Data Firehose转换数据格式后，将其存储至Amazon S3中。", "enus": "Use Amazon Kinesis Data Analytics to read and aggregate the data hourly. Transform the data and store it in Amazon S3 by using  Amazon Kinesis Data Firehose."}, "option_flag": true}, {"option_text": {"zhcn": "利用Amazon Kinesis Data Firehose每小时读取并聚合数据，再通过AWS Lambda对数据进行转换后存储至Amazon S3。", "enus": "Use Amazon Kinesis Data Firehose to read and aggregate the data hourly. Transform the data and store it in Amazon S3 by using AWS  Lambda."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Use Amazon Kinesis Data Analytics to read and aggregate the data hourly. Transform the data and store it in Amazon S3 by using Amazon Kinesis Data Firehose.” This solution requires the least customization as Amazon Kinesis Data Analytics is designed for real - time analytics on streaming data. It can easily handle the task of aggregating the incoming telemetry data on an hourly basis. After aggregation, Amazon Kinesis Data Firehose can smoothly transfer the transformed data to Amazon S3, which is a well - integrated and straightforward process.\n\nThe first fake option suggests using AWS Lambda. While Lambda can perform custom functions, it would require more coding and configuration to set up the hourly aggregation, making it less efficient and more customized. \n\nThe second and third fake options involve using Amazon Kinesis Data Firehose for aggregation. However, Kinesis Data Firehose is mainly used for loading streaming data into destinations like S3 and lacks built - in aggregation capabilities. The use of a short - lived Amazon EMR cluster or AWS Lambda in combination with it adds complexity and customization to the process that is not necessary. \n\nThe common pitfall here is thinking that AWS Lambda or Kinesis Data Firehose alone can efficiently handle the aggregation task without considering their primary functions and the additional work needed for customization. In contrast, Kinesis Data Analytics is specifically built for analytics on streaming data and thus provides a more streamlined solution.", "zhcn": "针对该问题，正确答案是\"使用Amazon Kinesis Data Analytics每小时读取并聚合数据，再通过Amazon Kinesis Data Firehose转换数据并存储至Amazon S3\"。此方案所需的定制化程度最低，因为Amazon Kinesis Data Analytics本就是为实时流数据分析而设计，能轻松实现遥测数据的小时级聚合。完成聚合后，Amazon Kinesis Data Firehose可将处理后的数据无缝传输至Amazon S3，这一流程既高度集成又简洁明了。\n\n第一个干扰项建议采用AWS Lambda方案。虽然Lambda支持自定义功能，但实现小时级聚合需要更多编码和配置工作，导致效率较低且定制化要求更高。第二、三个干扰项试图让Amazon Kinesis Data Firehose承担聚合功能，但该服务主要专长在于将流数据加载至S3等目标存储，本身并不具备原生聚合能力。若强行搭配临时Amazon EMR集群或AWS Lambda实现该功能，反而会徒增不必要的复杂度和定制化工作。\n\n常见误区在于误认为仅靠AWS Lambda或Kinesis Data Firehose就能高效完成聚合任务，却忽略了二者的核心功能定位及所需额外投入的定制化工作。相比之下，专为流数据分析而构建的Kinesis Data Analytics显然能提供更优雅的解决方案。"}, "answer": "C"}, {"id": "274", "question": {"enus": "A medical device company is building a machine learning (ML) model to predict the likelihood of device recall based on customer data that the company collects from a plain text survey. One of the survey questions asks which medications the customer is taking. The data for this field contains the names of medications that customers enter manually. Customers misspell some of the medication names. The column that contains the medication name data gives a categorical feature with high cardinality but redundancy. What is the MOST effective way to encode this categorical feature into a numeric feature? ", "zhcn": "一家医疗器械公司正基于客户填写的纯文本调查数据，构建机器学习模型以预测设备召回概率。其中一项调查询问客户当前服用药物名称，该字段数据由客户手动输入，存在药品名称拼写错误的情况。这使得包含药物名称的数据列呈现高基数且存在冗余的分类特征。将此分类特征转化为数值特征时，最高效的编码方式是什么？"}, "option": [{"option_text": {"zhcn": "对该列进行拼写检查。采用亚马逊SageMaker独热编码技术，将分类特征转换为数值特征。", "enus": "Spell check the column. Use Amazon SageMaker one-hot encoding on the column to transform a categorical feature to a numerical  feature."}, "option_flag": false}, {"option_text": {"zhcn": "使用字符级循环神经网络修正该列拼写错误。借助亚马逊SageMaker数据整理工具中的独热编码技术，将分类特征转换为数值特征。", "enus": "Fix the spelling in the column by using char-RNN. Use Amazon SageMaker Data Wrangler one-hot encoding to transform a categorical  feature to a numerical feature."}, "option_flag": false}, {"option_text": {"zhcn": "对指定列采用Amazon SageMaker Data Wrangler的相似度编码技术，将其转化为实数向量形式的嵌入表示。", "enus": "Use Amazon SageMaker Data Wrangler similarity encoding on the column to create embeddings of vectors of real numbers."}, "option_flag": false}, {"option_text": {"zhcn": "对指定列采用Amazon SageMaker Data Wrangler序数编码方法，将分类数据转换为介于0到该列总分类数之间的整数值。", "enus": "Use Amazon SageMaker Data Wrangler ordinal encoding on the column to encode categories into an integer between 0 and the total  number of categories in the column."}, "option_flag": true}], "analysis": {"enus": "The correct answer is to use Amazon SageMaker Data Wrangler ordinal encoding on the column to encode categories into an integer between 0 and the total number of categories in the column. The data has high cardinality (many unique values) and redundancy, and ordinal encoding is suitable as it can efficiently convert categorical data into numerical form without creating an extremely large number of features as one - hot encoding would.\n\nThe fake answer options that involve one - hot encoding are less effective. One - hot encoding creates a new binary column for each unique category. Given the high cardinality of the data, this would lead to a large number of columns, increasing the dimensionality of the data and potentially causing the curse of dimensionality. \n\nThe option of using similarity encoding to create embeddings of vectors of real numbers is not the most effective here. While it can capture semantic relationships, for a simple encoding task of a categorical feature with high cardinality and redundancy, ordinal encoding is a more straightforward and efficient solution. \n\nThe common misconception that might lead to choosing the fake options is thinking that one - hot encoding is always the best way to convert categorical data to numerical data. In cases of high cardinality, one - hot encoding can cause performance issues. Also, over - complicating the solution by choosing similarity encoding when a simpler encoding method suffices is another pitfall.", "zhcn": "正确答案是使用 Amazon SageMaker Data Wrangler 对列进行序数编码，将分类值转换为介于 0 到该列分类总数之间的整数。由于数据具有高基数（即大量唯一值）和冗余特性，序数编码能够在不产生极多特征维度的前提下，有效将分类数据转化为数值形式——这与独热编码会生成大量新列的特性形成鲜明对比。\n\n涉及独热编码的错误选项效果欠佳。独热编码会为每个唯一分类创建新的二值列，面对高基数数据时将导致列数激增，不仅增加数据维度，还可能引发维度灾难。而通过相似度编码生成实数向量的方案在此也非最优解：虽然它能捕捉语义关联，但对于具有高基数冗余特性的分类特征编码任务，序数编码是更简洁高效的解决方案。\n\n常见的认知误区可能促使人们选择错误选项，例如认为独热编码始终是分类数据数值化的最佳途径。但在高基数场景下，独热编码会引发性能问题。此外，当简单编码方法已能满足需求时，过度采用复杂的相似度编码也是另一个典型陷阱。"}, "answer": "D"}, {"id": "275", "question": {"enus": "A machine learning (ML) engineer has created a feature repository in Amazon SageMaker Feature Store for the company. The company has AWS accounts for development, integration, and production. The company hosts a feature store in the development account. The company uses Amazon S3 buckets to store feature values ofiine. The company wants to share features and to allow the integration account and the production account to reuse the features that are in the feature repository. Which combination of steps will meet these requirements? (Choose two.) ", "zhcn": "一位机器学习工程师在公司内部的Amazon SageMaker特征存储中创建了一个特征库。该公司分别设有开发、集成和生产环境的AWS账户，其中特征存储部署于开发账户，并采用Amazon S3存储桶离线保存特征值。现需实现特征共享功能，使集成账户与生产账户能够复用特征库中的特征。下列哪两项步骤组合可满足此需求？（请选择两项）"}, "option": [{"option_text": {"zhcn": "在开发账户中创建一个IAM角色，供集成账户和生产账户担任。为该角色附加IAM策略，允许其访问特征存储库和S3存储桶。", "enus": "Create an IAM role in the development account that the integration account and production account can assume. Attach IAM policies to  the role that allow access to the feature repository and the S3 buckets."}, "option_flag": true}, {"option_text": {"zhcn": "通过AWS资源访问管理器（AWS RAM），将开发账户中关联S3存储桶的特征库共享至集成账户与生产账户。", "enus": "Share the feature repository that is associated the S3 buckets from the development account to the integration account and the  production account by using AWS Resource Access Manager (AWS RAM)."}, "option_flag": false}, {"option_text": {"zhcn": "使用集成账户和生产账户中的AWS安全令牌服务（AWS STS）获取开发环境的访问凭证。", "enus": "Use AWS Security Token Service (AWS STS) from the integration account and the production account to retrieve credentials for the  development account."}, "option_flag": false}, {"option_text": {"zhcn": "在开发环境的S3存储桶与集成及生产环境的S3存储桶之间配置数据同步机制。", "enus": "Set up S3 replication between the development S3 buckets and the integration and production S3 buckets."}, "option_flag": true}, {"option_text": {"zhcn": "在开发账户中为 SageMaker 创建 AWS PrivateLink 端点。", "enus": "Create an AWS PrivateLink endpoint in the development account for SageMaker."}, "option_flag": false}], "analysis": {"enus": "The correct answers to the question are \"Create an IAM role in the development account that the integration account and production account can assume. Attach IAM policies to the role that allow access to the feature repository and the S3 buckets.\" and \"Set up S3 replication between the development S3 buckets and the integration and production S3 buckets.\"\n\nFor the IAM role option, creating an IAM role in the development account that other accounts can assume is a standard AWS practice for cross - account access. By attaching policies that allow access to the feature repository and S3 buckets, the integration and production accounts can securely access the required resources.\n\nSetting up S3 replication ensures that the offline feature values stored in the development S3 buckets are copied to the integration and production S3 buckets. This allows the other accounts to have access to the same feature values.\n\nThe option of using AWS RAM to share the feature repository associated with S3 buckets is incorrect because AWS RAM is not used for sharing Amazon SageMaker Feature Store resources. \n\nUsing AWS STS to retrieve credentials for the development account is not a proper solution as it does not provide a long - term and secure way for the other accounts to access the feature repository and S3 buckets. \n\nCreating an AWS PrivateLink endpoint in the development account for SageMaker is not relevant to the task of sharing features across different AWS accounts. It is mainly used for private connectivity to SageMaker services.\n\nIn summary, the real answer options provide valid and secure methods for sharing features and associated data across different AWS accounts, while the fake options are either not applicable or not the correct approach for this use - case.", "zhcn": "该问题的正确答案是：\"在开发账户中创建一个IAM角色，供集成账户和生产账户担任。为该角色附加允许访问特征存储库和S3存储桶的IAM策略\"以及\"设置开发环境S3存储桶与集成环境、生产环境S3存储桶之间的数据复制机制\"。\n\n对于IAM角色方案，在开发账户创建可供其他账户担任的IAM角色，是实现跨账户访问的标准AWS实践。通过附加允许访问特征存储库和S3存储桶的策略，集成账户和生产账户即可安全获取所需资源。\n\n配置S3数据复制能确保开发环境S3存储桶中的离线特征值同步至集成和生产环境存储桶，使各账户都能获取一致的特征数据。\n\n而通过AWS RAM共享关联S3存储桶的特征存储库方案并不成立，因为该服务并不适用于共享Amazon SageMaker特征存储资源。使用AWS STS获取开发账户凭证也非恰当方案，这无法为其他账户提供长期安全的特征存储库及S3存储桶访问方式。在开发账户为SageMaker创建AWS PrivateLink端点同样不适用，该服务主要用于实现与SageMaker服务的私有连接，与跨账户共享特征的任务无关。\n\n综上，正确答案提供了跨AWS账户共享特征及相关数据的有效安全方案，其余干扰选项要么不适用，要么不符合本场景的正确实施方法。"}, "answer": "AD"}, {"id": "276", "question": {"enus": "A company is building a new supervised classification model in an AWS environment. The company's data science team notices that the dataset has a large quantity of variables. All the variables are numeric. The model accuracy for training and validation is low. The model's processing time is affected by high latency. The data science team needs to increase the accuracy of the model and decrease the processing time. What should the data science team do to meet these requirements? ", "zhcn": "某公司正在AWS云环境中构建一个新型监督分类模型。数据科学团队发现数据集包含大量数值型变量，但当前模型的训练与验证准确率均不理想，且因延迟过高导致处理时间过长。为提升模型精度并缩短处理时长，数据科学团队应采取哪些措施？"}, "option": [{"option_text": {"zhcn": "生成新特征并构建交互变量。", "enus": "Create new features and interaction variables."}, "option_flag": false}, {"option_text": {"zhcn": "采用主成分分析（PCA）模型。", "enus": "Use a principal component analysis (PCA) model."}, "option_flag": true}, {"option_text": {"zhcn": "对特征集进行归一化处理。", "enus": "Apply normalization on the feature set."}, "option_flag": false}, {"option_text": {"zhcn": "采用多重对应分析（MCA）模型。", "enus": "Use a multiple correspondence analysis (MCA) model."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is ‘Use a principal component analysis (PCA) model.’ PCA is a dimensionality - reduction technique. When dealing with a dataset having a large number of variables, PCA can transform the original variables into a smaller set of uncorrelated variables called principal components. This reduces the complexity of the dataset, which in turn decreases the processing time as there are fewer variables to process. At the same time, by retaining the most important information in the data, it can potentially increase the model's accuracy.\n\n‘Create new features and interaction variables’ would likely increase the number of variables in the dataset. This would exacerbate the high - latency issue and might not necessarily improve accuracy, as it could lead to overfitting.\n\n‘Apply normalization on the feature set’ is mainly used to scale the features so that they have a similar range. While normalization can be beneficial in some cases for model performance, it does not address the issue of a large number of variables, and thus may not significantly improve processing time or accuracy in this context.\n\n‘Use a multiple correspondence analysis (MCA) model’ is typically used for categorical data, but the given dataset consists of only numeric variables. So, MCA is not appropriate for this scenario.\n\nIn summary, PCA's ability to reduce dimensionality is the key factor that makes it the real answer, distinguishing it from the fake options.", "zhcn": "该问题的正确答案是“采用主成分分析（PCA）模型”。PCA是一种降维技术，当处理变量繁多的数据集时，它能将原始变量转化为少量互不关联的主成分变量。这一过程不仅通过削减变量数量来降低数据复杂度，从而缩短处理时间，更能在保留数据核心信息的同时提升模型精度。\n\n而“创建新特征与交互变量”的做法反而可能增加变量数量，不仅会加剧高延迟问题，还可能因过度拟合导致准确率不升反降。“对特征集进行归一化处理”主要用于统一特征值的范围，虽在某些情况下有益于模型表现，但无法解决变量数量过多的问题，因此难以显著提升处理效率或预测准确性。“使用多重对应分析（MCA）模型”通常适用于分类数据，但本数据集仅含数值变量，故此法并不适用。\n\n综上，PCA凭借其降维特性成为破题关键，与其他选项形成本质区别。"}, "answer": "B"}, {"id": "277", "question": {"enus": "An exercise analytics company wants to predict running speeds for its customers by using a dataset that contains multiple health-related features for each customer. Some of the features originate from sensors that provide extremely noisy values. The company is training a regression model by using the built-in Amazon SageMaker linear learner algorithm to predict the running speeds. While the company is training the model, a data scientist observes that the training loss decreases to almost zero, but validation loss increases. Which technique should the data scientist use to optimally fit the model? ", "zhcn": "一家运动分析公司希望通过客户健康特征数据集预测其跑步速度，该数据集包含多项健康指标。其中部分指标来源于传感器，所采集的数据存在严重噪声干扰。该公司目前采用亚马逊SageMaker平台内置的线性学习算法训练回归模型，但在训练过程中，数据科学家发现训练损失值已趋近于零，验证损失值却持续上升。此时，数据科学家应采用何种技术手段以实现模型的最优拟合？"}, "option": [{"option_text": {"zhcn": "在线性学习器回归模型中引入L1正则化。", "enus": "Add L1 regularization to the linear learner regression model."}, "option_flag": false}, {"option_text": {"zhcn": "对数据集进行主成分分析（PCA），并采用线性学习器回归模型进行建模。", "enus": "Perform a principal component analysis (PCA) on the dataset. Use the linear learner regression model."}, "option_flag": false}, {"option_text": {"zhcn": "通过引入二次项与三次项进行特征工程，随后训练线性学习回归模型。", "enus": "Perform feature engineering by including quadratic and cubic terms. Train the linear learner regression model."}, "option_flag": true}, {"option_text": {"zhcn": "在线性回归学习模型中引入L2正则化。", "enus": "Add L2 regularization to the linear learner regression model."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Perform feature engineering by including quadratic and cubic terms. Train the linear learner regression model.” The problem described is overfitting, where the training loss is almost zero but the validation loss increases, meaning the model is too tailored to the training data and fails to generalize.\n\nAdding quadratic and cubic terms through feature engineering can help the model capture more complex relationships in the data. This can potentially improve the model's ability to generalize and reduce the validation loss, as it allows the model to fit the underlying patterns in the data better rather than just memorizing the training data.\n\nThe fake options are not as suitable. Adding L1 or L2 regularization (the other two options) is typically used to prevent overfitting by penalizing large coefficients. However, the root cause here might be the model's inability to capture complex relationships due to the noisy data, not just large coefficients. Performing PCA (Principal Component Analysis) reduces the dimensionality of the data, but this might discard important information and doesn't directly address the issue of the model's inability to capture complex patterns. \n\nA common misconception could be that regularization is always the best solution for overfitting, without considering that the model might need to capture more complex relationships in the data. Similarly, using PCA might seem like a good way to handle noisy data, but it may not help in this case where the model's complexity needs to be increased.", "zhcn": "针对题目所描述的过拟合现象（即训练损失几乎为零，但验证损失却上升，表明模型过度契合训练数据而泛化能力不足），正确的解决方法是：**通过引入二次项与三次项进行特征工程，并训练线性学习器回归模型**。  \n\n过拟合的根源在于模型未能充分捕捉数据中的复杂关系。通过特征工程增加二次项和三次项，能够帮助模型更好地识别数据中的潜在模式，从而提升其泛化能力、降低验证损失——这相当于让模型学会理解数据的内在规律，而非简单记忆训练样本。  \n\n其余选项则不够适宜：  \n- 添加L1或L2正则化（另两个选项）通常通过惩罚过大系数来抑制过拟合，但本题的核心问题在于模型因数据噪声影响而无法捕捉复杂关系，而非单纯系数过大；  \n- 主成分分析（PCA）虽能降低数据维度，却可能损失关键信息，且无法直接解决模型复杂度不足的问题。  \n\n常见的误解在于认为正则化永远是应对过拟合的最佳方案，却忽略了模型可能需要增强对数据复杂关系的捕捉能力；同样，PCA看似能处理噪声数据，但在此类需要提升模型复杂度的场景中并无助益。"}, "answer": "C"}, {"id": "278", "question": {"enus": "A company's machine learning (ML) specialist is building a computer vision model to classify 10 different trafic signs. The company has stored 100 images of each class in Amazon S3, and the company has another 10,000 unlabeled images. All the images come from dash cameras and are a size of 224 pixels × 224 pixels. After several training runs, the model is overfitting on the training data. Which actions should the ML specialist take to address this problem? (Choose two.) ", "zhcn": "某公司的机器学习专家正在构建一个计算机视觉模型，旨在对10种不同的交通标志进行分类。该公司已将每个类别的100张图像存储于Amazon S3中，同时还有10,000张未标注的图像。所有图像均采集自行车记录仪，尺寸为224像素×224像素。经过多次训练后，模型在训练数据上出现了过拟合现象。机器学习专家应采取哪两项措施来解决此问题？（请选择两项）"}, "option": [{"option_text": {"zhcn": "利用Amazon SageMaker Ground Truth对未标注图像进行智能标记。", "enus": "Use Amazon SageMaker Ground Truth to label the unlabeled images."}, "option_flag": false}, {"option_text": {"zhcn": "运用图像预处理技术将图片转换为灰度图像。", "enus": "Use image preprocessing to transform the images into grayscale images."}, "option_flag": false}, {"option_text": {"zhcn": "对标注图像进行旋转与平移的数据增强处理。", "enus": "Use data augmentation to rotate and translate the labeled images."}, "option_flag": false}, {"option_text": {"zhcn": "将最后一层的激活函数替换为S形函数。", "enus": "Replace the activation of the last layer with a sigmoid."}, "option_flag": true}, {"option_text": {"zhcn": "利用亚马逊SageMaker平台的k近邻（k-NN）算法，对未标注图像进行智能分类。", "enus": "Use the Amazon SageMaker k-nearest neighbors (k-NN) algorithm to label the unlabeled images."}, "option_flag": true}], "analysis": {"enus": "The correct answers to address the overfitting problem in the computer - vision model are “Replace the activation of the last layer with a sigmoid.” and “Use the Amazon SageMaker k - nearest neighbors (k - NN) algorithm to label the unlabeled images.”\n\nReplacing the activation of the last layer with a sigmoid can help in better probability distribution and can regularize the model, reducing overfitting. Using the k - NN algorithm to label the unlabeled images adds more data to the training set. More data generally helps the model to learn better patterns and reduces the chance of overfitting on the existing small labeled dataset.\n\nThe option “Use Amazon SageMaker Ground Truth to label the unlabeled images” is a valid way to label data, but it is not as cost - effective as using k - NN in this case. Ground Truth involves human intervention which can be time - consuming and expensive.\n\nTransforming images into grayscale with “Use image preprocessing to transform the images into grayscale images” might lead to loss of important color information in traffic signs, and it does not directly address overfitting.\n\n“Use data augmentation to rotate and translate the labeled images” is a common method to address overfitting, but with only 100 images per class, it may not be sufficient. Also, it does not increase the variety of data as effectively as adding labeled unlabeled data. \n\nThe key factors that distinguish the real answer options are their direct impact on reducing overfitting either through model regularization or increasing the size and diversity of the training data. The fake options either have limitations in addressing overfitting or are less efficient in the given context.", "zhcn": "针对计算机视觉模型过拟合问题的有效解决方案是\"将最后一层激活函数替换为Sigmoid函数\"和\"使用Amazon SageMaker k近邻(k-NN)算法标注未标记图像\"。将末层激活函数改为Sigmoid有助于优化概率分布，通过正则化抑制模型过拟合；而采用k-NN算法标注未标记图像则能扩充训练数据集——更多数据通常能提升模型对特征规律的学习能力，降低基于有限标注数据产生过拟合的风险。\n\n至于\"使用Amazon SageMaker Ground Truth标注未标记图像\"的方案，虽能实现数据标注，但在此场景下经济性不及k-NN算法。Ground Truth需人工介入，往往耗时且成本高昂。\n\n而\"通过图像预处理将图片转为灰度图\"的方案可能导致交通标志重要色彩信息丢失，且无法直接缓解过拟合现象。虽然\"对标注图像进行旋转平移的数据增强\"是常用过拟合应对策略，但在每类仅100张图像的情况下效果有限，其数据多样性的提升效果也不如直接增补标注数据。\n\n区分有效方案的核心在于其对过拟合的直接改善作用：要么通过模型正则化，要么通过提升训练数据的规模与多样性。其他方案或因改善效果有限，或在特定场景下效率不足而稍逊一筹。"}, "answer": "DE"}, {"id": "279", "question": {"enus": "A data science team is working with a tabular dataset that the team stores in Amazon S3. The team wants to experiment with different feature transformations such as categorical feature encoding. Then the team wants to visualize the resulting distribution of the dataset. After the team finds an appropriate set of feature transformations, the team wants to automate the workfiow for feature transformations. Which solution will meet these requirements with the MOST operational eficiency? ", "zhcn": "一支数据科学团队正在处理存储在Amazon S3中的表格数据集。团队需尝试多种特征变换方法（如分类特征编码），继而将变换后的数据分布进行可视化分析。在确定合适的特征变换组合后，团队希望将特征变换流程自动化。要同时满足这些需求且实现最高运营效率，下列哪种解决方案最为适宜？"}, "option": [{"option_text": {"zhcn": "利用Amazon SageMaker Data Wrangler预置的转换功能，可对特征变换进行探索分析。通过SageMaker Data Wrangler提供的可视化模板，实现数据特征的直观呈现。将特征处理工作流导出至SageMaker管道，即可实现全流程自动化部署。", "enus": "Use Amazon SageMaker Data Wrangler preconfigured transformations to explore feature transformations. Use SageMaker Data  Wrangler templates for visualization. Export the feature processing workfiow to a SageMaker pipeline for automation."}, "option_flag": false}, {"option_text": {"zhcn": "利用Amazon SageMaker笔记本实例进行多样化特征转换实验，将处理后的特征数据存储至Amazon S3。通过Amazon QuickSight实现可视化分析，并将特征处理流程封装为AWS Lambda函数以实现自动化运行。", "enus": "Use an Amazon SageMaker notebook instance to experiment with different feature transformations. Save the transformations to  Amazon S3. Use Amazon QuickSight for visualization. Package the feature processing steps into an AWS Lambda function for automation."}, "option_flag": false}, {"option_text": {"zhcn": "运用AWS Glue Studio结合自定义代码，尝试多种特征转换方案，并将转换结果存储至Amazon S3。通过Amazon QuickSight实现数据可视化，最后将特征处理流程封装至AWS Lambda函数，实现自动化运行。", "enus": "Use AWS Glue Studio with custom code to experiment with different feature transformations. Save the transformations to Amazon S3.  Use Amazon QuickSight for visualization. Package the feature processing steps into an AWS Lambda function for automation."}, "option_flag": true}, {"option_text": {"zhcn": "运用Amazon SageMaker Data Wrangler预置的数据转换功能，可灵活尝试多种特征转换方案。将转换后的数据存储至Amazon S3中，并通过Amazon QuickSight实现可视化呈现。每个特征转换环节应封装为独立的AWS Lambda函数，再借助AWS Step Functions实现工作流程的自动化编排。", "enus": "Use Amazon SageMaker Data Wrangler preconfigured transformations to experiment with different feature transformations. Save the  transformations to Amazon S3. Use Amazon QuickSight for visualization. Package each feature transformation step into a separate AWS  Lambda function. Use AWS Step Functions for workfiow automation."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Use AWS Glue Studio with custom code to experiment with different feature transformations. Save the transformations to Amazon S3. Use Amazon QuickSight for visualization. Package the feature processing steps into an AWS Lambda function for automation.” This solution offers the most operational efficiency.\n\nAWS Glue Studio allows for custom - coded data transformations, which gives the data science team greater flexibility in experimenting with various feature transformations compared to relying on pre - configured transformations in Amazon SageMaker Data Wrangler. \n\nAmong the fake options, using Amazon SageMaker Data Wrangler preconfigured transformations limits the team's ability to implement unique or complex transformations. Also, using SageMaker Data Wrangler templates for visualization might not be as comprehensive as Amazon QuickSight, which is a dedicated business intelligence service. \n\nUsing an Amazon SageMaker notebook instance is a more manual and less streamlined approach compared to AWS Glue Studio for data transformation tasks. \n\nPackaging each feature transformation step into a separate AWS Lambda function and using AWS Step Functions for workflow automation in one of the fake options adds unnecessary complexity. Instead, packaging all feature processing steps into a single AWS Lambda function simplifies the automation process. \n\nIn summary, the real answer provides a more flexible, efficient, and less complex solution for the data science team's requirements, which is why it is the best choice over the fake answer options.", "zhcn": "针对该问题，正确答案是\"运用AWS Glue Studio编写自定义代码来试验不同的特征转换，将转换结果保存至Amazon S3，通过Amazon QuickSight实现可视化，最后将特征处理流程封装至AWS Lambda函数实现自动化\"。这一方案能最大化运营效率。\n\n相较于依赖Amazon SageMaker Data Wrangler预置转换功能，AWS Glue Studio支持自定义代码的数据转换方式，使数据科学团队在尝试多样化特征转换时拥有更高灵活性。在干扰选项中，使用Amazon SageMaker Data Wrangler预配置转换会限制团队实施特殊或复杂转换的能力。此外，采用SageMaker Data Wrangler模板进行可视化分析的效果不如专精商业智能的Amazon QuickSight全面。对于数据转换任务而言，采用Amazon SageMaker笔记本实例相比AWS Glue Studio更为手动化且缺乏流程优化。\n\n某干扰方案中将每个特征转换步骤封装为独立AWS Lambda函数，并配合AWS Step Functions实现工作流自动化，这反而增加了不必要的复杂性。正确的做法是将所有特征处理步骤整合至单个Lambda函数，从而简化自动化流程。\n\n综上所述，正确答案为数据科学团队的需求提供了更灵活、高效且简洁的解决方案，因此相较于其他干扰选项是最佳选择。"}, "answer": "C"}, {"id": "280", "question": {"enus": "A company plans to build a custom natural language processing (NLP) model to classify and prioritize user feedback. The company hosts the data and all machine learning (ML) infrastructure in the AWS Cloud. The ML team works from the company's ofice, which has an IPsec VPN connection to one VPC in the AWS Cloud. The company has set both the enableDnsHostnames attribute and the enableDnsSupport attribute of the VPC to true. The company's DNS resolvers point to the VPC DNS. The company does not allow the ML team to access Amazon SageMaker notebooks through connections that use the public internet. The connection must stay within a private network and within the AWS internal network. Which solution will meet these requirements with the LEAST development effort? ", "zhcn": "某公司计划构建一个定制化的自然语言处理模型，用于对用户反馈进行分类和优先级排序。该公司将所有数据及机器学习基础设施部署在AWS云平台，其机器学习团队通过IPsec VPN从公司办公室连接至AWS云内的某个虚拟私有云（VPC）。该VPC已同时开启DNS主机名支持与DNS解析支持功能，且公司DNS解析器指向VPC的DNS服务。公司要求机器学习团队不得通过公共互联网访问Amazon SageMaker笔记本，所有连接必须严格限定在私有网络及AWS内部网络环境中。在满足上述要求的前提下，以下哪种解决方案能最大限度降低开发复杂度？"}, "option": [{"option_text": {"zhcn": "在VPC内为SageMaker笔记本创建接口端点。通过VPN连接及VPC端点访问该笔记本。", "enus": "Create a VPC interface endpoint for the SageMaker notebook in the VPC. Access the notebook through a VPN connection and the VPC  endpoint."}, "option_flag": false}, {"option_text": {"zhcn": "在虚拟私有云（VPC）的公共子网中，通过亚马逊EC2实例构建堡垒主机。", "enus": "Create a bastion host by using Amazon EC2 in a public subnet within the VP"}, "option_flag": true}, {"option_text": {"zhcn": "通过VPN连接登录至堡垒主机，经由堡垒主机访问SageMaker笔记本。  \nC. 在配备NAT网关的VPC私有子网中，使用Amazon EC2创建堡垒主机。通过VPN连接登录堡垒主机后，即可从该主机访问SageMaker笔记本。", "enus": "Log in to the bastion host through a VPN connection.  Access the SageMaker notebook from the bastion host.  C. Create a bastion host by using Amazon EC2 in a private subnet within the VPC with a NAT gateway. Log in to the bastion host through a  VPN connection. Access the SageMaker notebook from the bastion host."}, "option_flag": false}, {"option_text": {"zhcn": "在该VPC中创建NAT网关。通过VPN连接及NAT网关访问SageMaker笔记本的HTTPS端点。", "enus": "Create a NAT gateway in the VPC. Access the SageMaker notebook HTTPS endpoint through a VPN connection and the NAT gateway."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is “Create a bastion host by using Amazon EC2 in a public subnet within the VPC”. This solution meets the requirements with the least development effort. A bastion host in a public subnet allows the ML team to log in through the existing VPN connection. From there, they can access the SageMaker notebook while keeping the connection within the private and AWS internal networks.\n\nThe option “Create a VPC interface endpoint for the SageMaker notebook in the VPC. Access the notebook through a VPN connection and the VPC endpoint” requires setting up and configuring an interface endpoint, which adds complexity and development effort. \n\nThe option “Log in to the bastion host through a VPN connection. Access the SageMaker notebook from the bastion host. C. Create a bastion host by using Amazon EC2 in a private subnet within the VPC with a NAT gateway. Log in to the bastion host through a VPN connection. Access the SageMaker notebook from the bastion host” is overly complex. Using a private subnet with a NAT gateway is an unnecessary complication as it adds extra components and configuration steps.\n\nThe option “Create a NAT gateway in the VPC. Access the SageMaker notebook HTTPS endpoint through a VPN connection and the NAT gateway” is also not the best choice. A NAT gateway is mainly for allowing private resources in a VPC to access the internet, and using it to access the SageMaker notebook doesn't align with the requirement of keeping the connection private and within the AWS internal network. \n\nCommon misconceptions might lead one to choose the fake options. For example, thinking that endpoints or NAT gateways are always the best solutions without considering the additional setup and complexity they bring. Also, not fully understanding the purpose of a bastion host and how it can simplify access while meeting security requirements.", "zhcn": "针对该问题，正确答案是\"在VPC的公有子网中通过Amazon EC2创建堡垒机\"。此方案以最低的开发成本满足所有要求。部署在公有子网的堡垒机允许机器学习团队通过现有VPN连接登录，进而访问SageMaker笔记本实例，同时确保连接始终处于私有网络和AWS内部网络环境中。\n\n若选择\"为SageMaker笔记本创建VPC接口端点，通过VPN连接和VPC端点访问\"方案，则需配置接口端点，这会增加复杂性和开发工作量。而\"通过VPN连接登录堡垒机访问SageMaker笔记本，并在VPC私有子网中配置NAT网关创建堡垒机\"方案过于复杂——使用带有NAT网关的私有子网会引入多余组件和配置步骤，实无必要。\n\n至于\"在VPC中创建NAT网关，通过VPN连接和NAT网关访问SageMaker笔记本HTTPS端点\"方案亦非最佳选择。NAT网关主要用于允许VPC内私有资源访问互联网，将其用于访问SageMaker笔记本既不符合保持连接私有化的要求，也违背了AWS内部网络传输的原则。\n\n常见误区往往导致选择错误方案：例如未充分考虑端点或NAT网关带来的额外配置负担就认定其为最优解；或未能透彻理解堡垒机在满足安全要求前提下简化访问机制的实质价值。"}, "answer": "B"}, {"id": "281", "question": {"enus": "A data scientist is using Amazon Comprehend to perform sentiment analysis on a dataset of one million social media posts. Which approach will process the dataset in the LEAST time? ", "zhcn": "一位数据科学家正借助Amazon Comprehend对百万条社交媒体帖子进行情感分析。下列哪种方案能以最短时间完成该数据集的处理？"}, "option": [{"option_text": {"zhcn": "采用AWS Step Functions与AWS Lambda函数相结合的方式，逐条同步调用DetectSentiment接口对帖子进行情感分析。", "enus": "Use a combination of AWS Step Functions and an AWS Lambda function to call the DetectSentiment API operation for each post  synchronously."}, "option_flag": false}, {"option_text": {"zhcn": "采用AWS Step Functions与AWS Lambda函数相结合的方式，每次调用BatchDetectSentiment API时可批量处理最多25条帖子。", "enus": "Use a combination of AWS Step Functions and an AWS Lambda function to call the BatchDetectSentiment API operation with batches of  up to 25 posts at a time."}, "option_flag": false}, {"option_text": {"zhcn": "将文章内容上传至亚马逊S3存储服务，随后将S3存储路径传递给调用StartSentimentDetectionJob接口的AWS Lambda函数。", "enus": "Upload the posts to Amazon S3. Pass the S3 storage path to an AWS Lambda function that calls the StartSentimentDetectionJob API  operation."}, "option_flag": false}, {"option_text": {"zhcn": "利用AWS Lambda函数调用BatchDetectSentiment接口，对完整数据集进行情感分析。", "enus": "Use an AWS Lambda function to call the BatchDetectSentiment API operation with the whole dataset."}, "option_flag": true}], "analysis": {"enus": "The correct answer to the question ‘A data scientist is using Amazon Comprehend to perform sentiment analysis on a dataset of one million social media posts. Which approach will process the dataset in the LEAST time?’ is ‘Use an AWS Lambda function to call the BatchDetectSentiment API operation with the whole dataset.’ This is because calling the BatchDetectSentiment API with the entire dataset allows for processing a large number of records in a single operation, minimizing the overhead associated with multiple API calls.\n\nThe fake option ‘Use a combination of AWS Step Functions and an AWS Lambda function to call the DetectSentiment API operation for each post synchronously’ is inefficient as it processes each post one - by - one, resulting in a large number of individual API calls and significant processing time. The option of using BatchDetectSentiment with batches of up to 25 posts at a time also has more overhead compared to processing the whole dataset at once due to the need for multiple batches. And the approach of uploading to S3 and starting a sentiment detection job is more suitable for asynchronous, long - running tasks rather than achieving the least processing time as it involves additional steps like uploading to S3 and job management. These factors make the real answer the most time - efficient option.", "zhcn": "针对“数据科学家使用Amazon Comprehend对一百万条社交媒体帖子进行情感分析，哪种方案耗时最短？”这一问题，正确答案是“通过AWS Lambda函数调用BatchDetectSentiment接口，一次性处理整个数据集”。因为该方案通过单次API调用即可处理海量记录，最大限度减少了多次请求带来的性能损耗。  \n\n而干扰项“结合AWS Step Functions与Lambda函数同步调用DetectSentiment接口逐条处理帖子”效率低下——逐条处理会产生大量独立API调用，导致处理时间显著增加。另一方案“分批调用BatchDetectSentiment（每批最多25条）”因需多次分批处理，其开销也高于一次性整体处理。至于“上传数据至S3后启动情感检测任务”的方案涉及文件上传与作业管理等额外步骤，更适用于异步长时任务，而非追求最短处理时间。  \n\n综上，正选方案具有最优的时间效率。"}, "answer": "D"}, {"id": "282", "question": {"enus": "A machine learning (ML) specialist at a retail company must build a system to forecast the daily sales for one of the company's stores. The company provided the ML specialist with sales data for this store from the past 10 years. The historical dataset includes the total amount of sales on each day for the store. Approximately 10% of the days in the historical dataset are missing sales data. The ML specialist builds a forecasting model based on the historical dataset. The specialist discovers that the model does not meet the performance standards that the company requires. Which action will MOST likely improve the performance for the forecasting model? ", "zhcn": "某零售公司的机器学习专家需要构建一套系统，用于预测旗下某门店的每日销售额。公司向该专家提供了该门店过去十年的销售数据，这份历史数据集包含该门店每日销售总额，但其中约10%的日期存在数据缺失。基于此历史数据集，专家构建了预测模型，却发现模型未能达到公司要求的性能标准。下列哪项措施最有可能提升该预测模型的性能？"}, "option": [{"option_text": {"zhcn": "同一地理区域内各门店的销售总额。", "enus": "Aggregate sales from stores in the same geographic area."}, "option_flag": true}, {"option_text": {"zhcn": "对数据进行平滑处理以修正季节性波动。", "enus": "Apply smoothing to correct for seasonal variation."}, "option_flag": false}, {"option_text": {"zhcn": "将预测频率由每日调整为每周。", "enus": "Change the forecast frequency from daily to weekly."}, "option_flag": false}, {"option_text": {"zhcn": "采用线性插值法填补数据集中的缺失值。", "enus": "Replace missing values in the dataset by using linear interpolation."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is “Aggregate sales from stores in the same geographic area.” The key issue here is that the model built on the single - store historical data with 10% missing values fails to meet performance standards. Aggregating sales from stores in the same geographic area can provide more comprehensive and diverse data. This additional data can help the model capture broader market trends and patterns that might not be visible in the limited single - store data, thus improving its forecasting ability.\n\nThe option “Apply smoothing to correct for seasonal variation” only addresses seasonal patterns. While seasonality can affect sales, it doesn't deal with the problem of missing data and the potential lack of enough information in the single - store dataset.\n\n“Change the forecast frequency from daily to weekly” just changes the time frame of the forecast. It doesn't add more data or address the root cause of the poor performance, which is likely due to insufficient data.\n\n“Replace missing values in the dataset by using linear interpolation” only fills in the gaps in the existing single - store data. It doesn't bring in new information from other relevant stores, so it has limited potential to significantly improve the model's performance. This shows why “Aggregate sales from stores in the same geographic area” is the most likely action to enhance the model's performance.", "zhcn": "问题的正确答案是\"汇总同一地理区域内各门店的销售额\"。关键在于，基于单店历史数据构建的模型存在10%的缺失值，导致无法达到性能标准。通过整合相同区域其他门店的销售数据，能够获得更全面多元的信息基础。这些新增数据有助于模型捕捉更广泛的市场趋势与规律——这些隐藏在单一门店有限数据背后的深层信息，将有效提升模型的预测能力。\n\n至于\"采用平滑法修正季节性波动\"这一选项，其作用仅限于处理季节性规律。虽然季节性因素确实会影响销售表现，但该方法既不能解决数据缺失问题，也无法改善单店数据集信息量不足的现状。\n\n而\"将预测频率从每日调整为每周\"只是改变了预测的时间维度。这种做法既没有增加数据总量，也未能触及模型表现不佳的根本症结——数据基础薄弱。\n\n\"通过线性插值填补数据集缺失值\"的方案，仅仅是对现有单店数据的修补补。由于未能引入其他相关门店的新信息，该方法对模型性能的提升空间十分有限。由此可见，\"汇总同一地理区域内门店销售额\"才是提升模型性能的最优选择。"}, "answer": "A"}, {"id": "283", "question": {"enus": "A mining company wants to use machine learning (ML) models to identify mineral images in real time. A data science team built an image recognition model that is based on convolutional neural network (CNN). The team trained the model on Amazon SageMaker by using GPU instances. The team will deploy the model to a SageMaker endpoint. The data science team already knows the workload trafic patterns. The team must determine instance type and configuration for the workloads. Which solution will meet these requirements with the LEAST development effort? ", "zhcn": "一家矿业公司希望运用机器学习模型实时识别矿物图像。某数据科学团队基于卷积神经网络开发了图像识别模型，并借助GPU实例在Amazon SageMaker平台上完成了模型训练。团队计划将该模型部署至SageMaker终端节点。鉴于已掌握工作负载的流量规律，团队需为运算任务确定最合适的实例类型与配置方案。在满足所有需求的前提下，何种解决方案能最大程度降低开发投入？"}, "option": [{"option_text": {"zhcn": "将模型制品及容器注册至SageMaker模型注册库。选用SageMaker推理推荐器的默认任务类型。通过提供已知流量模式进行负载测试，从而根据工作负载筛选最优实例类型与配置方案。", "enus": "Register the model artifact and container to the SageMaker Model Registry. Use the SageMaker Inference Recommender Default job  type. Provide the known trafic pattern for load testing to select the best instance type and configuration based on the workloads."}, "option_flag": false}, {"option_text": {"zhcn": "将模型制品及相关容器注册至SageMaker模型注册库。选用SageMaker推理推荐器的高级任务模式，并提交已知流量模式以进行负载测试，从而根据实际工作负载筛选最优实例类型与配置方案。", "enus": "Register the model artifact and container to the SageMaker Model Registry. Use the SageMaker Inference Recommender Advanced job  type. Provide the known trafic pattern for load testing to select the best instance type and configuration based on the workloads."}, "option_flag": true}, {"option_text": {"zhcn": "将模型部署至基于GPU实例的终端节点。利用AWS Lambda与Amazon API Gateway处理来自网络的调用请求，并借助开源工具对终端节点进行负载测试，以选定最优实例类型与配置方案。", "enus": "Deploy the model to an endpoint by using GPU instances. Use AWS Lambda and Amazon API Gateway to handle invocations from the  web. Use open-source tools to perform load testing against the endpoint and to select the best instance type and configuration."}, "option_flag": false}, {"option_text": {"zhcn": "使用CPU实例将模型部署至服务终端。通过AWS Lambda与Amazon API Gateway处理来自网络的调用请求，并借助开源工具对终端进行负载测试，以选定最优实例类型与配置方案。", "enus": "Deploy the model to an endpoint by using CPU instances. Use AWS Lambda and Amazon API Gateway to handle invocations from the  web. Use open-source tools to perform load testing against the endpoint and to select the best instance type and configuration."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to register the model artifact and container to the SageMaker Model Registry and use the SageMaker Inference Recommender Advanced job type, providing the known traffic pattern for load testing to select the best instance type and configuration based on the workloads. \n\nThe SageMaker Inference Recommender Advanced job type is designed to handle specific traffic patterns. Since the data - science team already knows the workload traffic patterns, this option allows for a more tailored and accurate recommendation of the instance type and configuration, minimizing development effort as it uses Amazon SageMaker's built - in capabilities.\n\nThe Fake Answer Option with the SageMaker Inference Recommender Default job type is less suitable because the Default job type may not account for the specific traffic patterns known to the team as effectively as the Advanced job type.\n\nThe options that involve deploying the model to an endpoint using GPU or CPU instances, and then using AWS Lambda, Amazon API Gateway, and open - source tools for load testing require more development effort. These solutions involve setting up additional services (Lambda and API Gateway) and using open - source tools, which need to be configured, integrated, and maintained. This is in contrast to leveraging the built - in features of SageMaker, which offer a more streamlined and less development - intensive approach.", "zhcn": "针对该问题，正确答案是将模型工件及容器注册至SageMaker模型注册表，并选用SageMaker推理推荐器的高级任务类型，通过输入已知流量模式进行负载测试，从而根据工作负载选择最优实例类型与配置。SageMaker推理推荐器的高级任务类型专为处理特定流量模式设计。鉴于数据科学团队已掌握工作负载的流量特征，此选项能提供更精准定制化的实例类型与配置建议，同时充分利用Amazon SageMaker内置功能以最小化开发投入。\n\n而采用SageMaker推理推荐器默认任务类型的方案并不适宜，因为默认类型可能无法像高级任务类型那样有效适配团队已知的特定流量模式。\n\n至于将模型部署至GPU或CPU实例的终端节点，再结合AWS Lambda、Amazon API Gateway及开源工具进行负载测试的方案，则需要更高的开发成本。这类方案需配置额外服务（Lambda与API Gateway）并集成开源工具，涉及复杂的设置、整合及维护工作。相较之下，直接运用SageMaker的内置功能可实现更简洁高效、低开发负担的解决方案。"}, "answer": "B"}, {"id": "284", "question": {"enus": "A company is building custom deep learning models in Amazon SageMaker by using training and inference containers that run on Amazon EC2 instances. The company wants to reduce training costs but does not want to change the current architecture. The SageMaker training job can finish after interruptions. The company can wait days for the results. Which combination of resources should the company use to meet these requirements MOST cost-effectively? (Choose two.) ", "zhcn": "某公司正通过运行在亚马逊EC2实例上的训练与推理容器，在Amazon SageMaker中构建定制深度学习模型。公司希望降低训练成本，但需维持现有架构不变。当前SageMaker训练任务在中断后仍可完成，且公司能够接受数日的结果等待周期。要最高性价比地满足这些需求，应选择哪两种资源组合？（请选择两项）"}, "option": [{"option_text": {"zhcn": "按需实例", "enus": "On-Demand Instances"}, "option_flag": false}, {"option_text": {"zhcn": "检查点", "enus": "Checkpoints"}, "option_flag": false}, {"option_text": {"zhcn": "预留实例", "enus": "Reserved Instances"}, "option_flag": true}, {"option_text": {"zhcn": "渐进式训练", "enus": "Incremental training"}, "option_flag": false}, {"option_text": {"zhcn": "竞价实例", "enus": "Spot instances"}, "option_flag": true}], "analysis": {"enus": "The correct answers to the question are ‘Reserved Instances’ and ‘Spot instances’. Reserved Instances offer significant cost savings as the company commits to using the instances for a specific period. This is suitable as the company doesn't plan to change the architecture, so long - term use of resources can be predicted. Spot instances are also a great choice because they are spare Amazon EC2 capacity that is available at a much lower cost compared to On - Demand Instances. Since the SageMaker training job can tolerate interruptions and the company can wait days for results, using Spot instances won't disrupt the overall training process.\n\n‘On - Demand Instances’ are the most expensive option as they are billed based on the actual usage time without any long - term commitment discounts. ‘Checkpoints’ are a mechanism for saving the state of a training job, not a resource type for reducing costs. ‘Incremental training’ is a technique for updating an existing model rather than a resource that directly reduces training costs. The key factors that distinguish the real answer options are their cost - saving nature, which aligns with the company's goal of reducing training costs without changing the architecture.", "zhcn": "该问题的正确答案是“预留实例”与“竞价实例”。预留实例能大幅节约成本，因为公司承诺在特定期限内持续使用该实例。由于企业不打算调整架构，可预见资源的长期使用需求，因此该方案非常适用。竞价实例同样是理想选择，因为它们是亚马逊EC2的闲置容量资源，相比按需实例价格显著降低。鉴于SageMaker训练任务能够容忍中断，且公司可以接受数日等待结果，使用竞价实例不会影响整体训练进程。\n\n“按需实例”成本最高，因其按实际使用时长计费，不享受长期承诺折扣。“检查点”是保存训练任务状态的机制，而非降低成本资源类型。“增量训练”属于更新现有模型的技术手段，并非直接降低训练成本的资源。真正答案选项的核心区分点在于其节省成本的特性，这与公司不改变架构却要降低训练成本的目标高度契合。"}, "answer": "CE"}, {"id": "285", "question": {"enus": "A company hosts a public web application on AWS. The application provides a user feedback feature that consists of free-text fields where users can submit text to provide feedback. The company receives a large amount of free-text user feedback from the online web application. The product managers at the company classify the feedback into a set of fixed categories including user interface issues, performance issues, new feature request, and chat issues for further actions by the company's engineering teams. A machine learning (ML) engineer at the company must automate the classification of new user feedback into these fixed categories by using Amazon SageMaker. A large set of accurate data is available from the historical user feedback that the product managers previously classified. Which solution should the ML engineer apply to perform multi-class text classification of the user feedback? ", "zhcn": "一家公司在AWS上托管了一款公共网络应用程序。该应用程序设有一个用户反馈功能，包含自由文本字段供用户提交反馈意见。公司通过这款在线网络应用接收到大量自由文本形式的用户反馈。公司的产品经理将这些反馈按固定类别进行分类，包括界面问题、性能问题、新功能请求和聊天问题，以便工程团队后续处理。公司的一位机器学习工程师需要利用Amazon SageMaker服务，将新增用户反馈自动归类至这些固定类别。目前已有大量由产品经理预先分类过的历史用户反馈数据可供使用。针对用户反馈的多类别文本分类需求，这位机器学习工程师应当采用何种解决方案？"}, "option": [{"option_text": {"zhcn": "使用SageMaker平台的隐含狄利克雷分布（LDA）算法。", "enus": "Use the SageMaker Latent Dirichlet Allocation (LDA) algorithm."}, "option_flag": false}, {"option_text": {"zhcn": "请使用SageMaker BlazingText算法。", "enus": "Use the SageMaker BlazingText algorithm."}, "option_flag": true}, {"option_text": {"zhcn": "请使用SageMaker神经主题模型（NTM）算法。", "enus": "Use the SageMaker Neural Topic Model (NTM) algorithm."}, "option_flag": false}, {"option_text": {"zhcn": "请使用 SageMaker CatBoost 算法。", "enus": "Use the SageMaker CatBoost algorithm."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Use the SageMaker BlazingText algorithm.” This algorithm is well - suited for multi - class text classification tasks. It can efficiently handle large amounts of text data and classify it into multiple categories, which is exactly what the ML engineer needs to do with the user feedback.\n\nThe “SageMaker Latent Dirichlet Allocation (LDA) algorithm” and the “SageMaker Neural Topic Model (NTM) algorithm” are used for topic modeling. They aim to discover hidden topics in a text corpus rather than classifying text into predefined categories. So, they are not appropriate for this multi - class text classification task.\n\nThe “SageMaker CatBoost algorithm” is a gradient - boosting library that is mainly used for tabular data and structured data, not for text classification. \n\nA common misconception might be confusing topic modeling algorithms with classification algorithms, or assuming that an algorithm suitable for tabular data can also handle text data effectively. These misunderstandings could lead one to choose the fake answer options over the correct one.", "zhcn": "对于该问题的正确答案是\"使用SageMaker BlazingText算法\"。该算法特别适用于多类别文本分类任务，能够高效处理海量文本数据并将其归入不同类别，这正是机器学习工程师处理用户反馈时所需的功能。\n\n而\"SageMaker隐狄利克雷分布（LDA）算法\"与\"SageMaker神经主题模型（NTM）算法\"主要用于主题建模，其核心目标是发现文本语料库中的潜在主题，而非将文本划分至预设类别，因此不适用于当前的多类别文本分类场景。\n\n至于\"SageMaker CatBoost算法\"，作为梯度提升库主要面向表格化数据和结构化数据，并不适用于文本分类任务。常见的认知误区在于混淆主题建模算法与分类算法，或误判适用于表格数据的算法同样能有效处理文本数据——这些误解可能导致选择错误答案而非正确选项。"}, "answer": "B"}, {"id": "286", "question": {"enus": "A digital media company wants to build a customer churn prediction model by using tabular data. The model should clearly indicate whether a customer will stop using the company's services. The company wants to clean the data because the data contains some empty fields, duplicate values, and rare values. Which solution will meet these requirements with the LEAST development effort? ", "zhcn": "一家数字媒体公司计划利用表格数据构建客户流失预测模型。该模型需明确显示客户是否会停止使用公司服务。由于数据中存在部分空白字段、重复值及罕见数值，公司需要对数据进行清洗。哪种方案能够以最小的开发量满足这些需求？"}, "option": [{"option_text": {"zhcn": "利用SageMaker Canvas自动完成数据清洗工作，并构建分类模型。", "enus": "Use SageMaker Canvas to automatically clean the data and to prepare a categorical model."}, "option_flag": true}, {"option_text": {"zhcn": "利用SageMaker Data Wrangler进行数据清洗，并借助内置的SageMaker XGBoost算法训练分类模型。", "enus": "Use SageMaker Data Wrangler to clean the data. Use the built-in SageMaker XGBoost algorithm to train a classification model."}, "option_flag": false}, {"option_text": {"zhcn": "运用SageMaker Canvas的自动化数据清洗与整理工具，通过内置的SageMaker XGBoost算法训练回归模型。", "enus": "Use SageMaker Canvas automatic data cleaning and preparation tools. Use the built-in SageMaker XGBoost algorithm to train a  regression model."}, "option_flag": false}, {"option_text": {"zhcn": "利用SageMaker Data Wrangler进行数据清洗，并通过SageMaker Autopilot训练回归模型。", "enus": "Use SageMaker Data Wrangler to clean the data. Use the SageMaker Autopilot to train a regression model"}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is ‘Use SageMaker Canvas to automatically clean the data and to prepare a categorical model.’ This is because the question asks for a solution with the least development effort. SageMaker Canvas is a no - code/low - code tool that can automatically clean data and prepare a model, which significantly reduces the development effort for the digital media company.\n\nThe fake option ‘Use SageMaker Data Wrangler to clean the data. Use the built - in SageMaker XGBoost algorithm to train a classification model’ requires more development effort as it involves using Data Wrangler for data cleaning and then manually selecting and training an XGBoost model. \n\nThe option ‘Use SageMaker Canvas automatic data cleaning and preparation tools. Use the built - in SageMaker XGBoost algorithm to train a regression model’ is incorrect because a regression model is not suitable for the problem of predicting whether a customer will stop using services (a binary categorical problem), and it also adds the complexity of training an XGBoost model, increasing development effort.\n\nThe option ‘Use SageMaker Data Wrangler to clean the data. Use the SageMaker Autopilot to train a regression model’ has two issues. First, a regression model is not appropriate for this categorical prediction task. Second, using Data Wrangler and Autopilot still requires more hands - on work compared to the fully automated approach of SageMaker Canvas, thus having more development effort. \n\nThe key factor that makes the real answer the best choice is its ability to handle data cleaning and model preparation with minimal development effort, while the fake options either involve more manual steps or use an inappropriate model type.", "zhcn": "对于本题，正确答案应为\"使用SageMaker Canvas自动完成数据清洗并构建分类模型\"。原因在于题目要求选择开发投入最少的解决方案。SageMaker Canvas作为无代码/低代码工具，能够自动化实现数据清洗与模型准备工作，可显著降低数字媒体公司的开发成本。\n\n而干扰项\"使用SageMaker Data Wrangler清洗数据，采用内置SageMaker XGBoost算法训练分类模型\"需要更多开发投入，因其涉及数据清洗工具与手动训练XGBoost模型的双重操作。另一选项\"使用SageMaker Canvas自动数据清洗工具，采用内置SageMaker XGBoost算法训练回归模型\"存在两个问题：首先回归模型不适用于预测客户流失的二元分类问题；其次引入XGBoost模型训练会增加开发复杂度。\n\n至于\"使用SageMaker Data Wrangler清洗数据，通过SageMaker Autopilot训练回归模型\"这一选项，不仅存在模型类型误用问题（分类任务错用回归模型），其组合方案相比Canvas全自动化流程仍需更多人工干预，导致开发成本上升。因此，正确答案的核心优势在于以最小开发成本同步解决数据清洗与模型准备问题，而干扰项或因操作流程复杂、或因模型选择不当均不符合最优解要求。"}, "answer": "A"}, {"id": "287", "question": {"enus": "A data engineer is evaluating customer data in Amazon SageMaker Data Wrangler. The data engineer will use the customer data to create a new model to predict customer behavior. The engineer needs to increase the model performance by checking for multicollinearity in the dataset. Which steps can the data engineer take to accomplish this with the LEAST operational effort? (Choose two.) ", "zhcn": "一位数据工程师正在亚马逊SageMaker数据整理平台中评估客户数据。该工程师计划利用这些客户数据构建预测用户行为的新模型。为提升模型性能，需检测数据集中的多重共线性现象。以下哪两项措施能以最小操作量实现此目标？（请选择两项）"}, "option": [{"option_text": {"zhcn": "利用SageMaker Data Wrangler对数据集进行重构与转换，通过对分类变量实施独热编码处理。", "enus": "Use SageMaker Data Wrangler to refit and transform the dataset by applying one-hot encoding to category-based variables."}, "option_flag": true}, {"option_text": {"zhcn": "运用SageMaker Data Wrangler的诊断可视化功能，通过主成分分析（PCA）与奇异值分解（SVD）方法计算奇异值。", "enus": "Use SageMaker Data Wrangler diagnostic visualization. Use principal components analysis (PCA) and singular value decomposition  (SVD) to calculate singular values."}, "option_flag": false}, {"option_text": {"zhcn": "利用SageMaker Data Wrangler的快速模型可视化功能，可迅速评估数据集并生成各特征的重要性评分。", "enus": "Use the SageMaker Data Wrangler Quick Model visualization to quickly evaluate the dataset and to produce importance scores for each  feature."}, "option_flag": false}, {"option_text": {"zhcn": "使用SageMaker Data Wrangler的最小最大缩放器转换功能对数据进行归一化处理。", "enus": "Use the SageMaker Data Wrangler Min Max Scaler transform to normalize the data."}, "option_flag": false}, {"option_text": {"zhcn": "使用SageMaker Data Wrangler的诊断可视化功能。通过最小绝对值收敛选择算子（LASSO）算法，对基于该数据集训练的LASSO模型绘制系数值分布图。", "enus": "Use SageMaker Data Wrangler diagnostic visualization. Use least absolute shrinkage and selection operator (LASSO) to plot coeficient  values from a LASSO model that is trained on the dataset."}, "option_flag": true}], "analysis": {"enus": "The question asks for steps a data engineer can take with the least operational effort to check for multicollinearity in a dataset in Amazon SageMaker Data Wrangler.\n\nThe first real answer, “Use SageMaker Data Wrangler to refit and transform the dataset by applying one - hot encoding to category - based variables,” helps in dealing with categorical variables. Multicollinearity can be affected by the way categorical variables are represented. One - hot encoding provides a standard way to represent these variables numerically, reducing potential issues related to multicollinearity caused by improper encoding.\n\nThe second real answer, “Use SageMaker Data Wrangler diagnostic visualization. Use least absolute shrinkage and selection operator (LASSO) to plot coefficient values from a LASSO model that is trained on the dataset,” is useful because LASSO can automatically select relevant features and shrink the coefficients of multicollinear features towards zero. By visualizing these coefficients, the data engineer can easily identify multicollinear features.\n\nThe first fake answer involves using PCA and SVD to calculate singular values. While PCA and SVD are useful for dimensionality reduction, calculating singular values is not a direct way to check for multicollinearity and requires more in - depth understanding and analysis, thus involving more operational effort.\n\nThe second fake answer about using the Quick Model visualization to get feature importance scores doesn't directly address multicollinearity. Feature importance scores are more about identifying which features are most influential in the model, not about detecting the presence of multicollinearity.\n\nThe third fake answer of using the Min Max Scaler transform to normalize the data is mainly for scaling the data to a specific range. Normalization does not directly help in checking for multicollinearity.\n\nIn conclusion, the real answer options directly target the issue of multicollinearity with relatively less operational effort compared to the fake options.", "zhcn": "问题要求数据工程师以最小操作成本，在亚马逊SageMaker Data Wrangler中检查数据集多重共线性的方法。第一条有效方案\"使用SageMaker Data Wrangler对分类变量进行独热编码后重新拟合和转换数据集\"，能有效处理分类变量。分类变量的表示方式会影响多重共线性，独热编码通过标准化数值转换可减少因编码不当引发的共线性问题。\n\n第二条有效方案\"使用SageMaker Data Wrangler诊断可视化功能，基于数据集训练LASSO模型并绘制系数图\"具有实用价值。LASSO算法能自动筛选相关特征并将共线性特征的系数压缩至零，通过可视化系数图可直观识别共线性特征。\n\n而第一条无效方案采用PCA和SVD计算奇异值，虽然这两种方法适用于降维，但奇异值计算并非检测多重共线性的直接方式，且需要更深入的理解分析，操作成本较高。\n\n第二条无效方案建议通过快速模型可视化获取特征重要性评分，这并未直接解决多重共线性问题。特征重要性主要用于识别模型中最具影响力的特征，而非检测共线性存在。\n\n第三条无效方案使用最小最大缩放器进行数据归一化，其主要作用是将数据缩放至特定范围，归一化操作本身无助于直接检测多重共线性。\n\n综上可知，与无效方案相比，两条有效方案能以相对较低的操作成本直接针对多重共线性问题提出解决方案。"}, "answer": "AE"}, {"id": "288", "question": {"enus": "A company processes millions of orders every day. The company uses Amazon DynamoDB tables to store order information. When customers submit new orders, the new orders are immediately added to the DynamoDB tables. New orders arrive in the DynamoDB tables continuously. A data scientist must build a peak-time prediction solution. The data scientist must also create an Amazon QuickSight dashboard to display near real-time order insights. The data scientist needs to build a solution that will give QuickSight access to the data as soon as new order information arrives. Which solution will meet these requirements with the LEAST delay between when a new order is processed and when QuickSight can access the new order information? ", "zhcn": "一家公司每日需处理数百万笔订单。该公司采用Amazon DynamoDB数据表存储订单信息。当客户提交新订单时，这些订单会即时录入DynamoDB数据表中。新订单数据持续不断地流入DynamoDB数据表。数据科学家需要构建一套高峰时段预测方案，同时创建Amazon QuickSight仪表板以呈现近实时订单洞察。该方案需确保QuickSight能在新订单数据录入后立即获取信息。请问在满足以下条件的前提下，哪种方案能最大程度缩短新订单处理完成与QuickSight获取新订单信息之间的延迟？"}, "option": [{"option_text": {"zhcn": "使用AWS Glue将数据从Amazon DynamoDB导出至Amazon S3，并配置QuickSight以访问Amazon S3中的数据。", "enus": "Use AWS Glue to export the data from Amazon DynamoDB to Amazon S3. Configure QuickSight to access the data in Amazon S3."}, "option_flag": false}, {"option_text": {"zhcn": "利用Amazon Kinesis Data Streams将Amazon DynamoDB中的数据导出至Amazon S3，并配置QuickSight以访问Amazon S3内的数据。", "enus": "Use Amazon Kinesis Data Streams to export the data from Amazon DynamoDB to Amazon S3. Configure QuickSight to access the data  in Amazon S3."}, "option_flag": true}, {"option_text": {"zhcn": "借助 QuickSight 的 API 接口，可直接调用存储在 Amazon DynamoDB 中的数据。", "enus": "Use an API call from QuickSight to access the data that is in Amazon DynamoDB directly."}, "option_flag": false}, {"option_text": {"zhcn": "利用Amazon Kinesis Data Firehose将Amazon DynamoDB中的数据导出至Amazon S3存储服务，并配置QuickSight数据分析工具以访问Amazon S3内的数据资源。", "enus": "Use Amazon Kinesis Data Firehose to export the data from Amazon DynamoDB to Amazon S3. Configure QuickSight to access the data  in Amazon S3."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Use Amazon Kinesis Data Streams to export the data from Amazon DynamoDB to Amazon S3. Configure QuickSight to access the data in Amazon S3.” This is because Kinesis Data Streams is designed for real - time data ingestion and processing. It can handle the continuous stream of new orders arriving in DynamoDB and transfer the data to S3 with very low latency, allowing QuickSight to access the new order information almost immediately.\n\nThe option of using AWS Glue to export data from DynamoDB to S3 involves scheduled or batch - based data processing. It is not a real - time solution and will cause significant delays between new order processing and QuickSight's access to the data.\n\nDirectly accessing DynamoDB from QuickSight via an API call is not efficient for handling the high - volume, continuous stream of new orders. DynamoDB is optimized for transactional operations, and QuickSight may face performance issues and high cost when trying to access data directly in this high - velocity scenario.\n\nAmazon Kinesis Data Firehose is more suitable for buffering and loading data into destinations in a more batch - like manner, which has a higher latency compared to Kinesis Data Streams for real - time data access requirements.\n\nIn summary, the real answer option's ability to provide near - real - time data transfer makes it the best choice to meet the requirements with the least delay, distinguishing it from the fake options.", "zhcn": "针对该问题的正确答案是“使用Amazon Kinesis Data Streams将数据从Amazon DynamoDB导出至Amazon S3，并配置QuickSight访问S3中的数据”。原因在于Kinesis Data Streams专为实时数据摄取与处理而设计，能够持续处理DynamoDB中源源不断的新订单数据，并以极低延迟将数据同步至S3，使得QuickSight几乎可即时获取最新订单信息。\n\n若采用AWS Glue实现DynamoDB到S3的数据导出，其基于定时任务或批处理的特性决定了它并非实时解决方案，会导致新订单处理与QuickSight数据访问之间存在显著延迟。\n\n通过API调用让QuickSight直接访问DynamoDB的方式，在面对高并发、持续流动的新订单流时效率低下。DynamoDB本身针对事务性操作优化，而QuickSight在此高频率数据访问场景下可能遭遇性能瓶颈及高昂成本。\n\n至于Amazon Kinesis Data Firehose，其更适用于以类批处理模式缓冲并加载数据至目标存储，相较于Kinesis Data Streams，在满足实时数据访问需求时延迟较高。\n\n综上，正确答案方案凭借其近乎实时的数据传输能力，在满足需求的同时将延迟降至最低，这一核心优势使其从其他备选方案中脱颖而出。"}, "answer": "B"}, {"id": "289", "question": {"enus": "A data engineer is preparing a dataset that a retail company will use to predict the number of visitors to stores. The data engineer created an Amazon S3 bucket. The engineer subscribed the S3 bucket to an AWS Data Exchange data product for general economic indicators. The data engineer wants to join the economic indicator data to an existing table in Amazon Athena to merge with the business data. All these transformations must finish running in 30-60 minutes. Which solution will meet these requirements MOST cost-effectively? ", "zhcn": "一位数据工程师正在为某零售公司准备用于预测门店客流量的数据集。该工程师创建了一个Amazon S3存储桶，并为其订阅了AWS Data Exchange中关于通用经济指标的数据产品。现需将经济指标数据与Amazon Athena内现有业务数据表进行关联整合，且所有数据转换操作必须在30-60分钟内完成。下列哪种解决方案能以最具成本效益的方式满足这些需求？"}, "option": [{"option_text": {"zhcn": "将AWS Data Exchange产品配置为Amazon Kinesis数据流的生产源，通过Amazon Kinesis Data Firehose传输流将数据实时输送至Amazon S3存储桶。随后运行AWS Glue作业，将既有业务数据与Athena数据表进行整合处理，最终将处理结果回写至Amazon S3。", "enus": "Configure the AWS Data Exchange product as a producer for an Amazon Kinesis data stream. Use an Amazon Kinesis Data Firehose  delivery stream to transfer the data to Amazon S3. Run an AWS Glue job that will merge the existing business data with the Athena table.  Write the result set back to Amazon S3."}, "option_flag": false}, {"option_text": {"zhcn": "在AWS Data Exchange S3存储桶上配置S3事件以触发AWS Lambda函数。通过Lambda函数调用Amazon SageMaker Data Wrangler，将现有业务数据与Athena数据表进行整合处理，并将最终结果集回传至Amazon S3存储空间。", "enus": "Use an S3 event on the AWS Data Exchange S3 bucket to invoke an AWS Lambda function. Program the Lambda function to use Amazon  SageMaker Data Wrangler to merge the existing business data with the Athena table. Write the result set back to Amazon S3."}, "option_flag": false}, {"option_text": {"zhcn": "在AWS Data Exchange S3存储桶上配置S3事件以触发AWS Lambda函数。通过Lambda函数调用AWS Glue作业，将现有业务数据与Athena表进行整合，最终将处理结果回传至Amazon S3。", "enus": "Use an S3 event on the AWS Data Exchange S3 bucket to invoke an AWS Lambda function. Program the Lambda function to run an AWS  Glue job that will merge the existing business data with the Athena table. Write the results back to Amazon S3."}, "option_flag": false}, {"option_text": {"zhcn": "部署一套Amazon Redshift集群，订阅AWS Data Exchange服务并利用该服务创建Amazon Redshift数据表。在Redshift中完成数据整合处理，最终将处理结果回传至Amazon S3存储空间。", "enus": "Provision an Amazon Redshift cluster. Subscribe to the AWS Data Exchange product and use the product to create an Amazon Redshift  table. Merge the data in Amazon Redshift. Write the results back to Amazon S3."}, "option_flag": true}], "analysis": {"enus": "The correct answer to the question is “Provision an Amazon Redshift cluster. Subscribe to the AWS Data Exchange product and use the product to create an Amazon Redshift table. Merge the data in Amazon Redshift. Write the results back to Amazon S3.” This is because Amazon Redshift is a fast, fully managed data warehousing service optimized for large - scale data analysis and data merging operations. It can handle the required data transformations within the 30 - 60 minutes time frame efficiently.\n\nThe fake answer “Configure the AWS Data Exchange product as a producer for an Amazon Kinesis data stream. Use an Amazon Kinesis Data Firehose delivery stream to transfer the data to Amazon S3. Run an AWS Glue job that will merge the existing business data with the Athena table. Write the result set back to Amazon S3.” is not ideal. Kinesis is mainly for real - time data streaming, and using it for this batch - oriented data transformation task adds unnecessary complexity and cost. AWS Glue jobs might take longer to complete and are not as optimized for large - scale data merging as Redshift.\n\nThe option “Use an S3 event on the AWS Data Exchange S3 bucket to invoke an AWS Lambda function. Program the Lambda function to use Amazon SageMaker Data Wrangler to merge the existing business data with the Athena table. Write the result set back to Amazon S3.” is also incorrect. SageMaker Data Wrangler is more focused on data preparation for machine learning models, not specifically for large - scale data merging and transformation within a short time frame. Additionally, Lambda has limitations in terms of execution time and memory, which might not be sufficient for this task.\n\nThe last fake option “Use an S3 event on the AWS Data Exchange S3 bucket to invoke an AWS Lambda function. Program the Lambda function to run an AWS Glue job that will merge the existing business data with the Athena table. Write the results back to Amazon S3.” has similar issues. Lambda's limitations and the relatively slower performance of AWS Glue jobs compared to Redshift make it a less cost - effective and timely solution. Amazon Redshift's ability to handle large - scale data operations quickly and cost - effectively is the key reason it is the real answer.", "zhcn": "针对该问题的正确答案是：\"配置一个Amazon Redshift集群，订阅AWS Data Exchange产品并利用该产品创建Amazon Redshift表。在Redshift中完成数据整合后，将结果回写至Amazon S3。\"这是因为Amazon Redshift作为一款专为大规模数据分析优化的全托管数据仓库服务，其数据合并操作能在30-60分钟内高效完成。\n\n而干扰项\"将AWS Data Exchange产品配置为Amazon Kinesis数据流的生产者，通过Kinesis Data Firehose传输数据至S3，再运行AWS Glue作业合并现有业务数据与Athena表，最终将结果集写入S3\"并不理想。Kinesis主要适用于实时数据流处理，用于此类批处理任务会徒增复杂度与成本；且AWS Glue作业的执行效率不及Redshift的大规模数据合并优化能力。\n\n另一干扰项\"利用AWS Data Exchange的S3存储桶事件触发Lambda函数，通过编程让Lambda调用Amazon SageMaker Data Wrangler合并业务数据与Athena表，最后将结果写入S3\"同样不妥。SageMaker Data Wrangler侧重于机器学习数据预处理，并非为短时大规模数据合并场景设计，加之Lambda存在运行时长与内存限制，难以胜任此任务。\n\n最后一个错误选项\"通过S3存储桶事件触发Lambda运行AWS Glue作业进行数据合并\"也存在类似问题：Lambda的固有限制与Glue相对较慢的处理速度，使其在成本效益和执行效率上均逊色于Redshift。正是Redshift快速处理海量数据的能力，使其成为本题的最优解。"}, "answer": "D"}, {"id": "290", "question": {"enus": "A company operates large cranes at a busy port The company plans to use machine learning (ML) for predictive maintenance of the cranes to avoid unexpected breakdowns and to improve productivity. The company already uses sensor data from each crane to monitor the health of the cranes in real time. The sensor data includes rotation speed, tension, energy consumption, vibration, pressure, and temperature for each crane. The company contracts AWS ML experts to implement an ML solution. Which potential findings would indicate that an ML-based solution is suitable for this scenario? (Choose two.) ", "zhcn": "某公司在繁忙港口运营大型起重机，计划采用机器学习技术实施预测性维护，以期避免意外停机并提升作业效率。目前公司已通过每台起重机的传感器数据实时监测设备运行状态，采集指标包括旋转速度、张力、能耗、振动、压力及温度等。现聘请AWS机器学习专家部署解决方案。下列哪两项潜在发现可表明该场景适合采用基于机器学习的解决方案？（请选择两项）"}, "option": [{"option_text": {"zhcn": "特定时段的历史传感器数据在数据点数量与属性维度上均存在显著缺失。", "enus": "The historical sensor data does not include a significant number of data points and attributes for certain time periods."}, "option_flag": true}, {"option_text": {"zhcn": "历史传感器数据表明，基于规则的简单阈值设定即可预测起重机故障。", "enus": "The historical sensor data shows that simple rule-based thresholds can predict crane failures."}, "option_flag": false}, {"option_text": {"zhcn": "现有历史传感器数据仅涵盖一种在役起重机型号的故障记录，而多数其他在役起重机类型的故障数据尚属空白。", "enus": "The historical sensor data contains failure data for only one type of crane model that is in operation and lacks failure data of most  other types of crane that are in operation."}, "option_flag": false}, {"option_text": {"zhcn": "过去三年间，起重机的历史传感器数据均以精细粒度完整记录在册。", "enus": "The historical sensor data from the cranes are available with high granularity for the last 3 years."}, "option_flag": true}, {"option_text": {"zhcn": "历史传感器数据涵盖了该公司希望预测的大部分常见起重机故障类型。", "enus": "The historical sensor data contains most common types of crane failures that the company wants to predict."}, "option_flag": false}], "analysis": {"enus": "The correct answers for determining if an ML - based solution is suitable for the crane predictive maintenance scenario are “The historical sensor data does not include a significant number of data points and attributes for certain time periods” and “The historical sensor data from the cranes are available with high granularity for the last 3 years”.\n\nFor the first real answer, even with missing data in certain time - periods, ML algorithms can often handle and fill in the gaps through techniques like interpolation or using probabilistic models. This shows that ML can be useful in making sense of imperfect data. The second real answer is crucial because high - granularity historical data over 3 years provides a rich source of information for ML models to learn patterns related to crane health and potential failures.\n\nThe first fake answer “The historical sensor data shows that simple rule - based thresholds can predict crane failures” indicates that there's no need for a complex ML solution as basic rules can do the job. The second fake answer “The historical sensor data contains failure data for only one type of crane model that is in operation and lacks failure data of most other types of crane that are in operation” is a problem because ML models need diverse data to generalize well. With limited failure data for most crane types, the model may not be able to accurately predict failures for all cranes. The third fake answer “The historical sensor data contains most common types of crane failures that the company wants to predict” might seem positive, but it doesn't necessarily mean ML is the best approach. If the patterns are straightforward, simple rule - based systems could be sufficient.\n\nIn summary, the real answers highlight the unique capabilities of ML to handle data with gaps and learn from high - quality historical data, while the fake answers point to situations where ML may not be the most appropriate or necessary solution.", "zhcn": "在判断基于机器学习的解决方案是否适用于起重机预测性维护场景时，正确答案应为：“历史传感器数据在某些时间段内缺乏足够数量的数据点和属性”以及“过去三年内起重机的历史传感器数据具有高粒度精度”。  \n\n对于第一个有效答案，即使某些时间段存在数据缺失，机器学习算法通常也能通过插值或概率模型等技术处理并填补空白。这表明机器学习能够有效解析不完整的数据。第二个有效答案至关重要，因为三年期的高粒度历史数据为机器学习模型提供了丰富的信息源，使其能够学习与起重机健康状况及潜在故障相关的模式。  \n\n第一个干扰项“历史传感器数据显示基于规则的简单阈值即可预测起重机故障”表明无需复杂的机器学习方案，因为基础规则已能胜任。第二个干扰项“历史传感器数据仅包含在役起重机中某一型号的故障数据，缺乏多数其他在役型号的故障记录”则暴露出关键缺陷——机器学习模型需要多样化数据以实现良好泛化能力。若多数起重机类型的故障数据有限，模型将难以准确预测所有设备的故障。第三个干扰项“历史传感器数据包含企业待预测的大多数常见起重机故障类型”看似积极，但未必意味着机器学习是最优解。若故障模式简单直接，基于规则的系统可能已足够应对。  \n\n总之，有效答案凸显了机器学习处理残缺数据并从高质量历史数据中学习的独特能力，而干扰项则揭示了机器学习未必适用或必要的场景。"}, "answer": "AD"}, {"id": "291", "question": {"enus": "A company wants to create an artificial intelligence (AШ) yoga instructor that can lead large classes of students. The company needs to create a feature that can accurately count the number of students who are in a class. The company also needs a feature that can differentiate students who are performing a yoga stretch correctly from students who are performing a stretch incorrectly. Determine whether students are performing a stretch correctly, the solution needs to measure the location and angle of each student’s arms and legs. A data scientist must use Amazon SageMaker to access video footage of a yoga class by extracting image frames and applying computer vision models. Which combination of models will meet these requirements with the LEAST effort? (Choose two.) ", "zhcn": "一家公司计划开发人工智能瑜伽教练系统，用于指导大规模团体课程。该系统需具备两项核心功能：一是精确统计课堂学员人数，二是能准确区分学员的瑜伽伸展动作是否标准。为实现动作标准度判定，解决方案需测量每位学员四肢的位置与角度数据。数据科学家需利用Amazon SageMaker平台，通过提取视频图像帧并应用计算机视觉模型来处理瑜伽课堂录像。为以最小工作量满足上述需求，应选择哪两种模型组合？（请选择两项）"}, "option": [{"option_text": {"zhcn": "图像分类", "enus": "Image Classification"}, "option_flag": true}, {"option_text": {"zhcn": "光学字符识别（OCR）", "enus": "Optical Character Recognition (OCR)"}, "option_flag": false}, {"option_text": {"zhcn": "目标检测", "enus": "Object Detection"}, "option_flag": false}, {"option_text": {"zhcn": "姿态估计", "enus": "Pose estimation"}, "option_flag": false}, {"option_text": {"zhcn": "图像生成对抗网络（GANs）", "enus": "Image Generative Adversarial Networks (GANs)"}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is ‘Image Classification’. Image Classification can be used to distinguish between students performing a yoga stretch correctly and incorrectly. By training a model on a dataset of correct and incorrect yoga poses, the model can classify new images of students in the class accordingly.\n\n‘Optical Character Recognition (OCR)’ is used for extracting text from images, which has no relevance to counting students or evaluating yoga poses, so it is not suitable. ‘Object Detection’ can detect objects in an image but may not be as effective as Image Classification for the specific task of classifying correct and incorrect poses. ‘Pose estimation’ could be used to measure the location and angle of arms and legs, but the question asks for the least - effort solution, and Image Classification can achieve the goal more straightforwardly for pose correctness. ‘Image Generative Adversarial Networks (GANs)’ are used for generating new images, not for counting students or evaluating yoga poses.\n\nThe key factor that makes Image Classification the real answer is its direct applicability to the task of differentiating correct and incorrect yoga poses with relatively less complexity compared to other models, thus meeting the requirements with the least effort.", "zhcn": "问题的最佳答案是“图像分类”。该技术能够有效判别学生瑜伽动作的正误——通过对正确与错误瑜伽姿势的数据集进行模型训练，该模型即可对课堂中新采集的学生图像进行自动分类。  \n\n“光学字符识别（OCR）”专用于从图像中提取文字信息，与统计学生人数或评估瑜伽姿势无关，故不适用。“目标检测”虽能识别图像中的物体，但在专门判别姿势正误这一特定任务上，其效果不如图像分类精准。“姿态估计”技术虽可测量四肢的位置与角度，但本题要求选择最轻量级的解决方案，而图像分类能以更直接的方式实现姿势正确性判断。“图像生成对抗网络（GANs）”主要用于生成新图像，与人数统计或瑜伽姿势评估无关。  \n\n图像分类之所以成为正确答案的关键在于：相较于其他模型，它能以较低复杂度直接适用于瑜伽姿势正误判别任务，从而以最小成本满足需求。"}, "answer": "A"}, {"id": "292", "question": {"enus": "An ecommerce company has used Amazon SageMaker to deploy a factorization machines (FM) model to suggest products for customers. The company’s data science team has developed two new models by using the TensorFlow and PyTorch deep learning frameworks. The company needs to use A/B testing to evaluate the new models against the deployed model. The required A/B testing setup is as follows: • Send 70% of trafic to the FM model, 15% of trafic to the TensorFlow model, and 15% of trafic to the PyTorch model. • For customers who are from Europe, send all trafic to the TensorFlow model. Which architecture can the company use to implement the required A/B testing setup? ", "zhcn": "一家电商公司目前正运用Amazon SageMaker平台部署了因子分解机（FM）模型，用于向客户推荐商品。该公司的数据科学团队近期基于TensorFlow和PyTorch两种深度学习框架，开发了两款全新模型。现需通过A/B测试将新模型与已部署模型进行效果评估，具体要求如下：  \n• 将70%的流量分配至FM模型，TensorFlow模型与PyTorch模型各获得15%的流量；  \n• 对欧洲地区用户，全部流量定向至TensorFlow模型。  \n请问该公司可采用何种架构方案实现此A/B测试需求？"}, "option": [{"option_text": {"zhcn": "在现有SageMaker端点基础上，为TensorFlow和PyTorch模型分别创建两个新的SageMaker端点。部署一个应用负载均衡器，并为每个端点创建对应的目标群组。配置监听器规则并为各目标群组设置流量权重。针对欧洲地区用户，需额外设置监听器规则，将其访问流量定向至TensorFlow模型对应的目标群组。", "enus": "Create two new SageMaker endpoints for the TensorFlow and PyTorch models in addition to the existing SageMaker endpoint. Create  an Application Load Balancer. Create a target group for each endpoint. Configure listener rules and add weight to the target groups. To  send trafic to the TensorFlow model for customers who are from Europe, create an additional listener rule to forward trafic to the  TensorFlow target group."}, "option_flag": false}, {"option_text": {"zhcn": "为TensorFlow与PyTorch模型分别创建两个生产版本。配置自动扩缩策略并设定流量分配权重，以引导请求分发至各生产版本。将自动扩缩策略应用于现有SageMaker端点的更新。针对欧洲地区用户，需在请求中设置TargetVariant头部，将其指向TensorFlow模型对应的版本名称以实现定向流量分发。", "enus": "Create two production variants for the TensorFlow and PyTorch models. Create an auto scaling policy and configure the desired A/B  weights to direct trafic to each production variant. Update the existing SageMaker endpoint with the auto scaling policy. To send trafic to  the TensorFlow model for customers who are from Europe, set the TargetVariant header in the request to point to the variant name of the  TensorFlow model."}, "option_flag": false}, {"option_text": {"zhcn": "在为现有SageMaker端点提供服务的基础上，需为TensorFlow与PyTorch模型分别创建新的SageMaker端点。随后部署网络负载均衡器，并为每个端点创建对应目标组。通过配置监听器规则为各目标组分配流量权重。针对欧洲地区用户，需专门增设监听器规则，将其访问请求定向至TensorFlow模型对应的目标组。", "enus": "Create two new SageMaker endpoints for the TensorFlow and PyTorch models in addition to the existing SageMaker endpoint. Create a  Network Load Balancer. Create a target group for each endpoint. Configure listener rules and add weight to the target groups. To send  trafic to the TensorFlow model for customers who are from Europe, create an additional listener rule to forward trafic to the TensorFlow  target group."}, "option_flag": false}, {"option_text": {"zhcn": "为TensorFlow与PyTorch模型分别创建两个生产版本。在SageMaker端点配置中指定各生产版本的流量分配权重，并依据新配置更新现有SageMaker端点。针对欧洲地区用户，需在请求中设置TargetVariant头部指向TensorFlow模型对应的版本名称，以确保流量定向至该模型。", "enus": "Create two production variants for the TensorFlow and PyTorch models. Specify the weight for each production variant in the  SageMaker endpoint configuration. Update the existing SageMaker endpoint with the new configuration. To send trafic to the TensorFlow  model for customers who are from Europe, set the TargetVariant header in the request to point to the variant name of the TensorFlow  model."}, "option_flag": true}], "analysis": {"enus": "The correct answer to the question is to create two production variants for the TensorFlow and PyTorch models, specify the weight for each in the SageMaker endpoint configuration, update the existing endpoint, and set the TargetVariant header for European customers. This is the most straightforward and efficient approach as Amazon SageMaker natively supports production variants and weight - based traffic distribution in endpoint configurations.\n\nThe fake options that involve creating new SageMaker endpoints and using load balancers (Application Load Balancer or Network Load Balancer) add unnecessary complexity. These methods require additional resources and management overhead, such as setting up and configuring load balancers, target groups, and listener rules. \n\nThe option about using an auto - scaling policy for A/B testing weights is also incorrect. Auto - scaling policies are mainly used to adjust the number of instances based on metrics like CPU utilization or request rate, not for directing a specific percentage of traffic for A/B testing. \n\nIn summary, the real answer takes advantage of SageMaker's built - in features for traffic management, making it the most suitable choice for the given A/B testing requirements.", "zhcn": "针对该问题的正确解决方案是：为TensorFlow和PyTorch模型创建两个生产变体，在SageMaker终端节点配置中设定各自的流量权重，更新现有终端节点，并为欧洲客户设置TargetVariant请求头。这是最直接高效的实现方式，因为Amazon SageMaker原生支持在终端节点配置中通过生产变体实现基于权重的流量分配。  \n\n那些涉及创建新SageMaker终端节点和使用负载均衡器（应用型或网络型负载均衡器）的干扰选项只会徒增复杂性。这些方法需要额外资源和管理成本，例如配置负载均衡器、目标组和监听规则。而关于使用自动扩缩策略调整A/B测试权重的选项同样不正确——自动扩缩策略主要用于根据CPU利用率或请求率等指标调整实例数量，而非定向分配A/B测试的特定流量比例。  \n\n总而言之，正确答案充分利用了SageMaker内置的流量管理功能，使其成为满足当前A/B测试需求的最优选择。"}, "answer": "D"}, {"id": "293", "question": {"enus": "A data scientist stores financial datasets in Amazon S3. The data scientist uses Amazon Athena to query the datasets by using SQL. The data scientist uses Amazon SageMaker to deploy a machine learning (ML) model. The data scientist wants to obtain inferences from the model at the SageMaker endpoint. However, when the data scientist attempts to invoke the SageMaker endpoint, the data scientist receives SQL statement failures. The data scientist’s IAM user is currently unable to invoke the SageMaker endpoint. Which combination of actions will give the data scientist’s IAM user the ability to invoke the SageMaker endpoint? (Choose three.) ", "zhcn": "一位数据科学家将金融数据集存储于Amazon S3中，并借助SQL语言通过Amazon Athena对这些数据集进行查询。随后，该科学家使用Amazon SageMaker部署了一套机器学习模型，并期望通过SageMaker端点从模型中获取推断结果。然而，在尝试调用SageMaker端点时，却出现了SQL语句执行失败的问题。目前，该数据科学家的IAM用户权限尚无法成功调用SageMaker端点。请问需要采取哪三项组合措施，方可赋予该IAM用户调用SageMaker端点的权限？（请选择三项。）"}, "option": [{"option_text": {"zhcn": "为该用户身份附加AmazonAthenaFullAccess这一AWS托管策略。", "enus": "Attach the AmazonAthenaFullAccess AWS managed policy to the user identity."}, "option_flag": false}, {"option_text": {"zhcn": "为数据科学家的IAM用户添加一项策略声明，允许该用户执行sagemaker:InvokeEndpoint操作。", "enus": "Include a policy statement for the data scientist's IAM user that allows the IAM user to perform the sagemaker:InvokeEndpoint action."}, "option_flag": false}, {"option_text": {"zhcn": "为数据科学家的IAM用户添加内联策略，使其能够通过SageMaker读取S3存储桶中的对象。", "enus": "Include an inline policy for the data scientist’s IAM user that allows SageMaker to read S3 objects."}, "option_flag": false}, {"option_text": {"zhcn": "为数据科学家的IAM用户添加策略声明，允许该IAM用户执行sagemaker:GetRecord操作。", "enus": "Include a policy statement for the data scientist’s IAM user that allows the IAM user to perform the sagemaker:GetRecord action."}, "option_flag": true}, {"option_text": {"zhcn": "在Athena SQL查询中需加入以下SQL语句：\"USING EXTERNAL FUNCTION ml_function_name\"。", "enus": "Include the SQL statement \"USING EXTERNAL FUNCTION ml_function_name'' in the Athena SQL query."}, "option_flag": false}, {"option_text": {"zhcn": "在SageMaker中执行用户重映射，将当前IAM用户关联至托管终端节点上的另一IAM用户。", "enus": "Perform a user remapping in SageMaker to map the IAM user to another IAM user that is on the hosted endpoint."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Include a policy statement for the data scientist’s IAM user that allows the IAM user to perform the sagemaker:GetRecord action.” This is because to obtain inferences from the SageMaker endpoint, the IAM user needs appropriate permissions. The sagemaker:GetRecord action is relevant as it likely pertains to retrieving the necessary data records for making inferences from the model.\n\nLet's analyze the fake answer options:\n- “Attach the AmazonAthenaFullAccess AWS managed policy to the user identity”: This policy is focused on Athena full - access and does not directly address the issue of invoking the SageMaker endpoint. The problem is related to SageMaker endpoint invocation, not Athena access.\n- “Include a policy statement for the data scientist's IAM user that allows the IAM user to perform the sagemaker:InvokeEndpoint action.”: While this seems relevant, the question might have specific requirements that the sagemaker:GetRecord action is more in line with, perhaps related to data retrieval before invocation.\n- “Include an inline policy for the data scientist’s IAM user that allows SageMaker to read S3 objects.”: This is about S3 access for SageMaker and does not directly solve the problem of the IAM user being unable to invoke the SageMaker endpoint.\n- “Include the SQL statement 'USING EXTERNAL FUNCTION ml_function_name' in the Athena SQL query.”: This is an Athena SQL statement and has no direct bearing on the IAM user's ability to invoke the SageMaker endpoint.\n- “Perform a user remapping in SageMaker to map the IAM user to another IAM user that is on the hosted endpoint.”: This is an over - complicated and unnecessary solution. The issue can be resolved by granting the appropriate permissions to the existing IAM user rather than remapping.\n\nIn summary, the key factor is that the sagemaker:GetRecord action is directly related to the data retrieval needed for making inferences from the SageMaker endpoint, which is why it is the correct choice over the fake options.", "zhcn": "针对该问题的正确答案是\"为数据科学家的IAM用户添加允许执行sagemaker:GetRecord操作策略声明\"。这是因为要从SageMaker端点获取推断结果，IAM用户需具备相应权限。sagemaker:GetRecord操作涉及从模型获取推断所需的数据记录，因而具有直接关联性。\n\n以下是对错误选项的逐一分析：\n\n- \"为用户身份附加AmazonAthenaFullAccess托管策略\"：该策略专注于Athena全权限访问，与调用SageMaker端点的问题无直接关联。当前问题涉及SageMaker端点调用而非Athena访问。\n\n- \"为数据科学家的IAM用户添加允许执行sagemaker:InvokeEndpoint操作的策略声明\"：虽看似相关，但题目可能对数据检索环节有特定要求，故sagemaker:GetRecord操作更符合实际需求。\n\n- \"为数据科学家的IAM用户添加允许SageMaker读取S3对象的行内策略\"：此方案涉及SageMaker的S3访问权限，并未直接解决IAM用户无法调用SageMaker端点的问题。\n\n- \"在Athena SQL查询中加入'USING EXTERNAL FUNCTION ml_function_name'语句\"：此为Athena SQL语法操作，与IAM用户调用SageMaker端点的权限无关。\n\n- \"在SageMaker中执行用户重映射，将IAM用户映射至托管端点上的其他IAM用户\"：此方案过于复杂且无必要。通过为现有IAM用户授予适当权限即可解决问题，无需进行用户重映射。\n\n综上所述，关键因素在于sagemaker:GetRecord操作直接关联从SageMaker端点获取推断结果所需的数据检索流程，因此相较于其他选项更为准确。"}, "answer": "D"}, {"id": "294", "question": {"enus": "A data scientist is building a linear regression model. The scientist inspects the dataset and notices that the mode of the distribution is lower than the median, and the median is lower than the mean. Which data transformation will give the data scientist the ability to apply a linear regression model? ", "zhcn": "一位数据科学家正在构建线性回归模型。在检查数据集时，他发现数据分布的众数低于中位数，而中位数又低于均值。哪种数据变换方法能让这位科学家成功应用线性回归模型？"}, "option": [{"option_text": {"zhcn": "指数级蜕变", "enus": "Exponential transformation"}, "option_flag": false}, {"option_text": {"zhcn": "对数变换", "enus": "Logarithmic transformation"}, "option_flag": false}, {"option_text": {"zhcn": "多项式变换", "enus": "Polynomial transformation"}, "option_flag": false}, {"option_text": {"zhcn": "正弦变换", "enus": "Sinusoidal transformation"}, "option_flag": true}], "analysis": {"enus": "The correct answer to the question is ‘Sinusoidal transformation’. When the mode is lower than the median, and the median is lower than the mean, the data is right - skewed. A sinusoidal transformation can help in handling complex, non - linear patterns that might be present in such skewed data and make it more suitable for a linear regression model.\n\n‘Exponential transformation’ would typically exacerbate the right - skew as it makes the large values even larger, moving the data further away from a linear relationship. ‘Logarithmic transformation’ is commonly used for right - skewed data, but it mainly works well when the data follows a multiplicative pattern, and it might not be sufficient to address all the non - linearities in this case. ‘Polynomial transformation’ adds polynomial terms to the model, which can overfit the data and may not effectively deal with the underlying non - linearity caused by the skew. The ability of the sinusoidal transformation to capture complex patterns is the key factor that makes it the real answer over the fake options.", "zhcn": "该问题的正确答案为\"Sinusoidal transformation\"。当众数低于中位数，且中位数低于均值时，数据呈右偏分布。正弦变换能够有效处理此类偏态数据中可能存在的复杂非线性模式，使其更适用于线性回归模型。\"指数变换\"通常会加剧右偏趋势，因为它会放大较大数值，导致数据更偏离线性关系。\"对数变换\"虽常用于右偏数据，但其主要适用于乘数模式的数据集，对于本案例中的非线性特征可能无法全面处理。\"多项式变换\"通过增加多项式项来调整模型，但容易导致过拟合，且难以有效解决偏态引起的本质非线性问题。正弦变换捕捉复杂模式的能力，正是其区别于干扰选项的关键所在。"}, "answer": "D"}, {"id": "295", "question": {"enus": "A data scientist receives a collection of insurance claim records. Each record includes a claim ID. the final outcome of the insurance claim, and the date of the final outcome. The final outcome of each claim is a selection from among 200 outcome categories. Some claim records include only partial information. However, incomplete claim records include only 3 or 4 outcome categories from among the 200 available outcome categories. The collection includes hundreds of records for each outcome category. The records are from the previous 3 years. The data scientist must create a solution to predict the number of claims that will be in each outcome category every month, several months in advance. Which solution will meet these requirements? ", "zhcn": "一位数据科学家收到一组保险理赔记录。每份记录包含理赔编号、理赔最终结果及其确定日期。每项理赔的最终结果均从200种分类中选定。部分记录存在信息缺失，但残缺记录仅涉及200个分类中的3至4种结果类别。该数据集收录了过去三年的记录，每个结果类别下均有数百条数据。数据科学家需要构建一个预测模型，能够提前数月精准预测每月各分类下的理赔数量。何种解决方案可满足这些要求？"}, "option": [{"option_text": {"zhcn": "每月依据理赔内容，通过监督学习方式对200项结果类别进行分类处理。", "enus": "Perform classification every month by using supervised learning of the 200 outcome categories based on claim contents."}, "option_flag": false}, {"option_text": {"zhcn": "利用理赔编号与日期信息开展强化学习，指导提交理赔记录的保险代理人按月预估各结果分类项的预期理赔数量。", "enus": "Perform reinforcement learning by using claim IDs and dates. Instruct the insurance agents who submit the claim records to estimate  the expected number of claims in each outcome category every month."}, "option_flag": false}, {"option_text": {"zhcn": "通过索赔编号与日期进行预测，以确定每月各结果类别中的预期索赔数量。", "enus": "Perform forecasting by using claim IDs and dates to identify the expected number of claims in each outcome category every month."}, "option_flag": true}, {"option_text": {"zhcn": "对已提供部分索赔内容信息的赔付类别，采用监督学习进行分类预测；对其余类别的赔付结果，则依据索赔编号与日期进行趋势推演。", "enus": "Perform classification by using supervised learning of the outcome categories for which partial information on claim contents is  provided. Perform forecasting by using claim IDs and dates for all other outcome categories."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is “Perform forecasting by using claim IDs and dates to identify the expected number of claims in each outcome category every month.” The goal is to predict the number of claims in each outcome category monthly, several months in advance. Claim IDs and dates can help track the historical distribution of claims over time, enabling the data scientist to forecast future claim counts in each category.\n\nThe option “Perform classification every month by using supervised learning of the 200 outcome categories based on claim contents” focuses on classifying claims into outcome categories rather than forecasting the number of claims in each category, so it doesn't meet the requirement.\n\nThe “Perform reinforcement learning by using claim IDs and dates. Instruct the insurance agents who submit the claim records to estimate the expected number of claims in each outcome category every month” is wrong because it involves manual estimation by agents, which is not a reliable or automated solution for forecasting. Also, reinforcement learning is not the most appropriate approach for this kind of forecasting task.\n\nThe “Perform classification by using supervised learning of the outcome categories for which partial information on claim contents is provided. Perform forecasting by using claim IDs and dates for all other outcome categories” is overly complex and unnecessary. The task is about overall forecasting of claim numbers in each category, not a split approach of classification and forecasting based on information completeness.\n\nThe key factor distinguishing the real answer is its direct focus on using relevant historical data (claim IDs and dates) for forecasting the number of claims in each outcome category, which aligns with the problem requirements.", "zhcn": "该问题的正确答案是\"利用索赔编号和日期进行预测，以确定每月各结果类别中的预期索赔数量\"。其核心目标是提前数月预测出每月各个结果类别下的索赔数量。索赔编号和日期有助于追踪索赔数据随时间的分布规律，使数据科学家能够预测未来每个类别的索赔数量。\n\n而\"基于索赔内容，通过监督学习对200个结果类别进行每月分类\"这一方案，重点在于将索赔归类到不同结果类别，而非预测各类别的索赔数量，不符合题目要求。\n\n\"利用索赔编号和日期进行强化学习，指导提交索赔记录的保险代理人估算每月各结果类别的预期索赔数量\"这一选项存在两个问题：其一依赖代理人的人工估算，这种预测方式既不可靠也非自动化；其二强化学习并非此类预测任务的最优方法。\n\n至于\"对已提供部分索赔信息的结果类别实施监督学习分类，对其余类别则采用索赔编号和日期进行预测\"这一方案，不仅过于复杂且无必要。本任务关注的是对各类别索赔数量的整体预测，而非根据信息完整度采取分类与预测割裂的方案。\n\n正确答案的关键在于直接聚焦于利用相关历史数据（索赔编号和日期）预测各结果类别的索赔数量，这与问题需求高度契合。"}, "answer": "C"}, {"id": "296", "question": {"enus": "A retail company stores 100 GB of daily transactional data in Amazon S3 at periodic intervals. The company wants to identify the schema of the transactional data. The company also wants to perform transformations on the transactional data that is in Amazon S3. The company wants to use a machine learning (ML) approach to detect fraud in the transformed data. Which combination of solutions will meet these requirements with the LEAST operational overhead? (Choose three.) ", "zhcn": "一家零售企业定期将每日100 GB的交易数据存储于亚马逊S3中。该公司需要明确这些交易数据的结构模式，并对其中的数据进行转换处理。此外，企业还希望采用机器学习方法，在转换后的数据中实现欺诈行为检测。若要同时满足这些需求且将运维负担降至最低，应选择哪三种解决方案的组合？（请选出三项。）"}, "option": [{"option_text": {"zhcn": "利用Amazon Athena对数据进行扫描并解析其结构。", "enus": "Use Amazon Athena to scan the data and identify the schema."}, "option_flag": false}, {"option_text": {"zhcn": "借助AWS Glue爬虫程序自动扫描数据并智能识别其结构模式。", "enus": "Use AWS Glue crawlers to scan the data and identify the schema."}, "option_flag": true}, {"option_text": {"zhcn": "利用Amazon Redshift存储过程，实现数据转换处理。", "enus": "Use Amazon Redshift to store procedures to perform data transformations."}, "option_flag": false}, {"option_text": {"zhcn": "借助AWS Glue工作流与作业功能，实现数据转换处理。", "enus": "Use AWS Glue workfiows and AWS Glue jobs to perform data transformations."}, "option_flag": true}, {"option_text": {"zhcn": "利用Amazon Redshift ML训练模型以识别欺诈行为。", "enus": "Use Amazon Redshift ML to train a model to detect fraud."}, "option_flag": false}, {"option_text": {"zhcn": "利用Amazon Fraud Detector训练模型以识别欺诈行为。", "enus": "Use Amazon Fraud Detector to train a model to detect fraud."}, "option_flag": true}], "analysis": {"enus": "The correct answers for identifying the schema, performing data transformations, and detecting fraud with the least operational overhead are using AWS Glue crawlers, AWS Glue workflows and jobs, and Amazon Fraud Detector. \n\nAWS Glue crawlers are designed to automatically scan data in S3 and infer the schema, which is an efficient way to identify the structure of the transactional data without manual intervention. AWS Glue workflows and jobs simplify data transformation tasks. They handle the orchestration and execution of data transformation jobs, reducing the need for complex infrastructure management. Amazon Fraud Detector is a fully - managed service. It can quickly train fraud - detection models using pre - built templates and algorithms, minimizing the effort required for model development.\n\nAmazon Athena is mainly for querying data in S3. While it can be used to understand data, it is not optimized for schema discovery like AWS Glue crawlers, so it is less suitable for this task. Amazon Redshift is a data warehousing service. Using its stored procedures for data transformation and Amazon Redshift ML for fraud detection would require more setup, maintenance, and management compared to the AWS Glue and Amazon Fraud Detector solutions, adding more operational overhead.", "zhcn": "在识别数据模式、执行数据转换及以最小运维成本实现欺诈检测的场景中，最优解决方案是采用AWS Glue爬虫程序、AWS Glue工作流与任务以及Amazon Fraud Detector。AWS Glue爬虫能自动扫描S3存储中的数据并智能推断结构模式，无需人工干预即可高效解析交易数据的组织形式。AWS Glue工作流与任务则大幅简化数据转换流程，通过自动化编排与执行转换任务，有效降低复杂基础设施的管理负担。\n\nAmazon Fraud Detector作为全托管服务，依托预置模板与算法可快速训练欺诈检测模型，极大缩减模型开发所需投入。而Amazon Athena虽可用于查询S3数据，其核心功能聚焦于数据探查而非结构识别，在模式发现方面不如AWS Glue爬虫专业，故不适用于此场景。至于Amazon Redshift数据仓库服务，若通过其存储过程进行数据转换并借助Redshift ML实现欺诈检测，将需要更多配置维护工作，相较于AWS Glue与Amazon Fraud Detector组合方案会显著增加运维复杂度。"}, "answer": "BDF"}, {"id": "297", "question": {"enus": "A data scientist uses Amazon SageMaker Data Wrangler to define and perform transformations and feature engineering on historical data. The data scientist saves the transformations to SageMaker Feature Store. The historical data is periodically uploaded to an Amazon S3 bucket. The data scientist needs to transform the new historic data and add it to the online feature store. The data scientist needs to prepare the new historic data for training and inference by using native integrations. Which solution will meet these requirements with the LEAST development effort? ", "zhcn": "一位数据科学家借助Amazon SageMaker Data Wrangler对历史数据进行转换与特征工程定义，并将转换流程保存至SageMaker Feature Store。历史数据会定期上传至亚马逊S3存储桶。该科学家需对新入库的历史数据实施相同转换，并将其添加入线特征库，同时通过原生集成功能为模型训练与推理准备数据。要满足上述需求且最大限度降低开发工作量，应当采用何种解决方案？"}, "option": [{"option_text": {"zhcn": "利用AWS Lambda触发预设的SageMaker流程，对每份上传至S3存储桶的新数据集自动执行转换操作。", "enus": "Use AWS Lambda to run a predefined SageMaker pipeline to perform the transformations on each new dataset that arrives in the S3  bucket."}, "option_flag": false}, {"option_text": {"zhcn": "每当S3存储桶中有新的数据集抵达时，系统将自动执行AWS Step Functions工作流步骤，并调用预定义的SageMaker管道来完成数据转换处理。", "enus": "Run an AWS Step Functions step and a predefined SageMaker pipeline to perform the transformations on each new dataset that arrives  in the S3 bucket."}, "option_flag": false}, {"option_text": {"zhcn": "利用Apache Airflow对传入S3存储桶的每个新数据集执行一系列预定义的数据转换流程编排。", "enus": "Use Apache Airfiow to orchestrate a set of predefined transformations on each new dataset that arrives in the S3 bucket."}, "option_flag": true}, {"option_text": {"zhcn": "当检测到S3存储桶中出现新数据时，配置Amazon EventBridge以运行预定义的SageMaker管道来执行数据转换操作。", "enus": "Configure Amazon EventBridge to run a predefined SageMaker pipeline to perform the transformations when a new data is detected in  the S3 bucket."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is “Use Apache Airflow to orchestrate a set of predefined transformations on each new dataset that arrives in the S3 bucket.” This option requires the least development effort as Apache Airflow has native integrations with Amazon SageMaker Data Wrangler. It can easily manage and schedule the transformation tasks on new data as it aligns well with the need to perform predefined operations.\n\nThe fake answer options involve using AWS Lambda, AWS Step Functions, or Amazon EventBridge in combination with a SageMaker pipeline. While these services can achieve the goal, they typically demand more development work. For instance, AWS Lambda requires writing custom code to trigger and manage the SageMaker pipeline. AWS Step Functions need careful definition of state machines and orchestration logic. Amazon EventBridge also needs proper configuration to detect new data and trigger the pipeline. A common misconception might be thinking that using AWS - native services will always be the simplest, but in this case, Apache Airflow's existing integrations make it the most straightforward solution for minimizing development effort.", "zhcn": "对于题目中“如何对传入S3存储桶的新数据集执行预定义转换”这一问题，正确答案是“采用Apache Airflow编排预定义的转换流程”。该方案能最大程度降低开发复杂度，因为Apache Airflow原生集成了Amazon SageMaker Data Wrangler，可轻松实现对新数据转换任务的管理与调度，完美契合批量执行标准化操作的需求。  \n  \n干扰项中提出的方案包括使用AWS Lambda、AWS Step Functions或结合Amazon EventBridge与SageMaker流水线。虽然这些服务也能实现目标，但通常需要更多开发投入。例如AWS Lambda需编写定制代码来触发和管理SageMaker流水线；AWS Step Functions要求精确定义状态机与编排逻辑；Amazon EventBridge也需复杂配置来监测新数据并触发流水线。常见的误解是认为原生AWS服务必然最简便，但本案中Apache Airflow的现成集成机制反而成为减少开发工作量的最直接选择。"}, "answer": "C"}, {"id": "298", "question": {"enus": "An insurance company developed a new experimental machine learning (ML) model to replace an existing model that is in production. The company must validate the quality of predictions from the new experimental model in a production environment before the company uses the new experimental model to serve general user requests. New one model can serve user requests at a time. The company must measure the performance of the new experimental model without affecting the current live trafic. Which solution will meet these requirements? ", "zhcn": "一家保险公司研发出一款全新的实验性机器学习模型，旨在替代当前投入生产的现有模型。在将该实验模型正式用于处理常规用户请求之前，公司需在生产环境中验证其预测质量。系统每次仅能启用一个模型处理用户请求。公司必须在不影响现有实时流量的前提下，评估新实验模型的性能表现。何种解决方案可满足上述需求？"}, "option": [{"option_text": {"zhcn": "A/B 测试", "enus": "A/B testing"}, "option_flag": false}, {"option_text": {"zhcn": "金丝雀发布", "enus": "Canary release"}, "option_flag": false}, {"option_text": {"zhcn": "暗影部署", "enus": "Shadow deployment"}, "option_flag": false}, {"option_text": {"zhcn": "蓝绿部署", "enus": "Blue/green deployment"}, "option_flag": true}], "analysis": {"enus": "The correct answer to the question is ‘Blue/green deployment’. In this approach, there are two identical environments: the ‘blue’ (current production) and the ‘green’ (new experimental model). The live traffic runs on the blue environment, and the green environment with the new model can be thoroughly tested without affecting the live traffic. Once the new model is validated, the traffic can be switched from blue to green.\n\n‘A/B testing’ involves splitting the traffic between two or more versions of a model to compare their performance. This would affect the live traffic as some users would be directed to the new experimental model. ‘Canary release’ gradually rolls out the new model to a small subset of users, which also impacts live traffic. ‘Shadow deployment’ runs the new model in parallel with the existing one and sends live traffic to both, but it still has an impact on the live traffic as the new model processes real - time data. \n\nThe key factor distinguishing blue/green deployment is that it allows for full - scale testing of the new model in a production - like environment without any interference with the live traffic, making it the best solution to meet the requirements of the question.", "zhcn": "该问题的正确答案是\"蓝绿部署\"。这种部署模式包含两个完全一致的环境：\"蓝色\"环境代表当前生产版本，\"绿色\"环境则对应新上线的实验模型。线上真实流量始终在蓝色环境中运行，而搭载新模型的绿色环境可进行全面测试，完全不影响线上服务。待新模型通过验证后，只需将流量从蓝色环境切换至绿色环境即可。\n\n\"A/B测试\"会将流量分流至多个不同版本的模型进行效果对比，这种做法会使部分用户访问新模型，从而影响线上流量。\"金丝雀发布\"采用渐进式推广策略，逐步向小范围用户开放新模型，同样会波及线上服务。\"影子部署\"虽然让新旧模型并行运行，同时处理实时数据，但依然会对线上流量产生实际影响。\n\n蓝绿部署的核心优势在于：它能在类生产环境中实现新模型的全面测试，且完全不影响线上实时流量，这使其成为满足本题要求的最佳解决方案。"}, "answer": "D"}, {"id": "299", "question": {"enus": "A company deployed a machine learning (ML) model on the company website to predict real estate prices. Several months after deployment, an ML engineer notices that the accuracy of the model has gradually decreased. The ML engineer needs to improve the accuracy of the model. The engineer also needs to receive notifications for any future performance issues. Which solution will meet these requirements? ", "zhcn": "某公司在官方网站部署了一套机器学习模型，用于预测房地产价格。上线数月后，机器学习工程师发现模型预测准确度逐渐下降。该工程师需提升模型精度，同时建立未来性能异常的自动通知机制。请问下列哪种方案能同时满足这些需求？"}, "option": [{"option_text": {"zhcn": "对模型进行增量训练以完成更新。启用亚马逊SageMaker模型监控功能，以便检测模型性能问题并发送通知。", "enus": "Perform incremental training to update the model. Activate Amazon SageMaker Model Monitor to detect model performance issues and  to send notifications."}, "option_flag": false}, {"option_text": {"zhcn": "使用Amazon SageMaker模型治理功能。通过配置模型治理自动调整模型超参数。在Amazon CloudWatch中创建性能阈值告警以便发送通知。", "enus": "Use Amazon SageMaker Model Governance. Configure Model Governance to automatically adjust model hyperparameters. Create a  performance threshold alarm in Amazon CloudWatch to send notifications."}, "option_flag": false}, {"option_text": {"zhcn": "合理设定阈值以启用Amazon SageMaker Debugger，配置调试器向团队发送Amazon CloudWatch警报。仅采用过去数月的数据对模型进行重新训练。", "enus": "Use Amazon SageMaker Debugger with appropriate thresholds. Configure Debugger to send Amazon CloudWatch alarms to alert the  team. Retrain the model by using only data from the previous several months."}, "option_flag": true}, {"option_text": {"zhcn": "仅采用近数月的数据进行增量训练，以完成模型迭代更新。通过亚马逊SageMaker模型监测平台及时侦测模型性能异常，并自动发送预警通知。", "enus": "Use only data from the previous several months to perform incremental training to update the model. Use Amazon SageMaker Model  Monitor to detect model performance issues and to send notifications."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Use Amazon SageMaker Debugger with appropriate thresholds. Configure Debugger to send Amazon CloudWatch alarms to alert the team. Retrain the model by using only data from the previous several months.” This solution addresses both requirements effectively. Retraining the model with recent data helps adapt to any changes in the real - estate market that may have caused the accuracy decline. Amazon SageMaker Debugger can monitor the model's performance, and by setting appropriate thresholds and integrating with CloudWatch alarms, the engineer can be notified of future performance issues.\n\nThe first fake option suggests incremental training and using Amazon SageMaker Model Monitor. While incremental training can update the model, it may not fully account for the changes in the data distribution over several months. Also, Model Monitor is more focused on monitoring data drift and model quality in production, and it might not be as effective as Debugger for directly improving the model's accuracy during retraining.\n\nThe second fake option involves Amazon SageMaker Model Governance. Model Governance is mainly about managing and governing the model lifecycle, including aspects like approval workflows and compliance. Automatically adjusting hyperparameters may not be sufficient to address the root cause of the accuracy decline, which could be due to changes in the data.\n\nThe third fake option combines incremental training with recent data and Model Monitor. Similar to the first fake option, incremental training may not fully capture the new data patterns, and Model Monitor is better for monitoring rather than directly improving the model's accuracy during retraining.\n\nIn summary, the real answer option is the most suitable as it directly tackles the need to improve the model's accuracy through retraining with recent data and provides a reliable way to receive notifications for future performance issues.", "zhcn": "针对该问题的正确答案是：\"使用 Amazon SageMaker Debugger 并设置合理阈值，配置 Debugger 向团队发送 Amazon CloudWatch 警报，仅采用近几个月的数据重新训练模型。\" 这一方案能同时满足两项需求：通过最新数据重新训练模型，可有效适应可能导致准确率下降的房地产市场变化；借助 Amazon SageMaker Debugger 监控模型性能，结合阈值设置与 CloudWatch 警报机制，工程师可及时获知未来可能出现的性能问题。\n\n首个干扰项建议采用增量训练并搭配 Amazon SageMaker Model Monitor。虽然增量训练能更新模型，但难以完全应对数月来数据分布的变化。且 Model Monitor 更侧重于监控生产环境中的数据漂移和模型质量，在重新训练过程中直接提升模型准确性的效果不如 Debugger。\n\n第二个干扰项涉及 Amazon SageMaker Model Governance。该功能主要专注于模型生命周期的管理与治理，包括审批流程和合规性等方面。自动调整超参数未必能解决准确率下降的根本原因——这很可能源于数据本身的变化。\n\n第三个干扰项结合了增量训练与 Model Monitor。其局限性与前两个类似：增量训练难以完整捕捉新数据特征，而 Model Monitor 更适用于监控场景，而非在重新训练阶段直接提升模型准确性。\n\n综上所述，正确答案选项最为适宜——它通过近期数据重训练直指提升模型准确性的核心需求，同时为未来性能问题提供了可靠的通知机制。"}, "answer": "C"}, {"id": "300", "question": {"enus": "A university wants to develop a targeted recruitment strategy to increase new student enrollment. A data scientist gathers information about the academic performance history of students. The data scientist wants to use the data to build student profiles. The university will use the profiles to direct resources to recruit students who are likely to enroll in the university. Which combination of steps should the data scientist take to predict whether a particular student applicant is likely to enroll in the university? (Choose two.) ", "zhcn": "某大学计划制定精准招生策略以提升新生录取率。一位数据科学家着手收集学生过往学业表现的相关信息，旨在通过数据分析构建学生画像。校方将借助这些画像精准配置招生资源，重点吸纳入学意愿强烈的申请者。为预测特定申请人是否倾向于就读该校，数据科学家应当采取下列哪两项组合步骤？（请选择两项）"}, "option": [{"option_text": {"zhcn": "利用Amazon SageMaker Ground Truth将数据归类至\"enrolled\"（已注册）与\"not enrolled\"（未注册）两个分组中。", "enus": "Use Amazon SageMaker Ground Truth to sort the data into two groups named \"enrolled\" or \"not enrolled.\""}, "option_flag": false}, {"option_text": {"zhcn": "运用预测算法进行趋势推演。", "enus": "Use a forecasting algorithm to run predictions."}, "option_flag": false}, {"option_text": {"zhcn": "运用回归算法进行预测分析。", "enus": "Use a regression algorithm to run predictions."}, "option_flag": false}, {"option_text": {"zhcn": "运用分类算法进行预测分析。", "enus": "Use a classification algorithm to run predictions."}, "option_flag": true}, {"option_text": {"zhcn": "运用亚马逊SageMaker内置的k均值算法，将数据划分为名为\"已注册\"与\"未注册\"的两个群组。", "enus": "Use the built-in Amazon SageMaker k-means algorithm to cluster the data into two groups named \"enrolled\" or \"not enrolled.\""}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Use a classification algorithm to run predictions.” This is because the goal is to predict whether a particular student applicant is likely to enroll in the university, which is a binary classification problem (either the student will enroll or not). Classification algorithms are designed to assign data points to predefined classes, making them suitable for this task.\n\nThe option “Use Amazon SageMaker Ground Truth to sort the data into two groups named 'enrolled' or 'not enrolled'” is incorrect. Amazon SageMaker Ground Truth is mainly used for data labeling, not for making predictions about future enrollment. \n\n“Use a forecasting algorithm to run predictions” is wrong because forecasting algorithms are typically used for predicting future numerical values like sales or stock prices over time, not for classifying whether a student will enroll.\n\n“Use a regression algorithm to run predictions” is also incorrect. Regression algorithms are used to predict continuous numerical values, such as predicting a student's GPA, rather than a binary outcome like enrollment.\n\n“Use the built - in Amazon SageMaker k - means algorithm to cluster the data into two groups named 'enrolled' or 'not enrolled'” is not appropriate. K - means is an unsupervised learning algorithm used for clustering data based on similarity, without predefined classes. In this case, we need a supervised learning approach to predict the enrollment status. The nature of the problem and the suitability of the classification algorithm for binary prediction are the key factors that distinguish the real answer from the fake options.", "zhcn": "对于该问题的正确答案是\"采用分类算法进行预测\"。原因在于，我们需要预测特定申请者是否有可能入读该校，这本质上是一个二分类问题（学生要么入学，要么不入）。分类算法的设计初衷就是将数据点划分到预定义的类别中，因此非常适合处理此类任务。\n\n而\"使用Amazon SageMaker Ground Truth将数据标记为'已入学'或'未入学'两组\"这一选项并不正确。该工具主要功能是数据标注，而非对未来入学情况进行预测。\"采用预测算法进行预测\"同样错误，因为预测算法通常用于预估随时间变化的数值（如销售额或股价），不适用于判断学生是否入学这类分类问题。\n\n\"采用回归算法进行预测\"亦不恰当。回归算法适用于预测连续型数值（如学生绩点），而非入学状态这样的二元结果。\n\n\"使用Amazon SageMaker内置k均值算法将数据聚类为'已入学'或'未入学'两组\"也不合适。K均值属于无监督学习算法，其聚类依据是数据相似性而非预设类别。而本案需要采用有监督学习方法来预测入学状态。正是问题的本质特征及分类算法在二分类预测中的适用性，构成了正确答案与其他干扰项的根本区别。"}, "answer": "D"}, {"id": "301", "question": {"enus": "A machine learning (ML) specialist is using the Amazon SageMaker DeepAR forecasting algorithm to train a model on CPU-based Amazon EC2 On-Demand instances. The model currently takes multiple hours to train. The ML specialist wants to decrease the training time of the model. Which approaches will meet this requirement? (Choose two.) ", "zhcn": "一位机器学习专家正利用基于CPU的亚马逊EC2按需实例，通过Amazon SageMaker平台的DeepAR预测算法训练模型。当前模型训练耗时数小时之久。该专家希望缩短模型训练时长，下列哪两种方法可实现此目标？（请选择两项正确答案）"}, "option": [{"option_text": {"zhcn": "将按需实例替换为竞价实例。", "enus": "Replace On-Demand Instances with Spot Instances."}, "option_flag": true}, {"option_text": {"zhcn": "根据负载变化动态配置模型自动扩缩容，实现实例数量自主调节。", "enus": "Configure model auto scaling dynamically to adjust the number of instances automatically."}, "option_flag": false}, {"option_text": {"zhcn": "将基于CPU的EC2实例更换为基于GPU的EC2实例。", "enus": "Replace CPU-based EC2 instances with GPU-based EC2 instances."}, "option_flag": false}, {"option_text": {"zhcn": "采用多组训练样本。", "enus": "Use multiple training instances."}, "option_flag": true}, {"option_text": {"zhcn": "建议使用模型的预训练版本，并在此基础上进行增量训练。", "enus": "Use a pre-trained version of the model. Run incremental training."}, "option_flag": false}], "analysis": {"enus": "The correct answers to the question “A machine learning (ML) specialist is using the Amazon SageMaker DeepAR forecasting algorithm to train a model on CPU - based Amazon EC2 On - Demand instances. The model currently takes multiple hours to train. The ML specialist wants to decrease the training time of the model. Which approaches will meet this requirement?” are “Replace On - Demand Instances with Spot Instances.” and “Use multiple training instances.”\n\nReplacing On - Demand Instances with Spot Instances can reduce training time because Spot Instances are spare Amazon EC2 computing capacity that is available at a significantly lower cost and can be quickly provisioned. This allows for potentially faster access to resources and thus can speed up the training process. Using multiple training instances enables parallel processing. The workload of training the model can be divided among these instances, which can greatly reduce the overall training time.\n\nThe option “Configure model auto scaling dynamically to adjust the number of instances automatically” is incorrect because auto - scaling is more about adjusting resources based on demand during the inference phase rather than directly reducing training time. “Replace CPU - based EC2 instances with GPU - based EC2 instances” is not valid here as the DeepAR algorithm is designed to run efficiently on CPUs, and switching to GPUs may not necessarily lead to a decrease in training time. “Use a pre - trained version of the model. Run incremental training” is wrong because incremental training is more about updating an existing model with new data, and it doesn't directly address the issue of reducing the initial long training time.\n\nThese key differences between the real and fake answer options, based on the algorithm's characteristics and the nature of the proposed solutions, are why the selected real answer options are the correct choices to decrease the model's training time.", "zhcn": "针对问题“一位机器学习专家正使用基于CPU的Amazon EC2按需实例，通过Amazon SageMaker DeepAR预测算法训练模型。当前模型训练耗时数小时。该专家希望缩短训练时间，下列哪种方法可实现此目标？”的正确答案为“将按需实例替换为Spot实例”与“采用多个训练实例”。\n\n将按需实例替换为Spot实例可缩短训练时间，因为Spot实例是亚马逊EC2的闲置计算资源，成本显著更低且能快速部署。这使得资源获取可能更迅速，从而加速训练进程。使用多个训练实例则能实现并行处理：模型训练的工作负载可分摊至这些实例上，大幅缩短整体训练时长。\n\n而“配置模型自动扩展功能以动态调整实例数量”这一选项并不正确，因为自动扩展主要针对推理阶段根据需求调整资源，而非直接缩短训练时间；“将基于CPU的EC2实例替换为基于GPU的实例”在此场景下无效，因DeepAR算法本身针对CPU运行效率优化，改用GPU未必能降低训练时长；“使用预训练模型版本并进行增量训练”亦属错误，增量训练侧重于通过新数据更新现有模型，并未直接解决初始训练耗时过长的问题。\n\n正是基于DeepAR算法的特性及各解决方案的本质差异，上述两个正确选项才能成为缩短模型训练时间的有效策略。"}, "answer": "AD"}, {"id": "302", "question": {"enus": "A chemical company has developed several machine learning (ML) solutions to identify chemical process abnormalities. The time series values of independent variables and the labels are available for the past 2 years and are suficient to accurately model the problem. The regular operation label is marked as 0 The abnormal operation label is marked as 1. Process abnormalities have a significant negative effect on the company’s profits. The company must avoid these abnormalities. Which metrics will indicate an ML solution that will provide the GREATEST probability of detecting an abnormality? ", "zhcn": "某化工企业已开发出多项机器学习解决方案，用于识别化工流程异常。过去两年的自变量时间序列数据和对应标签完备可用，足以精准构建问题模型。正常工况标记为0，异常工况标记为1。流程异常会对企业利润产生重大负面影响，必须彻底规避此类异常。在下列评估指标中，哪项能最能确保机器学习方案捕获异常现象的最大概率？"}, "option": [{"option_text": {"zhcn": "精确率 = 0.91 - 召回率 = 0.6", "enus": "Precision = 0.91 -  Recall = 0.6"}, "option_flag": true}, {"option_text": {"zhcn": "精确率 = 0.61 - 召回率 = 0.98", "enus": "Precision = 0.61 -  Recall = 0.98"}, "option_flag": false}, {"option_text": {"zhcn": "精确率 = 0.7 - 召回率 = 0.9", "enus": "Precision = 0.7 -  Recall = 0.9"}, "option_flag": false}, {"option_text": {"zhcn": "精确率 = 0.98 - 召回率 = 0.8", "enus": "Precision = 0.98 -  Recall = 0.8"}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is “Precision = 0.91 - Recall = 0.6”. In this context, precision measures the proportion of correctly predicted positive cases (abnormal operations) out of all predicted positive cases. Recall measures the proportion of actual positive cases that are correctly predicted.\n\nThe company's main goal is to avoid process abnormalities which have a significant negative impact on profits. A high - precision model is crucial as it means that when the model predicts an abnormality, it is very likely to be correct. This helps in preventing false alarms that could lead to unnecessary actions and costs.\n\nThe option “Precision = 0.91 - Recall = 0.6” has a relatively high precision, which indicates that most of the predicted abnormalities are actual abnormalities. \n\nThe fake answer “Precision = 0.61 - Recall = 0.98” has a very high recall but low precision. This means that while it can detect most of the actual abnormalities, a large number of the predicted abnormalities are false positives, which can lead to unnecessary actions. \n\nThe option “Precision = 0.7 - Recall = 0.9” also has a relatively lower precision compared to the real answer, so there is a higher chance of false positives. \n\nThe option “Precision = 0.98 - Recall = 0.8” has high precision but lower recall than the real answer, which means it may miss some actual abnormalities. The key factor here is that high precision is more important for the company to avoid false alarms and focus on real threats to profits, which is why “Precision = 0.91 - Recall = 0.6” is the correct choice.", "zhcn": "该问题的正确答案为“精确率 = 0.91 - 召回率 = 0.6”。在此场景中，精确率衡量的是预测为正例（异常工况）中真实正例的比例，而召回率则反映实际正例被正确预测的比例。由于公司核心目标是规避对利润产生重大负面影响的生产异常，高精确率模型至关重要——当模型预警异常时，其判断结果具有高度可信性，能有效避免因误报引发不必要的干预措施及成本损耗。\n\n选项“精确率 = 0.91 - 召回率 = 0.6”具有较高精确率，意味着绝大多数预测异常确属真实异常。错误选项“精确率 = 0.61 - 召回率 = 0.98”虽召回率极高但精确率偏低，虽能捕捉大部分真实异常，却会产生大量误报，易导致过度干预。选项“精确率 = 0.7 - 召回率 = 0.9”与正确答案相比精确率较低，误报风险更高。而“精确率 = 0.98 - 召回率 = 0.8”虽精确率较高，但召回率低于正确答案，可能遗漏部分真实异常。综合考虑企业需优先防范误报、聚焦真实利润威胁的核心诉求，高精确率更具战略价值，故“精确率 = 0.91 - 召回率 = 0.6”为最优选择。"}, "answer": "A"}, {"id": "303", "question": {"enus": "An online delivery company wants to choose the fastest courier for each delivery at the moment an order is placed. The company wants to implement this feature for existing users and new users of its application. Data scientists have trained separate models with XGBoost for this purpose, and the models are stored in Amazon S3. There is one model for each city where the company operates. Operation engineers are hosting these models in Amazon EC2 for responding to the web client requests, with one instance for each model, but the instances have only a 5% utilization in CPU and memory. The operation engineers want to avoid managing unnecessary resources. Which solution will enable the company to achieve its goal with the LEAST operational overhead? ", "zhcn": "一家外卖配送公司希望在用户下单时，就能为每笔订单匹配最快的骑手。公司计划为现有用户及新用户的应用端同步实现这一功能。数据科学家已基于XGBoost算法针对不同城市训练了独立预测模型，并将模型存储于亚马逊S3服务中。目前运维团队为每个城市模型单独配置了亚马逊EC2实例以响应客户端请求，但实例的CPU与内存利用率仅达5%。为避免资源空置，运维团队希望尽可能减少冗余管理成本。下列哪种方案能以最低运维负担实现该目标？"}, "option": [{"option_text": {"zhcn": "创建一个Amazon SageMaker笔记本实例，用于通过boto3库从Amazon S3拉取所有模型。删除现有实例，并利用该笔记本执行SageMaker批量转换任务，为所有城市中的潜在用户实现离线推理。将结果以独立文件形式存储于Amazon S3中，并将网络客户端指向这些文件。", "enus": "Create an Amazon SageMaker notebook instance for pulling all the models from Amazon S3 using the boto3 library. Remove the  existing instances and use the notebook to perform a SageMaker batch transform for performing inferences ofiine for all the possible  users in all the cities. Store the results in different files in Amazon S3. Point the web client to the files."}, "option_flag": false}, {"option_text": {"zhcn": "基于开源多模型服务器，构建一个亚马逊SageMaker的Docker容器。移除现有实例，转而在SageMaker中创建多模型端点，并将其指向存储所有模型的S3存储桶。在运行时通过Web客户端调用该端点，并依据每项请求对应的城市信息指定TargetModel参数。", "enus": "Prepare an Amazon SageMaker Docker container based on the open-source multi-model server. Remove the existing instances and  create a multi-model endpoint in SageMaker instead, pointing to the S3 bucket containing all the models. Invoke the endpoint from the  web client at runtime, specifying the TargetModel parameter according to the city of each request."}, "option_flag": true}, {"option_text": {"zhcn": "仅保留单一EC2实例承载所有模型。在该实例中部署模型服务器，并通过从Amazon S3拉取模型文件的方式加载各模型。通过Amazon API网关将实例与网页客户端集成，实现实时请求响应，并依据每项请求所在城市指定目标资源。", "enus": "Keep only a single EC2 instance for hosting all the models. Install a model server in the instance and load each model by pulling it from  Amazon S3. Integrate the instance with the web client using Amazon API Gateway for responding to the requests in real time, specifying  the target resource according to the city of each request."}, "option_flag": false}, {"option_text": {"zhcn": "基于亚马逊SageMaker预构建镜像准备Docker容器。将现有实例替换为独立的SageMaker端点，为该公司运营的每个城市分别部署一个。通过网页客户端调用这些端点，根据请求所属城市指定对应的URL和端点名称参数。", "enus": "Prepare a Docker container based on the prebuilt images in Amazon SageMaker. Replace the existing instances with separate  SageMaker endpoints, one for each city where the company operates. Invoke the endpoints from the web client, specifying the URL and  EndpointName parameter according to the city of each request."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to prepare an Amazon SageMaker Docker container based on the open - source multi - model server, remove the existing instances, create a multi - model endpoint in SageMaker, and point it to the S3 bucket with all models. This solution has the least operational overhead as it allows multiple models to be hosted on a single endpoint. When a request comes in, the appropriate model can be selected at runtime by specifying the TargetModel parameter, which simplifies management.\n\nThe first fake answer involves using a SageMaker notebook instance for batch transforms. This is not suitable for real - time decision - making at the moment an order is placed, as it performs offline inferences and stores results in files. The company needs to choose the fastest courier in real - time, so this option is not a good fit.\n\nThe second fake answer keeps a single EC2 instance and installs a model server. This still requires the operation engineers to manage the EC2 instance, including software installation, security, and maintenance, which goes against the goal of minimizing operational overhead.\n\nThe third fake answer creates separate SageMaker endpoints for each city. This leads to more endpoints to manage compared to a single multi - model endpoint, increasing the complexity and operational overhead. \n\nIn summary, the real answer option provides a streamlined and efficient way to host and serve multiple models with minimal management, making it the best choice for the company's requirements.", "zhcn": "针对该问题的正确答案是：基于开源多模型服务器构建Amazon SageMaker Docker容器，移除现有实例，在SageMaker中创建多模型端点，并将其指向存储所有模型的S3存储桶。此方案具有最低运维成本，因为多个模型可共用一个端点服务。当请求到达时，只需在运行时通过指定TargetModel参数即可动态选择对应模型，极大简化了管理流程。\n\n首个干扰项采用SageMaker笔记本实例执行批量转换。由于该方案进行离线推理并将结果存储于文件，无法在用户下单时实现实时决策。而公司需要实时匹配最快物流商，故此方案不适用。\n\n第二个干扰项保留单一EC2实例并安装模型服务器。这仍需要运维工程师管理EC2实例的软件安装、安全维护等工作，与降低运维负担的目标相悖。\n\n第三个干扰项为每个城市创建独立SageMaker端点。相较于单一多模型端点方案，该做法会导致端点数量激增，增加系统复杂度和运维压力。\n\n综上所述，正确答案通过集约化托管多模型的方式，以最小管理成本实现了高效服务部署，最契合该企业的实际需求。"}, "answer": "B"}, {"id": "304", "question": {"enus": "A company builds computer-vision models that use deep learning for the autonomous vehicle industry. A machine learning (ML) specialist uses an Amazon EC2 instance that has a CPU:GPU ratio of 12:1 to train the models. The ML specialist examines the instance metric logs and notices that the GPU is idle half of the time. The ML specialist must reduce training costs without increasing the duration of the training jobs. Which solution will meet these requirements? ", "zhcn": "一家公司为自动驾驶汽车行业开发基于深度学习的计算机视觉模型。一位机器学习专家采用CPU与GPU配比为12:1的亚马逊EC2实例进行模型训练。该专家在分析实例运行指标时发现，GPU有半数时间处于闲置状态。现需在不延长训练时长的前提下降低训练成本，下列哪项方案符合此要求？"}, "option": [{"option_text": {"zhcn": "切换至仅配备CPU的实例类型。", "enus": "Switch to an instance type that has only CPUs."}, "option_flag": false}, {"option_text": {"zhcn": "在一个异构集群中部署两组不同的实例组。", "enus": "Use a heterogeneous cluster that has two different instances groups."}, "option_flag": false}, {"option_text": {"zhcn": "训练任务可采用内存优化的EC2竞价型实例。", "enus": "Use memory-optimized EC2 Spot Instances for the training jobs."}, "option_flag": true}, {"option_text": {"zhcn": "请切换至CPU与GPU配比为6:1的实例类型。", "enus": "Switch to an instance type that has a CPU:GPU ratio of 6:1."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is “Use memory - optimized EC2 Spot Instances for the training jobs.” This solution meets the requirements because Spot Instances are spare Amazon EC2 computing capacity available at a significantly lower cost compared to On - Demand Instances. By using memory - optimized ones, the ML specialist can leverage the available memory for training the computer - vision models, and since the GPU is idle half of the time, the cost savings from Spot Instances can be substantial without necessarily increasing the training duration.\n\nThe option “Switch to an instance type that has only CPUs” is incorrect. Computer - vision models using deep learning often benefit from the parallel processing power of GPUs, and removing GPUs altogether may significantly increase the training duration, which violates the requirement.\n\nThe option “Use a heterogeneous cluster that has two different instances groups” is complex and may not directly address the issue of the idle GPU and cost - reduction. Setting up and managing a heterogeneous cluster can be time - consuming and may introduce additional costs.\n\nThe option “Switch to an instance type that has a CPU:GPU ratio of 6:1” is not ideal. It increases the proportion of GPU relative to CPU, but the problem is that the existing GPU is already idle half of the time. This change may not reduce costs and could potentially increase them.\n\nIn summary, the key to choosing the real answer lies in its ability to reduce costs without increasing training duration, which is not achieved by the fake answer options.", "zhcn": "对于该问题的正确答案是“采用内存优化的EC2竞价型实例处理训练任务”。这一方案符合要求，因为竞价型实例属于亚马逊EC2的闲置计算资源，其成本较按需实例显著降低。通过选用内存优化型实例，机器学习专家可利用充足内存训练计算机视觉模型；同时鉴于GPU存在半数时间处于闲置状态，采用竞价型实例可在不延长训练时长的前提下实现可观的成本节约。\n\n“切换至纯CPU实例类型”的方案并不恰当。采用深度学习的计算机视觉模型往往依赖GPU的并行计算能力，完全移除GPU可能导致训练时长大幅增加，违背了既定要求。\n\n“采用包含两种实例组的异构集群”方案过于复杂，且无法直接解决GPU闲置与成本优化问题。搭建和管理异构集群既耗时又可能产生额外开销。\n\n“改用CPU与GPU配比为6:1的实例类型”并非最优解。虽然该方案提升了GPU相对CPU的比例，但现有GPU本就存在半数闲置时间。此举不仅无法有效降低成本，反而可能增加费用支出。\n\n综上所述，正确答案的选择关键在于实现成本压缩与训练时长控制的平衡，而其余选项均未能同时满足这两项核心要求。"}, "answer": "C"}, {"id": "305", "question": {"enus": "A company wants to forecast the daily price of newly launched products based on 3 years of data for older product prices, sales, and rebates. The time-series data has irregular timestamps and is missing some values. Data scientist must build a dataset to replace the missing values. The data scientist needs a solution that resamples the data daily and exports the data for further modeling. Which solution will meet these requirements with the LEAST implementation effort? ", "zhcn": "某公司希望依据过去三年旧产品的价格、销量及折扣数据，预测新产品的每日价格。现有时间序列数据存在时间戳不规则及部分数值缺失的问题。数据科学家需构建数据集以填补缺失值，并要求解决方案能实现每日数据重采样，同时导出数据供后续建模使用。在满足上述需求的前提下，哪种方案能以最小实施成本达成目标？"}, "option": [{"option_text": {"zhcn": "借助 Amazon EMR Serverless 运行 PySpark 作业。", "enus": "Use Amazon EMR Serverless with PySpark."}, "option_flag": false}, {"option_text": {"zhcn": "使用 AWS Glue DataBrew。", "enus": "Use AWS Glue DataBrew."}, "option_flag": true}, {"option_text": {"zhcn": "请使用 Amazon SageStudio 数据整理器。", "enus": "Use Amazon SageMaker Studio Data Wrangler."}, "option_flag": false}, {"option_text": {"zhcn": "在亚马逊SageMaker Studio Notebook中运用Pandas进行数据分析。", "enus": "Use Amazon SageMaker Studio Notebook with Pandas."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is ‘Use AWS Glue DataBrew.’ AWS Glue DataBrew is a visual data preparation tool that simplifies data cleaning and preparation. It can handle resampling time - series data daily and filling in missing values with minimal coding effort. It has built - in features and a graphical interface that make it easy to perform these operations and export the data for further modeling.\n\n‘Amazon EMR Serverless with PySpark’ requires writing PySpark code to handle data resampling, filling missing values, and exporting the data. This demands in - depth knowledge of programming and PySpark, resulting in more implementation effort.\n\n‘Amazon SageMaker Studio Data Wrangler’ is more focused on data exploration and transformation in the context of machine learning in SageMaker. While it can handle data, it may not be as straightforward for the specific task of resampling irregular time - series data as AWS Glue DataBrew.\n\n‘Amazon SageMaker Studio Notebook with Pandas’ also requires writing Python code using the Pandas library. This involves coding skills and more manual work to implement the resampling and missing value handling, which is more effort - intensive compared to using the visual and automated features of AWS Glue DataBrew.\n\nThe key factor distinguishing the real answer is the low - code, visual nature of AWS Glue DataBrew, which reduces the implementation effort for the given data preparation tasks.", "zhcn": "该问题的正确答案是“使用AWS Glue DataBrew”。AWS Glue DataBrew作为可视化数据预处理工具，能够以极简的编码需求实现时间序列数据的每日重采样与缺失值填补。其内置功能与图形化界面让数据操作变得直观高效，并可轻松导出数据以供后续建模使用。\n\n相比之下，“采用PySpark的Amazon EMR Serverless”方案需编写PySpark代码来完成数据重采样、缺失值填补及导出操作，要求使用者具备扎实的编程与PySpark知识，实施复杂度显著更高。\n\n“Amazon SageMaker Studio Data Wrangler”更侧重于SageMaker机器学习场景下的数据探索与转换。虽然具备数据处理能力，但在处理不规则时间序列数据重采样的特定任务上，其便捷性不及AWS Glue DataBrew。\n\n而“基于Pandas库的Amazon SageMaker Studio Notebook”同样需要编写Python代码，依赖人工实现重采样与缺失值处理。相较于Glue DataBrew的可视化与自动化特性，此方案需投入更多开发精力。\n\n核心区别在于：AWS Glue DataBrew通过低代码、可视化的特性，显著降低了所述数据预处理任务的实施门槛。"}, "answer": "B"}, {"id": "306", "question": {"enus": "A data scientist is building a forecasting model for a retail company by using the most recent 5 years of sales records that are stored in a data warehouse. The dataset contains sales records for each of the company’s stores across five commercial regions. The data scientist creates a working dataset with StoreID. Region. Date, and Sales Amount as columns. The data scientist wants to analyze yearly average sales for each region. The scientist also wants to compare how each region performed compared to average sales across all commercial regions. Which visualization will help the data scientist better understand the data trend? ", "zhcn": "一位数据科学家正在利用数据仓库中近五年的销售记录为某零售企业构建预测模型。该数据集涵盖五大商业区域各门店的销售记录。科学家已创建包含门店编号、所属区域、日期及销售额的工作数据集。为分析各区域年度平均销售额，并对比各区域与整体商业区域平均值的表现差异，应采用何种可视化方案方能更清晰地呈现数据趋势？"}, "option": [{"option_text": {"zhcn": "使用Pandas的GroupBy功能按年份和门店汇总数据，生成各门店逐年平均销售额的聚合数据集。以年份为分面绘制柱状图，展示各门店平均销售额。每个分面中添加独立柱体，用以呈现整体平均销售额水平。", "enus": "Create an aggregated dataset by using the Pandas GroupBy function to get average sales for each year for each store. Create a bar  plot, faceted by year, of average sales for each store. Add an extra bar in each facet to represent average sales."}, "option_flag": true}, {"option_text": {"zhcn": "请使用Pandas的GroupBy功能创建聚合数据集，计算每家门店每年的平均销售额。绘制按地区着色的柱状图，以年份为分面展示各门店平均销售额，并在每个分面中添加代表平均销售额的水平参考线。", "enus": "Create an aggregated dataset by using the Pandas GroupBy function to get average sales for each year for each store. Create a bar  plot, colored by region and faceted by year, of average sales for each store. Add a horizontal line in each facet to represent average sales."}, "option_flag": false}, {"option_text": {"zhcn": "请使用Pandas的GroupBy功能创建聚合数据集，获取各区域每年平均销售额。绘制各区域平均销售额的条形图，并在每个分区中添加额外条形以表示平均销售额。", "enus": "Create an aggregated dataset by using the Pandas GroupBy function to get average sales for each year for each region. Create a bar  plot of average sales for each region. Add an extra bar in each facet to represent average sales."}, "option_flag": false}, {"option_text": {"zhcn": "使用Pandas的GroupBy功能按年份和地区汇总数据，生成各区域每年平均销售额的数据集。通过分面柱状图展示各区域平均销售额，每个年份单独呈现一个子图，并在各子图中添加代表平均销售额的水平参考线。", "enus": "Create an aggregated dataset by using the Pandas GroupBy function to get average sales for each year for each region. Create a bar  plot, faceted by year, of average sales for each region. Add a horizontal line in each facet to represent average sales."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Create an aggregated dataset by using the Pandas GroupBy function to get average sales for each year for each store. Create a bar plot, faceted by year, of average sales for each store. Add an extra bar in each facet to represent average sales.” This approach directly addresses the data scientist's needs. By faceting the bar plot by year, it becomes easy to analyze the yearly average sales. The extra bar representing the average sales allows for a straightforward comparison of each store's performance against the overall average.\n\nThe first fake option colors the bars by region and adds a horizontal line for average sales. Coloring by region is not necessary as the focus is on each store's comparison with the overall average, and a horizontal line might not be as intuitive as an extra bar for this comparison.\n\nThe second fake option aggregates by region instead of store, which does not meet the requirement of analyzing each store's performance.\n\nThe third fake option also aggregates by region and uses a horizontal line. Similar to the other fake options, it fails to provide the detailed store - level analysis and the most intuitive way to compare with the average sales as the real answer does.\n\nThe key factor distinguishing the real answer is its ability to provide a clear and direct visualization for analyzing each store's yearly average sales and comparing it with the overall average, which aligns precisely with the data scientist's goals.", "zhcn": "针对该问题的正确方法是：\"利用Pandas的GroupBy功能创建聚合数据集，计算各门店每年的平均销售额。以年份为分面绘制各门店平均销售额的条形图，并在每个分面中添加代表整体平均销售额的辅助条形。\"这一方案直击数据科学家的核心需求。通过按年份分面展示条形图，可以清晰分析各年度销售趋势；而每个分面中的辅助条形则能直观呈现各门店业绩与整体均值的对比关系。\n\n首个干扰项采用区域色彩区分条形并添加平均水平线。但区域着色并非必要，因为分析重点在于门店与整体均值的对比，且水平线的直观性远不及辅助条形。\n\n第二个干扰项将聚合维度错误设定为区域而非门店，无法满足门店级精细分析的要求。\n\n第三个干扰项同样存在区域聚合偏差，且沿用水平线标识。与其他干扰项类似，该方法既未能提供门店层级的详细分析，也缺乏真实答案所具有的均值对比直观性。\n\n真正解决方案的核心优势在于：通过精准的可视化设计，既清晰展示各门店年度销售趋势，又提供与整体均值直接对比的最佳途径，完美契合数据科学家的分析目标。"}, "answer": "A"}, {"id": "307", "question": {"enus": "A company uses sensors on devices such as motor engines and factory machines to measure parameters, temperature and pressure. The company wants to use the sensor data to predict equipment malfunctions and reduce services outages. Machine learning (ML) specialist needs to gather the sensors data to train a model to predict device malfunctions. The ML specialist must ensure that the data does not contain outliers before training the model. How can the ML specialist meet these requirements with the LEAST operational overhead? ", "zhcn": "某企业通过在电机引擎与工厂机械等设备上安装传感器，用以监测各项运行参数、温度及压力数据。该企业旨在运用传感器数据预测设备故障，从而减少服务中断情况。机器学习专家需要采集传感器数据以训练预测设备故障的模型。在模型训练前，专家必须确保数据不含异常值。请问机器学习专家如何以最低运维成本满足这些需求？"}, "option": [{"option_text": {"zhcn": "将数据载入Amazon SageMaker Studio笔记本，计算第一与第三四分位数值。随后通过SageMaker Data Wrangler数据流处理功能，精准剔除仅超出该四分位数范围的数据点。", "enus": "Load the data into an Amazon SageMaker Studio notebook. Calculate the first and third quartile. Use a SageMaker Data Wrangler data  fiow to remove only values that are outside of those quartiles."}, "option_flag": false}, {"option_text": {"zhcn": "利用亚马逊SageMaker数据整理工具的偏差报告识别数据集中的异常值，随后通过数据流处理功能，依据偏差分析结果剔除异常数据。", "enus": "Use an Amazon SageMaker Data Wrangler bias report to find outliers in the dataset. Use a Data Wrangler data fiow to remove outliers  based on the bias report."}, "option_flag": false}, {"option_text": {"zhcn": "借助亚马逊SageMaker数据整理器的异常检测可视化功能，可精准定位数据集中的异常值。通过在数据整理流程中添加转换步骤，即可有效剔除异常数据点。", "enus": "Use an Amazon SageMaker Data Wrangler anomaly detection visualization to find outliers in the dataset. Add a transformation to a  Data Wrangler data fiow to remove outliers."}, "option_flag": true}, {"option_text": {"zhcn": "运用亚马逊设备监测服务（Amazon Lookout for Equipment）从数据集中识别并剔除异常值。", "enus": "Use Amazon Lookout for Equipment to find and remove outliers from the dataset."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Use an Amazon SageMaker Data Wrangler anomaly detection visualization to find outliers in the dataset. Add a transformation to a Data Wrangler data flow to remove outliers.” This approach offers the least operational overhead as it directly uses the built - in anomaly detection visualization in Data Wrangler. It allows the ML specialist to quickly identify outliers and then easily remove them through a transformation in the data flow.\n\nThe option of “Load the data into an Amazon SageMaker Studio notebook. Calculate the first and third quartile. Use a SageMaker Data Wrangler data flow to remove only values that are outside of those quartiles” has more overhead. It requires manual calculation of quartiles in a notebook, which is a more complex and time - consuming process compared to using the pre - built anomaly detection feature.\n\nUsing an “Amazon SageMaker Data Wrangler bias report to find outliers in the dataset” is incorrect because a bias report is used to detect bias in data, not outliers. This is a common misconception where one might mix up the purpose of different analytical reports in the SageMaker ecosystem.\n\n“Use Amazon Lookout for Equipment to find and remove outliers from the dataset” is also not the best choice. Amazon Lookout for Equipment is mainly designed for detecting equipment failures, not specifically for outlier detection and removal. It would be overkill for this task and add unnecessary complexity and potentially higher costs.\n\nIn summary, the real answer option is the most efficient way to meet the requirements with the least operational overhead, distinguishing it from the fake options.", "zhcn": "针对该问题的正确答案是：\"利用Amazon SageMaker Data Wrangler的异常检测可视化功能识别数据集中的离群值，通过在Data Wrangler数据流中添加转换步骤来剔除这些异常值。\"此方案具有最低的操作复杂度，因为它直接调用Data Wrangler内置的异常检测可视化工具。机器学习专家可快速定位异常数据，并借助数据流中的转换模块轻松完成剔除。\n\n而\"将数据加载至Amazon SageMaker Studio笔记本中，手动计算第一与第三四分位数，再通过Data Wrangler数据流仅剔除四分位数范围外的数值\"这一方案则操作更为繁琐。它需要在笔记本中人工计算四分位数，相较于使用预置的异常检测功能，该过程既复杂又耗时。\n\n至于\"采用Amazon SageMaker Data Wrangler偏差报告检测异常值\"的方案并不正确——偏差报告专用于数据偏差分析，与异常值检测功能有本质区别。在SageMaker生态中，需注意避免混淆不同分析报告的用途。\n\n同样，\"使用Amazon Lookout for Equipment发现并移除异常值\"也非最佳选择。该服务主要针对设备故障预警设计，并非专为数据离群值处理场景打造。采用此方案不仅大材小用，还会引入不必要的操作复杂度及潜在成本提升。\n\n综上，正确答案选项以最小操作成本高效满足了需求，这与其余干扰选项形成鲜明对比。"}, "answer": "C"}, {"id": "308", "question": {"enus": "A data scientist obtains a tabular dataset that contains 150 correlated features with different ranges to build a regression model. The data scientist needs to achieve more eficient model training by implementing a solution that minimizes impact on the model’s performance. The data scientist decides to perform a principal component analysis (PCA) preprocessing step to reduce the number of features to a smaller set of independent features before the data scientist uses the new features in the regression model. Which preprocessing step will meet these requirements? ", "zhcn": "一位数据科学家获得了一个包含150个相关特征且数值范围各异的表格数据集，旨在构建回归模型。为实现更高效的模型训练，需采用一种对模型性能影响最小的解决方案。该科学家决定在执行回归模型前，先通过主成分分析（PCA）预处理步骤，将特征数量缩减为少量独立的新特征。何种预处理方法可满足上述需求？"}, "option": [{"option_text": {"zhcn": "对数据集应用Amazon SageMaker内置的主成分分析算法，以实现数据转换。", "enus": "Use the Amazon SageMaker built-in algorithm for PCA on the dataset to transform the data."}, "option_flag": false}, {"option_text": {"zhcn": "将数据载入亚马逊SageMaker数据整理平台，通过最小-最大缩放转换步骤对数据进行标准化处理。随后在缩放后的数据集上运用SageMaker内置的主成分分析算法，完成数据转换。", "enus": "Load the data into Amazon SageMaker Data Wrangler. Scale the data with a Min Max Scaler transformation step. Use the SageMaker  built-in algorithm for PCA on the scaled dataset to transform the data."}, "option_flag": false}, {"option_text": {"zhcn": "通过剔除相关性最高的特征来降低数据集的维度。将数据加载至Amazon SageMaker Data Wrangler后，执行标准缩放转换步骤以规范化数据尺度。随后在缩放后的数据集上运用SageMaker内置的PCA算法实现数据转换。", "enus": "Reduce the dimensionality of the dataset by removing the features that have the highest correlation. Load the data into Amazon  SageMaker Data Wrangler. Perform a Standard Scaler transformation step to scale the data. Use the SageMaker built-in algorithm for PCA  on the scaled dataset to transform the data."}, "option_flag": false}, {"option_text": {"zhcn": "通过剔除相关性最弱的特征来降低数据集的维度。将数据加载至Amazon SageMaker Data Wrangler后，执行最小最大缩放变换步骤以标准化数据范围。随后在缩放后的数据集上运用SageMaker内置的主成分分析算法，实现数据转换。", "enus": "Reduce the dimensionality of the dataset by removing the features that have the lowest correlation. Load the data into Amazon  SageMaker Data Wrangler. Perform a Min Max Scaler transformation step to scale the data. Use the SageMaker built-in algorithm for PCA  on the scaled dataset to transform the data."}, "option_flag": true}], "analysis": {"enus": "The correct answer to the question is to “Reduce the dimensionality of the dataset by removing the features that have the lowest correlation. Load the data into Amazon  SageMaker Data Wrangler. Perform a Min Max Scaler transformation step to scale the data. Use the SageMaker built - in algorithm for PCA  on the scaled dataset to transform the data.” \n\nFirst, removing features with the lowest correlation helps in reducing the number of features in a way that retains the most relevant information, which is crucial for efficient model training. Scaling the data using a Min - Max Scaler is appropriate as the original features have different ranges, and this transformation normalizes the data to a specific range (usually between 0 and 1). Then applying PCA on the scaled data ensures that the PCA algorithm can work effectively, as PCA is sensitive to the scale of the features.\n\nThe first fake option “Use the Amazon SageMaker built - in algorithm for PCA on the dataset to transform the data” is incorrect because it skips the important steps of feature reduction and data scaling. Without scaling, PCA may give inaccurate results as it is affected by the different ranges of the original features.\n\nThe second fake option “Load the data into Amazon SageMaker Data Wrangler. Scale the data with a Min Max Scaler transformation step. Use the SageMaker  built - in algorithm for PCA on the scaled dataset to transform the data” lacks the initial step of reducing the dimensionality by removing low - correlated features. This may lead to a larger number of features being processed by PCA, which can be computationally expensive and may not be as efficient.\n\nThe third fake option “Reduce the dimensionality of the dataset by removing the features that have the highest correlation. Load the data into Amazon  SageMaker Data Wrangler. Perform a Standard Scaler transformation step to scale the data. Use the SageMaker built - in algorithm for PCA  on the scaled dataset to transform the data” makes a wrong choice in feature removal. Removing features with the highest correlation may remove important information as highly correlated features can contribute significantly to the model. Also, while a Standard Scaler can be used for scaling, Min - Max Scaler is more suitable when we want to preserve the original distribution's shape within a fixed range.\n\nThe key factors of feature reduction, appropriate scaling, and then applying PCA distinguish the real answer from the fake options, and common misconceptions like skipping necessary steps or making the wrong choice of feature removal can lead to choosing the incorrect options.", "zhcn": "对于该问题的正确答案是：\"通过剔除相关性最低的特征来降低数据集维度，将数据加载至Amazon SageMaker Data Wrangler后，执行最小-最大缩放转换步骤对数据进行标准化处理，最后在缩放后的数据集上使用SageMaker内置PCA算法完成数据转换。\"首先，剔除低相关性特征能在保留最关键信息的前提下精简特征数量，这对提升模型训练效率至关重要。由于原始特征值范围存在差异，采用最小-最大缩放器进行数据标准化处理（通常将数据映射到0-1区间）尤为合适。随后对标准化数据实施主成分分析（PCA），可确保该算法充分发挥效能，因为PCA对特征尺度具有敏感性。\n\n第一个干扰项\"直接使用Amazon SageMaker内置PCA算法对数据集进行转换\"存在谬误，因为它跳过了特征降维和数据标准化这两个关键步骤。若未进行标准化处理，PCA可能因原始特征量纲差异而得出失真结果。\n\n第二个干扰项\"将数据加载至Amazon SageMaker Data Wrangler，通过最小-最大缩放器标准化数据后，在缩放数据集上使用SageMaker内置PCA算法\"缺失了初始的低相关特征剔除环节。这将导致PCA需要处理更多特征，不仅增加计算成本，也会影响分析效率。\n\n第三个干扰项\"通过剔除高相关性特征降维，加载数据至Amazon SageMaker Data Wrangler，执行标准缩放器转换后使用内置PCA算法\"在特征选择上存在原则性错误。移除高相关性特征可能导致重要信息丢失，因为这类特征往往对模型构建具有显著贡献。此外，虽然标准缩放器也可用于数据标准化，但当需要将数据分布形态保持于固定区间时，最小-最大缩放器显然是更优选择。\n\n正是特征降维、适度标准化及后续PCA应用的环环相扣，构成了正确答案与干扰项的本质区别。常见认知误区如跳过必要步骤或错误选择特征剔除方式，往往会导致误选无效方案。"}, "answer": "D"}, {"id": "309", "question": {"enus": "An online retailer collects the following data on customer orders: demographics, behaviors, location, shipment progress, and delivery time. A data scientist joins all the collected datasets. The result is a single dataset that includes 980 variables. The data scientist must develop a machine learning (ML) model to identify groups of customers who are likely to respond to a marketing campaign. Which combination of algorithms should the data scientist use to meet this requirement? (Choose two.) ", "zhcn": "某电商平台收集了以下客户订单数据：用户画像、行为特征、地理位置、物流状态及交付时长。数据科学家将全部采集到的数据集进行整合后，生成了一个包含980个变量的统一数据集。此时需要开发一个机器学习模型，用于精准定位可能对营销活动产生兴趣的客户群体。为达成此目标，数据科学家应当采用哪两种算法的组合方案？（请选择两项）"}, "option": [{"option_text": {"zhcn": "潜在狄利克雷分布（LDA）", "enus": "Latent Dirichlet Allocation (LDA)"}, "option_flag": true}, {"option_text": {"zhcn": "K-means 聚类算法", "enus": "K-means"}, "option_flag": false}, {"option_text": {"zhcn": "语义分割", "enus": "Semantic segmentation"}, "option_flag": false}, {"option_text": {"zhcn": "主成分分析（PCA）", "enus": "Principal component analysis (PCA)"}, "option_flag": false}, {"option_text": {"zhcn": "因子分解机（Factorization Machines，简称FM）", "enus": "Factorization machines (FM)"}, "option_flag": true}], "analysis": {"enus": "The correct answer to the question is ‘Latent Dirichlet Allocation (LDA)’ and ‘Factorization machines (FM)’. LDA is useful for topic modeling, which can help in uncovering hidden patterns in customer data such as common interests or preferences, which are valuable for identifying groups likely to respond to a marketing campaign. FM can handle high - dimensional sparse data well, like the 980 - variable dataset here, and can model the interactions between variables to predict customer responses.\n\nThe fake answer ‘K - means’ is a clustering algorithm mainly for partitioning data into groups based on similarity in feature space, but it may not effectively capture the complex relationships in the data for predicting response to a marketing campaign. ‘Semantic segmentation’ is typically used in computer vision for image analysis and has no direct relevance to customer data analysis for marketing campaigns. ‘Principal component analysis (PCA)’ is a dimensionality reduction technique and doesn't directly identify groups of customers likely to respond to a campaign; it just simplifies the data. This is why LDA and FM are the real answer options, distinct from the fake ones.", "zhcn": "问题的正确答案是\"潜在狄利克雷分布（LDA）\"与\"因子分解机（FM）\"。LDA适用于主题建模，能有效挖掘客户数据中的潜在模式——例如共同兴趣或偏好，这对识别可能响应营销活动的客户群体具有重要价值。FM擅长处理高维稀疏数据（如本题包含980个变量的数据集），并能通过变量间交互作用建模来预测客户反馈。\n\n而干扰项\"K均值\"作为一种聚类算法，主要依据特征空间相似度划分数据组别，但难以有效捕捉数据中影响营销响应的复杂关联；\"语义分割\"通常用于计算机视觉领域的图像分析，与营销活动的客户数据分析并无直接关联；\"主成分分析（PCA）\"属于降维技术，虽能简化数据结构却无法直接识别潜在响应活动的客户群体。正因如此，LDA和FM才是区别于干扰项的正确选择。"}, "answer": "AE"}, {"id": "310", "question": {"enus": "A machine learning engineer is building a bird classification model. The engineer randomly separates a dataset into a training dataset and a validation dataset. During the training phase, the model achieves very high accuracy. However, the model did not generalize well during validation of the validation dataset. The engineer realizes that the original dataset was imbalanced. What should the engineer do to improve the validation accuracy of the model? ", "zhcn": "一位机器学习工程师正在构建鸟类分类模型。该工程师将数据集随机划分为训练集和验证集。训练阶段模型表现出极高的准确率，但在验证集上却未能展现出良好的泛化能力。工程师意识到原数据集存在样本失衡问题。为提升模型在验证集上的准确率，该采取哪些改进措施？"}, "option": [{"option_text": {"zhcn": "对原始数据集进行分层抽样。", "enus": "Perform stratified sampling on the original dataset."}, "option_flag": true}, {"option_text": {"zhcn": "在原数据集中，对多数类别进行进一步的数据采集。", "enus": "Acquire additional data about the majority classes in the original dataset."}, "option_flag": false}, {"option_text": {"zhcn": "采用规模更小、经过随机抽样的训练数据集版本。", "enus": "Use a smaller, randomly sampled version of the training dataset."}, "option_flag": false}, {"option_text": {"zhcn": "对原始数据集进行系统抽样。", "enus": "Perform systematic sampling on the original dataset."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is ‘Perform stratified sampling on the original dataset.’ When the original dataset is imbalanced, it can lead to a model being biased towards the majority classes, resulting in poor generalization on the validation set. Stratified sampling ensures that each class in the dataset is represented proportionally in both the training and validation sets. This way, the model gets a more balanced exposure to all classes during training, which helps it learn better patterns and generalize well on the validation data.\n\n‘Acquiring additional data about the majority classes in the original dataset’ will only exacerbate the imbalance, making the model even more biased towards the majority classes. ‘Using a smaller, randomly sampled version of the training dataset’ might not address the imbalance issue and could lead to the model having insufficient data to learn effectively. ‘Performing systematic sampling on the original dataset’ does not take into account the class distribution, so it won't help in dealing with the imbalance problem. The key factor here is to address the class imbalance, which is why stratified sampling is the real answer option, distinguishing it from the fake options.", "zhcn": "问题的正确答案是\"对原始数据集进行分层抽样\"。当原始数据集存在类别不平衡时，容易导致模型偏向多数类，从而在验证集上表现欠佳。分层抽样能确保每个类别按比例均匀分布在训练集和验证集中，使模型在训练过程中能均衡学习所有类别特征，从而掌握更有效的规律，在验证数据上展现更好的泛化能力。\n\n\"增加多数类样本数量\"只会加剧数据失衡，使模型更倾向于多数类；\"采用随机抽样的精简训练集\"可能无法解决不平衡问题，还会导致模型训练数据不足；\"对原始数据进行系统抽样\"由于未考虑类别分布，同样无法改善失衡状况。解决类别不平衡才是关键所在，这正是分层抽样成为正确答案的根本原因，也是识别干扰选项的重要依据。"}, "answer": "A"}, {"id": "311", "question": {"enus": "A data engineer wants to perform exploratory data analysis (EDA) on a petabyte of data. The data engineer does not want to manage compute resources and wants to pay only for queries that are run. The data engineer must write the analysis by using Python from a Jupyter notebook. Which solution will meet these requirements? ", "zhcn": "一位数据工程师希望对PB级数据进行探索性数据分析（EDA）。该工程师不愿自行管理计算资源，且仅希望按实际执行的查询量付费。分析代码需通过Jupyter笔记本使用Python编写。何种方案可满足这些需求？"}, "option": [{"option_text": {"zhcn": "在亚马逊 Athena 中集成使用 Apache Spark。", "enus": "Use Apache Spark from within Amazon Athena."}, "option_flag": false}, {"option_text": {"zhcn": "在Amazon SageMaker环境中集成Apache Spark进行数据处理。", "enus": "Use Apache Spark from within Amazon SageMaker."}, "option_flag": true}, {"option_text": {"zhcn": "在 Amazon EMR 集群环境中运行 Apache Spark。", "enus": "Use Apache Spark from within an Amazon EMR cluster."}, "option_flag": false}, {"option_text": {"zhcn": "通过集成Amazon Redshift使用Apache Spark。", "enus": "Use Apache Spark through an integration with Amazon Redshift."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is ‘Use Apache Spark from within Amazon SageMaker.’ This is because Amazon SageMaker allows data engineers to write Python code in a Jupyter notebook for analysis. It also abstracts the need to manage compute resources as it takes care of the underlying infrastructure, and users pay only for the queries they run.\n\nThe option ‘Use Apache Spark from within Amazon Athena’ is incorrect because Athena is mainly used for querying data in Amazon S3 using SQL, not specifically for running Python - based EDA from a Jupyter notebook and using Apache Spark. \n\n‘Use Apache Spark from within an Amazon EMR cluster’ requires the user to manage the EMR cluster, including provisioning and scaling compute resources, which goes against the requirement of not wanting to manage compute resources. \n\n‘Use Apache Spark through an integration with Amazon Redshift’ is wrong as Amazon Redshift is a data warehousing solution, and while it can integrate with various tools, it doesn't directly meet the requirements of running Python - based EDA from a Jupyter notebook without managing compute resources and paying only for queries.\n\nThe key factor distinguishing the real answer is its ability to meet all the requirements of the data engineer, such as using Python in a Jupyter notebook, not managing compute resources, and paying only for queries. Common misconceptions might lead one to choose the fake options if they misunderstand the core functionalities and use - cases of these Amazon services.", "zhcn": "问题的正确答案是\"在Amazon SageMaker中使用Apache Spark\"。这是因为Amazon SageMaker允许数据工程师在Jupyter笔记本中编写Python代码进行分析，同时抽象了计算资源管理的需求——该服务会自动管理底层基础设施，用户只需为实际运行的查询付费。\n\n而\"在Amazon Athena中使用Apache Spark\"这一选项并不正确，因为Athenia主要用于通过SQL查询Amazon S3中的数据，并不专门支持在Jupyter笔记本中运行基于Python的探索性数据分析或使用Apache Spark。\"在Amazon EMR集群中使用Apache Spark\"需要用户自行管理EMR集群，包括配置和扩展计算资源，这违背了无需管理计算资源的要求。至于\"通过Amazon Redshift集成使用Apache Spark\"，由于Redshift是数据仓库解决方案，虽然能集成多种工具，但无法直接满足在Jupyter笔记本中运行Python、免于管理计算资源且按查询付费的全部需求。\n\n真正的答案关键在于能否满足数据工程师的所有需求：在Jupyter笔记本中使用Python、免于管理计算资源、按查询付费。若使用者对这些亚马逊服务的核心功能存在误解，则可能误选其他干扰选项。"}, "answer": "B"}, {"id": "312", "question": {"enus": "A data scientist receives a new dataset in .csv format and stores the dataset in Amazon S3. The data scientist will use the dataset to train a machine learning (ML) model. The data scientist first needs to identify any potential data quality issues in the dataset. The data scientist must identify values that are missing or values that are not valid. The data scientist must also identify the number of outliers in the dataset. Which solution will meet these requirements with the LEAST operational effort? ", "zhcn": "一位数据科学家收到一份.csv格式的新数据集，并将其存储于Amazon S3中。该数据集将用于训练机器学习模型。数据科学家首先需要识别其中潜在的数据质量问题，包括缺失值、无效数值以及异常值数量。在满足这些要求的前提下，何种解决方案能以最小的操作量实现目标？"}, "option": [{"option_text": {"zhcn": "创建一个AWS Glue作业，用于将数据从.csv格式转换为Apache Parquet格式。通过配置AWS Glue爬虫程序，结合Amazon Athena并运用恰当的SQL查询语句来提取所需信息。", "enus": "Create an AWS Glue job to transform the data from .csv format to Apache Parquet format. Use an AWS Glue crawler and Amazon Athena  with appropriate SQL queries to retrieve the required information."}, "option_flag": false}, {"option_text": {"zhcn": "请将数据集保留为.csv格式，通过AWS Glue爬虫程序与Amazon Athena服务，配合恰当的SQL查询语句来提取所需信息。", "enus": "Leave the dataset in .csv format. Use an AWS Glue crawler and Amazon Athena with appropriate SQL queries to retrieve the required  information."}, "option_flag": false}, {"option_text": {"zhcn": "创建一项AWS Glue作业，用于将数据从.csv格式转换为Apache Parquet格式。将处理后的数据导入Amazon SageMaker Data Wrangler，随后通过数据质量与洞察报告获取所需分析信息。", "enus": "Create an AWS Glue job to transform the data from .csv format to Apache Parquet format. Import the data into Amazon SageMaker Data  Wrangler. Use the Data Quality and Insights Report to retrieve the required information."}, "option_flag": false}, {"option_text": {"zhcn": "将数据集保留为.csv格式，将其导入Amazon SageMaker Data Wrangler中，随后通过数据质量与洞察报告获取所需信息。", "enus": "Leave the dataset in .csv format. Import the data into Amazon SageMaker Data Wrangler. Use the Data Quality and Insights Report to  retrieve the required information."}, "option_flag": true}], "analysis": {"enus": "The correct answer to the question is to “Leave the dataset in .csv format. Import the data into Amazon SageMaker Data Wrangler. Use the Data Quality and Insights Report to retrieve the required information.” This solution requires the least operational effort because it allows the data scientist to work directly with the existing .csv dataset and uses SageMaker Data Wrangler's built - in report feature to quickly get the needed data quality information.\n\nThe option of creating an AWS Glue job to transform the data to Apache Parquet format adds unnecessary complexity and operational overhead. Even if the transformation is followed by using an AWS Glue crawler and Amazon Athena or importing into SageMaker Data Wrangler, the initial conversion step is not required for the task at hand. \n\nUsing an AWS Glue crawler and Amazon Athena with SQL queries also demands more work. Writing appropriate SQL queries to identify missing values, invalid values, and outliers is more time - consuming compared to using the pre - built Data Quality and Insights Report in SageMaker Data Wrangler. \n\nCommon misconceptions might lead one to choose the fake options. For example, some might think that converting to Apache Parquet is always beneficial for data processing, without realizing it adds extra steps here. Others might overestimate the power of SQL queries and underestimate the simplicity of using a dedicated data quality report tool.", "zhcn": "针对该问题的正确答案是\"保持数据集为.csv格式，将其导入Amazon SageMaker Data Wrangler，通过数据质量与洞察报告获取所需信息\"。此方案操作成本最低，既允许数据科学家直接处理现有.csv数据集，又能借助SageMaker Data Wler内置报告功能快速获得数据质量分析。  \n  \n若选择通过AWS Glue作业将数据转换为Apache Parquet格式，则会引入不必要的复杂性和运维负担。即便后续使用AWS Glue爬虫程序配合Amazon Athena查询，或重新导入SageMaker Data Wrangler，初始转换步骤对本任务而言实属多余。采用AWS Glue爬虫与Amazon Athena执行SQL查询同样会增加工作量——相比直接使用SageMaker Data Wrangler中预置的数据质量洞察报告，编写识别缺失值、无效值及异常值的SQL语句显然更为耗时。  \n  \n常见误解可能导致人们选择错误选项。例如有人可能认为转换为Parquet格式总能提升数据处理效率，却未意识到此举在当前场景下徒增步骤；亦或高估SQL查询的灵活性，而低估专用数据质量报告工具的便捷性。"}, "answer": "D"}, {"id": "313", "question": {"enus": "An ecommerce company has developed a XGBoost model in Amazon SageMaker to predict whether a customer will return a purchased item. The dataset is imbalanced. Only 5% of customers return items. A data scientist must find the hyperparameters to capture as many instances of returned items as possible. The company has a small budget for compute. How should the data scientist meet these requirements MOST cost-effectively? ", "zhcn": "一家电商公司利用亚马逊SageMaker平台开发了XGBoost模型，用于预测顾客是否会退回所购商品。当前数据集存在不平衡问题，仅5%的顾客选择退货。数据科学家需在有限的计算资源预算内，通过超参数调优尽可能精准识别退货案例。在此条件下，如何以最具成本效益的方式达成该目标？"}, "option": [{"option_text": {"zhcn": "采用自动模型调优（AMT）对所有可调超参数进行优化。优化目标设定为：{\"HyperParameterTuningJobObjective\": {\"MetricName\": \"validation:accuracy\", \"Type\": \"Maximize\"}}，以最大化验证集准确率为导向。", "enus": "Tune all possible hyperparameters by using automatic model tuning (AMT). Optimize on {\"HyperParameterTuningJobObjective\":  {\"MetricName\": \"validation:accuracy\", \"Type\": \"Maximize\"}}."}, "option_flag": false}, {"option_text": {"zhcn": "通过自动模型调优（AMT）对csv_weight超参数与scale_pos_weight超参数进行调校。优化目标设定为：{\"HyperParameterTuningJobObjective\": {\"MetricName\": \"validation'll\", \"Type\": \"Maximize\"}}。", "enus": "Tune the csv_weight hyperparameter and the scale_pos_weight hyperparameter by using automatic model tuning (AMT). Optimize on  {\"HyperParameterTuningJobObjective\": {\"MetricName\": \"validation'll\", \"Type\": \"Maximize\"}}."}, "option_flag": false}, {"option_text": {"zhcn": "通过自动模型调优（AMT）对所有可调超参数进行优化，以最大化验证集F1分数（{\"HyperParameterTuningJobObjective\": {\"MetricName\": \"validation:f1\", \"Type\": \"Maximize\"}}）为目标进行调优。", "enus": "Tune all possible hyperparameters by using automatic model tuning (AMT). Optimize on {\"HyperParameterTuningJobObjective\":  {\"MetricName\": \"validation:f1\", \"Type\": \"Maximize\"}}."}, "option_flag": true}, {"option_text": {"zhcn": "通过自动模型调优（AMT）调整 `csv_weight` 超参数与 `scale_pos_weight` 超参数，并以最小化验证集F1分数为目标进行优化：`{\"HyperParameterTuningJobObjective\": {\"MetricName\": \"validation:f1\", \"Type\": \"Minimize\"}}`。", "enus": "Tune the csv_weight hyperparameter and the scale_pos_weight hyperparameter by using automatic model tuning (AMT). Optimize on  {\"HyperParameterTuningJobObjective\": {\"MetricName\": \"validation:f1\", \"Type\": \"Minimize\"}}."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Tune all possible hyperparameters by using automatic model tuning (AMT). Optimize on {\"HyperParameterTuningJobObjective\":  {\"MetricName\": \"validation:f1\", \"Type\": \"Maximize\"}}”. The key goal is to capture as many instances of returned items as possible, and the dataset is imbalanced with only 5% of customers returning items.\n\nThe F1 - score is a suitable metric for imbalanced datasets as it balances precision and recall. Maximizing the validation F1 - score helps in finding the hyperparameters that perform well in identifying the minority class (customers who return items).\n\nThe first fake option suggests optimizing on validation accuracy. In an imbalanced dataset, accuracy can be misleading because a model can achieve high accuracy by simply predicting the majority class (customers who don't return items) most of the time. So, this metric won't effectively capture the instances of returned items.\n\nThe second fake option mentions optimizing on an unknown metric “validation'll”. Since this isn't a recognized metric in the context of XGBoost model evaluation, it can't be used to find the appropriate hyperparameters.\n\nThe third fake option suggests minimizing the validation F1 - score. The goal is to maximize the ability to capture returned items, and minimizing the F1 - score would lead to a model that performs poorly in identifying the minority class.\n\nIn summary, using AMT to tune all possible hyperparameters and optimizing on the validation F1 - score is the most cost - effective way to meet the requirements, which is why it is the real answer option.", "zhcn": "该问题的正确答案是\"通过自动模型调优（AMT）调整所有可能的超参数，并以最大化验证集F1分数为目标进行优化\"。核心目标是尽可能准确地识别退货客户，而当前数据集存在不平衡问题——仅有5%的客户发生退货行为。F1分数能均衡考量精确率与召回率，特别适合处理不平衡数据集。通过最大化验证集F1分数，可有效筛选出最能识别少数类（退货客户）的超参数组合。\n\n第一个干扰选项提议优化验证集准确率。但在数据不平衡场景下，准确率容易产生误导——模型仅通过频繁预测多数类（未退货客户）即可获得高准确率，这将导致无法有效捕捉退货实例。\n\n第二个干扰选项提及优化未知指标\"validation'll\"。由于该指标并非XGBoost模型评估中的公认度量标准，故无法用于超参数筛选。\n\n第三个干扰选项建议最小化验证集F1分数。但我们的目标是提升识别退货客户的能力，最小化F1分数将导致模型对少数类的识别性能恶化。\n\n综上所述，采用AMT技术全面调参并最大化验证集F1分数，是满足需求最具成本效益的方案，因此被确定为正确选项。"}, "answer": "C"}, {"id": "314", "question": {"enus": "A data scientist is trying to improve the accuracy of a neural network classification model. The data scientist wants to run a large hyperparameter tuning job in Amazon SageMaker. However, previous smaller tuning jobs on the same model often ran for several weeks. The ML specialist wants to reduce the computation time required to run the tuning job. Which actions will MOST reduce the computation time for the hyperparameter tuning job? (Choose two.) ", "zhcn": "一位数据科学家正致力于提升神经网络分类模型的准确率。他计划在Amazon SageMaker平台上运行大规模超参数调优任务，但此前相同模型的较小规模调优作业往往需耗时数周。为缩短调优任务的计算时间，该机器学习专家应采取哪两项最能显著提升效率的措施？（请选择两项）"}, "option": [{"option_text": {"zhcn": "采用超带优化策略。", "enus": "Use the Hyperband tuning strategy."}, "option_flag": true}, {"option_text": {"zhcn": "增加超参数的数量。", "enus": "Increase the number of hyperparameters."}, "option_flag": false}, {"option_text": {"zhcn": "将 MaxNumberOfTrainingJobs 参数的值适当调低。", "enus": "Set a lower value for the MaxNumberOfTrainingJobs parameter."}, "option_flag": true}, {"option_text": {"zhcn": "采用网格搜索调优策略。", "enus": "Use the grid search tuning strategy."}, "option_flag": false}, {"option_text": {"zhcn": "将 MaxParallelTrainingJobs 参数的值适当调低。", "enus": "Set a lower value for the MaxParallelTrainingJobs parameter."}, "option_flag": false}], "analysis": {"enus": "The two actions that will most reduce the computation time for the hyperparameter tuning job are “Use the Hyperband tuning strategy.” and “Set a lower value for the MaxNumberOfTrainingJobs parameter.”\n\nThe Hyperband tuning strategy is designed to quickly identify the most promising hyperparameter configurations and prune unpromising ones, allowing the tuning process to focus on the more likely successful combinations and thus reducing overall computation time. \n\nSetting a lower value for the MaxNumberOfTrainingJobs parameter directly limits the total number of training jobs that will be run during the tuning process. Fewer training jobs mean less computational resources are used and less time is spent on the overall tuning job.\n\nIncreasing the number of hyperparameters will actually increase the search space, leading to a larger number of possible combinations to evaluate and thus increasing the computation time. \n\nThe grid search tuning strategy exhaustively searches through all possible combinations of hyperparameters, which can be extremely time - consuming, especially for large search spaces. \n\nSetting a lower value for the MaxParallelTrainingJobs parameter reduces the number of training jobs that can run simultaneously. This slows down the overall tuning process as it takes longer to complete the entire set of training jobs, rather than reducing the computation time. \n\nThese are the reasons why the real answer options are correct and distinguish them from the fake answer options.", "zhcn": "在超参数调优任务中，最能显著缩短计算时间的两种方法是\"采用Hyperband调优策略\"与\"降低MaxNumberOfTrainingJobs参数的设定值\"。Hyperband调优策略的核心优势在于能快速筛选出潜力较大的超参数组合，并及时剔除无效配置，使调优过程聚焦于成功概率更高的参数区间，从而全面提升计算效率。而限制MaxNumberOfTrainingJobs参数值可直接控制调优过程中的总训练任务量，任务数量的减少意味着计算资源的节约与整体耗时的压缩。\n\n需要特别注意的是，增加超参数数量反而会扩大搜索空间，导致需要评估的参数组合呈指数级增长，进而延长计算时间。网格搜索调优策略虽能遍历所有超参数组合，但在大规模搜索空间中会产生惊人的时间成本。若降低MaxParallelTrainingJobs参数值，虽然会减少并行训练任务数量，但会导致整体调优进程放缓——因为完成全部训练任务需要更长时间，这并非真正意义上的计算效率提升。以上解析既阐明了正确答案的合理性，也揭示了干扰选项的谬误所在。"}, "answer": "AC"}, {"id": "315", "question": {"enus": "A machine learning (ML) specialist needs to solve a binary classification problem for a marketing dataset. The ML specialist must maximize the Area Under the ROC Curve (AUC) of the algorithm by training an XGBoost algorithm. The ML specialist must find values for the eta, alpha, min_child_weight, and max_depth hyperparameters that will generate the most accurate model. Which approach will meet these requirements with the LEAST operational overhead? ", "zhcn": "一位机器学习专家需要针对营销数据集解决二分类问题。该专家必须通过训练XGBoost算法来最大化模型的ROC曲线下面积（AUC），并寻找能使模型达到最高准确度的eta、alpha、min_child_weight和max_depth超参数组合。在满足这些要求的前提下，哪种方法能以最小的操作成本实现目标？"}, "option": [{"option_text": {"zhcn": "在Amazon EMR集群上通过引导脚本安装scikit-learn库。部署EMR集群后，对算法采用k折交叉验证方法进行评估。", "enus": "Use a bootstrap script to install scikit-learn on an Amazon EMR cluster. Deploy the EMR cluster. Apply k-fold cross-validation methods to  the algorithm."}, "option_flag": false}, {"option_text": {"zhcn": "部署预置scikit-learn环境的Amazon SageMaker Docker镜像，对算法实施k折交叉验证方法。", "enus": "Deploy Amazon SageMaker prebuilt Docker images that have scikit-learn installed. Apply k-fold cross-validation methods to the  algorithm."}, "option_flag": false}, {"option_text": {"zhcn": "使用Amazon SageMaker自动模型调优（AMT）功能。为每个超参数设定一个取值范围。", "enus": "Use Amazon SageMaker automatic model tuning (AMT). Specify a range of values for each hyperparameter."}, "option_flag": true}, {"option_text": {"zhcn": "订阅一款发布于AWS Marketplace的AUC算法。为每个超参数设定相应的数值范围。", "enus": "Subscribe to an AUC algorithm that is on AWS Marketplace. Specify a range of values for each hyperparameter."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Use Amazon SageMaker automatic model tuning (AMT). Specify a range of values for each hyperparameter.” This approach meets the requirements with the least operational overhead because SageMaker AMT automates the hyperparameter tuning process. It efficiently searches through the specified ranges of hyperparameters (eta, alpha, min_child_weight, and max_depth in this case) to find the optimal values that maximize the AUC, saving the ML specialist from manual tuning efforts.\n\nThe option of using a bootstrap script to install scikit - learn on an Amazon EMR cluster and applying k - fold cross - validation has high operational overhead. It involves setting up an EMR cluster, installing scikit - learn, and implementing the cross - validation process manually.\n\nDeploying Amazon SageMaker prebuilt Docker images with scikit - learn and applying k - fold cross - validation also requires more work. The ML specialist still needs to manage the cross - validation process, which is time - consuming and error - prone.\n\nSubscribing to an AUC algorithm on AWS Marketplace and specifying hyperparameter ranges doesn't guarantee that the algorithm is optimized for the given marketing dataset or XGBoost. It may also involve additional costs and integration efforts. \n\nIn summary, SageMaker AMT's automation is the key factor that makes it the best option with the least operational overhead, distinguishing it from the other more complex and resource - intensive fake answer options.", "zhcn": "针对该问题的正确答案是\"使用Amazon SageMaker自动模型调优（AMT）功能，为每个超参数设定取值范围\"。这一方案既能满足需求，又可实现最低运维负担——因为SageMaker AMT能够自动化执行超参数调优流程。该系统会高效遍历指定超参数（本案中的eta、alpha、min_child_weight和max_depth）的取值范围，自动寻找能最大化AUC指标的最佳参数组合，使机器学习专家无需再手动调参。\n\n若选择通过引导脚本在Amazon EMR集群上安装scikit-learn并实施k折交叉验证，将产生较高的运维成本。此方案不仅需要配置EMR集群、安装scikit-learn，还需手动实现交叉验证流程。\n\n采用预装scikit-learn的Amazon SageMaker预制Docker镜像并执行k折交叉验证同样会增加工作量。机器学习专家仍需耗时费力地管理交叉验证过程，且该过程容易出错。\n\n至于通过AWS Marketplace订阅AUC算法并指定超参数范围的方法，既无法保证该算法针对特定营销数据集或XGBoost进行过优化，还可能产生额外成本与集成负担。\n\n综上所述，SageMaker AMT的自动化特性是其区别于其他复杂且耗费资源的干扰选项的关键优势，使其成为运维负担最轻的最佳选择。"}, "answer": "C"}, {"id": "316", "question": {"enus": "A machine learning (ML) developer for an online retailer recently uploaded a sales dataset into Amazon SageMaker Studio. The ML developer wants to obtain importance scores for each feature of the dataset. The ML developer will use the importance scores to feature engineer the dataset. Which solution will meet this requirement with the LEAST development effort? ", "zhcn": "某在线零售商的机器学习开发人员近日将一份销售数据集上传至Amazon SageMaker Studio。该开发人员需要获取数据集中各特征的重要性评分，以便用于特征工程处理。在满足此需求的前提下，下列哪种解决方案所需开发工作量最小？"}, "option": [{"option_text": {"zhcn": "利用SageMaker Data Wrangler进行基尼重要性评分分析。", "enus": "Use SageMaker Data Wrangler to perform a Gini importance score analysis."}, "option_flag": true}, {"option_text": {"zhcn": "使用SageMaker笔记实例执行主成分分析（PCA）。", "enus": "Use a SageMaker notebook instance to perform principal component analysis (PCA)."}, "option_flag": false}, {"option_text": {"zhcn": "利用SageMaker笔记本实例进行奇异值分解分析。", "enus": "Use a SageMaker notebook instance to perform a singular value decomposition analysis."}, "option_flag": false}, {"option_text": {"zhcn": "运用多重共线性特性进行LASSO特征筛选，进而完成重要性评分分析。", "enus": "Use the multicollinearity feature to perform a lasso feature selection to perform an importance scores analysis."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question “A machine learning (ML) developer for an online retailer recently uploaded a sales dataset into Amazon SageMaker Studio. The ML developer wants to obtain importance scores for each feature of the dataset. The ML developer will use the importance scores to feature engineer the dataset. Which solution will meet this requirement with the LEAST development effort?” is “Use SageMaker Data Wrangler to perform a Gini importance score analysis.” This is because SageMaker Data Wrangler provides a user - friendly interface that allows for quick and easy analysis of feature importance using Gini scores, minimizing the need for extensive coding and development.\n\nThe option “Use a SageMaker notebook instance to perform principal component analysis (PCA)” is incorrect. PCA is mainly used for dimensionality reduction by transforming the data into a new set of uncorrelated variables, not for directly obtaining feature importance scores. \n\n“Use a SageMaker notebook instance to perform a singular value decomposition analysis” is also wrong. Singular value decomposition is used for various purposes like matrix factorization and data compression, and it does not directly provide feature importance scores. \n\n“Use the multicollinearity feature to perform a lasso feature selection to perform an importance scores analysis” requires more development effort as it involves implementing lasso feature selection, which needs a good understanding of statistical concepts and more coding compared to using SageMaker Data Wrangler. Common misconceptions might lead one to choose these fake options if they misunderstand the purpose of these techniques or overestimate the simplicity of using notebook instances for these complex analyses.", "zhcn": "针对“某在线零售商的机器学习开发者近期将销售数据集上传至Amazon SageMaker Studio，需获取各特征的重要性评分以用于特征工程，下列哪种方案能以最小开发量满足需求？”这一问题，正确答案为“使用SageMaker Data Wrangler进行基尼重要性评分分析”。因为该工具提供直观界面，可快速通过基尼系数计算特征重要性，极大减少了编码工作量。\n\n而“使用SageMaker笔记本实例执行主成分分析（PCA）”的方案并不适用。PCA主要用于通过数据转换实现降维，而非直接获取特征重要性评分。“使用SageMaker笔记本实例进行奇异值分解分析”同样错误，该技术常用于矩阵分解和数据压缩，无法直接生成特征重要性指标。至于“利用多重共线性功能执行LASSO特征选择以分析重要性评分”，该方法需开发者掌握统计知识并编写更多代码，开发成本明显高于直接使用SageMaker Data Wrangler。\n\n常见误区在于：若未能准确理解各分析技术的用途，或低估了在笔记本实例中实现复杂分析的难度，则易误选上述干扰项。"}, "answer": "A"}, {"id": "317", "question": {"enus": "A company is setting up a mechanism for data scientists and engineers from different departments to access an Amazon SageMaker Studio domain. Each department has a unique SageMaker Studio domain. The company wants to build a central proxy application that data scientists and engineers can log in to by using their corporate credentials. The proxy application will authenticate users by using the company's existing Identity provider (IdP). The application will then route users to the appropriate SageMaker Studio domain. The company plans to maintain a table in Amazon DynamoDB that contains SageMaker domains for each department. How should the company meet these requirements? ", "zhcn": "某公司正着手建立一套机制，使不同部门的数据科学家与工程师能够访问各自的亚马逊SageMaker Studio工作域。每个部门均拥有独立的SageMaker Studio域环境。该公司计划构建一个中央代理应用程序，科研人员可通过企业身份凭证登录该应用。该代理程序将借助企业现有身份提供商（IdP）完成用户认证，随后将用户引导至对应的SageMaker Studio域。公司拟在Amazon DynamoDB中维护一张数据表，用于存储各部门对应的SageMaker域信息。请问应如何设计该解决方案以满足上述需求？"}, "option": [{"option_text": {"zhcn": "请调用SageMaker的CreatePresignedDomainUrl接口，依据DynamoDB表中的每个域名生成对应的预签名网址，并将该网址传递至代理应用程序。", "enus": "Use the SageMaker CreatePresignedDomainUrl API to generate a presigned URL for each domain according to the DynamoDB table.  Pass the presigned URL to the proxy application."}, "option_flag": true}, {"option_text": {"zhcn": "请使用 SageMaker CreateHumanTaskUi API 生成用户界面链接，并将该链接传递给代理应用程序。", "enus": "Use the SageMaker CreateHumanTaskUi API to generate a UI URL. Pass the URL to the proxy application."}, "option_flag": false}, {"option_text": {"zhcn": "请调用Amazon SageMaker的ListHumanTaskUis接口获取所有任务界面URL，并将对应地址传递至DynamoDB表中，以便代理应用程序调用该链接。", "enus": "Use the Amazon SageMaker ListHumanTaskUis API to list all UI URLs. Pass the appropriate URL to the DynamoDB table so that the  proxy application can use the URL."}, "option_flag": false}, {"option_text": {"zhcn": "请调用 SageMaker 的 CreatePresignedNotebookInstanceUrl 接口生成预签名网址，并将该网址传递至代理应用程序。", "enus": "Use the SageMaker CreatePresignedNotebooklnstanceUrl API to generate a presigned URL. Pass the presigned URL to the proxy  application."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Use the SageMaker CreatePresignedDomainUrl API to generate a presigned URL for each domain according to the DynamoDB table. Pass the presigned URL to the proxy application.” This is because the goal is to route users to the appropriate SageMaker Studio domain after authentication. The CreatePresignedDomainUrl API is designed to generate a presigned URL that allows users to access a specific SageMaker Studio domain, which precisely meets the company's requirements.\n\nThe fake answer “Use the SageMaker CreateHumanTaskUi API to generate a UI URL. Pass the URL to the proxy application” is incorrect because the CreateHumanTaskUi API is used for creating user interfaces for human tasks in Amazon SageMaker Ground Truth, not for accessing SageMaker Studio domains.\n\nThe option “Use the Amazon SageMaker ListHumanTaskUis API to list all UI URLs. Pass the appropriate URL to the DynamoDB table so that the proxy application can use the URL” is wrong as the ListHumanTaskUis API is for listing human task UIs in SageMaker Ground Truth, not related to accessing Studio domains.\n\nThe answer “Use the SageMaker CreatePresignedNotebooklnstanceUrl API to generate a presigned URL. Pass the presigned URL to the proxy application” is also incorrect. This API is for generating a presigned URL to access a SageMaker notebook instance, not a SageMaker Studio domain.\n\nCommon misconceptions might arise from confusing different SageMaker APIs and their intended use cases. One might choose the fake options if they do not fully understand the specific functionality of each API and assume that any URL - generating API can be used for accessing SageMaker Studio domains.", "zhcn": "针对该问题的正确答案是：“使用SageMaker CreatePresignedDomainUrl API根据DynamoDB表为每个域生成预签名URL，并将预签名URL传递给代理应用程序”。因为该方案的核心目标是在用户完成身份验证后，将其路由至对应的SageMaker Studio域。CreatePresignedDomainUrl API专用于生成允许用户访问特定SageMaker Studio域的预签名URL，这与企业的需求完全契合。\n\n而错误答案“使用SageMaker CreateHumanTaskUi API生成UI URL并传递给代理应用程序”不可行，因为该API仅适用于在Amazon SageMaker Ground Truth中创建人工任务界面，与访问SageMaker Studio域无关。\n\n另一干扰项“使用Amazon SageMaker ListHumanTaskUis API列出所有UI URL，将对应URL存入DynamoDB表供代理应用程序调用”同样不正确，因为此API仅用于列举SageMaker Ground Truth中的人工任务界面，与Studio域的访问功能无关。\n\n至于“使用SageMaker CreatePresignedNotebookInstanceUrl API生成预签名URL并传递给代理应用程序”这一选项，其错误在于该API仅能为SageMaker笔记本实例生成访问链接，而非针对SageMaker Studio域。\n\n常见误解往往源于混淆不同SageMaker API的功能边界。若未能准确理解各API的具体应用场景，可能会误认为任何具备生成URL功能的API都适用于访问SageMaker Studio域，从而选择错误答案。"}, "answer": "A"}, {"id": "318", "question": {"enus": "An insurance company is creating an application to automate car insurance claims. A machine learning (ML) specialist used an Amazon SageMaker Object Detection - TensorFlow built-in algorithm to train a model to detect scratches and dents in images of cars. After the model was trained, the ML specialist noticed that the model performed better on the training dataset than on the testing dataset. Which approach should the ML specialist use to improve the performance of the model on the testing data? ", "zhcn": "一家保险公司正在开发一款自动化车险理赔应用程序。机器学习专家采用亚马逊SageMaker平台内置的TensorFlow目标检测算法，训练出可识别汽车图像中刮痕和凹痕的模型。训练完成后，专家发现该模型在训练数据集上的表现优于测试数据集。为提升模型在测试数据上的性能表现，专家应当采取何种优化策略？"}, "option": [{"option_text": {"zhcn": "增大动量超参数的数值。", "enus": "Increase the value of the momentum hyperparameter."}, "option_flag": false}, {"option_text": {"zhcn": "适当调低dropout_rate超参数的数值。", "enus": "Reduce the value of the dropout_rate hyperparameter."}, "option_flag": true}, {"option_text": {"zhcn": "降低学习率超参数的数值。", "enus": "Reduce the value of the learning_rate hyperparameter"}, "option_flag": false}, {"option_text": {"zhcn": "提升L2超参数的数值。", "enus": "Increase the value of the L2 hyperparameter."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question “An insurance company is creating an application to automate car insurance claims... Which approach should the ML specialist use to improve the performance of the model on the testing data?” is “Reduce the value of the dropout_rate hyperparameter.” The model performing better on the training dataset than the testing dataset indicates overfitting. Dropout is a regularization technique that randomly “drops out” (sets to zero) a fraction of the input units during training to prevent overfitting. Reducing the dropout rate allows more units to contribute to the learning process, which can help the model generalize better and thus improve performance on the testing data.\n\nIncreasing the value of the momentum hyperparameter mainly affects the speed and stability of the optimization algorithm during training, and it doesn't directly address overfitting. Reducing the learning rate controls the step size in the optimization process, which can help with convergence but isn't a direct solution for overfitting. Increasing the L2 hyperparameter is a form of regularization, but it adds a penalty term to the loss function, and increasing it too much can lead to underfitting rather than solving the overfitting problem. These are the reasons why the other options are incorrect, making “Reduce the value of the dropout_rate hyperparameter” the real answer.", "zhcn": "针对“某保险公司正开发一款自动化车险理赔应用程序...机器学习专家应采用哪种方法提升模型在测试数据上的表现？”这一问题，正确答案为“降低dropout_rate超参数的数值”。当模型在训练集上表现优于测试集时，表明出现了过拟合现象。Dropout作为一种正则化技术，通过在训练过程中随机“丢弃”（置零）部分输入单元来抑制过拟合。降低丢弃率可使更多单元参与学习过程，从而增强模型的泛化能力，进而提升测试数据上的表现。\n\n若增加动量超参数值，主要影响训练过程中优化算法的速度与稳定性，并不能直接解决过拟合问题；降低学习率虽能控制优化过程的步长、促进收敛，却非针对过拟合的直接措施；而增大L2超参数虽属正则化手段，但过度增加会对损失函数引入惩罚项，反而可能导致欠拟合而非改善过拟合。正因其他选项存在上述局限性，“降低dropout_rate超参数值”成为唯一正确的选择。"}, "answer": "B"}, {"id": "319", "question": {"enus": "A developer at a retail company is creating a daily demand forecasting model. The company stores the historical hourly demand data in an Amazon S3 bucket. However, the historical data does not include demand data for some hours. The developer wants to verify that an autoregressive integrated moving average (ARIMA) approach will be a suitable model for the use case. How should the developer verify the suitability of an ARIMA approach? ", "zhcn": "某零售企业的一位开发人员正在构建每日需求预测模型。该公司将历史每小时需求数据存储在亚马逊S3存储桶中，但部分时段的历史需求数据存在缺失。开发人员希望验证自回归积分滑动平均模型（ARIMA）是否适用于该场景。请问应如何评估ARIMA模型在此案例中的适用性？"}, "option": [{"option_text": {"zhcn": "使用Amazon SageMaker Data Wrangler。从Amazon S3导入数据。对每小时缺失数据进行填补。执行季节性趋势分解。", "enus": "Use Amazon SageMaker Data Wrangler. Import the data from Amazon S3. Impute hourly missing data. Perform a Seasonal Trend  decomposition."}, "option_flag": false}, {"option_text": {"zhcn": "使用Amazon SageMaker Autopilot，创建一个指定S3数据位置的新实验。选择ARIMA作为机器学习问题类型，并评估模型性能。", "enus": "Use Amazon SageMaker Autopilot. Create a new experiment that specifies the S3 data location. Choose ARIMA as the machine learning  (ML) problem. Check the model performance."}, "option_flag": false}, {"option_text": {"zhcn": "使用 Amazon SageMaker Data Wrangler。从 Amazon S3 导入数据，通过聚合日总量进行数据重采样，并执行季节性趋势分解。", "enus": "Use Amazon SageMaker Data Wrangler. Import the data from Amazon S3. Resample data by using the aggregate daily total. Perform a  Seasonal Trend decomposition."}, "option_flag": true}, {"option_text": {"zhcn": "使用Amazon SageMaker Autopilot，创建一项新实验并指定S3数据存储路径。对缺失的每小时数据进行填补处理。选择ARIMA作为机器学习（ML）问题类型，最后评估模型性能。", "enus": "Use Amazon SageMaker Autopilot. Create a new experiment that specifies the S3 data location. Impute missing hourly values. Choose  ARIMA as the machine learning (ML) problem. Check the model performance."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Use Amazon SageMaker Data Wrangler. Import the data from Amazon S3. Resample data by using the aggregate daily total. Perform a Seasonal Trend decomposition.” This is because the goal is to create a daily demand - forecasting model, so resampling the data to a daily aggregate is a logical step. Seasonal Trend decomposition helps in understanding the underlying patterns in the data, which is crucial to determine if an ARIMA approach is suitable as ARIMA relies on the stationarity and seasonality of the data.\n\nThe first fake option mentions imputing hourly missing data. Since the end - goal is a daily model, focusing on hourly data imputation may not be the most efficient or relevant step. The second and third fake options involve using Amazon SageMaker Autopilot. Autopilot is more about automating the model building process rather than specifically verifying the suitability of an ARIMA approach. Also, in the third option, imputing hourly values is again not the right approach for a daily forecasting model. The key factor that makes the real answer correct is its alignment with the daily forecasting requirement and the use of techniques to understand the data's seasonality for ARIMA suitability.", "zhcn": "对于该问题的正确答案是：\"使用Amazon SageMaker Data Wrangler，从Amazon S3导入数据，通过聚合每日总值进行数据重采样，并执行季节性趋势分解。\" 这是因为构建每日需求预测模型的目标决定了将数据重采样为日聚合值是合理的处理步骤。季节性趋势分解有助于理解数据的潜在规律，这对于判断ARIMA模型是否适用至关重要——毕竟ARIMA方法的效果取决于数据的平稳性与季节性特征。\n\n第一个干扰项提及对每小时缺失数据进行填补，但最终目标是建立每日模型，专注于小时级数据填补既非最有效也不够相关。第二和第三个干扰项都涉及使用Amazon SageMaker Autopilot，而该工具主要专注于自动化建模流程，并非专门用于验证ARIMA方法的适用性。此外第三个选项中再次出现的小时值填补操作，同样不适用于每日预测模型的构建需求。真正答案的关键优势在于其严格契合每日预测需求，并采用恰当技术解析数据季节性以判断ARIMA模型的适用性。"}, "answer": "C"}, {"id": "320", "question": {"enus": "A company decides to use Amazon SageMaker to develop machine learning (ML) models. The company will host SageMaker notebook instances in a VPC. The company stores training data in an Amazon S3 bucket. Company security policy states that SageMaker notebook instances must not have internet connectivity. Which solution will meet the company’s security requirements? ", "zhcn": "一家公司决定采用Amazon SageMaker进行机器学习模型的研发。该公司计划将SageMaker笔记本实例部署在虚拟私有云（VPC）中，并将训练数据存储于亚马逊S3存储桶。根据企业安全政策要求，SageMaker笔记本实例需禁止连接互联网。何种解决方案能够满足该公司的安全要求？"}, "option": [{"option_text": {"zhcn": "通过AWS站点到站点VPN连接位于VPC内的SageMaker笔记本实例，对所有出站互联网流量进行加密传输。配置VPC流日志监控功能，全面追踪网络流量动态，以便及时侦测并阻断任何恶意活动。", "enus": "Connect the SageMaker notebook instances that are in the VPC by using AWS Site-to-Site VPN to encrypt all internet-bound trafic.  Configure VPC fiow logs. Monitor all network trafic to detect and prevent any malicious activity."}, "option_flag": false}, {"option_text": {"zhcn": "请将包含SageMaker笔记本实例的VPC配置为使用VPC接口端点来建立训练和托管连接。修改与VPC接口端点关联的所有现有安全组，仅允许训练和托管所需的出站连接。", "enus": "Configure the VPC that contains the SageMaker notebook instances to use VPC interface endpoints to establish connections for  training and hosting. Modify any existing security groups that are associated with the VPC interface endpoint to allow only outbound  connections for training and hosting."}, "option_flag": true}, {"option_text": {"zhcn": "创建一项禁止访问互联网的IAM策略。将该IAM策略应用于某个IAM角色。除了实例已分配的任何IAM角色外，还需将此IAM角色分配给SageMaker笔记本实例。", "enus": "Create an IAM policy that prevents access the internet. Apply the IAM policy to an IAM role. Assign the IAM role to the SageMaker  notebook instances in addition to any IAM roles that are already assigned to the instances."}, "option_flag": false}, {"option_text": {"zhcn": "创建虚拟私有云安全组以阻断所有出入流量，并将该安全组配置至SageMaker笔记本实例。", "enus": "Create VPC security groups to prevent all incoming and outgoing trafic. Assign the security groups to the SageMaker notebook  instances."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Configure the VPC that contains the SageMaker notebook instances to use VPC interface endpoints to establish connections for training and hosting. Modify any existing security groups that are associated with the VPC interface endpoint to allow only outbound connections for training and hosting.” This solution meets the security requirement of no internet connectivity because VPC interface endpoints enable private communication between the SageMaker notebook instances in the VPC and other AWS services like Amazon S3, without the need for internet access.\n\nThe first fake option involves using AWS Site - to - Site VPN and VPC flow logs. A VPN connection still implies internet connectivity, which violates the security policy. The second fake option uses an IAM policy to prevent internet access. However, IAM policies control access to AWS services, not network connectivity, so this won't stop the instances from having internet access. The third fake option creates security groups to block all traffic. While it does prevent internet access, it also blocks the necessary communication for training and hosting ML models, making it an impractical solution. These common misconceptions about the scope of IAM policies, the nature of VPN connections, and the need for specific network communication in ML development might lead one to choose the fake options.", "zhcn": "针对该问题的正确答案是：\"配置包含SageMaker笔记本实例的VPC，使用VPC接口终端节点建立训练和托管连接。修改与VPC接口终端节点关联的所有安全组，仅允许训练和托管所需的出站连接。\"此方案满足无互联网连接的安全要求，因为VPC接口终端节点可实现VPC内的SageMaker笔记本实例与Amazon S3等其他AWS服务之间的私有通信，无需通过互联网。\n\n第一个干扰选项涉及使用AWS站点到站点VPN和VPC流日志。VPN连接仍意味着互联网连通性，这违反了安全策略。第二个干扰选项试图通过IAM策略阻止互联网访问。但IAM策略仅控制对AWS服务的访问权限，无法管理网络连接性，因此不能阻止实例访问互联网。第三个干扰方案创建安全组以阻断所有流量。虽然这确实能阻止互联网访问，但同时也阻断了机器学习模型训练和托管所需的必要通信，导致该方案不具备可行性。这些关于IAM策略适用范围、VPN连接本质以及机器学习开发中特定网络通信需求的常见误解，可能会使人误选干扰选项。"}, "answer": "B"}, {"id": "321", "question": {"enus": "A machine learning (ML) engineer uses Bayesian optimization for a hyperpara meter tuning job in Amazon SageMaker. The ML engineer uses precision as the objective metric. The ML engineer wants to use recall as the objective metric. The ML engineer also wants to expand the hyperparameter range for a new hyperparameter tuning job. The new hyperparameter range will include the range of the previously performed tuning job. Which approach will run the new hyperparameter tuning job in the LEAST amount of time? ", "zhcn": "一位机器学习工程师在亚马逊SageMaker平台上使用贝叶斯优化进行超参数调优任务。该工程师原采用精确率作为优化目标指标，现计划改用召回率作为新目标指标，并希望扩展超参数范围至包含此前已完成的调优作业区间。若要实现新的超参数调优任务，何种方案能以最短耗时完成？"}, "option": [{"option_text": {"zhcn": "采用热启动超参数调优任务。", "enus": "Use a warm start hyperparameter tuning job."}, "option_flag": true}, {"option_text": {"zhcn": "采用检查点超参数调优任务。", "enus": "Use a checkpointing hyperparameter tuning job."}, "option_flag": false}, {"option_text": {"zhcn": "为超参数调优任务使用相同的随机种子。", "enus": "Use the same random seed for the hyperparameter tuning job."}, "option_flag": false}, {"option_text": {"zhcn": "为超参数调优任务并行运行多个作业。", "enus": "Use multiple jobs in parallel for the hyperparameter tuning job."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is ‘Use a warm start hyperparameter tuning job.’ A warm - start hyperparameter tuning job in Amazon SageMaker allows the new tuning job to leverage the knowledge from a previous job. Since the new hyperparameter range includes the previous one, the warm - start can reuse the information from the previous runs, significantly reducing the time needed to explore the parameter space.\n\n‘Use a checkpointing hyperparameter tuning job’ is mainly for resuming a job that was interrupted. It doesn't use the results of a previous job to speed up a new job with an expanded parameter range. \n\n‘Use the same random seed for the hyperparameter tuning job’ just ensures reproducibility of the randomness in the tuning process. It doesn't help in reducing the time of the new job by leveraging previous results. \n\n‘Use multiple jobs in parallel for the hyperparameter tuning job’ can speed up the overall process, but it doesn't reuse previous results. Instead, it runs multiple independent jobs simultaneously, which may not be as efficient as a warm - start in this case. \n\nThe key factor here is the ability to reuse previous information, which is why a warm - start hyperparameter tuning job is the best option for minimizing the time of the new job.", "zhcn": "该问题的正确答案是\"采用热启动超参数调优任务\"。亚马逊SageMaker中的热启动超参数调优功能，可使新调优任务继承先前任务的调优经验。由于新参数范围完全覆盖原有区间，热启动能直接复用历史调优数据，从而大幅缩短参数空间的探索时间。\n\n而\"采用检查点机制的调优任务\"主要用于中断任务的恢复运行，无法通过历史结果来加速扩展参数范围的新任务；\"为调优任务设置相同随机数种子\"仅能保证调优过程中随机操作的复现性，无法借助既有成果缩短新任务时长；\"并行运行多个调优任务\"虽可提升整体效率，但属于同时执行多个独立任务，其效率在此场景下不及能复用历史信息的热启动方案。\n\n关键在于对历史调优信息的复用能力，因此热启动超参数调优是实现新任务耗时最小化的最优选择。"}, "answer": "A"}, {"id": "322", "question": {"enus": "A news company is developing an article search tool for its editors. The search tool should look for the articles that are most relevant and representative for particular words that are queried among a corpus of historical news documents. The editors test the first version of the tool and report that the tool seems to look for word matches in general. The editors have to spend additional time to filter the results to look for the articles where the queried words are most important. A group of data scientists must redesign the tool so that it isolates the most frequently used words in a document. The tool also must capture the relevance and importance of words for each document in the corpus. Which solution meets these requirements? ", "zhcn": "一家新闻机构正为其编辑研发一款文章检索工具。该工具需从历史新闻文档库中精准找出与查询词汇最相关且最具代表性的文章。编辑们对初版工具进行测试后反馈，现有检索机制仅停留在普通词汇匹配层面，导致他们需要耗费额外时间筛选结果，才能找到查询词汇处于核心地位的文章。数据科学家团队需要重新设计该工具，使其能自动识别文档中的高频词汇，同时精准捕捉每个文档内词汇的相关性与重要程度。现有方案中哪项能满足这些要求？"}, "option": [{"option_text": {"zhcn": "运用隐狄利克雷分布（LDA）主题建模技术从每篇文章中提取主题，并通过累加文章中各词项的主题频次作为评分，构建主题词频统计表。配置该工具时，设定检索规则为：当查询词在文章中的主题词频评分较高时，即优先调取相应文章。", "enus": "Extract the topics from each article by using Latent Dirichlet Allocation (LDA) topic modeling. Create a topic table by assigning the sum  of the topic counts as a score for each word in the articles. Configure the tool to retrieve the articles where this topic count score is higher  for the queried words."}, "option_flag": false}, {"option_text": {"zhcn": "为每篇文章中的词语构建一个按文章长度加权的词频指标，同时基于语料库全部文献为每个词语计算逆向文档频率。将这两项频率指标的乘积定义为最终的高亮评分。将此工具配置为：当查询词条的高亮评分较高时，即可检索出对应文献。", "enus": "Build a term frequency for each word in the articles that is weighted with the article's length. Build an inverse document frequency for  each word that is weighted with all articles in the corpus. Define a final highlight score as the product of both of these frequencies.  Configure the tool to retrieve the articles where this highlight score is higher for the queried words."}, "option_flag": true}, {"option_text": {"zhcn": "下载预训练的词嵌入对照表。为语料库中每篇文章计算标题词嵌入的平均值，构建标题嵌入表。定义每个词的凸显分数，使其与词嵌入和标题嵌入之间的空间距离成反比。配置检索工具，使其能够根据查询词的凸显分数高低筛选出相关文章。", "enus": "Download a pretrained word-embedding lookup table. Create a titles-embedding table by averaging the title's word embedding for each  article in the corpus. Define a highlight score for each word as inversely proportional to the distance between its embedding and the title  embedding. Configure the tool to retrieve the articles where this highlight score is higher for the queried words."}, "option_flag": false}, {"option_text": {"zhcn": "为语料库中每篇文章的词汇建立词频评分表。停用词一律记零分。其余词汇按其在该文章中的出现频次计分。将工具设置为可检索查询词汇得分较高的文章。", "enus": "Build a term frequency score table for each word in each article of the corpus. Assign a score of zero to all stop words. For any other  words, assign a score as the word’s frequency in the article. Configure the tool to retrieve the articles where this frequency score is higher  for the queried words."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to build a term frequency weighted by article length and an inverse document frequency weighted by all articles in the corpus, then define a final highlight score as their product and retrieve articles with a higher score for queried words. This approach meets the requirements as term frequency (TF) helps identify how often a word appears in a single document, and inverse document frequency (IDF) measures the importance of a word across the entire corpus. The product of TF and IDF (TF - IDF) effectively captures both the frequency of a word in a document and its relevance across the corpus.\n\nThe first fake option using LDA topic modeling focuses on topic extraction and topic - count scores. While it can group articles by topics, it doesn't directly measure the frequency of individual words in a document and their importance across the corpus as required.\n\nThe second fake option using a pretrained word - embedding lookup table and title embeddings mainly considers the relationship between word embeddings and title embeddings. It doesn't isolate the most frequently used words in a document and their importance in the context of the entire corpus.\n\nThe third fake option only builds a term frequency score table and assigns scores based on word frequency in an article, ignoring the importance of words across the whole corpus. A common pitfall could be choosing these fake options if one doesn't fully understand the need to balance word frequency within a document and across the corpus, or if they misinterpret the capabilities of the techniques such as LDA or word embeddings for this specific task.", "zhcn": "问题的正确答案是：构建一个以文章长度为权重的词项频率（TF）指标，以及一个以语料库中全部文章为权重的逆文档频率（IDF）指标，随后将二者的乘积作为最终的高亮分数，并据此检索查询词汇得分较高的文章。这种方法完全符合要求——词项频率能识别特定词语在单篇文章中的出现频次，而逆文档频率则可衡量该词在整个语料库中的重要程度。TF与IDF的乘积（TF-IDF）能同时精准捕捉词语在文档内的出现频率及其在全局语料中的相关性。\n\n第一个干扰选项采用的LDA主题建模技术侧重于主题提取与主题计数评分。虽然它能按主题对文章进行聚类，但无法直接衡量特定词语在单篇文章中的出现频率及其在语料库中的重要性，因此不符合核心要求。\n\n第二个干扰选项使用预训练词嵌入查询表和标题嵌入向量，其主要考量的是词向量与标题向量之间的关联性。这种方法既不能分离出文档中最常使用的词汇，也无法评估这些词汇在整体语料库背景下的重要程度。\n\n第三个干扰选项仅构建词频评分表，根据词语在文章中的出现频次进行评分，完全忽略了词汇在全局语料中的重要程度。若未能充分理解平衡文档内词频与跨语料库重要性的必要性，或错误解读LDA、词嵌入等技术在此特定任务中的适用性，则容易陷入选择这些干扰项的误区。"}, "answer": "B"}, {"id": "323", "question": {"enus": "A growing company has a business-critical key performance indicator (KPI) for the uptime of a machine learning (ML) recommendation system. The company is using Amazon SageMaker hosting services to develop a recommendation model in a single Availability Zone within an AWS Region. A machine learning (ML) specialist must develop a solution to achieve high availability. The solution must have a recovery time objective (RTO) of 5 minutes. Which solution will meet these requirements with the LEAST effort? ", "zhcn": "一家处于成长期的企业将其机器学习推荐系统的持续运行时间视为关键业务指标。该公司目前使用Amazon SageMaker托管服务，在AWS区域的单个可用区内开发推荐模型。为确保系统高可用性，机器学习专家需制定解决方案，且必须满足5分钟恢复时间目标。下列哪种方案能以最小成本满足这些要求？"}, "option": [{"option_text": {"zhcn": "在横跨至少两个区域（Region）的虚拟私有云（VPC）中，为每个终端节点部署多个实例。", "enus": "Deploy multiple instances for each endpoint in a VPC that spans at least two Regions."}, "option_flag": false}, {"option_text": {"zhcn": "为托管的推荐模型启用SageMaker自动扩缩容功能。", "enus": "Use the SageMaker auto scaling feature for the hosted recommendation models."}, "option_flag": false}, {"option_text": {"zhcn": "为每个生产端点部署多个实例，这些实例应置于跨越至少两个子网的虚拟私有云中，且这些子网需位于不同的可用区。", "enus": "Deploy multiple instances for each production endpoint in a VPC that spans least two subnets that are in a second Availability Zone."}, "option_flag": true}, {"option_text": {"zhcn": "请定期为生产推荐模型生成备份，并将备份部署于第二区域。", "enus": "Frequently generate backups of the production recommendation model. Deploy the backups in a second Region."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Deploy multiple instances for each production endpoint in a VPC that spans least two subnets that are in a second Availability Zone.” This solution meets the high - availability requirement and the 5 - minute RTO with the least effort. Availability Zones within an AWS Region are physically separate but interconnected, allowing for quick failover in case of an issue in one zone, which helps achieve the short RTO.\n\nThe option “Deploy multiple instances for each endpoint in a VPC that spans at least two Regions” is incorrect. Spanning multiple Regions involves dealing with higher latency, more complex networking, and additional compliance and management overhead. It requires more effort than using multiple Availability Zones within a single Region.\n\nThe “SageMaker auto - scaling feature” is mainly for handling varying workloads by adjusting the number of instances. It does not address high - availability in the context of a single Availability Zone failure, so it cannot meet the requirement of high availability and the 5 - minute RTO.\n\nThe option of frequently generating backups and deploying them in a second Region also has significant drawbacks. It involves the time - consuming process of backup generation and restoration, which is likely to exceed the 5 - minute RTO. Moreover, it requires more management effort compared to using multiple Availability Zones.\n\nThe key factor that makes the real answer correct is its balance of simplicity and effectiveness in achieving high availability within the given RTO constraint, distinguishing it from the more complex and less suitable fake options.", "zhcn": "针对该问题的正确答案是：\"在横跨至少两个位于不同可用区的子网的VPC中，为每个生产端点部署多个实例。\"此方案以最小成本同时满足了高可用性要求和5分钟恢复时间目标。AWS区域内的可用区虽物理隔离但互联互通，可在单一可用区出现故障时实现快速故障转移，从而达成短期恢复目标。\n\n而\"在跨越至少两个区域的VPC中为每个端点部署多个实例\"的方案并不恰当。跨区域部署会面临更高延迟、更复杂的网络配置，以及额外的合规管理负担，其实现难度远高于单区域多可用区方案。\n\n至于\"SageMaker自动扩缩容功能\"，其主要作用是通过动态调整实例数量应对工作负载波动，无法解决单可用区故障场景下的高可用性问题，故不能满足5分钟恢复时间目标。\n\n频繁生成备份并部署至第二区域的方案同样存在明显缺陷。备份创建与恢复过程耗时较长，极易超出5分钟恢复时限，且相比多可用区方案需要投入更多管理精力。\n\n正确答案的核心优势在于：在满足恢复时间约束的前提下，以最简洁有效的部署架构实现高可用性，这与其它复杂而不适用的选项形成鲜明对比。"}, "answer": "C"}, {"id": "324", "question": {"enus": "A global company receives and processes hundreds of documents daily. The documents are in printed .pdf format or .jpg format. A machine learning (ML) specialist wants to build an automated document processing workfiow to extract text from specific fields from the documents and to classify the documents. The ML specialist wants a solution that requires low maintenance. Which solution will meet these requirements with the LEAST operational effort? ", "zhcn": "一家跨国企业每日需接收并处理数百份文件，这些文件以打印版PDF或JPG格式存在。一位机器学习专家计划构建自动化文档处理流程，旨在从文件中特定区域提取文本内容并对文档进行分类。该专家希望采用运维需求较低的解决方案。在满足上述条件的前提下，何种方案能以最小的运维投入实现目标？"}, "option": [{"option_text": {"zhcn": "在 Amazon SageMaker 中调用 PaddleOCR 模型以检测并提取所需文本及字段，并借助 SageMaker 文本分类模型对文档进行自动归类。", "enus": "Use a PaddleOCR model in Amazon SageMaker to detect and extract the required text and fields. Use a SageMaker text classification  model to classify the document."}, "option_flag": false}, {"option_text": {"zhcn": "在 Amazon SageMaker 中调用 PaddleOCR 模型，对所需文本及字段进行检测与提取，并借助 Amazon Comprehend 实现文档的智能分类。", "enus": "Use a PaddleOCR model in Amazon SageMaker to detect and extract the required text and fields. Use Amazon Comprehend to classify  the document."}, "option_flag": false}, {"option_text": {"zhcn": "借助Amazon Textract识别并提取所需文本与字段，运用Amazon Rekognition对文档进行智能分类。", "enus": "Use Amazon Textract to detect and extract the required text and fields. Use Amazon Rekognition to classify the document."}, "option_flag": false}, {"option_text": {"zhcn": "借助Amazon Textract精准识别并提取所需文本与字段，运用Amazon Comprehend对文档进行智能分类。", "enus": "Use Amazon Textract to detect and extract the required text and fields. Use Amazon Comprehend to classify the document."}, "option_flag": true}], "analysis": {"enus": "The correct answer to this question is to “Use Amazon Textract to detect and extract the required text and fields. Use Amazon Comprehend to classify the document.” This solution requires the least operational effort because both Amazon Textract and Amazon Comprehend are fully - managed AWS services. They are pre - trained and optimized for their respective tasks of text extraction and document classification, which means the ML specialist doesn't need to worry about infrastructure management, model training, or software updates, thus minimizing maintenance.\n\nThe fake options involve using a PaddleOCR model in Amazon SageMaker. While SageMaker is a powerful ML platform, using a custom model like PaddleOCR demands more operational effort. The specialist has to handle the model deployment, tuning, and maintenance, which goes against the requirement of low maintenance. \n\nThe option of using Amazon Rekognition for document classification is also incorrect. Amazon Rekognition is mainly designed for image and video analysis, not specifically for document classification. It lacks the specialized capabilities for understanding the context and content of documents as effectively as Amazon Comprehend does. These factors clearly distinguish the real answer from the fake options.", "zhcn": "该问题的正确答案是\"使用Amazon Textract检测并提取所需文本和字段，再通过Amazon Comprehend进行文档分类\"。这一解决方案所需的运维投入最低，因为这两项均为AWS全托管服务。它们经过预先训练并针对文本提取与文档分类任务专项优化，这意味着机器学习专家无需操心基础设施管理、模型训练或软件更新，从而将维护成本降至最低。\n\n干扰选项中提到的在Amazon SageMaker中部署PaddleOCR模型虽具可行性，但SageMaker作为机器学习平台虽功能强大，使用PaddleOCR这类定制模型将显著增加运维负担。专家需要亲自处理模型部署、调优及维护工作，这与低维护需求的要求背道而驰。而使用Amazon Rekognition进行文档分类的方案同样不妥——该服务主要专精于图像视频分析，并未针对文档分类场景优化，其理解文档上下文与内容的能力远不及专门化的Amazon Comprehend。这些关键差异使得正确答案从干扰项中清晰凸显出来。"}, "answer": "D"}, {"id": "325", "question": {"enus": "A company wants to detect credit card fraud. The company has observed that an average of 2% of credit card transactions are fraudulent. A data scientist trains a classifier on a year's worth of credit card transaction data. The classifier needs to identify the fraudulent transactions. The company wants to accurately capture as many fraudulent transactions as possible. Which metrics should the data scientist use to optimize the classifier? (Choose two.) ", "zhcn": "一家公司希望检测信用卡欺诈行为。据该公司观察，信用卡交易中平均有2%属于欺诈交易。数据科学家利用一整年的信用卡交易数据训练了一个分类器，该分类器需要识别出欺诈交易。公司希望尽可能准确地捕捉尽可能多的欺诈交易。数据科学家应采用哪些指标来优化该分类器？（请选择两项。）"}, "option": [{"option_text": {"zhcn": "精准", "enus": "Specificity"}, "option_flag": false}, {"option_text": {"zhcn": "误报率", "enus": "False positive rate"}, "option_flag": false}, {"option_text": {"zhcn": "精确", "enus": "Accuracy"}, "option_flag": false}, {"option_text": {"zhcn": "F1分数", "enus": "F1 score"}, "option_flag": true}, {"option_text": {"zhcn": "真阳性率", "enus": "True positive rate"}, "option_flag": true}], "analysis": {"enus": "The correct answers for optimizing the classifier to accurately capture as many fraudulent transactions as possible are the “F1 score” and “True positive rate”. The True positive rate, also known as recall, measures the proportion of actual fraudulent transactions that the classifier correctly identifies. Since the company wants to catch as many fraudulent transactions as it can, a high True positive rate is crucial.\n\nThe F1 score is the harmonic mean of precision and recall. It provides a balanced measure that takes into account both the ability to correctly identify positives (precision) and the ability to find all the positives (recall). In the context of credit - card fraud detection, it helps in achieving a good balance between correctly identifying frauds and not misclassifying legitimate transactions as frauds.\n\n“Specificity” measures the proportion of non - fraudulent transactions that are correctly identified. While it is important for minimizing false alarms, it does not directly focus on capturing more fraudulent transactions. The “False positive rate” is the proportion of non - fraudulent transactions incorrectly classified as fraudulent, which is more about reducing false alarms rather than maximizing the capture of real frauds. “Accuracy” is the ratio of correctly predicted observations to the total observations. In a highly imbalanced dataset like this one (only 2% fraudulent transactions), a classifier can achieve high accuracy by simply predicting all transactions as non - fraudulent, so it is not a suitable metric for this purpose.\n\nCommon misconceptions might lead one to choose “Accuracy” because it is a commonly used metric in general classification problems. However, in imbalanced datasets, it can be misleading. Choosing “Specificity” or “False positive rate” could be a result of over - emphasizing the need to avoid false alarms, rather than focusing on the primary goal of detecting as many real fraudulent transactions as possible.", "zhcn": "在优化分类器以尽可能精准捕捉欺诈交易时，正确的评估指标应为“F1分数”和“真阳性率”。真阳性率亦称召回率，用于衡量分类器正确识别出的真实欺诈交易比例。由于企业需要最大限度捕获欺诈交易，较高的真阳性率至关重要。\n\nF1分数是精确率与召回率的调和平均数，能均衡考量模型准确识别阳性样本的能力（精确率）和全面捕捉所有阳性样本的能力（召回率）。在信用卡欺诈检测场景中，该指标有助于在精准识别欺诈交易与避免误判合法交易之间取得平衡。\n\n“特异度”衡量的是正确识别的非欺诈交易比例。虽然该指标对降低误报风险很重要，但并未直接聚焦于提升欺诈交易的捕获数量。“假阳性率”指被误判为欺诈的非欺诈交易比例，其核心在于减少误报而非最大化捕捉真实欺诈行为。“准确率”是正确预测样本占总样本的比例。在如此类高度不平衡的数据集（仅含2%欺诈交易）中，分类器仅需将所有交易预测为非欺诈即可获得高准确率，故不适用于本场景。\n\n常见误解可能导致选择“准确率”，因其在常规分类问题中应用广泛。然而面对不平衡数据集时，该指标具有误导性。若选择“特异度”或“假阳性率”，则可能是过度强调避免误报需求，而偏离了最大限度检测真实欺诈交易的核心目标。"}, "answer": "DE"}, {"id": "326", "question": {"enus": "A data scientist is designing a repository that will contain many images of vehicles. The repository must scale automatically in size to store new images every day. The repository must support versioning of the images. The data scientist must implement a solution that maintains multiple immediately accessible copies of the data in different AWS Regions. Which solution will meet these requirements? ", "zhcn": "一位数据科学家正在设计一个用于存储大量车辆图像的资料库。该资料库需具备自动扩容能力，以应对每日新增的图像存储需求，同时必须支持图像版本管理。此外，资料库方案需实现在不同AWS区域保持多个可即时调取的数据副本。何种方案能够满足上述所有要求？"}, "option": [{"option_text": {"zhcn": "\"Amazon S3 跨区域复制（CRR）功能\"", "enus": "Amazon S3 with S3 Cross-Region Replication (CRR)"}, "option_flag": true}, {"option_text": {"zhcn": "在辅助区域共享快照的亚马逊弹性块存储（Amazon EBS）", "enus": "Amazon Elastic Block Store (Amazon EBS) with snapshots that are shared in a secondary Region"}, "option_flag": false}, {"option_text": {"zhcn": "亚马逊弹性文件系统（Amazon EFS）标准存储，采用区域可用性配置。", "enus": "Amazon Elastic File System (Amazon EFS) Standard storage that is configured with Regional availability"}, "option_flag": false}, {"option_text": {"zhcn": "AWS存储网关之卷网关", "enus": "AWS Storage Gateway Volume Gateway"}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is ‘Amazon S3 with S3 Cross - Region Replication (CRR)’. Amazon S3 can scale automatically to store an unlimited amount of data, which is essential for storing new vehicle images daily. It also supports versioning of objects, allowing the data scientist to keep track of different versions of the images. Moreover, S3 Cross - Region Replication enables maintaining multiple immediately accessible copies of the data in different AWS Regions.\n\n‘Amazon Elastic Block Store (Amazon EBS) with snapshots that are shared in a secondary Region’ is incorrect because EBS is mainly used as block storage for EC2 instances and does not provide the same level of scalability and ease of use for storing a large number of images as S3. Also, sharing snapshots in a secondary Region is not the same as having immediately accessible real - time copies.\n\n‘Amazon Elastic File System (Amazon EFS) Standard storage that is configured with Regional availability’ is wrong as EFS is a file - based storage system. While it can scale, it is not optimized for storing a large number of images, and the Regional availability option does not meet the requirement of having copies in different AWS Regions.\n\n‘AWS Storage Gateway Volume Gateway’ is used to connect on - premises environments to cloud storage and is not designed to meet the requirements of automatic scaling, versioning, and maintaining multiple copies in different AWS Regions for an image repository.\n\nThe key factors that make Amazon S3 with CRR the right choice are its scalability, versioning support, and the ability to replicate data across regions, distinguishing it from the other fake options.", "zhcn": "该问题的正确答案是“采用跨区域复制功能的亚马逊S3存储服务”。亚马逊S3能够自动扩展以存储无限量的数据，这对于每日新增车辆图像存储至关重要。同时其支持对象版本控制功能，便于数据科学家追踪图像的不同迭代版本。此外，S3跨区域复制技术可在不同AWS区域中维护多个立即可用的数据副本。\n\n而“采用跨区域快照共享的亚马逊弹性块存储”并非正确选项，因为EBS主要作为EC2实例的块级存储使用，在存储海量图像时既不具备S3级别的扩展性，也不具备同等的易用性。况且跨区域共享快照并不等同于实时可访问的数据副本。\n\n“配置区域可用性的亚马逊弹性文件系统标准存储”亦不正确，EFS作为基于文件的存储系统虽具备扩展能力，但并未针对海量图像存储进行优化，且区域可用性选项无法满足跨AWS区域存储副本的要求。\n\n“AWS存储网关卷网关”主要用于连接本地环境与云存储，其设计初衷并不满足图像存储库所需的自动扩展、版本控制及跨区域多副本维护等核心需求。\n\n亚马逊S3配合跨区域复制功能之所以成为正确选择，关键在于其卓越的扩展性、版本控制支持以及跨区域数据复制能力，这些特性使其在所有备选方案中脱颖而出。"}, "answer": "A"}, {"id": "327", "question": {"enus": "An ecommerce company wants to update a production real-time machine learning (ML) recommendation engine API that uses Amazon SageMaker. The company wants to release a new model but does not want to make changes to applications that rely on the API. The company also wants to evaluate the performance of the new model in production trafic before the company fully rolls out the new model to all users. Which solution will meet these requirements with the LEAST operational overhead? ", "zhcn": "一家电子商务公司计划升级其基于亚马逊SageMaker的生产级实时机器学习推荐引擎API。在保持依赖该API的应用程序无需改动的前提下，公司希望部署新模型，并计划在向全体用户全面推广前，先于实际生产流量中评估新模型的性能。哪种方案能以最低运维成本满足这些需求？"}, "option": [{"option_text": {"zhcn": "为新型号创建全新的SageMaker终端节点。配置应用负载均衡器（ALB），使流量在旧模型与新模型之间实现智能分发。", "enus": "Create a new SageMaker endpoint for the new model. Configure an Application Load Balancer (ALB) to distribute trafic between the  old model and the new model."}, "option_flag": false}, {"option_text": {"zhcn": "将现有终端节点调整为使用SageMaker生产变体，以便在旧模型与新模型之间分配流量。", "enus": "Modify the existing endpoint to use SageMaker production variants to distribute trafic between the old model and the new model."}, "option_flag": true}, {"option_text": {"zhcn": "对现有端点进行改造，采用SageMaker批量转换技术，实现新旧模型之间的流量分配。", "enus": "Modify the existing endpoint to use SageMaker batch transform to distribute trafic between the old model and the new model."}, "option_flag": false}, {"option_text": {"zhcn": "为新型号创建全新的SageMaker终端节点。配置网络负载均衡器（NLB），以便在旧模型与新模型之间实现流量分发。", "enus": "Create a new SageMaker endpoint for the new model. Configure a Network Load Balancer (NLB) to distribute trafic between the old  model and the new model."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is ‘Modify the existing endpoint to use SageMaker production variants to distribute traffic between the old model and the new model.’ This approach meets the requirements with the least operational overhead. SageMaker production variants allow distributing traffic between different models on the same endpoint. This means there's no need to create new endpoints or configure external load - balancers, and applications relying on the API can continue using the existing endpoint without any changes.\n\nThe option of creating a new SageMaker endpoint and using an Application Load Balancer (ALB) or a Network Load Balancer (NLB) to distribute traffic adds significant operational complexity. New endpoints need to be managed, and the load - balancers require configuration and maintenance, increasing overhead.\n\nThe option of using SageMaker batch transform is incorrect because batch transform is designed for offline, batch - based processing, not for real - time traffic distribution between models in a production environment. \n\nCommon misconceptions might lead one to choose the fake options. For example, thinking that using external load - balancers provides more flexibility without considering the added complexity. Or confusing the purpose of batch transform with real - time traffic management. These factors make the real answer the most suitable choice in terms of meeting requirements with minimal overhead.", "zhcn": "问题的正确答案是\"修改现有端点，使其利用SageMaker生产变体在旧模型与新模型之间分配流量\"。此方案能以最低运维成本满足需求。SageMaker生产变体支持在同一端点下为不同模型配置流量分配，这意味着既无需创建新端点，也不必配置外部负载均衡器，依赖该API的应用程序可继续使用现有端点且无需任何改动。\n\n若选择创建新SageMaker端点并搭配应用负载均衡器（ALB）或网络负载均衡器（NLB）进行流量分配，将显著增加运维复杂度。新端点需要管理维护，负载均衡器还需额外配置与运维，导致运营成本上升。\n\n采用SageMaker批量转换的方案并不适用，因为该功能专为离线批处理设计，而非生产环境中模型间的实时流量分配。常见误解可能导致选择错误选项，例如认为外部负载均衡器能提供更高灵活性却忽略其复杂性问题，或误将批量转换功能与实时流量管理相混淆。正是这些因素使得原答案成为满足需求且运维成本最优的最佳选择。"}, "answer": "B"}, {"id": "328", "question": {"enus": "A machine learning (ML) specialist at a manufacturing company uses Amazon SageMaker DeepAR to forecast input materials and energy requirements for the company. Most of the data in the training dataset is missing values for the target variable. The company stores the training dataset as JSON files. The ML specialist develop a solution by using Amazon SageMaker DeepAR to account for the missing values in the training dataset. Which approach will meet these requirements with the LEAST development effort? ", "zhcn": "某制造企业的机器学习专家运用亚马逊SageMaker DeepAR平台，旨在精准预测企业所需的原材料与能源消耗量。然而训练数据集中的目标变量存在大量数值缺失，且企业当前以JSON格式存储训练数据。该专家需基于亚马逊SageMaker DeepAR框架，以最小开发成本构建能够处理训练数据缺失值的解决方案。下列哪种方法最高效契合需求？"}, "option": [{"option_text": {"zhcn": "采用线性回归方法对缺失值进行填补，继而利用完整数据集及填补后的数值训练DeepAR模型。", "enus": "Impute the missing values by using the linear regression method. Use the entire dataset and the imputed values to train the DeepAR  model."}, "option_flag": false}, {"option_text": {"zhcn": "将缺失值替换为NaN（非数值）。利用完整数据集及经过编码的缺失值来训练DeepAR模型。", "enus": "Replace the missing values with not a number (NaN). Use the entire dataset and the encoded missing values to train the DeepAR  model."}, "option_flag": false}, {"option_text": {"zhcn": "采用前向填充法补全缺失值，并运用完整数据集及填补后的数值训练DeepAR模型。", "enus": "Impute the missing values by using a forward fill. Use the entire dataset and the imputed values to train the DeepAR model."}, "option_flag": false}, {"option_text": {"zhcn": "采用均值填补缺失值后，结合完整数据集与填补后的数值训练DeepAR模型。", "enus": "Impute the missing values by using the mean value. Use the entire dataset and the imputed values to train the DeepAR model."}, "option_flag": true}], "analysis": {"enus": "The correct answer to the question is to “Impute the missing values by using the mean value. Use the entire dataset and the imputed values to train the DeepAR model.” This approach requires the least development effort because calculating the mean is a simple and straightforward statistical operation. It involves just summing all the available values of the target variable and dividing by the number of non - missing values.\n\nThe option of using linear regression to impute missing values is more complex. Linear regression requires building a regression model, which involves tasks like feature selection, model training, and parameter tuning. This demands more development work compared to a simple mean calculation.\n\nReplacing missing values with NaN is not a valid approach for training most machine - learning models, including DeepAR. Most models cannot handle NaN values directly, and it will likely lead to errors during the training process.\n\nUsing forward fill to impute missing values is also more complex than using the mean. Forward fill requires keeping track of the previous non - missing value in the sequence, which can be more involved, especially in large datasets. \n\nIn summary, the simplicity of calculating the mean value makes it the option with the least development effort, distinguishing it from the other more complex or invalid fake answer options.", "zhcn": "对于题目中提出的缺失值处理问题，正确答案是\"采用均值填充法补全缺失值，并基于完整数据集与填充后的数据训练DeepAR模型\"。这种方案所需开发投入最低，因为均值计算作为基础统计方法，仅需对目标变量的有效数值求和后除以非空值数量即可完成。\n\n相比之下，采用线性回归进行缺失值填补则更为复杂。该方法需构建回归模型，涉及特征筛选、模型训练及参数调优等环节，开发工作量远超简单的均值计算。\n\n若将缺失值替换为NaN，则不符合大多数机器学习模型（包括DeepAR）的训练要求。模型通常无法直接处理NaN值，此举极易导致训练过程报错。\n\n前向填充法在实现复杂度上也高于均值法。该方法需要追踪序列中的前一个有效值，对于大规模数据集而言操作更为繁琐。\n\n综上，均值法以其计算简洁性成为开发成本最低的选项，与其他复杂或无效的备选方案形成鲜明对比。"}, "answer": "D"}, {"id": "329", "question": {"enus": "A law firm handles thousands of contracts every day. Every contract must be signed. Currently, a lawyer manually checks all contracts for signatures. The law firm is developing a machine learning (ML) solution to automate signature detection for each contract. The ML solution must also provide a confidence score for each contract page. Which Amazon Textract API action can the law firm use to generate a confidence score for each page of each contract? ", "zhcn": "一家律师事务所每日处理数以千计的合同文件，每份合同均需完成签署。目前由律师人工核验所有合同的签名情况。该事务所正研发机器学习解决方案，旨在实现合同签名自动识别功能。此方案还需为每页合同生成可信度评分。请问律师事务所应采用亚马逊Textract的哪项API操作，才能为每份合同的每一页生成可信度评分？"}, "option": [{"option_text": {"zhcn": "请调用AnalyzeDocument API接口，将FeatureTypes参数设置为SIGNATURES，并返回每一页签名区域的置信度评分。", "enus": "Use the AnalyzeDocument API action. Set the FeatureTypes parameter to SIGNATURES. Return the confidence scores for each page."}, "option_flag": false}, {"option_text": {"zhcn": "对文档调用预测接口，返回每页的签名信息及置信度评分。", "enus": "Use the Prediction API call on the documents. Return the signatures and confidence scores for each page."}, "option_flag": false}, {"option_text": {"zhcn": "调用StartDocumentAnalysis接口操作以检测签名区域，并返回每页签名的置信度评分。", "enus": "Use the StartDocumentAnalysis API action to detect the signatures. Return the confidence scores for each page."}, "option_flag": true}, {"option_text": {"zhcn": "调用GetDocumentAnalysis接口功能以检测文档中的签名区域，并返回每一页签名的置信度评分。", "enus": "Use the GetDocumentAnalysis API action to detect the signatures. Return the confidence scores for each page."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Use the StartDocumentAnalysis API action to detect the signatures and return the confidence scores for each page.” The StartDocumentAnalysis API is an asynchronous operation suitable for processing large - scale documents, which aligns well with the law firm's need to handle thousands of contracts daily. It can analyze various document features, including signatures, and provides confidence scores for the detected elements on each page.\n\nThe “AnalyzeDocument” API is a synchronous operation. It is more suitable for small - scale document processing. Given the large volume of contracts the law firm deals with, using this API would likely lead to performance issues and time - consuming processing, so it is not the best choice.\n\nThere is no “Prediction API call” in Amazon Textract. This option is a non - existent API in the context of Amazon Textract, which makes it clearly incorrect.\n\nThe “GetDocumentAnalysis” API is used to retrieve the results of a previously started StartDocumentAnalysis operation. It cannot be used independently to start the signature detection process, so it is also an incorrect option. These incorrect options might mislead users who are not familiar with the specific capabilities and usage scenarios of Amazon Textract APIs.", "zhcn": "针对该问题的正确答案是\"使用StartDocumentAnalysis API操作来检测签名并返回每页的可信度评分\"。StartDocumentAnalysis API作为异步操作，特别适合处理大规模文档，这与律师事务所每日需处理数千份合同的需求高度契合。该API能够分析包括签名在内的多种文档特征，并为每页检测到的元素提供可信度评分。\n\n而AnalyzeDocument API属于同步操作，更适用于小规模文档处理。考虑到律师事务所处理的合同量级，使用此API可能导致性能问题和处理耗时，故而非最佳选择。\n\nAmazon Textract中并不存在名为\"Prediction API call\"的接口。该选项在Amazon Textract的语境下属于不存在的API，因此明显错误。\n\nGetDocumentAnalysis API用于获取已启动的StartDocumentAnalysis操作结果，无法独立启动签名检测流程，故同样为错误选项。这些错误选项可能会对不熟悉Amazon Textract API具体功能及使用场景的用户产生误导。"}, "answer": "C"}, {"id": "330", "question": {"enus": "A company that operates oil platforms uses drones to photograph locations on oil platforms that are difficult for humans to access to search for corrosion. Experienced engineers review the photos to determine the severity of corrosion. There can be several corroded areas in a single photo. The engineers determine whether the identified corrosion needs to be fixed immediately, scheduled for future maintenance, or requires no action. The corrosion appears in an average of 0.1% of all photos. A data science team needs to create a solution that automates the process of reviewing the photos and classifying the need for maintenance. Which combination of steps will meet these requirements? (Choose three.) ", "zhcn": "一家运营海上石油平台的企业采用无人机拍摄平台人员难以抵达区域的照片，以探查腐蚀状况。经验丰富的工程师通过审阅这些照片评估腐蚀严重程度，单张图像中可能呈现多处腐蚀区域。工程师需判断已识别的腐蚀点是需要立即修复、安排后续维护，抑或无需采取行动。在所有拍摄图像中，腐蚀现象的出现概率平均为0.1%。数据科学团队需构建一套自动化解决方案，实现照片审阅及维护需求分类的智能化处理。下列哪三项步骤组合能够满足上述需求？（请选择三项）"}, "option": [{"option_text": {"zhcn": "采用目标检测算法训练模型，用于识别照片中的腐蚀区域。最高赞方案", "enus": "Use an object detection algorithm to train a model to identify corrosion areas of a photo. Most Voted"}, "option_flag": true}, {"option_text": {"zhcn": "对照片启用亚马逊Rekognition的标签识别功能。", "enus": "Use Amazon Rekognition with label detection on the photos."}, "option_flag": false}, {"option_text": {"zhcn": "采用k均值聚类算法训练模型，实现对照片中腐蚀程度的智能分级。", "enus": "Use a k-means clustering algorithm to train a model to classify the severity of corrosion in a photo."}, "option_flag": false}, {"option_text": {"zhcn": "采用XGBoost算法训练模型，对照片中的腐蚀程度进行等级分类。最高票选方案。", "enus": "Use an XGBoost algorithm to train a model to classify the severity of corrosion in a photo. Most Voted"}, "option_flag": true}, {"option_text": {"zhcn": "对含有腐蚀痕迹的照片进行图像增强处理。最多赞同", "enus": "Perform image augmentation on photos that contain corrosion. Most Voted"}, "option_flag": true}, {"option_text": {"zhcn": "对不含腐蚀痕迹的照片进行图像增强处理。", "enus": "Perform image augmentation on photos that do not contain corrosion."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question involves three steps: using an object - detection algorithm to identify corrosion areas, using an XGBoost algorithm to classify corrosion severity, and performing image augmentation on photos with corrosion.\n\nUsing an object - detection algorithm is essential as it can precisely locate the corroded areas in the photos, which is the first step in the automated review process. XGBoost is a powerful machine - learning algorithm well - suited for classification tasks. It can effectively classify the severity of corrosion based on the identified areas, thus automating the decision - making process for maintenance. Image augmentation on photos with corrosion helps increase the diversity of the training data, improving the model's generalization ability.\n\nThe fake answer options are incorrect for the following reasons. Amazon Rekognition with label detection is a general - purpose image analysis tool. It may not be specifically tailored to detect and classify corrosion, which is a very specialized task. A k - means clustering algorithm is an unsupervised learning method used for grouping data points based on similarity. It is not designed for the classification of corrosion severity, which requires a supervised approach. Performing image augmentation on photos without corrosion doesn't contribute to training the model to detect and classify corrosion, as the focus should be on the corroded areas. These common misconceptions might lead one to choose the fake options due to a lack of understanding of the specific requirements of the task and the capabilities of different algorithms.", "zhcn": "针对该问题的正确答案包含三个步骤：首先采用目标检测算法精确定位腐蚀区域，随后运用XGBoost算法进行腐蚀程度分级，最后对含腐蚀图像进行数据增强。目标检测算法能像探针般精准捕捉图像中的腐蚀部位，这是实现自动化检测的首要环节。XGBoost作为高效的机器学习算法，在分类任务中表现卓越，可基于已识别的腐蚀区域实现维护决策的自动化判断。而对含腐蚀图像进行数据增强，则能有效扩充训练样本的多样性，提升模型的泛化能力。\n\n至于干扰选项的谬误之处在于：亚马逊Rekognition标签识别作为通用图像分析工具，难以胜任腐蚀检测这类专业任务；K均值聚类属于无监督学习方法，无法实现腐蚀程度分类这种需要监督学习的场景；对无腐蚀图像进行数据增强更是偏离核心目标——毕竟训练模型的重点在于让算法学会识别腐蚀特征。这些常见误解往往源于对任务特殊性及算法适用性的认知不足。"}, "answer": "ADE"}, {"id": "331", "question": {"enus": "A company maintains a 2 TB dataset that contains information about customer behaviors. The company stores the dataset in Amazon S3. The company stores a trained model container in Amazon Elastic Container Registry (Amazon ECR). A machine learning (ML) specialist needs to score a batch model for the dataset to predict customer behavior. The ML specialist must select a scalable approach to score the model. Which solution will meet these requirements MOST cost-effectively? ", "zhcn": "某公司存有一套容量为2 TB的客户行为数据集，存放于亚马逊S3云存储服务中。该公司已将训练好的模型容器托管于亚马逊弹性容器注册表（Amazon ECR）。一位机器学习专家需对该数据集进行批量模型评分以预测客户行为，此时必须选择可扩展的评分方案。下列哪种解决方案最能符合成本效益要求？"}, "option": [{"option_text": {"zhcn": "利用AWS Batch管理的亚马逊EC2预留实例对模型进行评分。创建亚马逊EC2实例存储卷，并将其挂载至预留实例。", "enus": "Score the model by using AWS Batch managed Amazon EC2 Reserved Instances. Create an Amazon EC2 instance store volume and mount it to the Reserved Instances."}, "option_flag": false}, {"option_text": {"zhcn": "采用AWS Batch托管型Amazon EC2竞价型实例对模型进行评分。创建Amazon FSx for Lustre存储卷并将其挂载至竞价型实例。获赞最多方案", "enus": "Score the model by using AWS Batch managed Amazon EC2 Spot Instances. Create an Amazon FSx for Lustre volume and mount it to the Spot Instances. Most Voted"}, "option_flag": true}, {"option_text": {"zhcn": "在亚马逊EC2预留实例上运行Amazon SageMaker笔记本以评估模型性能。创建亚马逊EBS存储卷并将其挂载至预留实例。", "enus": "Score the model by using an Amazon SageMaker notebook on Amazon EC2 Reserved Instances. Create an Amazon EBS volume and mount it to the Reserved Instances."}, "option_flag": false}, {"option_text": {"zhcn": "在亚马逊EC2 Spot实例上通过Amazon SageMaker笔记本对模型进行评分。创建亚马逊弹性文件系统（Amazon EFS）并挂载至Spot实例。B（100%）", "enus": "Score the model by using Amazon SageMaker notebook on Amazon EC2 Spot Instances. Create an Amazon Elastic File System (Amazon EFS) file system and mount it to the Spot Instances.  B (100%)"}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to score the model by using AWS Batch managed Amazon EC2 Spot Instances and creating an Amazon FSx for Lustre volume to mount on the Spot Instances. This approach is the most cost - effective because Spot Instances offer significant cost savings compared to Reserved Instances as they use unused EC2 capacity. FSx for Lustre is a high - performance file system well - suited for large - scale data processing, which is ideal for scoring a batch model on a 2 TB dataset.\n\nThe option of using AWS Batch with Amazon EC2 Reserved Instances and an EC2 instance store volume is less cost - effective. Reserved Instances require upfront payment and long - term commitment, which is not necessary for a potentially short - lived batch scoring task. Also, an instance store volume is ephemeral and not suitable for long - term data storage.\n\nUsing an Amazon SageMaker notebook on Amazon EC2 Reserved Instances and an EBS volume is also more expensive due to the use of Reserved Instances. Additionally, SageMaker notebooks are more commonly used for interactive model development rather than large - scale batch scoring.\n\nUsing an Amazon SageMaker notebook on Spot Instances with an Amazon EFS file system is not the best choice. While Spot Instances are cost - effective, EFS is not as performant as FSx for Lustre when dealing with large - scale data processing. The main factors that make the real answer superior are cost savings from Spot Instances and high - performance data access with FSx for Lustre, which distinguish it from the fake options.", "zhcn": "针对该问题，正确答案是采用AWS Batch托管的Amazon EC2竞价型实例，并创建Amazon FSx for Lustre存储卷挂载至竞价实例。此方案最具成本效益：竞价型实例利用EC2闲置资源，较之预留实例可大幅节省开支；而FSx for Lustre高性能文件系统专为海量数据处理设计，非常适合对2TB数据集进行批量模型评分。\n\n若采用AWS Batch搭配Amazon EC2预留实例及EC2实例存储卷，则成本效益较低。预留实例需预付费用且要求长期承诺，对于可能短时运行的批量评分任务并不必要；此外实例存储卷为临时存储，不适用于长期数据保存。\n\n若在Amazon EC2预留实例上运行Amazon SageMaker笔记本并配合EBS存储卷，会因预留实例的使用导致成本偏高。况且SageMaker笔记本通常用于交互式模型开发，而非大规模批量评分任务。\n\n若采用竞价型实例运行Amazon SageMaker笔记本并配合EFS文件系统，亦非最佳选择。虽然竞价实例成本可控，但EFS在处理大规模数据时的性能表现不及FSx for Lustre。真正答案的优越性正体现在竞价实例的成本优势与FSx for Lustre的高性能数据访问能力，这使其与其他错误选项形成鲜明对比。"}, "answer": "B"}, {"id": "332", "question": {"enus": "A data scientist is implementing a deep learning neural network model for an object detection task on images. The data scientist wants to experiment with a large number of parallel hyperparameter tuning jobs to find hyperparameters that optimize compute time. The data scientist must ensure that jobs that underperform are stopped. The data scientist must allocate computational resources to well-performing hyperparameter configurations. The data scientist is using the hyperparameter tuning job to tune the stochastic gradient descent (SGD) learning rate, momentum, epoch, and mini-batch size. Which technique will meet these requirements with LEAST computational time? ", "zhcn": "一位数据科学家正在为图像目标检测任务部署深度学习神经网络模型。该数据科学家希望通过并行运行大量超参数调优任务，寻找能最大化计算效率的最佳参数组合。在此过程中，需及时终止表现不佳的训练任务，并将计算资源动态分配给表现优异的参数配置。本次超参数调优主要针对随机梯度下降法（SGD）的学习率、动量参数、训练轮次及小批量样本规模。若要满足上述需求且最大限度缩短计算时间，应采用下列哪种技术方案？"}, "option": [{"option_text": {"zhcn": "网格搜索", "enus": "Grid search"}, "option_flag": false}, {"option_text": {"zhcn": "随机寻优", "enus": "Random search"}, "option_flag": false}, {"option_text": {"zhcn": "贝叶斯优化", "enus": "Bayesian optimization"}, "option_flag": false}, {"option_text": {"zhcn": "超频优选（Hyperband Most Voted）\n\n注：采用\"超频\"对应\"Hyper\"的技术感，\"优选\"对应\"Most Voted\"的集体决策内涵，既保留算法领域特性，又通过四字格提升中文韵律美感。专有名词部分保留英文原称置于括号内，符合学术规范。", "enus": "Hyperband Most Voted"}, "option_flag": true}], "analysis": {"enus": "The correct answer to the question is ‘Hyperband Most Voted’. Hyperband is designed to efficiently allocate resources to well - performing hyperparameter configurations and stop underperforming jobs. It does this by using a multi - stage process where it quickly eliminates less promising hyperparameter settings, thus saving computational time.\n\n‘Grid search’ evaluates all possible combinations of hyperparameters in a predefined grid. This can be extremely computationally expensive, especially when there are a large number of hyperparameters to tune, as in this case with learning rate, momentum, epoch, and mini - batch size.\n\n‘Random search’ randomly samples hyperparameter combinations from a defined range. While it can be more efficient than grid search, it does not have a mechanism to actively stop underperforming jobs and allocate resources to better ones as effectively as Hyperband.\n\n‘Bayesian optimization’ builds a probabilistic model of the objective function to predict which hyperparameters are likely to perform well. However, it can be computationally intensive, especially in the early stages, and may not be as efficient as Hyperband in quickly eliminating bad configurations.\n\nThe ability of Hyperband Most Voted to rapidly prune underperforming jobs and focus resources on better ones is the key reason it is the real answer option, distinguishing it from the computationally more expensive fake options.", "zhcn": "对于该问题的正确答案是\"Hyperband Most Voted\"。Hyperband的核心设计在于通过多阶段筛选机制，高效分配资源给表现优异的超参数组合，并及时终止潜力不足的方案。其运作方式如同精密的筛选系统：先快速淘汰表现欠佳的超参数设置以节约计算资源，再集中资源深化潜力组合的调优。\n\n\"网格搜索\"采用穷举式策略，在预设参数网格中遍历所有组合。当需要调整学习率、动量参数、训练轮次和小批量规模等多组参数时，这种方法的计算成本会呈指数级增长。\n\n\"随机搜索\"通过在设定范围内随机抽取超参数组合进行尝试。虽然相比网格搜索更具效率优势，但缺乏动态终止低效任务、智能调配资源至优势方案的机制。\n\n\"贝叶斯优化\"通过构建目标函数的概率模型来预测优质参数区间。然而其计算负荷较大，尤其在初始阶段难以实现Hyperband那样快速淘汰劣质配置的效率。\n\n\"Hyperband Most Voted\"能成为真正正确答案的关键在于：它具备动态剪枝机制，可快速终止低效任务并将资源聚焦于优势方案，这种智能资源调配能力使其显著区别于其他计算成本高昂的备选方案。"}, "answer": "D"}, {"id": "333", "question": {"enus": "An agriculture company wants to improve crop yield forecasting for the upcoming season by using crop yields from the last three seasons. The company wants to compare the performance of its new scikit-learn model to the benchmark. A data scientist needs to package the code into a container that computes both the new model forecast and the benchmark. The data scientist wants AWS to be responsible for the operational maintenance of the container. Which solution will meet these requirements? ", "zhcn": "一家农业公司希望利用过去三个季度的作物产量数据，提升对新一季作物产量的预测精度。该公司计划将其新开发的scikit-learn模型与基准模型进行性能比较。一位数据科学家需要将相关代码封装至容器中，使其能够同时运行新模型预测与基准模型计算。该数据科学家希望由AWS负责容器的运维管理。何种解决方案可满足上述需求？"}, "option": [{"option_text": {"zhcn": "将代码打包为适用于 Amazon SageMaker scikit-learn 容器的训练脚本。", "enus": "Package the code as the training script for an Amazon SageMaker scikit-learn container."}, "option_flag": false}, {"option_text": {"zhcn": "将代码封装至定制容器中，随后把容器推送至亚马逊弹性容器仓库（Amazon ECR）。", "enus": "Package the code into a custom-built container. Push the container to Amazon Elastic Container Registry (Amazon ECR)."}, "option_flag": false}, {"option_text": {"zhcn": "将代码封装至定制容器中，随后将该容器推送至AWS Fargate服务平台。", "enus": "Package the code into a custom-built container. Push the container to AWS Fargate."}, "option_flag": false}, {"option_text": {"zhcn": "通过扩展亚马逊SageMaker的scikit-learn容器对代码进行封装。投票结果：D选项（50%）获最高支持，A选项（33%）次之，C选项（17%）位列第三。", "enus": "Package the code by extending an Amazon SageMaker scikit-learn container. Most Voted  D (50%)  A (33%)  C (17%)"}, "option_flag": true}], "analysis": {"enus": "The correct answer to the question is to “Package the code by extending an Amazon SageMaker scikit - learn container”. This option meets the requirement of having AWS be responsible for the operational maintenance of the container because Amazon SageMaker manages the underlying infrastructure and operational aspects for its pre - built containers. When you extend a SageMaker scikit - learn container, you can leverage this built - in maintenance support.\n\nThe option “Package the code as the training script for an Amazon SageMaker scikit - learn container” is incorrect as it focuses on the training script rather than packaging the code for computing both the new model forecast and the benchmark, and doesn't directly address the container maintenance aspect.\n\n“Package the code into a custom - built container. Push the container to Amazon Elastic Container Registry (Amazon ECR)” requires the user to handle more of the operational maintenance as it is a custom - built container. ECR is mainly for container storage, not for taking over the operational management of the container.\n\n“Package the code into a custom - built container. Push the container to AWS Fargate” also involves a custom - built container, and while Fargate simplifies some aspects of running containers, it still doesn't offer the same level of operational maintenance as using a pre - built and managed SageMaker container. \n\nThe key factor that makes the real answer correct is the use of a managed SageMaker container, which relieves the data scientist from the burden of operational maintenance, distinguishing it from the fake options that rely on custom - built containers. A common misconception might be thinking that custom - built containers offer the same level of management as pre - built SageMaker containers, leading one to choose the fake options.", "zhcn": "针对该问题的正确答案是\"通过扩展亚马逊SageMaker的scikit-learn容器来封装代码\"。此方案符合\"由AWS负责容器运维\"的要求，因为亚马逊SageMaker会为其预置容器管理底层基础设施和运维事务。当您扩展SageMaker的scikit-learn容器时，便可直接利用这种内置的运维支持。\n\n而\"将代码打包为亚马逊SageMaker scikit-learn容器的训练脚本\"这一选项并不正确——它侧重于训练脚本的封装，并未涉及同时计算新模型预测值与基准值的代码打包需求，也未直接解决容器运维问题。\n\n若选择\"将代码打包至自定义容器并推送至亚马逊弹性容器仓库（ECR）\"，则需用户承担更多运维工作，毕竟自定义容器不在AWS全托管范畴。ECR仅提供容器存储服务，并不负责容器运维管理。\n\n同样，\"将代码打包至自定义容器并部署至AWS Fargate\"的方案也涉及自定义容器。尽管Fargate简化了容器运行流程，但其运维支持力度仍不及使用预置的SageMaker托管容器。\n\n正确方案的核心优势在于：采用SageMaker托管容器可让数据科学家从运维负担中彻底解放。常见的认知误区是认为自定义容器能提供与预置SageMaker容器同等的管理效能，从而导致选择错误答案。"}, "answer": "D"}, {"id": "334", "question": {"enus": "A cybersecurity company is collecting on-premises server logs, mobile app logs, and IoT sensor data. The company backs up the ingested data in an Amazon S3 bucket and sends the ingested data to Amazon OpenSearch Service for further analysis. Currently, the company has a custom ingestion pipeline that is running on Amazon EC2 instances. The company needs to implement a new serverless ingestion pipeline that can automatically scale to handle sudden changes in the data flow. Which solution will meet these requirements MOST cost-effectively? ", "zhcn": "一家网络安全公司正在采集本地服务器日志、移动应用日志及物联网传感器数据。该公司将采集的数据备份至亚马逊S3存储桶，并传送至亚马逊OpenSearch服务进行深度分析。当前其采用的自定义数据摄取管道运行于亚马逊EC2实例之上。现需构建一套全新的无服务器数据摄取管道，该管道需具备自动扩展能力以应对数据流的突发波动。在满足这些需求的前提下，何种解决方案能实现最优成本效益？"}, "option": [{"option_text": {"zhcn": "创建两条亚马逊数据火线（Amazon Data Firehose）传输流，用于将数据分别传送至S3存储桶与OpenSearch服务。配置数据源以使其向这两条传输流发送数据。", "enus": "Create two Amazon Data Firehose delivery streams to send data to the S3 bucket and OpenSearch Service. Configure the data sources to send data to the delivery streams."}, "option_flag": false}, {"option_text": {"zhcn": "创建一条Amazon Kinesis数据流。  \n设立两条Amazon Data Firehose传输流，分别将数据传送至S3存储桶与OpenSearch服务。  \n将两条传输流与数据流建立连接。  \n配置各数据源，使其向数据流持续输送数据。", "enus": "Create one Amazon Kinesis data stream. Create two Amazon Data Firehose delivery streams to send data to the S3 bucket and OpenSearch Service. Connect the delivery streams to the data stream. Configure the data sources to send data to the data stream."}, "option_flag": false}, {"option_text": {"zhcn": "创建一条亚马逊数据火线（Amazon Data Firehose）传输流，将数据传送至OpenSearch服务。配置该传输流时，需将原始数据备份至S3存储桶。同时设置数据源，使其能够向传输流发送数据。", "enus": "Create one Amazon Data Firehose delivery stream to send data to OpenSearch Service. Configure the delivery stream to back up the raw data to the S3 bucket. Configure the data sources to send data to the delivery stream. Most Voted"}, "option_flag": true}, {"option_text": {"zhcn": "创建一条Amazon Kinesis数据流。建立一条Amazon Data Firehose传输流，将数据发送至OpenSearch Service。配置该传输流将数据备份至S3存储桶。把传输流与数据流相连接。配置数据源使其向数据流发送数据。C (100%)", "enus": "Create one Amazon Kinesis data stream. Create one Amazon Data Firehose delivery stream to send data to OpenSearch Service. Configure the delivery stream to back up the data to the S3 bucket. Connect the delivery stream to the data stream. Configure the data sources to send data to the data stream.  C (100%)"}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to create one Amazon Data Firehose delivery stream to send data to OpenSearch Service, configure it to back up raw data to the S3 bucket, and have data sources send data to the delivery stream. This solution is the most cost - effective because it uses a single delivery stream to achieve both the data analysis (sending to OpenSearch Service) and backup (to S3) tasks. Amazon Data Firehose is a fully managed service that can automatically scale to handle sudden changes in data flow, which meets the serverless requirement.\n\nThe option of creating two Amazon Data Firehose delivery streams to send data to the S3 bucket and OpenSearch Service is less cost - effective as it involves managing and paying for two separate delivery streams when the same can be achieved with one.\n\nThe options that involve creating an Amazon Kinesis data stream add an extra layer of complexity and cost. Kinesis data streams require more management and resources compared to directly using Data Firehose. The company's goal can be met without the need for the additional Kinesis data stream, so these options are not the most cost - effective.\n\nCommon misconceptions might lead one to choose the fake options. For example, thinking that having separate delivery streams for backup and analysis is more reliable or that using Kinesis data streams is always necessary for data ingestion. However, in this case, these additional components are unnecessary and increase costs.", "zhcn": "针对该问题的正确答案是：创建一条亚马逊数据火线（Amazon Data Firehose）传输流，将数据发送至OpenSearch服务，同时配置其将原始数据备份至S3存储桶，并由数据源直接向该传输流发送数据。此方案最具成本效益，因为仅通过单条传输流即可同时实现数据分析（发送至OpenSearch服务）与备份（至S3存储桶）两大功能。亚马逊数据火线作为全托管服务，可自动扩展以应对数据流量的突发波动，完美契合无服务器架构要求。\n\n若创建两条独立数据火线传输流分别处理S3存储桶和OpenSearch服务的数据传输，则需承担双倍资源管理成本，其经济效益显然不及单流方案。而涉及创建亚马逊Kinesis数据流的选项不仅增加了架构复杂度，还会带来更高成本——相比直接使用数据火线，Kinesis数据流需投入更多管理精力与资源。鉴于企业目标无需Kinesis数据流即可实现，此类方案并非最优成本选择。\n\n常见误区可能导致选择错误选项，例如误认为备份与分析采用独立传输流更可靠，或认定数据摄取必须依赖Kinesis数据流。但本场景中，这些附加组件实非必要，只会徒增成本。"}, "answer": "C"}, {"id": "335", "question": {"enus": "A bank has collected customer data for 10 years in CSV format. The bank stores the data in an on-premises server. A data science team wants to use Amazon SageMaker to build and train a machine learning (ML) model to predict churn probability. The team will use the historical data. The data scientists want to perform data transformations quickly and to generate data insights before the team builds a model for production. Which solution will meet these requirements with the LEAST development effort? ", "zhcn": "一家银行以CSV格式积累了长达十年的客户数据，这些数据存储于本地服务器。数据科学团队计划利用Amazon SageMaker构建并训练机器学习模型，用于预测客户流失概率。团队将基于历史数据开展工作，希望在构建生产模型前快速完成数据转换并生成数据洞察。要满足上述需求且开发投入最少，应当采用哪种解决方案？"}, "option": [{"option_text": {"zhcn": "将数据直接上传至SageMaker Data Wrangler控制台，即可在平台内完成数据转换并生成深度分析报告。", "enus": "Upload the data into the SageMaker Data Wrangler console directly. Perform data transformations and generate insights within Data Wrangler."}, "option_flag": false}, {"option_text": {"zhcn": "将数据上传至Amazon S3存储桶，并授权SageMaker访问桶内数据。随后将数据从S3存储桶导入SageMaker Data Wrangler，通过该工具进行数据转换并生成分析洞察。此为最高票选方案。", "enus": "Upload the data into an Amazon S3 bucket. Allow SageMaker to access the data that is in the bucket. Import the data from the S3 bucket into SageMaker Data Wrangler. Perform data transformations and generate insights within Data Wrangler. Most Voted"}, "option_flag": true}, {"option_text": {"zhcn": "将数据直接上传至SageMaker Data Wrangler控制台。授权SageMaker与Amazon QuickSight访问存储于Amazon S3存储桶中的数据。在Data Wrangler中执行数据转换操作，并将处理后的数据保存至另一个S3存储桶。最后通过QuickSight生成数据洞察分析结果。", "enus": "Upload the data into the SageMaker Data Wrangler console directly. Allow SageMaker and Amazon QuickSight to access the data that is in an Amazon S3 bucket. Perform data transformations in Data Wrangler and save the transformed data into a second S3 bucket. Use QuickSight to generate data insights."}, "option_flag": false}, {"option_text": {"zhcn": "将数据上传至Amazon S3存储桶，并授权SageMaker访问桶内数据。将数据从存储桶导入SageMaker Data Wrangler后，在Data Wrangler中进行数据转换处理。完成转换后将数据保存至第二个S3存储桶，最终通过SageMaker Studio笔记本生成数据洞察分析。", "enus": "Upload the data into an Amazon S3 bucket. Allow SageMaker to access the data that is in the bucket. Import the data from the bucket into SageMaker Data Wrangler. Perform data transformations in Data Wrangler. Save the data into a second S3 bucket. Use a SageMaker Studio notebook to generate data insights."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to upload the data into an Amazon S3 bucket, allow SageMaker to access it, import the data into SageMaker Data Wrangler, and perform data transformations and generate insights within Data Wrangler. This option meets the requirements with the least development effort because Amazon S3 is a highly scalable and reliable data storage service that SageMaker integrates well with. SageMaker Data Wrangler has built - in tools for both data transformation and generating insights, enabling the data science team to complete these tasks in a single environment.\n\nThe first fake option of uploading data directly into the SageMaker Data Wrangler console is incorrect. Data Wrangler is not designed to accept direct uploads of large - scale historical data. It is more efficient to store data in S3 first and then import it into Data Wrangler.\nThe second fake option that involves using Amazon QuickSight adds unnecessary complexity. QuickSight is a business intelligence service and using it adds an extra step and tool, increasing development effort when Data Wrangler can already generate insights.\nThe third fake option of saving transformed data into a second S3 bucket and using a SageMaker Studio notebook to generate insights also adds extra steps. It is more straightforward to generate insights directly within Data Wrangler instead of going through the process of first saving the data to another bucket and then using a notebook.", "zhcn": "针对该问题，正确答案是将数据上传至Amazon S3存储桶，授权SageMaker访问权限，随后将数据导入SageMaker Data Wrangler，并直接在该工具内完成数据转换与洞察分析。此方案能以最小开发量满足需求：Amazon S3作为高扩展性、高可靠性的存储服务，与SageMaker无缝集成；而Data Wrangler内置数据转换和洞察分析功能，可使数据科学团队在统一环境中高效完成任务。\n\n首个干扰选项建议直接上传数据至Data Wrangler控制台，这种做法并不正确。该工具本身不支持直接大规模历史数据上传，更高效的方式是先将数据存储于S3再行导入。\n\n第二个干扰方案涉及使用Amazon QuickSight，这会徒增复杂性。作为商业智能工具，QuickSight的引入意味着额外操作步骤和工具切换，而Data Wrangler本身已具备洞察生成能力。\n\n第三个干扰选项主张将转换后数据存入另一个S3存储桶，再通过SageMaker Studio笔记本生成洞察。此流程同样存在冗余：相较于直接通过Data Wrangler生成洞察，先转存数据再调用笔记本的方式显然不够简洁。"}, "answer": "B"}, {"id": "336", "question": {"enus": "A media company wants to deploy a machine learning (ML) model that uses Amazon SageMaker to recommend new articles to the company’s readers. The company's readers are primarily located in a single city. The company notices that the heaviest reader traffic predictably occurs early in the morning, after lunch, and again after work hours. There is very little traffic at other times of day. The media company needs to minimize the time required to deliver recommendations to its readers. The expected amount of data that the API call will return for inference is less than 4 MB. Which solution will meet these requirements in the MOST cost-effective way? ", "zhcn": "一家传媒公司计划部署基于亚马逊SageMaker的机器学习模型，用于向读者推荐新闻资讯。该公司读者主要集中在单一城市，数据显示访问流量高峰呈现规律性分布：清晨、午休后及下班后时段最为密集，其余时段流量显著回落。为确保读者能即时获取推荐内容，公司需最大限度缩短推荐模型的响应延迟。已知API调用返回的推理数据量预计低于4MB。请问下列哪种解决方案能以最具成本效益的方式满足上述需求？"}, "option": [{"option_text": {"zhcn": "自动扩缩容实时推理", "enus": "Real-time inference with auto scaling"}, "option_flag": false}, {"option_text": {"zhcn": "无服务器推理与预置并发\n最高票选\nB（83%）\nA（17%）", "enus": "Serverless inference with provisioned concurrency Most Voted  B (83%)  A (17%)"}, "option_flag": true}, {"option_text": {"zhcn": "异步推理", "enus": "Asynchronous inference"}, "option_flag": false}, {"option_text": {"zhcn": "批量转换任务", "enus": "A batch transform task"}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is ‘Serverless inference with provisioned concurrency’. This solution is the most cost - effective for the media company's needs. Since the reader traffic has predictable peaks and valleys, serverless inference allows the company to pay only for the actual usage of the ML model. Provisioned concurrency ensures that there is enough capacity during peak traffic times, minimizing the time required to deliver recommendations.\n\n‘Real - time inference with auto scaling’ can be more expensive as it may require maintaining a certain level of infrastructure even during low - traffic periods. It also might not handle the quick spikes in traffic as efficiently as serverless with provisioned concurrency. \n\n‘Asynchronous inference’ is not ideal because the company needs to minimize the time to deliver recommendations. Asynchronous inference is better suited for tasks where immediate results are not required. \n\n‘A batch transform task’ is used for processing large amounts of data in batches and is not suitable for real - time or near - real - time article recommendations that the company aims to provide. \n\nThe key factors that distinguish serverless inference with provisioned concurrency are its cost - effectiveness based on usage and its ability to handle traffic spikes quickly, making it the best choice over the fake answer options.", "zhcn": "针对该问题，正确答案应为\"采用预置并发的无服务器推理方案\"。这一方案最能满足该媒体公司的成本效益需求。鉴于读者流量存在可预测的高峰与低谷，无服务器推理使公司仅需为机器学习模型的实际使用量付费。预置并发功能则能确保在流量高峰期间维持充足的服务容量，从而最大限度缩短文章推荐结果的生成时间。\n\n相比之下，\"支持自动扩缩容的实时推理\"方案成本较高，因其在低流量时段仍需维持一定规模的基础设施；且在应对突发流量峰值时，其效率不及预置并发的无服务器方案。\"异步推理\"方案亦非理想选择——由于公司需要最小化推荐延迟，而异步推理更适用于无需即时响应的任务场景。至于\"批量转换任务\"方案，其设计初衷是处理批量数据，并不适用于该公司期望实现的实时或近实时文章推荐场景。\n\n采用预置并发的无服务器推理方案具有两大核心优势：基于实际用量的成本效益优势，以及快速应对流量突增的能力，这使其明显优于其他干扰选项。"}, "answer": "B"}, {"id": "337", "question": {"enus": "A machine learning (ML) engineer is using Amazon SageMaker automatic model tuning (AMT) to optimize a model's hyperparameters. The ML engineer notices that the tuning jobs take a long time to run. The tuning jobs continue even when the jobs are not significantly improving against the objective metric. The ML engineer needs the training jobs to optimize the hyperparameters more quickly. How should the ML engineer configure the SageMaker AMT data types to meet these requirements? ", "zhcn": "一位机器学习工程师正借助Amazon SageMaker自动模型调优功能优化模型超参数。该工程师发现调优任务运行耗时过长，且即使目标指标未出现显著提升时，调优进程仍持续进行。为加速超参数优化进程，该工程师应如何配置SageMaker自动模型调优的数据类型以满足需求？"}, "option": [{"option_text": {"zhcn": "将策略设定为贝叶斯值。", "enus": "Set Strategy to the Bayesian value."}, "option_flag": false}, {"option_text": {"zhcn": "将重试策略设置为1。", "enus": "Set RetryStrategy to a value of 1."}, "option_flag": false}, {"option_text": {"zhcn": "将参数范围设定为基于先前超参数任务所推断出的精确区间。", "enus": "Set ParameterRanges to the narrow range Inferred from previous hyperparameter jobs."}, "option_flag": false}, {"option_text": {"zhcn": "将 TrainingJobEarlyStoppingType 设为 AUTO 值。此为最高票选方案。", "enus": "Set TrainingJobEarlyStoppingType to the AUTO value. Most Voted"}, "option_flag": true}], "analysis": {"enus": "The correct answer to the question is to “Set TrainingJobEarlyStoppingType to the AUTO value”. This setting enables Amazon SageMaker to automatically stop training jobs that show no significant improvement against the objective metric. By doing so, it saves time and resources, allowing the overall hyperparameter tuning process to complete more quickly.\n\nThe option “Set Strategy to the Bayesian value” is a method for hyperparameter optimization, but it doesn't directly address the issue of long - running jobs that aren't improving. Bayesian optimization focuses on building a probabilistic model to select the next set of hyperparameters, not on early termination.\n\n“Set RetryStrategy to a value of 1” is about retrying failed training jobs. It has no relation to speeding up the tuning process when jobs are not improving, as it only deals with job failures.\n\n“Set ParameterRanges to the narrow range Inferred from previous hyperparameter jobs” might limit the search space, but it doesn't solve the problem of jobs continuing to run when they are not making significant progress. It could potentially miss better hyperparameter combinations and doesn't address the early termination need.\n\nIn summary, setting the TrainingJobEarlyStoppingType to AUTO directly tackles the problem of long - running, non - improving jobs, which is why it is the real answer option over the fake ones.", "zhcn": "针对该问题的正确答案是\"将 TrainingJobEarlyStoppingType 设置为 AUTO 值\"。这一设置能让 Amazon SageMaker 自动终止在目标指标上未呈现显著改进的训练任务，从而有效节约时间与计算资源，使整体超参数调优过程加速完成。\n\n至于\"将 Strategy 设置为 Bayesian 值\"这一选项，它属于超参数优化的方法之一，但并未直接解决长时间运行却无进展的任务问题。贝叶斯优化法的核心在于建立概率模型以选择下一组超参数，而非实现提前终止机制。\n\n\"将 RetryStrategy 设为 1\"涉及的是对失败训练任务的重试机制。该设置与加速无进展任务的调优过程无关，因其仅针对任务失败的情况进行处理。\n\n而\"将 ParameterRanges 设置为根据既往超参数任务推断的狭窄范围\"可能缩小搜索空间，但无法解决任务持续运行却无实质进展的问题。这种做法不仅可能遗漏更优的超参数组合，也未能满足提前终止的需求。\n\n综上所述，将 TrainingJobEarlyStoppingType 设为 AUTO 能直接应对长时间运行却无改善的任务问题，这正是该选项优于其他干扰项的根本原因。"}, "answer": "D"}, {"id": "338", "question": {"enus": "A global bank requires a solution to predict whether customers will leave the bank and choose another bank. The bank is using a dataset to train a model to predict customer loss. The training dataset has 1,000 rows. The training dataset includes 100 instances of customers who left the bank. A machine learning (ML) specialist is using Amazon SageMaker Data Wrangler to train a churn prediction model by using a SageMaker training job. After training, the ML specialist notices that the model returns only false results. The ML specialist must correct the model so that it returns more accurate predictions. Which solution will meet these requirements? ", "zhcn": "一家国际银行需要一套解决方案，用于预测客户是否会流失并选择其他银行。该银行正利用某个数据集训练模型以预测客户流失情况，训练数据集包含1000条记录，其中涉及100例已流失客户。一位机器学习专家正在使用Amazon SageMaker Data Wrangler工具，通过SageMaker训练任务来训练客户流失预测模型。训练完成后，该专家发现模型仅返回错误结果。当前必须修正模型以提升预测准确性，请问下列哪种方案能满足需求？"}, "option": [{"option_text": {"zhcn": "在模型训练前，先运用异常检测技术剔除训练数据集中的离群值。", "enus": "Apply anomaly detection to remove outliers from the training dataset before training."}, "option_flag": false}, {"option_text": {"zhcn": "在模型训练前，对训练数据集采用合成少数类过采样技术（SMOTE）。最高票选方案。", "enus": "Apply Synthetic Minority Oversampling Technique (SMOTE) to the training dataset before training. Most Voted"}, "option_flag": true}, {"option_text": {"zhcn": "在训练开始前，需对训练集的特征数据进行归一化处理。", "enus": "Apply normalization to the features of the training dataset before training."}, "option_flag": false}, {"option_text": {"zhcn": "在训练开始前，对训练集进行欠采样处理。", "enus": "Apply undersampling to the training dataset before training."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Apply Synthetic Minority Oversampling Technique (SMOTE) to the training dataset before training”. The problem here is that the training dataset has a significant class imbalance, with only 100 instances of customers who left the bank out of 1,000 rows. This imbalance can cause the model to be biased towards the majority class (customers who stayed), resulting in only false results.\n\nSMOTE helps address this class - imbalance issue by generating synthetic samples of the minority class. This way, the model gets more data to learn from the minority class, improving its ability to predict churn accurately.\n\nThe option “Apply anomaly detection to remove outliers from the training dataset before training” is incorrect because the main problem is class imbalance, not outliers. Removing outliers won't fix the issue of the model being unable to learn from the minority class.\n\n“Apply normalization to the features of the training dataset before training” is wrong. Normalization is used to standardize the range of features, which is useful for some algorithms to converge faster. However, it doesn't solve the class - imbalance problem that is causing the inaccurate predictions.\n\n“Apply undersampling to the training dataset before training” is also incorrect. Undersampling reduces the size of the majority class. But with a small number of minority class instances (only 100 in this case), undersampling may lead to a loss of important information and an even smaller training set, making the model perform worse.\n\nIn summary, the class - imbalance problem is the root cause, and SMOTE is the appropriate solution to address it, distinguishing it from the other fake answer options.", "zhcn": "对于该问题的正确解答是：\"在训练前对训练数据集应用合成少数类过采样技术（SMOTE）\"。此处的问题在于训练数据集存在显著的类别失衡——1000条数据中仅有100个流失客户样本。这种不平衡会导致模型偏向多数类（未流失客户），从而只能产生错误预测。\n\nSMOTE技术通过生成少数类的合成样本，有效缓解类别失衡问题。这使得模型能够从少数类中获得更多学习素材，从而提升准确预测客户流失的能力。\n\n而\"在训练前对训练数据集进行异常值检测以剔除离群点\"这一选项并不恰当，因为核心问题在于类别失衡而非异常值。移除离群点无法解决模型难以从少数类中学习的问题。\n\n\"在训练前对训练数据集特征进行归一化处理\"同样错误。归一化旨在规范特征值范围，虽能加速某些算法的收敛，但无法根治导致预测失准的类别失衡问题。\n\n\"在训练前对训练数据集进行欠采样\"亦不可取。欠采样虽能缩减多数类样本量，但鉴于少数类样本本已稀少（本例中仅100个），此法可能导致重要信息丢失，使训练集进一步缩水，反而加剧模型性能恶化。\n\n综上所述，类别失衡才是问题根源所在。SMOTE作为针对性解决方案，其有效性使其从其他干扰选项中脱颖而出。"}, "answer": "B"}, {"id": "339", "question": {"enus": "A banking company provides financial products to customers around the world. A machine learning (ML) specialist collected transaction data from internal customers. The ML specialist split the dataset into training, testing, and validation datasets. The ML specialist analyzed the training dataset by using Amazon SageMaker Clarify. The analysis found that the training dataset contained fewer examples of customers in the 40 to 55 year-old age group compared to the other age groups. Which type of pretraining bias did the ML specialist observe in the training dataset? ", "zhcn": "一家银行企业为全球客户提供金融产品。一位机器学习专家从内部客户处收集了交易数据，并将数据集划分为训练集、测试集和验证集。该专家运用Amazon SageMaker Clarify工具对训练数据集进行分析，发现与其他年龄段相比，40至55岁年龄组客户的样本数量明显偏少。请问这位机器学习专家在训练数据集中观察到的是哪种预训练偏差？"}, "option": [{"option_text": {"zhcn": "标签比例差异（DPL）", "enus": "Difference in proportions of labels (DPL)"}, "option_flag": false}, {"option_text": {"zhcn": "\"类别不均衡（CI）最高票选\"", "enus": "Class imbalance (CI) Most Voted"}, "option_flag": true}, {"option_text": {"zhcn": "条件性人口差异（CDD）", "enus": "Conditional demographic disparity (CDD)"}, "option_flag": false}, {"option_text": {"zhcn": "柯尔莫哥洛夫-斯米尔诺夫检验", "enus": "Kolmogorov-Smirnov (KS)"}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is ‘Class imbalance (CI)’. Class imbalance occurs when the distribution of classes in a dataset is not uniform. In this case, the training dataset has fewer examples of customers in the 40 - 55 year - old age group compared to other age groups, which is a classic example of class imbalance.\n\n‘Difference in proportions of labels (DPL)’ focuses on the difference in the proportion of positive and negative labels in different groups, which is not described in the given scenario. ‘Conditional demographic disparity (CDD)’ is about the disparity in outcomes based on conditional factors related to demographics, and there is no mention of such conditional disparities in the question. ‘Kolmogorov - Smirnov (KS)’ is a statistical test used to compare two probability distributions, not directly related to the observed issue of unequal representation of an age group in the dataset.\n\nThe key factor distinguishing the real answer from the fake options is that class imbalance directly addresses the problem of unequal numbers of examples in different classes (age groups here), while the other options deal with different concepts in the context of bias analysis. A common misconception might be to choose other options if one is not clear about the specific definitions and applications of these bias - related terms and misinterprets the problem described in the question.", "zhcn": "问题的正确答案是\"类别不平衡（Class Imbalance）\"。当数据集中各类别的分布不均匀时，就会出现类别不平衡现象。本例中，与其他年龄段相比，训练数据集中40-55岁年龄组的客户样本数量明显偏少，这正是类别不平衡的典型表现。\n\n\"标签比例差异（DPL）\"关注的是不同组别中正负标签的比例差异，而题目描述的情境并未涉及这一概念。\"条件人口统计差异（CDD）\"强调基于人口统计条件因素导致的结果差异，但题目中并未提及此类条件差异。\"柯尔莫哥洛夫-斯米尔诺夫检验（KS）\"作为统计检验方法，主要用于比较两个概率分布，与数据集中年龄组代表数量不均的问题并无直接关联。\n\n正确答案与干扰项的核心区别在于：类别不平衡直指不同类别（此处指年龄组）样本数量不均的本质问题，而其他选项涉及的是偏差分析中的不同概念。若未能清晰理解这些偏差术语的具体定义和应用场景，就很容易误解题意而错选其他选项。"}, "answer": "B"}, {"id": "340", "question": {"enus": "A tourism company uses a machine learning (ML) model to make recommendations to customers. The company uses an Amazon SageMaker environment and set hyperparameter tuning completion criteria to MaxNumberOfTrainingJobs. An ML specialist wants to change the hyperparameter tuning completion criteria. The ML specialist wants to stop tuning immediately after an internal algorithm determines that tuning job is unlikely to improve more than 1% over the objective metric from the best training job. Which completion criteria will meet this requirement? ", "zhcn": "一家旅游公司采用机器学习模型为客户提供个性化推荐。该公司基于亚马逊SageMaker平台构建了算法环境，并将超参数调优的终止条件设定为\"最大训练任务数\"。现有一位机器学习专家需要调整该终止条件，希望当系统内部算法判定调优结果相比最佳训练任务的目标指标提升空间不足1%时，立即终止调优流程。下列哪种终止条件符合这一需求？"}, "option": [{"option_text": {"zhcn": "最大运行时长（秒）", "enus": "MaxRuntimeInSeconds"}, "option_flag": false}, {"option_text": {"zhcn": "目标指标数值", "enus": "TargetObjectiveMetricValue"}, "option_flag": false}, {"option_text": {"zhcn": "CompleteOnConvergence 最高票当选", "enus": "CompleteOnConvergence Most Voted"}, "option_flag": true}, {"option_text": {"zhcn": "训练任务数量已达上限但未见改善", "enus": "MaxNumberOfTrainingJobsNotImproving"}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is ‘CompleteOnConvergence’. This is because the ‘CompleteOnConvergence’ criterion stops the hyperparameter tuning job as soon as an internal algorithm determines that the job is unlikely to improve more than a specified percentage (in this case, 1%) over the objective metric from the best training job, which exactly meets the ML specialist's requirement.\n\n‘MaxRuntimeInSeconds’ is used to set a maximum time limit for the tuning job, so it doesn't relate to the improvement of the objective metric. ‘TargetObjectiveMetricValue’ stops the tuning when the objective metric reaches a specific pre - defined value, not based on the improvement percentage. ‘MaxNumberOfTrainingJobsNotImproving’ stops the job after a certain number of non - improving training jobs, rather than using an internal algorithm to assess the likelihood of future improvement. These differences are why ‘CompleteOnConvergence’ is the real answer, distinguishing it from the fake options.", "zhcn": "对于该问题，正确答案应为\"CompleteOnConvergence\"。因为当内部算法判定超参数调优任务的目标指标提升幅度（本例中为1%）已不太可能超越当前最佳训练任务时，\"CompleteOnConvergence\"准则便会立即终止调优进程，这恰好符合机器学习专家的需求。  \n\n而\"MaxRuntimeInSeconds\"用于设定调优任务的最大时长限制，与目标指标的提升幅度无关；\"TargetObjectiveMetricValue\"是在达到预设目标值时终止调优，而非基于百分比改进幅度；\"MaxNumberOfTrainingJobsNotImproving\"则是在连续若干次训练未产生改进后停止，并非通过内部算法评估未来改进可能性。正是这些本质区别，使得\"CompleteOnConvergence\"成为真正符合题意的选项。"}, "answer": "C"}, {"id": "341", "question": {"enus": "A car company has dealership locations in multiple cities. The company uses a machine learning (ML) recommendation system to market cars to its customers. An ML engineer trained the ML recommendation model on a dataset that includes multiple attributes about each car. The dataset includes attributes such as car brand, car type, fuel efficiency, and price. The ML engineer uses Amazon SageMaker Data Wrangler to analyze and visualize data. The ML engineer needs to identify the distribution of car prices for a specific type of car. Which type of visualization should the ML engineer use to meet these requirements? ", "zhcn": "一家汽车公司在多个城市设有经销网点。该公司采用机器学习推荐系统向客户进行汽车营销。一位机器学习工程师基于包含每辆汽车多项属性的数据集，训练了该推荐模型。数据集涵盖品牌、车型、燃油效率及价格等属性。该工程师运用Amazon SageMaker Data Wrangler进行数据分析和可视化，现需针对特定车型分析其价格分布规律。为满足此需求，应采用何种可视化图表类型？"}, "option": [{"option_text": {"zhcn": "利用SageMaker Data Wrangler的散点图可视化功能，可以直观地观察汽车价格与车型之间的关联分布。", "enus": "Use the SageMaker Data Wrangler scatter plot visualization to inspect the relationship between the car price and type of car."}, "option_flag": false}, {"option_text": {"zhcn": "利用SageMaker Data Wrangler的快速模型可视化功能，可迅速评估数据，并为汽车价格与车型生成重要性评分。", "enus": "Use the SageMaker Data Wrangler quick model visualization to quickly evaluate the data and produce importance scores for the car price and type of car."}, "option_flag": false}, {"option_text": {"zhcn": "借助SageMaker Data Wrangler的异常检测可视化功能，可精准定位特定特征中的异常数据点。", "enus": "Use the SageMaker Data Wrangler anomaly detection visualization to Identify outliers for the specific features."}, "option_flag": false}, {"option_text": {"zhcn": "借助SageMaker Data Wrangler的直方图可视化功能，可清晰呈现特定特征的数值分布范围。", "enus": "Use the SageMaker Data Wrangler histogram visualization to inspect the range of values for the specific feature.  Most Voted"}, "option_flag": true}], "analysis": {"enus": "The correct answer to the question is to “Use the SageMaker Data Wrangler histogram visualization to inspect the range of values for the specific feature.” A histogram is ideal for showing the distribution of a single variable, in this case, the car prices for a specific type of car. It groups the data into bins and shows how many data points fall into each bin, clearly depicting the price distribution.\n\nThe “scatter plot visualization” is used to show the relationship between two variables. Here, the goal is to find the distribution of car prices, not the relationship between price and car type, so this option is incorrect. The “quick model visualization” is for evaluating data and getting importance scores, which doesn't help in visualizing the price distribution. The “anomaly detection visualization” is focused on identifying outliers, not on showing the overall distribution of car prices. These fake options are likely to be chosen due to a misunderstanding of the purpose of each visualization type and the question requirements.", "zhcn": "对于本题的正确答案是\"使用SageMaker Data Wrangler的直方图可视化功能来查看特定特征的数值范围\"。直方图最适合展示单一变量的分布情况，在此例中即特定车型的汽车价格分布。它将数据划分为若干区间，并显示每个区间内的数据点数量，从而清晰呈现价格分布态势。</think>\n\n\"散点图可视化\"用于展现两个变量间的关联性。而本题目标是探究汽车价格分布规律，并非分析价格与车型的相互关系，故该选项不正确。\"快速模型可视化\"适用于数据评估和获取特征重要性评分，无助于直观呈现价格分布。\"异常检测可视化\"侧重于识别异常值，而非展示整体价格分布。这些干扰选项容易被误选，往往源于对各类可视化图表用途及题目要求的理解偏差。"}, "answer": "D"}, {"id": "342", "question": {"enus": "A media company is building a computer vision model to analyze images that are on social media. The model consists of CNNs that the company trained by using images that the company stores in Amazon S3. The company used an Amazon SageMaker training job in File mode with a single Amazon EC2 On-Demand Instance. Every day, the company updates the model by using about 10,000 images that the company has collected in the last 24 hours. The company configures training with only one epoch. The company wants to speed up training and lower costs without the need to make any code changes. Which solution will meet these requirements? ", "zhcn": "一家传媒公司正构建计算机视觉模型，用于分析社交媒体上的图像。该模型基于卷积神经网络，其训练数据来自公司存储在Amazon S3中的图像资源。公司目前采用Amazon SageMaker训练任务的文件模式，配合单台按需分配的Amazon EC2实例进行模型训练。每日，公司会使用过去24小时内收集的约一万张新图像更新模型，并将训练周期设定为单次迭代。为在无需修改代码的前提下加速训练过程并降低成本，下列哪项方案能同时满足这两项需求？"}, "option": [{"option_text": {"zhcn": "请将SageMaker训练任务配置为使用管道模式，而非文件模式。通过管道实时读取数据流。", "enus": "Instead of File mode, configure the SageMaker training job to use Pipe mode. Ingest the data from a pipe."}, "option_flag": false}, {"option_text": {"zhcn": "相较于文件模式，建议将SageMaker训练任务配置调整为快速文件模式，无需其他改动。此为最高票推荐方案。", "enus": "Instead of File mode, configure the SageMaker training job to use FastFile mode with no other changes. Most Voted"}, "option_flag": true}, {"option_text": {"zhcn": "“请将SageMaker训练任务配置为使用竞价型实例，而非按需实例。其余设置保持不变。”", "enus": "Instead of On-Demand Instances, configure the SageMaker training job to use Spot Instances. Make no other changes,"}, "option_flag": false}, {"option_text": {"zhcn": "可将SageMaker训练任务配置为使用竞价实例替代按需实例，并启用模型检查点功能。", "enus": "Instead of On-Demand Instances, configure the SageMaker training job to use Spot Instances, implement model checkpoints."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to configure the SageMaker training job to use FastFile mode instead of File mode with no other changes. FastFile mode allows for faster data access from Amazon S3 compared to File mode. It can cache data on the instance, which speeds up the process of reading the images during training, thus reducing the overall training time. And since no code changes are required, it aligns with the company's requirement.\n\nThe option of using Pipe mode involves ingesting data from a pipe. While Pipe mode can be useful for streaming data, it often requires code modifications to handle the streaming nature of the data, so it doesn't meet the no - code - change requirement.\n\nSwitching to Spot Instances can lower costs as they are generally cheaper than On - Demand Instances. However, Spot Instances can be interrupted, which can lead to training failures. Using Spot Instances without implementing model checkpoints (as in the third option) can cause the loss of training progress. And even when implementing model checkpoints (the fourth option), it doesn't directly address the issue of speeding up the training process. \n\nThe key factor here is that the company wants to speed up training and lower costs without code changes. FastFile mode directly addresses the speed issue while maintaining the simplicity of no code changes, which is why it is the real answer option, distinguishing it from the fake options provided.", "zhcn": "针对该问题，正确答案是将SageMaker训练任务配置为使用FastFile模式而非File模式，且无需其他改动。相较于File模式，FastFile模式能够更快地从Amazon S3读取数据。该模式可在实例上缓存数据，从而加速训练过程中的图像读取效率，有效缩短整体训练时长。由于无需修改代码，这一方案完全符合企业提出的零代码改动要求。\n\n而采用Pipe模式的方案需通过数据管道获取数据。虽然该模式适用于流式数据处理，但通常需要修改代码以适应数据流特性，因此无法满足零代码改动的前提。\n\n若改用Spot实例虽能降低成本（其价格通常低于按需实例），但此类实例可能被中断导致训练失败。若未配合模型检查点机制（如第三选项），训练进度可能丢失；即便启用检查点功能（如第四选项），仍无法直接解决加速训练进程的核心诉求。关键在于企业需要在零代码改动的前提下实现训练加速与成本优化。FastFile模式在保持零代码改动简洁性的同时，直指训练效率提升的痛点，这正是其区别于其他干扰选项的核心优势。"}, "answer": "B"}, {"id": "343", "question": {"enus": "A telecommunications company has deployed a machine learning model using Amazon SageMaker. The model identifies customers who are likely to cancel their contract when calling customer service. These customers are then directed to a specialist service team. The model has been trained on historical data from multiple years relating to customer contracts and customer service interactions in a single geographic region. The company is planning to launch a new global product that will use this model. Management is concerned that the model might incorrectly direct a large number of calls from customers in regions without historical data to the specialist service team. Which approach would MOST effectively address this issue? ", "zhcn": "一家电信公司运用亚马逊SageMaker平台部署了机器学习模型。该模型能识别出那些在致电客服时可能解约的客户，并将其转接至专家服务团队。此模型基于单一地理区域内多年积累的客户合同及客服互动历史数据训练而成。公司计划推出一项采用该模型的全球新产品，但管理层担忧模型可能误将来自缺乏历史数据地区的客户来电大量转接至专家团队。下列哪种方法能最高效地解决此问题？"}, "option": [{"option_text": {"zhcn": "启用模型端点的Amazon SageMaker模型监控数据捕获功能。基于训练数据集创建监控基线。设定定期监控任务。当区域客户数据的数值分布未通过基线漂移检验时，通过Amazon CloudWatch向数据科学家发送告警。利用更广泛的数据源重新评估训练集并优化模型。最高票选方案", "enus": "Enable Amazon SageMaker Model Monitor data capture on the model endpoint. Create a monitoring baseline on the training dataset. Schedule monitoring jobs. Use Amazon CloudWatch to alert the data scientists when the numerical distance of regional customer data fails the baseline drift check. Reevaluate the training set with the larger data source and retrain the model. Most Voted"}, "option_flag": true}, {"option_text": {"zhcn": "在模型端点上启用Amazon SageMaker Debugger功能。创建自定义规则以衡量与基准训练数据集的偏差程度。通过Amazon CloudWatch在规则触发时向数据科学家发送告警通知。利用更庞大的数据源重新评估训练集，并对模型进行迭代训练。", "enus": "Enable Amazon SageMaker Debugger on the model endpoint. Create a custom rule to measure the variance from the baseline training dataset. Use Amazon CloudWatch to alert the data scientists when the rule is invoked. Reevaluate the training set with the larger data source and retrain the model."}, "option_flag": false}, {"option_text": {"zhcn": "将转接至专家服务团队的所有客户通话录音存档于Amazon S3中。设定定时监控任务，抓取全部真阳性与真阴性判定结果，将其与训练数据集进行关联比对并计算准确率。当准确率出现下降时，通过Amazon CloudWatch向数据科学家发送预警。结合专家服务团队提供的增量数据重新评估训练集，并对模型进行迭代训练。", "enus": "Capture all customer calls routed to the specialist service team in Amazon S3. Schedule a monitoring job to capture all the true positives and true negatives, correlate them to the training dataset, and calculate the accuracy. Use Amazon CloudWatch to alert the data scientists when the accuracy decreases. Reevaluate the training set with the additional data from the specialist service team and retrain the model."}, "option_flag": false}, {"option_text": {"zhcn": "在模型端点上启用Amazon CloudWatch监控服务。通过Amazon CloudWatch日志捕获指标数据并传输至Amazon S3存储。将监测结果与训练数据基线进行比对分析，若发现偏离幅度超过区域客户差异阈值，则需重新评估训练集并优化模型。", "enus": "Enable Amazon CloudWatch on the model endpoint. Capture metrics using Amazon CloudWatch Logs and send them to Amazon S3. Analyze the monitored results against the training data baseline. When the variance from the baseline exceeds the regional customer variance, reevaluate the training set and retrain the model."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to enable Amazon SageMaker Model Monitor data capture on the model endpoint. This approach is most effective because it directly focuses on detecting data drift, which is the core issue in this scenario. By creating a monitoring baseline on the training dataset and scheduling monitoring jobs, it can closely track if the regional customer data starts to deviate from the original training data. Using Amazon CloudWatch to alert data - scientists when the numerical distance of regional customer data fails the baseline drift check allows for timely intervention. Then, reevaluating the training set with a larger data source and retraining the model can help the model adapt to new data from different regions.\n\nThe option of enabling Amazon SageMaker Debugger is incorrect. SageMaker Debugger is mainly used for debugging training jobs to identify issues like overfitting during the training process, not for detecting data drift in the deployed model, so it doesn't address the problem at hand.\n\nThe option of capturing all customer calls routed to the specialist service team in Amazon S3 and calculating accuracy is less effective. This method focuses on the results of the model's output rather than detecting data drift early. It may lead to a large number of incorrect calls being routed to the specialist team before the problem is detected.\n\nThe option of enabling Amazon CloudWatch on the model endpoint and capturing metrics has a similar problem. It doesn't specifically target data drift detection like SageMaker Model Monitor does. Instead, it tries to analyze variance in a more general way, which may not be as precise in identifying the root cause of incorrect call routing due to lack of regional historical data.", "zhcn": "针对该问题的正确答案是：在模型端点上启用Amazon SageMaker Model Monitor的数据捕获功能。此方案最为有效，因为它直接聚焦于检测数据漂移——这正是本场景中的核心问题。通过在训练数据集上建立监控基线并安排监控任务，可密切跟踪区域客户数据是否开始偏离原始训练数据。利用Amazon CloudWatch在区域客户数据的数值距离未通过基线漂移检查时向数据科学家发出警报，能够实现及时干预。随后，通过更广泛的数据源重新评估训练集并重新训练模型，有助于模型适应来自不同区域的新数据。\n\n启用Amazon SageMaker Debugger的方案并不正确。SageMaker Debugger主要用于调试训练任务，以识别训练过程中的过拟合等问题，而非用于检测已部署模型的数据漂移，因此无法解决当前问题。\n\n捕获所有路由至专家服务团队的客户呼叫并存入Amazon S3再计算准确率的方案效果较差。该方法侧重于模型输出的结果而非早期数据漂移检测，可能导致在发现问题前已有大量错误呼叫被转接至专家团队。\n\n在模型端点上启用Amazon CloudWatch并捕获指标的方案存在类似缺陷。它未能像SageMaker Model Monitor那样专门针对数据漂移进行检测，而是试图以更通用的方式分析方差，由于缺乏区域历史数据，可能难以精准定位错误呼叫路由的根本原因。"}, "answer": "A"}, {"id": "344", "question": {"enus": "A machine learning (ML) engineer is creating a binary classification model. The ML engineer will use the model in a highly sensitive environment. There is no cost associated with missing a positive label. However, the cost of making a false positive inference is extremely high. What is the most important metric to optimize the model for in this scenario? ", "zhcn": "机器学习工程师正在构建一个用于高敏感场景的二元分类模型。该场景下漏报阳性标签不会产生代价，但误判为阳性的代价极其高昂。在此情况下，优化模型时应优先考量哪个关键指标？"}, "option": [{"option_text": {"zhcn": "精准", "enus": "Accuracy"}, "option_flag": false}, {"option_text": {"zhcn": "“精准之选”", "enus": "Precision Most Voted"}, "option_flag": true}, {"option_text": {"zhcn": "忆起", "enus": "Recall"}, "option_flag": false}, {"option_text": {"zhcn": "一级方程式赛车", "enus": "F1"}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is ‘Precision Most Voted’. In a binary - classification model for a highly sensitive environment where there's no cost for missing a positive label but a very high cost for false positives, precision is the most crucial metric. Precision measures the proportion of correctly predicted positive instances out of all instances predicted as positive. By optimizing for precision, the model aims to minimize false positives, which aligns with the high - cost nature of such errors in this scenario.\n\n‘Accuracy’ is the ratio of correct predictions (both true positives and true negatives) to the total number of predictions. It doesn't specifically address the high cost of false positives, as it treats all types of errors equally. ‘Recall’ measures the proportion of actual positive instances that are correctly predicted as positive. Since there's no cost for missing a positive label, optimizing recall isn't a priority here. The ‘F1’ score is the harmonic mean of precision and recall, trying to balance both. But as the focus is on reducing false positives, precision alone is more important than this balanced metric. So, precision is the real answer, distinguishing it from the other fake options.", "zhcn": "对于该问题的正确答案是“精确度（Precision）”。在高度敏感场景下的二分类模型中，若漏报正例标签无代价而误报正例代价极高时，精确度便成为最关键的评价指标。精确度衡量的是在所有被预测为正类的样本中，真正属于正类的比例。通过优化精确度，模型旨在最大限度减少误报情况，这正符合此类场景中误报所导致的高代价特性。\n\n“准确率（Accuracy）”表示正确预测（包括真阳性与真阴性）占总预测数量的比例。由于该指标对所有错误类型一视同仁，无法针对性反映误报的高代价问题。“召回率（Recall）”衡量实际正例中被正确预测为正例的比例。鉴于漏报正例无需承担代价，优化召回率在此并非优先考量。“F1分数”作为精确度与召回率的调和平均数，试图平衡二者权重。但由于当前核心目标是降低误报，单独的精确度指标比这种平衡性指标更具实际意义。因此，精确度才是真正答案，与其他干扰选项存在本质区别。"}, "answer": "B"}, {"id": "345", "question": {"enus": "An ecommerce company discovers that the search tool for the company's website is not presenting the top search results to customers. The company needs to resolve the issue so the search tool will present results that customers are most likely to want to purchase. Which solution will meet this requirement with the LEAST operational effort? ", "zhcn": "一家电商企业发现，其网站搜索工具未能向客户展示最相关的搜索结果。该公司需要解决此问题，以确保搜索工具能呈现客户最可能有意向购买的商品。在满足这一需求的前提下，何种解决方案所需的运营投入最低？"}, "option": [{"option_text": {"zhcn": "运用Amazon SageMaker BlazingText算法，通过查询扩展技术为搜索结果增添语境信息。", "enus": "Use the Amazon SageMaker BlazingText algorithm to add context to search results through query expansion."}, "option_flag": false}, {"option_text": {"zhcn": "利用亚马逊SageMaker平台的XGBoost算法优化候选项目排序效果。", "enus": "Use the Amazon SageMaker XGBoost algorithm to improve candidate ranking."}, "option_flag": false}, {"option_text": {"zhcn": "采用亚马逊云搜索服务，并按搜索相关度得分对结果进行排序。最多赞同", "enus": "Use Amazon CloudSearch and sort results by the search relevance score. Most Voted"}, "option_flag": true}, {"option_text": {"zhcn": "利用亚马逊云搜索服务，并按照地理位置对结果进行排序。", "enus": "Use Amazon CloudSearch and sort results by the geographic location."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is ‘Use Amazon CloudSearch and sort results by the search relevance score’. This solution meets the requirement with the least operational effort as Amazon CloudSearch is a fully - managed service, and sorting by search relevance score directly addresses presenting the most relevant (and likely to be purchased) results.\n\nThe option ‘Use the Amazon SageMaker BlazingText algorithm to add context to search results through query expansion’ and ‘Use the Amazon SageMaker XGBoost algorithm to improve candidate ranking’ involve using Amazon SageMaker. SageMaker requires more setup, model training, and tuning, which demand significant operational effort.\n\nSorting results by geographic location in ‘Use Amazon CloudSearch and sort results by the geographic location’ does not ensure that the presented results are what customers are most likely to want to purchase. It focuses on location rather than search relevance. The simplicity and direct relevance - based approach of using Amazon CloudSearch with search relevance score sorting make it the best choice, distinguishing it from the more complex and less relevant fake options.", "zhcn": "对于该问题的正确答案是“使用Amazon CloudSearch并按搜索相关度得分排序”。这一方案以最小的运维投入满足了需求，因为Amazon CloudSearch是全托管服务，而按搜索相关度排序能直接确保呈现最相关（且最可能被购买）的结果。  \n“使用Amazon SageMaker BlazingText算法通过查询扩展为搜索结果添加上下文”及“使用Amazon SageMaker XGBoost算法改进候选排序”这两个选项均涉及Amazon SageMaker的使用。SageMaker需要进行更多设置、模型训练和调优，运维成本显著更高。  \n“使用Amazon CloudSearch并按地理位置排序”方案中，按地理位置排序无法保证呈现客户最可能购买的商品，其侧重点在于地理位置而非搜索相关度。  \n综合来看，采用Amazon CloudSearch并基于搜索相关度排序的方案兼具简洁性与直接相关性，使其从众多复杂且关联性较低的错误选项中脱颖而出，成为最佳选择。"}, "answer": "C"}, {"id": "346", "question": {"enus": "A machine learning (ML) specialist collected daily product usage data for a group of customers. The ML specialist appended customer metadata such as age and gender from an external data source. The ML specialist wants to understand product usage patterns for each day of the week for customers in specific age groups. The ML specialist creates two categorical features named dayofweek and binned_age, respectively. Which approach should the ML specialist use discover the relationship between the two new categorical features? ", "zhcn": "一位机器学习专家收集了一组客户的日常产品使用数据，并从外部数据源补充了客户的年龄、性别等元数据。为探究特定年龄段客户在一周内各天的产品使用规律，该专家创建了名为\"dayofweek\"（星期几）和\"binned_age\"（分段年龄）的两个分类特征。此时应采用何种分析方法来揭示这两个分类特征之间的关联性？"}, "option": [{"option_text": {"zhcn": "请绘制一张关于星期几与年龄段分布的散点图。", "enus": "Create a scatterplot for day_of_week and binned_age."}, "option_flag": false}, {"option_text": {"zhcn": "为\"day_of_week\"与\"binned_age\"创建交叉分析表。最多票选", "enus": "Create crosstabs for day_of_week and binned_age. Most Voted"}, "option_flag": true}, {"option_text": {"zhcn": "为“星期几”和“年龄分段”生成文字云图。", "enus": "Create word clouds for day_of_week and binned_age."}, "option_flag": false}, {"option_text": {"zhcn": "为\"星期几\"与\"年龄分段\"绘制箱线图。", "enus": "Create a boxplot for day_of_week and binned_age."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Create crosstabs for day_of_week and binned_age”. Crosstabs, or contingency tables, are ideal for analyzing the relationship between two categorical variables. They display the frequency distribution of the variables, allowing the ML specialist to see how the different age - groups are distributed across each day of the week, which directly helps in understanding product usage patterns for specific age groups on different days.\n\nA scatterplot is used to show the relationship between two numerical variables, not categorical ones like day_of_week and binned_age. So, creating a scatterplot is inappropriate as it won't effectively display the relationship between these categorical features.\n\nWord clouds are mainly used for visualizing text data, highlighting the frequency of words. Since day_of_week and binned_age are categorical variables, not text data, word clouds are not a suitable approach for discovering their relationship.\n\nA boxplot is used to show the distribution of numerical data, including measures like median, quartiles, and outliers. As day_of_week and binned_age are categorical, a boxplot cannot be used to analyze their relationship.\n\nIn summary, crosstabs are the right choice because they are designed to analyze the relationship between categorical variables, which is what the ML specialist needs to understand the product usage patterns in this case.", "zhcn": "该问题的正确答案是\"为星期几与年龄分组创建交叉表\"。交叉表（或称列联表）是分析两个分类变量关系的理想工具。它能清晰呈现变量的频数分布，使机器学习专家直观掌握不同年龄段在一周各天的分布情况，从而精准把握特定年龄群体在不同日期的产品使用规律。\n\n散点图适用于展现两个数值变量之间的关系，而星期几和年龄分组属于分类变量，因此散点图无法有效呈现这两个分类特征的内在关联。\n\n词云图主要用于文本数据可视化，通过字体大小突出词汇频次。鉴于当前变量为分类数据而非文本数据，此方法并不适用。\n\n箱形图用于展示数值数据的分布特征（如中位数、四分位点及异常值）。由于两个变量均为分类数据，箱形图无法进行有效分析。\n\n综上，交叉表能专业处理分类变量间的关联分析，正是机器学习专家本案中探索产品使用模式所需的合适工具。"}, "answer": "B"}, {"id": "347", "question": {"enus": "A company needs to develop a model that uses a machine learning (ML) model for risk analysis. An ML engineer needs to evaluate the contribution each feature of a training dataset makes to the prediction of the target variable before the ML engineer selects features. How should the ML engineer predict the contribution of each feature? ", "zhcn": "一家公司需要开发一个利用机器学习模型进行风险分析的解决方案。在筛选特征变量之前，机器学习工程师需先评估训练数据集中每个特征对目标变量预测的贡献度。请问工程师应当如何科学预测各特征的贡献程度？"}, "option": [{"option_text": {"zhcn": "利用Amazon SageMaker Data Wrangler的多重共线性检测功能，结合主成分分析（PCA）算法，可计算数据集在特征空间多维方向上的方差分布。", "enus": "Use the Amazon SageMaker Data Wrangler multicollinearity measurement features and the principal component analysis (PCA) algorithm to calculate the variance of the dataset along multiple directions in the feature space."}, "option_flag": false}, {"option_text": {"zhcn": "运用亚马逊SageMaker数据整理器的快速模型可视化功能，筛选出特征重要性评分介于0.5至1之间的结果。此为最高票选方案。", "enus": "Use an Amazon SageMaker Data Wrangler quick model visualization to find feature importance scores that are between 0.5 and 1. Most Voted"}, "option_flag": true}, {"option_text": {"zhcn": "利用Amazon SageMaker Data Wrangler的偏差报告，可识别特征工程相关数据中可能存在的潜在偏差。", "enus": "Use the Amazon SageMaker Data Wrangler bias report to identify potential biases in the data related to feature engineering."}, "option_flag": false}, {"option_text": {"zhcn": "利用Amazon SageMaker Data Wrangler数据流构建并优化数据预处理流程，同时手动添加特征评分。", "enus": "Use an Amazon SageMaker Data Wrangler data flow to create and modify a data preparation pipeline. Manually add the feature scores."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question ‘A company needs to develop a model that uses a machine learning (ML) model for risk analysis. An ML engineer needs to evaluate the contribution each feature of a training dataset makes to the prediction of the target variable before the ML engineer selects features. How should the ML engineer predict the contribution of each feature?’ is ‘Use an Amazon SageMaker Data Wrangler quick model visualization to find feature importance scores that are between 0.5 and 1’. This is because feature importance scores directly indicate the contribution of each feature to the prediction of the target variable, which is exactly what the ML engineer needs to evaluate.\n\nThe fake option ‘Use the Amazon SageMaker Data Wrangler multicollinearity measurement features and the principal component analysis (PCA) algorithm to calculate the variance of the dataset along multiple directions in the feature space’ focuses on measuring multicollinearity and variance, not on the contribution of individual features to the target variable. The option ‘Use the Amazon SageMaker Data Wrangler bias report to identify potential biases in the data related to feature engineering’ is about detecting biases, not feature contribution. And the option ‘Use an Amazon SageMaker Data Wrangler data flow to create and modify a data preparation pipeline. Manually add the feature scores’ is more about data preparation and a non - automated way of adding scores rather than evaluating the actual contribution of features. The key factor here is that the real answer directly addresses the need to evaluate feature contribution, which sets it apart from the fake options.", "zhcn": "针对“某公司需开发一个采用机器学习模型进行风险分析的解决方案。在特征选择前，机器学习工程师需要评估训练数据集中每个特征对目标变量预测的贡献度，请问该如何实现？”这一问题，正确答案是“使用Amazon SageMaker Data Wrangler的快速模型可视化功能，获取介于0.5到1之间的特征重要性评分”。因为特征重要性评分能直接反映每个特征对目标变量预测的贡献程度，完美契合工程师的评估需求。\n\n而干扰项“使用Amazon SageMaker Data Wrangler的多重共线性检测功能与主成分分析算法计算特征空间多方向方差”侧重于多重共线性和方差度量，与特征个体贡献度无关；“使用Amazon SageMaker Data Wrangler偏差报告识别特征工程相关数据偏差”关注的是偏差检测而非特征贡献；至于“使用Amazon SageMaker Data Wrangler数据流构建并修改数据准备流程，手动添加特征评分”则聚焦数据准备工作，且采用非自动化评分方式，未能真正评估特征贡献。核心区别在于，正确答案直指特征贡献度评估这一核心需求，从而与其他选项形成鲜明对比。"}, "answer": "B"}, {"id": "348", "question": {"enus": "A company is building a predictive maintenance system using real-time data from devices on remote sites. There is no AWS Direct Connect connection or VPN connection between the sites and the company's VPC. The data needs to be ingested in real time from the devices into Amazon S3. Transformation is needed to convert the raw data into clean .csv data to be fed into the machine learning (ML) model. The transformation needs to happen during the ingestion process. When transformation fails, the records need to be stored in a specific location in Amazon S3 for human review. The raw data before transformation also needs to be stored in Amazon S3. How should an ML specialist architect the solution to meet these requirements with the LEAST effort? ", "zhcn": "某公司正基于远程站点设备采集的实时数据构建预测性维护系统。站点与公司虚拟私有云（VPC）之间未配置AWS Direct Connect专线或VPN连接。需将设备生成的原始数据实时摄取至Amazon S3存储服务，并在数据注入过程中完成格式转换，将其处理为可供机器学习模型使用的规整CSV格式。若转换失败，相关记录需存储至Amazon S3的指定路径供人工核查，且转换前的原始数据也需保留在Amazon S3中。机器学习架构师应如何以最小工作量设计满足上述需求的解决方案？"}, "option": [{"option_text": {"zhcn": "将Amazon Data Firehose与Amazon S3搭配使用，并以后者作为数据目的地。配置Firehose调用AWS Lambda函数实现数据格式转换，同时启用Firehose的源记录备份功能。", "enus": "Use Amazon Data Firehose with Amazon S3 as the destination. Configure Firehose to invoke an AWS Lambda function for data transformation. Enable source record backup on Firehose."}, "option_flag": true}, {"option_text": {"zhcn": "采用Amazon Managed Streaming for Apache Kafka（全托管式Apache Kafka服务），在Amazon Elastic Container Service（亚马逊弹性容器服务，简称Amazon ECS）中部署工作节点，将数据从Kafka代理实时传输至Amazon S3存储服务，并在此过程中完成数据格式转换。需配置工作节点，将原始数据与转换失败的数据分别存储至不同的S3存储桶中。", "enus": "Use Amazon Managed Streaming for Apache Kafka. Set up workers in Amazon Elastic Container Service (Amazon ECS) to move data from Kafka brokers to Amazon S3 while transforming it. Configure workers to store raw and unsuccessfully transformed data in different S3 buckets."}, "option_flag": false}, {"option_text": {"zhcn": "以Amazon S3为目标端配置Amazon Data Firehose服务，设定Firehose调用AWS Glue中的Apache Spark作业进行数据转换。启用源数据记录备份功能并配置错误日志存储路径。此为最高票选方案。", "enus": "Use Amazon Data Firehose with Amazon S3 as the destination. Configure Firehose to invoke an Apache Spark job in AWS Glue for data transformation. Enable source record backup and configure the error prefix. Most Voted"}, "option_flag": false}, {"option_text": {"zhcn": "在Amazon Data Firehose前接入Amazon Kinesis数据流，通过Kinesis数据流与AWS Lambda的协同运作，将原始数据存储至Amazon S3。同时配置Firehose服务，使其调用Lambda函数进行数据转换处理，并以Amazon S3作为最终存储目的地。", "enus": "Use Amazon Kinesis Data Streams in front of Amazon Data Firehose. Use Kinesis Data Streams with AWS Lambda to store raw data in Amazon S3. Configure Firehose to invoke a Lambda function for data transformation with Amazon S3 as the destination."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question about architecting a predictive maintenance system is to “Use Amazon Data Firehose with Amazon S3 as the destination. Configure Firehose to invoke an AWS Lambda function for data transformation. Enable source record backup on Firehose.” This option requires the least effort as Amazon Data Firehose is a fully - managed service that can directly ingest real - time data into Amazon S3. AWS Lambda is a serverless compute service, which is easy to set up and integrate with Firehose for data transformation. Enabling source record backup on Firehose takes care of storing the raw data.\n\nThe option of using “Amazon Managed Streaming for Apache Kafka” is more complex. It requires setting up and managing Kafka brokers, as well as configuring workers in Amazon ECS. This adds a significant amount of infrastructure management effort compared to using a fully - managed service like Data Firehose.\n\nThe option of using “Amazon Data Firehose with an Apache Spark job in AWS Glue for data transformation” is more resource - intensive. Apache Spark jobs in AWS Glue are better suited for large - scale batch processing rather than real - time transformation during ingestion. It also adds unnecessary complexity compared to using a simple Lambda function.\n\nThe option of using “Amazon Kinesis Data Streams in front of Amazon Data Firehose” adds an extra layer of infrastructure. Kinesis Data Streams is mainly used for building custom real - time streaming data applications, and here it is an over - complication as Data Firehose can handle the ingestion directly.\n\nOverall, the real answer option is the simplest and requires the least effort in setting up and managing the system to meet the given requirements.", "zhcn": "关于构建预测性维护系统的正确答案是“采用Amazon Data Firehose服务并以Amazon S3为目标存储。配置Firehose调用AWS Lambda函数进行数据转换，同时开启原始记录备份功能”。此方案实施难度最低——Amazon Data Firehose作为全托管服务可直接将实时数据摄入S3，而无服务器计算的AWS Lambda既能便捷集成Firehose实施数据转换，又无需管理基础设施。启用源数据备份后，原始数据存储需求也得以自动满足。\n\n相比之下，“采用Amazon Managed Streaming for Apache Kafka”的方案更为复杂，需部署管理Kafka代理集群并配置Amazon ECS中的工作节点，其基础设施管理负担远超过全托管的Data Firehose方案。\n\n若选择“通过AWS Glue中的Apache Spark作业实现Data Firehose数据转换”，则需投入更多资源。AWS Glue的Spark作业更适用于大规模批处理场景，而非实时数据摄入阶段的转换操作，其复杂度也远超轻量级的Lambda函数方案。\n\n至于“在Data Firehose前增设Amazon Kinesis Data Streams”的方案，实属过度设计。Kinesis Data Streams通常用于构建定制化实时流数据处理应用，而本案中Data Firehose已能独立完成数据接入任务。\n\n综上，正确答案所提方案在满足需求的前提下，实现了系统搭建与管理的最小化投入。"}, "answer": "A"}, {"id": "349", "question": {"enus": "A company wants to use machine learning (ML) to improve its customer churn prediction model. The company stores data in an Amazon Redshift data warehouse. A data science team wants to use Amazon Redshift machine learning (Amazon Redshift ML) to build a model and run predictions for new data directly within the data warehouse. Which combination of steps should the company take to use Amazon Redshift ML to meet these requirements? (Choose three.) ", "zhcn": "某公司计划运用机器学习技术优化其客户流失预测模型。该企业将数据存储于Amazon Redshift数据仓库中，数据科学团队希望借助Amazon Redshift机器学习功能，直接在数据仓库内构建模型并对新数据执行预测。为实现这一目标，该公司应采取以下哪三项组合步骤？（请选择三项）"}, "option": [{"option_text": {"zhcn": "为构建客户流失预测模型，需明确特征变量与目标变量。", "enus": "Define the feature variables and target variable for the churn prediction model. Most Voted"}, "option_flag": true}, {"option_text": {"zhcn": "运用SOL EXPLAIN_MODEL函数执行预测分析。", "enus": "Use the SOL EXPLAIN_MODEL function to run predictions."}, "option_flag": false}, {"option_text": {"zhcn": "编写一条创建模型的CREATE MODEL SQL语句。最高票选方案", "enus": "Write a CREATE MODEL SQL statement to create a model. Most Voted"}, "option_flag": true}, {"option_text": {"zhcn": "利用Amazon Redshift Spectrum对模型进行训练。", "enus": "Use Amazon Redshift Spectrum to train the model."}, "option_flag": false}, {"option_text": {"zhcn": "请将训练数据手动导出至Amazon S3。", "enus": "Manually export the training data to Amazon S3."}, "option_flag": false}, {"option_text": {"zhcn": "运用SQL预测函数执行数据推演，采纳最高票选结果。", "enus": "Use the SQL prediction function to run predictions. Most Voted"}, "option_flag": true}], "analysis": {"enus": "The correct steps for the company to use Amazon Redshift ML for customer churn prediction are to define the feature and target variables, write a CREATE MODEL SQL statement, and use the SQL prediction function.\n\nDefining the feature and target variables is fundamental as it determines what data will be used to train the model and what the model is trying to predict. Writing a CREATE MODEL SQL statement is how the model is actually created within Amazon Redshift. Using the SQL prediction function allows the company to utilize the trained model to make predictions on new data directly in the data warehouse.\n\nThe fake answer “Use the SOL EXPLAIN_MODEL function to run predictions” is incorrect because the EXPLAIN_MODEL function is used to understand the model's characteristics, not to run predictions. “Use Amazon Redshift Spectrum to train the model” is wrong as Redshift Spectrum is for querying external data, not for model training. “Manually export the training data to Amazon S3” is unnecessary since Amazon Redshift ML can work directly with data in the Redshift data warehouse, and it goes against the goal of running the process directly within the data warehouse. These common misconceptions might lead one to choose the fake options if they misunderstand the functions of different Amazon Redshift components and the capabilities of Amazon Redshift ML.", "zhcn": "企业运用Amazon Redshift ML预测客户流失的正确流程包括：定义特征变量与目标变量、编写CREATE MODEL SQL语句，以及使用SQL预测函数。定义特征变量与目标变量是基础环节，它决定了模型训练所采用的数据维度及预测目标。通过CREATE MODEL SQL语句，可在Amazon Redshift环境中直接创建机器学习模型。而SQL预测函数则能让企业借助训练完成的模型，在数据仓库内直接对新数据进行预测分析。\n\n错误答案中提到的\"使用SOL EXPLAIN_MODEL函数执行预测\"实属谬误，因为EXPLAIN_MODEL功能旨在解析模型特征而非执行预测；\"通过Amazon Redshift Spectrum训练模型\"同样不正确，该服务专用于外部数据查询而非模型训练；至于\"将训练数据手动导出至Amazon S3\"的做法不仅多余——毕竟Redshift ML可直接处理数据仓库内的数据，更违背了在数据仓库内直接完成全流程的设计初衷。若对Amazon Redshift各组件功能及Redshift ML能力存在误解，便容易陷入这些常见认知误区而选择错误方案。"}, "answer": "ACF"}, {"id": "350", "question": {"enus": "A company’s machine learning (ML) team needs to build a system that can detect whether people in a collection of images are wearing the company’s logo. The company has a set of labeled training data. Which algorithm should the ML team use to meet this requirement? ", "zhcn": "某公司的机器学习团队需构建一套系统，用于检测图集中的人物是否佩戴公司标识。目前企业已具备标注完成的训练数据集。为达成此目标，该团队应采用何种算法更为适宜？"}, "option": [{"option_text": {"zhcn": "主成分分析（PCA）", "enus": "Principal component analysis (PCA)"}, "option_flag": false}, {"option_text": {"zhcn": "循环神经网络（RNN）", "enus": "Recurrent neural network (RNN)"}, "option_flag": false}, {"option_text": {"zhcn": "K-近邻算法（k-NN）", "enus": "К-nearest neighbors (k-NN)"}, "option_flag": false}, {"option_text": {"zhcn": "卷积神经网络（CNN） 高票精选", "enus": "Convolutional neural network (CNN) Most Voted"}, "option_flag": true}], "analysis": {"enus": "The correct answer to the question ‘A company’s machine learning (ML) team needs to build a system that can detect whether people in a collection of images are wearing the company’s logo. The company has a set of labeled training data. Which algorithm should the ML team use to meet this requirement?’ is ‘Convolutional neural network (CNN)’. This is because CNNs are specifically designed for image - related tasks. They can automatically and adaptively learn spatial hierarchies of features from images, which is crucial for detecting the company's logo in images.\n\n‘Principal component analysis (PCA)’ is mainly used for dimensionality reduction, not for image classification tasks like the one described, so it's not suitable. ‘Recurrent neural network (RNN)’ is designed to handle sequential data, making it more appropriate for tasks such as natural language processing and time - series analysis, rather than image detection. ‘К - nearest neighbors (k - NN)’ has limited performance for complex image classification as it struggles with high - dimensional data like images and doesn’t have the ability to learn hierarchical features like CNNs. \n\nThe ability of CNNs to learn features from images effectively is the key factor that makes it the real answer option, distinguishing it from the fake options provided. A common misconception could be choosing RNN due to a misunderstanding of its application scope, or PCA thinking that dimensionality reduction is the main requirement for this image - detection problem.", "zhcn": "对于“某公司机器学习团队需构建一套系统，用于检测图集中人物是否佩戴公司标识。公司已具备带标注的训练数据集，请问团队应采用哪种算法满足此需求？”这一问题，正确答案应为“卷积神经网络（CNN）”。原因在于CNN专为图像相关任务设计，能够自动自适应地学习图像中的空间层次化特征，这对检测图中公司标识至关重要。  \n\n主成分分析（PCA）主要用于降维，而非此类图像分类任务，故不适用。循环神经网络（RNN）适用于处理序列数据（如自然语言处理和时间序列分析），与图像检测需求不符。K近邻算法（k-NN）面对图像这类高维数据时表现受限，且无法像CNN那样学习层次化特征。  \n\nCNN从图像中高效学习特征的能力，使其在本题中成为区别于其他干扰项的核心选择。常见的误解可能包括：因对RNN应用场景的误解而误选该算法，或误认为降维是本图像检测问题的主要需求而选择PCA。"}, "answer": "D"}, {"id": "351", "question": {"enus": "A data scientist uses Amazon SageMaker Data Wrangler to obtain a feature summary from a dataset that the data scientist imported from Amazon S3. The data scientist notices that the prediction power for a dataset feature has a score of 1. What is the cause of the score? ", "zhcn": "数据科学家使用Amazon SageMaker Data Wrangler，对从Amazon S3导入的数据集进行特征摘要分析时，发现某一数据特征的预测力评分为1。此评分结果的可能成因是什么？"}, "option": [{"option_text": {"zhcn": "导入数据集中出现了目标变量泄露。多数投票结果如此。", "enus": "Target leakage occurred in the imported dataset. Most Voted"}, "option_flag": true}, {"option_text": {"zhcn": "数据科学家并未对训练集与验证集的划分进行精细调整。", "enus": "The data scientist did not fine-tune the training and validation split."}, "option_flag": false}, {"option_text": {"zhcn": "数据科学家采用的SageMaker Data Wrangler算法未能为每个特征找到最优的模型拟合方案，从而无法准确评估其预测效力。", "enus": "The SageMaker Data Wrangler algorithm that the data scientist used did not find an optimal model fit for each feature to calculate the prediction power."}, "option_flag": false}, {"option_text": {"zhcn": "数据科学家未能对特征进行充分处理，以致无法精确评估其预测效力。", "enus": "The data scientist did not process the features enough to accurately calculate prediction power."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question ‘A data scientist uses Amazon SageMaker Data Wrangler to obtain a feature summary from a dataset that the data scientist imported from Amazon S3. The data scientist notices that the prediction power for a dataset feature has a score of 1. What is the cause of the score?’ is ‘Target leakage occurred in the imported dataset.’ A prediction power score of 1 indicates that a feature has a perfect predictive relationship with the target variable, which is highly unlikely in real - world scenarios and is a strong sign of target leakage, where information from the target variable has inadvertently leaked into the feature.\n\nThe option ‘The data scientist did not fine - tune the training and validation split’ is incorrect because the split between training and validation data mainly affects model evaluation and overfitting, not the prediction power score of a single feature. The option ‘The SageMaker Data Wrangler algorithm that the data scientist used did not find an optimal model fit for each feature to calculate the prediction power’ is wrong as a score of 1 is not about non - optimal model fit but rather an abnormal relationship between the feature and the target. The option ‘The data scientist did not process the features enough to accurately calculate prediction power’ is also incorrect since insufficient feature processing would not lead to a perfect score of 1. The key factor distinguishing the real answer is the abnormal nature of a prediction power score of 1, which points to target leakage.", "zhcn": "针对问题“数据科学家使用Amazon SageMaker Data Wrangler对从Amazon S3导入的数据集进行特征摘要分析时，发现某特征的预测力评分为1。该评分出现的原因是什么？”的正确定义是“导入的数据集中存在目标变量泄露”。预测力评分达到1意味着该特征与目标变量存在完全预测关系，这在现实场景中极不寻常，强烈暗示目标变量信息意外渗入特征所导致的泄露。\n\n而“数据科学家未优化训练集与验证集的划分”这一选项并不成立，因为训练/验证数据的划分主要影响模型评估与过拟合问题，与单一特征的预测力评分无关；“数据科学家使用的SageMaker Data Wrangler算法未能为每个特征找到最优模型拟合以计算预测力”这一说法同样错误，因为评分值为1并非模型拟合欠佳所致，而是反映特征与目标变量间存在异常关联；“数据科学家未对特征进行充分处理以准确计算预测力”的选项亦不准确，因为特征处理不足不可能产生1分的完美评分。真正的关键区别在于：预测力评分达到1的异常现象直指目标变量泄露这一核心问题。"}, "answer": "A"}, {"id": "352", "question": {"enus": "A data scientist is conducting exploratory data analysis (EDA) on a dataset that contains information about product suppliers. The dataset records the country where each product supplier is located as a two-letter text code. For example, the code for New Zealand is \"NZ.\" The data scientist needs to transform the country codes for model training. The data scientist must choose the solution that will result in the smallest increase in dimensionality. The solution must not result in any information loss. Which solution will meet these requirements? ", "zhcn": "一位数据科学家正在对包含产品供应商信息的数据集进行探索性数据分析（EDA）。该数据集以双字母文本代码的形式记录每位产品供应商所在的国家，例如新西兰的代码为\"NZ\"。为进行模型训练，数据科学家需对国家代码进行转换，且必须选择能实现维度增加最小的解决方案，同时确保不丢失任何信息。何种方案可满足这些要求？"}, "option": [{"option_text": {"zhcn": "添加一个包含完整国家名称的新数据列。", "enus": "Add a new column of data that includes the full country name."}, "option_flag": false}, {"option_text": {"zhcn": "将国家代码通过相似性编码转化为数值变量。", "enus": "Encode the country codes into numeric variables by using similarity encoding."}, "option_flag": false}, {"option_text": {"zhcn": "将国家代码与对应的大洲名称进行映射。", "enus": "Map the country codes to continent names."}, "option_flag": false}, {"option_text": {"zhcn": "将国家代码通过独热编码转换为数值变量。最高票当选。", "enus": "Encode the country codes into numeric variables by using one-hot encoding. Most Voted"}, "option_flag": true}], "analysis": {"enus": "The correct answer to the question is to “Encode the country codes into numeric variables by using one - hot encoding”. One - hot encoding creates a binary column for each unique category (country code in this case). It ensures no information loss as each country code has its own distinct binary representation. And compared to other methods, it generally results in a relatively small increase in dimensionality when dealing with a set of country codes.\n\nAdding a new column with the full country name doesn't transform the data into a numeric format suitable for model training, so it doesn't meet the requirements. Similarity encoding is used when there is a concept of similarity between categories, which is not relevant for country codes. Mapping country codes to continent names leads to information loss as it aggregates countries into broader groups, losing the specific country - level details. These factors make one - hot encoding the best choice, distinguishing it from the fake answer options.", "zhcn": "针对该问题的正确答案是\"采用独热编码方式将国家代码转换为数值型变量\"。独热编码会为每个独特的类别（此处指国家代码）创建独立的二元数列，既能确保每个国家代码都有独特的二进制表示形式而不会丢失信息，又能在处理国家代码集合时保持维度增长相对可控。  \n\n若直接添加包含完整国家名称的新列，则无法将数据转换为适合模型训练的数值格式，故不符合要求。相似性编码适用于类别间存在相似度概念的场景，而国家代码之间并无此种关联。将国家代码映射至大洲名称会导致信息损失，因为这种聚合方式会模糊国家层面的具体细节。这些因素共同凸显了独热编码的优越性，使其从其他干扰选项中脱颖而出。"}, "answer": "D"}, {"id": "353", "question": {"enus": "A data scientist is building a new model for an ecommerce company. The model will predict how many minutes it will take to deliver a package. During model training, the data scientist needs to evaluate model performance. Which metrics should the data scientist use to meet this requirement? (Choose two.) ", "zhcn": "一位数据科学家正在为某电商企业构建新模型，该模型旨在预测包裹投递所需时长。在模型训练过程中，需对模型性能进行评估。为达成此目标，该数据科学家应采用哪两项评估指标？（请选择两项）"}, "option": [{"option_text": {"zhcn": "推理延迟", "enus": "InferenceLatency"}, "option_flag": false}, {"option_text": {"zhcn": "均方误差（MSE） 获赞最多", "enus": "Mean squared error (MSE) Most Voted"}, "option_flag": true}, {"option_text": {"zhcn": "均方根误差（RMSE） 获赞最多", "enus": "Root mean squared error (RMSE) Most Voted"}, "option_flag": true}, {"option_text": {"zhcn": "精准", "enus": "Precision"}, "option_flag": false}, {"option_text": {"zhcn": "精准", "enus": "Accuracy"}, "option_flag": false}], "analysis": {"enus": "The correct answers to the question “A data scientist is building a new model for an ecommerce company. The model will predict how many minutes it will take to deliver a package. During model training, the data scientist needs to evaluate model performance. Which metrics should the data scientist use to meet this requirement?” are “Mean squared error (MSE)” and “Root mean squared error (RMSE)”. These metrics are ideal for regression problems, which is what this package - delivery time prediction is. MSE calculates the average of the squared differences between the predicted and actual values, and RMSE is the square root of MSE. They provide a measure of how close the model's predictions are to the actual delivery times.\n\nThe fake answer “InferenceLatency” refers to the time it takes for the model to make a prediction, not the accuracy of those predictions. “Precision” and “Accuracy” are metrics commonly used for classification problems, where the goal is to correctly classify data into categories. In this package - delivery time prediction which is a regression task, using them would be inappropriate. This is why MSE and RMSE are the real answer options, distinguishing them from the fake ones.", "zhcn": "针对“某电商公司的数据科学家正在构建一个新模型，用于预测包裹配送所需时长。在模型训练过程中，需评估模型性能，此时应选用哪些指标？”这一问题，正确答案应为“均方误差（MSE）”与“均方根误差（RMSE）”。由于包裹送达时间预测属于回归问题，这两项指标尤为适用。MSE通过计算预测值与实际值偏差平方的平均值来衡量误差，RMSE则是MSE的平方根，二者能有效反映模型预测结果与真实配送时间的接近程度。\n\n而干扰项“推理延迟”指的是模型进行预测所需的时间消耗，与预测精度无关；“精确率”和“准确率”通常用于分类场景，其核心在于判断数据所属类别。将分类指标应用于回归类的配送时间预测任务显然不妥。正因如此，MSE与RMSE才是符合题意的正确选项，可与干扰项明确区分。"}, "answer": "BC"}, {"id": "354", "question": {"enus": "A machine learning (ML) specialist is developing a model for a company. The model will classify and predict sequences of objects that are displayed in a video. The ML specialist decides to use a hybrid architecture that consists of a convolutional neural network (CNN) followed by a classifier three-layer recurrent neural network (RNN). The company developed a similar model previously but trained the model to classify a different set of objects. The ML specialist wants to save time by using the previously trained model and adapting the model for the current use case and set of objects. Which combination of steps will accomplish this goal with the LEAST amount of effort? (Choose two.) ", "zhcn": "一位机器学习专家正为公司开发一款视频物体序列分类与预测模型。该专家决定采用由卷积神经网络（CNN）与三层循环神经网络（RNN）分类器构成的混合架构。该公司曾开发过类似模型，但当时训练所用物体类别与当前不同。为节省时间，专家计划基于已有模型进行适应性调整。以下哪两种步骤组合能以最小工作量实现这一目标？（请选择两项）"}, "option": [{"option_text": {"zhcn": "重新初始化整个卷积神经网络的权重。利用新的物体数据集，对网络进行图像分类任务的再次训练。", "enus": "Reinitialize the weights of the entire CNN. Retrain the CNN on the classification task by using the new set of objects."}, "option_flag": false}, {"option_text": {"zhcn": "重新初始化整个网络的权重。利用新的对象集合，对网络进行整体重构，以完成预测任务的训练。", "enus": "Reinitialize the weights of the entire network. Retrain the entire network on the prediction task by using the new set of objects."}, "option_flag": false}, {"option_text": {"zhcn": "重新初始化整个循环神经网络的权重参数，利用新增对象集合对模型进行完整重训练，以优化其预测性能。", "enus": "Reinitialize the weights of the entire RNN. Retrain the entire model on the prediction task by using the new set of objects."}, "option_flag": false}, {"option_text": {"zhcn": "重新初始化卷积神经网络末层全连接层的权重参数，并采用新版对象集对网络进行分类任务的再训练。最高票当选方案。", "enus": "Reinitialize the weights of the last fully connected layer of the CNN. Retrain the CNN on the classification task by using the new set of objects. Most Voted"}, "option_flag": true}, {"option_text": {"zhcn": "重新初始化循环神经网络最后一层的权重参数，并基于新增对象集对模型进行预测任务的完整重训练。采纳最高票选方案。", "enus": "Reinitialize the weights of the last layer of the RNN. Retrain the entire model on the prediction task by using the new set of objects. Most Voted"}, "option_flag": true}], "analysis": {"enus": "The correct answer to the question of how to adapt a previously - trained model for a new set of objects with the least effort is to reinitialize the weights of the last fully connected layer of the CNN and retrain the CNN on the new classification task, and reinitialize the weights of the last layer of the RNN and retrain the entire model on the new prediction task.\n\nThe main reason is that in transfer learning, the earlier layers of neural networks often learn general features that are useful across different tasks. By only reinitializing the last layers of the CNN and RNN, we can leverage the previously learned general features from the earlier layers. This way, we save a significant amount of training time and computational resources as we don't need to retrain the entire network from scratch.\n\nOn the other hand, the fake answer options involve reinitializing the entire CNN, the entire network, or the entire RNN. Retraining these large parts of the model requires a large amount of data and computational power, and it takes much more time. A common misconception might be thinking that since the new task has a different set of objects, the whole model needs to be retrained from the ground up. However, this is not necessary as the earlier layers can still provide valuable feature extraction capabilities for the new task.", "zhcn": "如何以最小代价使已训练模型适应新对象集？正确做法是：重新初始化卷积神经网络末层全连接层的权重，针对新分类任务微调CNN；同时重置循环神经网络最后一层权重，在新预测任务上对整体模型进行精训练。其核心原理在于：迁移学习中，神经网络底层通常已习得通用特征，这些特征具备跨任务适用性。通过仅调整CNN与RNN的末端层，既能充分利用先前学到的通用特征，又可大幅节省训练时间与计算资源——无需对整个网络进行推倒重来式的训练。  \n\n而错误选项则涉及重置整个CNN、全部网络或完整RNN。若对这些大型模块进行重新训练，不仅需要海量数据与算力支撑，耗时也将呈指数级增长。常见误解在于认为新任务涉及不同对象集就必须彻底重训模型，却忽略了底层网络特征提取能力对于新任务依然具有重要价值。"}, "answer": "DE"}, {"id": "355", "question": {"enus": "A company distributes an online multiple-choice survey to several thousand people. Respondents to the survey can select multiple options for each question. A machine learning (ML) engineer needs to comprehensively represent every response from all respondents in a dataset. The ML engineer will use the dataset to train a logistic regression model. Which solution will meet these requirements? ", "zhcn": "某公司向数千人分发了一份在线选择题问卷。受访者可为每个问题选择多个选项。一位机器学习工程师需要将全体受访者的每项回答完整呈现在数据集中。该工程师将使用该数据集训练逻辑回归模型。下列哪种方案能满足这些要求？"}, "option": [{"option_text": {"zhcn": "对问卷中每道题目的所有选项进行独热编码处理。最高票当选。", "enus": "Perform one-hot encoding on every possible option for each question of the survey. Most Voted"}, "option_flag": true}, {"option_text": {"zhcn": "对每位受访者在每道题目中所作的选择进行归类整理。", "enus": "Perform binning on all the answers each respondent selected for each question."}, "option_flag": false}, {"option_text": {"zhcn": "利用亚马逊土耳其机器人（Amazon Mechanical Turk）为每组可能的回答生成分类标签。", "enus": "Use Amazon Mechanical Turk to create categorical labels for each set of possible responses."}, "option_flag": false}, {"option_text": {"zhcn": "利用Amazon Textract为每组可能的回答生成数字特征。", "enus": "Use Amazon Textract to create numeric features for each set of possible responses."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is “Perform one - hot encoding on every possible option for each question of the survey.” One - hot encoding is ideal here because the survey allows respondents to select multiple options per question, and we need to comprehensively represent every response in the dataset for training a logistic regression model. One - hot encoding creates a binary column for each possible option, indicating whether that option was selected or not, which effectively captures all possible responses.\n\nThe option “Perform binning on all the answers each respondent selected for each question” is incorrect. Binning is used to group continuous data into intervals, not for handling multiple - choice responses where we need to represent each individual option.\n\n“Use Amazon Mechanical Turk to create categorical labels for each set of possible responses” is wrong. Amazon Mechanical Turk is for human - powered tasks like data annotation, but it doesn't directly transform the data into a format suitable for logistic regression to represent all individual responses.\n\n“Use Amazon Textract to create numeric features for each set of possible responses” is also incorrect. Amazon Textract is for extracting text from documents, not for processing multiple - choice survey response data in the way required for this task. The key factor that makes one - hot encoding the right choice is its ability to precisely represent all possible options in a format that logistic regression can work with, distinguishing it from the fake options.", "zhcn": "该问题的正确答案是“对问卷中每个问题的所有选项进行独热编码”。此处采用独热编码最为理想，因为问卷允许受访者为每个问题选择多个选项，而我们需要在数据集中完整呈现每个回答，以便训练逻辑回归模型。独热编码会为每个可能选项创建独立的二元标识列，清晰标记该选项是否被选中，从而精准捕捉所有可能的回答组合。\n\n“对每位受访者在每个问题上的答案进行分箱处理”这一选项是错误的。分箱技术适用于将连续数据分组到区间内，而非处理需要单独呈现每个选项的多选题回答。\n\n“使用亚马逊土耳其机器人（Amazon Mechanical Turk）为每组可能回答创建分类标签”亦属错误。该平台本用于众包形式的人工标注任务，并不能直接将数据转化为适合逻辑回归模型处理所有独立回答的格式。\n\n“使用亚马逊文本提取服务（Amazon Textract）为每组可能回答创建数值特征”同样不正确。此工具专用于从文档中提取文本信息，而非处理本任务所需的多选题问卷数据。独热编码之所以成为正确选择，关键在于它能以逻辑回归模型可处理的格式精确呈现所有选项，这一特性使其与错误选项形成本质区别。"}, "answer": "A"}, {"id": "356", "question": {"enus": "A manufacturing company stores production volume data in a PostgreSQL database. The company needs an end-to-end solution that will give business analysts the ability to prepare data for processing and to predict future production volume based the previous year's production volume. The solution must not require the company to have coding knowledge. Which solution will meet these requirements with the LEAST effort? ", "zhcn": "一家制造企业将其产量数据存储于PostgreSQL数据库中。该公司需要一套端到端的解决方案，使业务分析师能够为数据处理做好准备，并依据往年产量预测未来生产规模。该方案必须确保企业无需具备编程知识。哪种方案能以最小投入满足这些需求？"}, "option": [{"option_text": {"zhcn": "运用AWS数据库迁移服务（AWS DMS）将PostgreSQL数据库中的数据迁移至Amazon S3存储桶。随后创建Amazon EMR集群读取S3存储桶中的数据并进行预处理，最终通过Amazon SageMaker Studio平台完成预测模型的构建工作。", "enus": "Use AWS Database Migration Service (AWS DMS) to transfer the data from the PostgreSQL database to an Amazon S3 bucket. Create an Amazon EMR duster to read the S3 bucket and perform the data preparation. Use Amazon SageMaker Studio for the prediction modeling."}, "option_flag": false}, {"option_text": {"zhcn": "使用AWS Glue DataBrew从PostgreSQL数据库中读取数据并进行数据预处理，通过Amazon SageMaker Canvas平台实现预测建模。此为最高票选方案。", "enus": "Use AWS Glue DataBrew to read the data that is in the PostgreSQL database and to perform the data preparation. Use Amazon SageMaker Canvas for the prediction modeling. Most Voted"}, "option_flag": true}, {"option_text": {"zhcn": "使用AWS数据库迁移服务（AWS DMS）将PostgreSQL数据库中的数据迁移至Amazon S3存储桶。通过AWS Glue读取S3存储桶内的数据并进行预处理，最终借助Amazon SageMaker Canvas平台完成预测模型的构建。", "enus": "Use AWS Database Migration Service (AWS DMS) to transfer the data from the PostgreSQL database to an Amazon S3 bucket. Use AWS Glue to read the data in the S3 bucket and to perform the data preparation. Use Amazon SageMaker Canvas for the prediction modeling."}, "option_flag": false}, {"option_text": {"zhcn": "利用AWS Glue DataBrew读取PostgreSQL数据库中的数据并进行数据预处理，随后通过Amazon SageMaker Studio平台开展预测建模工作。", "enus": "Use AWS Glue DataBrew to read the data that is in the PostgreSQL database and to perform the data preparation. Use Amazon SageMaker Studio for the prediction modeling."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Use AWS Glue DataBrew to read the data that is in the PostgreSQL database and to perform the data preparation. Use Amazon SageMaker Canvas for the prediction modeling”. This solution meets the requirements with the least effort as it directly accesses the PostgreSQL database with AWS Glue DataBrew for data - preparation, eliminating the need for data transfer steps. Amazon SageMaker Canvas is a no - code tool, which aligns with the requirement of not needing coding knowledge for prediction modeling.\n\nThe fake answer “Use AWS Database Migration Service (AWS DMS) to transfer the data from the PostgreSQL database to an Amazon S3 bucket. Create an Amazon EMR duster to read the S3 bucket and perform the data preparation. Use Amazon SageMaker Studio for the prediction modeling” involves extra steps of data transfer and setting up an Amazon EMR cluster, which requires more effort. Also, Amazon SageMaker Studio usually requires coding skills.\n\nThe option “Use AWS Database Migration Service (AWS DMS) to transfer the data from the PostgreSQL database to an Amazon S3 bucket. Use AWS Glue to read the data in the S3 bucket and to perform the data preparation. Use Amazon SageMaker Canvas for the prediction modeling” adds an unnecessary data transfer step with AWS DMS, increasing the complexity and effort.\n\nThe answer “Use AWS Glue DataBrew to read the data that is in the PostgreSQL database and to perform the data preparation. Use Amazon SageMaker Studio for the prediction modeling” is incorrect because Amazon SageMaker Studio generally demands coding knowledge, which goes against the given requirement.", "zhcn": "针对该问题，正确答案是“使用AWS Glue DataBrew读取PostgreSQL数据库中的数据并进行数据预处理，再通过Amazon SageMaker Canvas完成预测建模”。此方案以最小成本满足需求：通过AWS Glue DataBrew直接访问PostgreSQL数据库进行数据预处理，避免了数据迁移步骤；而Amazon SageMaker Canvas作为无代码工具，符合预测建模无需编程能力的要求。\n\n错误答案“使用AWS数据库迁移服务将PostgreSQL数据转移至Amazon S3存储桶，创建Amazon EMR集群读取S3数据并执行预处理，再通过Amazon SageMaker Studio进行预测建模”存在多余的数据传输环节和EMR集群搭建步骤，实施成本更高。且Amazon SageMaker Studio通常需要编程能力。\n\n另一选项“使用AWS数据库迁移服务将数据转移至Amazon S3存储桶，通过AWS Glue读取S3数据并预处理，最后使用Amazon SageMaker Canvas进行预测建模”虽然选择了无代码建模工具，但通过DMS进行数据迁移的步骤实属冗余，增加了复杂性和工作量。\n\n而“使用AWS Glue DataBrew读取PostgreSQL数据并预处理，再通过Amazon SageMaker Studio进行预测建模”的方案因Amazon SageMaker Studio需具备编程知识，与题目要求相悖，故不可取。"}, "answer": "B"}, {"id": "357", "question": {"enus": "A data scientist needs to create a model for predictive maintenance. The model will be based on historical data to identify rare anomalies in the data. The historical data is stored in an Amazon S3 bucket. The data scientist needs to use Amazon SageMaker Data Wrangler to ingest the data. The data scientist also needs to perform exploratory data analysis (EDA) to understand the statistical properties of the data. Which solution will meet these requirements with the LEAST amount of compute resources? ", "zhcn": "数据科学家需要构建一个预测性维护模型。该模型将基于历史数据识别其中的罕见异常。历史数据存储于Amazon S3存储桶中，数据科学家需使用Amazon SageMaker Data Wrangler进行数据摄取，同时还需开展探索性数据分析（EDA）以理解数据的统计特性。哪种方案能够以最少的计算资源满足这些需求？"}, "option": [{"option_text": {"zhcn": "使用“无”选项导入数据。", "enus": "Import the data by using the None option."}, "option_flag": false}, {"option_text": {"zhcn": "采用分层抽样方式导入数据。", "enus": "Import the data by using the Stratified option."}, "option_flag": false}, {"option_text": {"zhcn": "使用“前K项”选项导入数据，并依据领域知识推断K的取值。", "enus": "Import the data by using the First K option. Infer the value of K from domain knowledge."}, "option_flag": true}, {"option_text": {"zhcn": "通过随机化选项导入数据，并依据领域知识推断随机样本规模。", "enus": "Import the data by using the Randomized option. Infer the random size from domain knowledge."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is “Import the data by using the First K option. Infer the value of K from domain knowledge.” When dealing with large historical data stored in an Amazon S3 bucket for anomaly - detection and exploratory data analysis (EDA), using the First K option allows the data scientist to import a subset of the data. By inferring the value of K from domain knowledge, they can get a representative sample that is sufficient for EDA and anomaly - identification, thus minimizing the compute resources needed.\n\nThe “None” option would import the entire dataset. This is resource - intensive as it processes all the data, which is unnecessary for initial EDA and anomaly - detection. The “Stratified” option is used when you want to maintain the proportion of different classes in the data during sampling. It is more complex and may require more resources as it involves additional calculations to ensure proper stratification. The “Randomized” option also samples data randomly, but inferring the random size from domain knowledge doesn't guarantee as efficient resource - usage as the First K option. The First K option is straightforward and can quickly provide a useful subset of data, making it the best choice for minimizing compute resources.", "zhcn": "对于该问题的正确答案是：“使用‘前K项’选项导入数据，并依据领域知识推断K的取值。”当处理存储在亚马逊S3存储桶中的大规模历史数据以进行异常检测和探索性数据分析时，采用“前K项”选项可使数据科学家导入数据子集。通过结合领域知识推定K值，他们能获得具有代表性且足以完成探索性分析与异常识别的数据样本，从而最大限度节约计算资源。\n\n若选择“全部导入”选项，则会完整加载整个数据集。这种方式需要处理全部数据，资源消耗较大，对于初步的探索性分析和异常检测而言实无必要。而“分层抽样”选项适用于需要在采样过程中保持数据内各类别比例的场景。该方式更为复杂，可能因需确保准确分层而消耗额外资源。“随机抽样”选项虽也能实现随机采样，但依据领域知识推断随机样本量的方法，在资源利用效率上不如“前K项”选项可控。“前K项”方案操作直观，能快速提供有效数据子集，因此成为最小化计算资源的最优选择。"}, "answer": "C"}, {"id": "358", "question": {"enus": "An ecommerce company has observed that customers who use the company's website rarely view items that the website recommends to customers. The company wants to recommend items to customers that customers are more likely to want to purchase. Which solution will meet this requirement in the SHORTEST amount of time? ", "zhcn": "一家电商公司发现，其网站向顾客推荐的商品很少被浏览。为提升推荐商品的购买转化率，该公司希望在最短时间内找到最有效的解决方案。下列哪种方式能最快实现这一目标？"}, "option": [{"option_text": {"zhcn": "将公司网站部署于亚马逊EC2加速计算实例，可显著提升网站响应速度。", "enus": "Host the company's website on Amazon EC2 Accelerated Computing instances to increase the website response speed."}, "option_flag": false}, {"option_text": {"zhcn": "将公司网站部署于亚马逊EC2 GPU实例之上，以提升网站搜索工具的响应速度。", "enus": "Host the company's website on Amazon EC2 GPU-based instances to increase the speed of the website's search tool."}, "option_flag": false}, {"option_text": {"zhcn": "将Amazon Personalize整合至公司官网，为顾客提供个性化推荐服务。", "enus": "Integrate Amazon Personalize into the company's website to provide customers with personalized recommendations."}, "option_flag": true}, {"option_text": {"zhcn": "利用亚马逊 SageMaker 训练神经协同过滤（NCF）模型，实现个性化商品推荐。", "enus": "Use Amazon SageMaker to train a Neural Collaborative Filtering (NCF) model to make product recommendations."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is ‘Integrate Amazon Personalize into the company's website to provide customers with personalized recommendations.’ This is because Amazon Personalize is a fully - managed service designed specifically for generating personalized item recommendations. It can quickly analyze customer behavior data and start providing relevant product suggestions, which directly addresses the company's need to recommend items customers are more likely to purchase in a short time.\n\nThe option ‘Host the company's website on Amazon EC2 Accelerated Computing instances to increase the website response speed’ only focuses on improving website speed, not on making better product recommendations. Similarly, ‘Host the company's website on Amazon EC2 GPU - based instances to increase the speed of the website's search tool’ is about enhancing the search tool's speed, not about generating more appealing product recommendations.\n\n‘Use Amazon SageMaker to train a Neural Collaborative Filtering (NCF) model to make product recommendations’ involves building and training a custom model. This process is time - consuming as it requires data preprocessing, model training, and tuning, which cannot meet the requirement of achieving the goal in the shortest amount of time. \n\nCommon misconceptions might lead one to choose the fake options if they misunderstand the question and think that improving website performance or building a custom model is the quickest way to solve the problem of generating better product recommendations. In reality, leveraging a ready - made service like Amazon Personalize is the most time - efficient solution.", "zhcn": "针对该问题的正确答案是\"将Amazon Personalize集成至公司官网，为客户提供个性化推荐\"。这是因为Amazon Personalize作为全托管服务，专精于生成个性化商品推荐方案。该服务能快速分析客户行为数据，即时输出相关产品建议，恰好契合企业在短期内实现精准商品推荐的诉求。\n\n而\"采用Amazon EC2加速计算实例部署官网以提升响应速度\"的方案仅着眼于网站性能优化，与优化推荐功能无直接关联；同理，\"基于Amazon EC2 GPU实例部署官网以加速站内搜索工具\"也只是提升搜索效率，并未解决增强商品推荐吸引力的核心需求。\n\n至于\"使用Amazon SageMaker训练神经协同过滤（NCF）模型进行商品推荐\"的方案，需要经历数据预处理、模型训练与参数调优等复杂流程，其开发周期无法满足\"最短时间内达成目标\"的要求。常见误区在于：若未能准确理解题意，可能误认为提升网站性能或构建定制模型是解决推荐功能优化的捷径，实则采用Amazon Personalize这类开箱即用的服务才是最高效的解决方案。"}, "answer": "C"}, {"id": "359", "question": {"enus": "A machine learning (ML) engineer is preparing a dataset for a classification model. The ML engineer notices that some continuous numeric features have a significantly greater value than most other features. A business expert explains that the features are independently informative and that the dataset is representative of the target distribution. After training, the model's inferences accuracy is lower than expected. Which preprocessing technique will result in the GREATEST increase of the model's inference accuracy? ", "zhcn": "一位机器学习工程师正在为分类模型准备数据集。他注意到某些连续数值型特征的量级远高于其他特征。业务专家解释称这些特征各自具有独立信息价值，且数据集能代表目标分布。然而模型训练后的推理准确率却低于预期。下列哪种预处理技术能最大程度提升模型的推理准确率？"}, "option": [{"option_text": {"zhcn": "化解棘手特征，使其归于和谐。", "enus": "Normalize the problematic features."}, "option_flag": true}, {"option_text": {"zhcn": "**启动问题功能。**", "enus": "Bootstrap the problematic features."}, "option_flag": false}, {"option_text": {"zhcn": "去除有问题的功能。", "enus": "Remove the problematic features."}, "option_flag": false}, {"option_text": {"zhcn": "推演合成特征。", "enus": "Extrapolate synthetic features."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is ‘Normalize the problematic features.’ When continuous numeric features have significantly greater values than others, it can cause issues in a classification model as algorithms may give more weight to these high - value features. Normalization scales these features to a common range, ensuring that all features contribute equally to the model training, which can greatly improve the model's inference accuracy.\n\n‘Bootstrap the problematic features’ involves resampling with replacement, which is mainly used for estimating statistics and reducing variance, not for dealing with the issue of features having large values. ‘Remove the problematic features’ is not ideal as the business expert has stated that these features are independently informative, so removing them would lead to a loss of valuable data. ‘Extrapolate synthetic features’ creates new features based on existing ones, but this does not address the core problem of the large - valued features overwhelming the model. The need to balance the influence of features is the key reason for choosing normalization over the other fake options.", "zhcn": "对于“如何处理数值差异过大的特征”这一问题，正确答案是“对问题特征进行归一化处理”。当连续型数值特征的量级显著高于其他特征时，分类模型可能因算法过度关注高值特征而产生偏差。归一化处理能将这类特征缩放至统一区间，确保所有特征在模型训练中发挥同等作用，从而显著提升模型推理的准确性。  \n\n“自助法处理问题特征”采用有放回抽样技术，主要用于统计量估计和方差削减，并不能解决特征数值过大的问题。“直接删除问题特征”并非良策，因为业务专家已明确指出这些特征本身具有独立信息价值，剔除会造成有效数据损失。“构造合成特征”虽能基于现有特征生成新变量，但并未触及高值特征主导模型这一核心矛盾。因此，通过归一化平衡特征影响力，才是区别于其他干扰选项的关键所在。"}, "answer": "A"}, {"id": "360", "question": {"enus": "A manufacturing company produces 100 types of steel rods. The rod types have varying material grades and dimensions. The company has sales data for the steel rods for the past 50 years. A data scientist needs to build a machine learning (ML) model to predict future sales of the steel rods. Which solution will meet this requirement in the MOST operationally efficient way? ", "zhcn": "一家钢铁制品公司生产百种规格的螺纹钢，其材质等级与尺寸参数各不相同。该公司拥有过去五十年间的螺纹钢销售数据，一位数据科学家需构建机器学习模型以预测未来销量。下列哪种解决方案能以最高运营效率满足此需求？"}, "option": [{"option_text": {"zhcn": "利用Amazon SageMaker的DeepAR预测算法，为所有产品构建统一预测模型。", "enus": "Use the Amazon SageMaker DeepAR forecasting algorithm to build a single model for all the products."}, "option_flag": true}, {"option_text": {"zhcn": "利用Amazon SageMaker平台的DeepAR预测算法，为每款产品分别建立专属预测模型。", "enus": "Use the Amazon SageMaker DeepAR forecasting algorithm to build separate models for each product."}, "option_flag": false}, {"option_text": {"zhcn": "利用Amazon SageMaker Autopilot，为所有产品构建统一模型。", "enus": "Use Amazon SageMaker Autopilot to build a single model for all the products."}, "option_flag": false}, {"option_text": {"zhcn": "利用Amazon SageMaker Autopilot为每款产品分别构建专属模型。完成度：百分之百。", "enus": "Use Amazon SageMaker Autopilot to build separate models for each product.  A (100%)"}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Use the Amazon SageMaker DeepAR forecasting algorithm to build a single model for all the products.” This is because the DeepAR algorithm is designed for time - series forecasting and can handle multiple related time - series data efficiently. Building a single model for all 100 types of steel rods takes advantage of the algorithm's ability to capture common patterns across different products, which is operationally more efficient in terms of training time, resource usage, and maintenance.\n\nThe option of using DeepAR to build separate models for each product is less efficient as it would require training 100 individual models, consuming more resources and time. Amazon SageMaker Autopilot is more suitable for tabular data and classification/regression tasks rather than time - series forecasting, so using it to build either a single model or separate models for the steel rods is not the best approach for this time - series sales prediction problem. This is why the chosen real answer is the most operationally efficient solution, distinguishing it from the fake options.", "zhcn": "针对该问题，正确答案应为\"使用亚马逊SageMaker DeepAR预测算法为所有产品构建统一模型\"。原因在于DeepAR算法专为时间序列预测设计，能高效处理多个关联时间序列数据。为100种钢棒产品建立单一模型，可充分发挥该算法捕捉不同产品间共性规律的优势，从而在训练时间、资源消耗及模型维护方面实现更优的运营效率。  \n  \n若采用DeepAR为每种产品单独建模，则需训练100个独立模型，将大幅增加资源与时间成本。而亚马逊SageMaker Autopilot更适用于表格数据及分类/回归任务，而非时间序列预测场景，因此无论用于构建单一模型还是独立模型，均非解决此类销售预测问题的最佳选择。正是基于运营效率的显著优势，该方案才从备选答案中脱颖而出。"}, "answer": "A"}, {"id": "361", "question": {"enus": "A machine learning (ML) specialist is building a credit score model for a financial institution. The ML specialist has collected data for the previous 3 years of transactions and third-party metadata that is related to the transactions. After the ML specialist builds the initial model, the ML specialist discovers that the model has low accuracy for both the training data and the test data. The ML specialist needs to improve the accuracy of the model. Which solutions will meet this requirement? (Choose two.) ", "zhcn": "一位机器学习专家正为某金融机构构建信用评分模型。该专家已收集了过去三年的交易数据及与之相关的第三方元数据。在完成初始模型构建后，专家发现该模型对训练数据和测试数据的准确度均不理想。现需提升模型精准度，下列哪两项措施可达成此目标？（请选择两项）"}, "option": [{"option_text": {"zhcn": "增加对现有训练数据的处理轮次。进一步优化超参数配置。", "enus": "Increase the number of passes on the existing training data. Perform more hyperparameter tuning."}, "option_flag": true}, {"option_text": {"zhcn": "增强正则化强度，减少特征组合的使用。", "enus": "Increase the amount of regularization. Use fewer feature combinations."}, "option_flag": false}, {"option_text": {"zhcn": "增添特定领域的新功能，采用更复杂的模型架构。", "enus": "Add new domain-specific features. Use more complex models."}, "option_flag": true}, {"option_text": {"zhcn": "减少特征组合数量。缩减数值属性分箱区间。", "enus": "Use fewer feature combinations. Decrease the number of numeric attribute bins."}, "option_flag": false}, {"option_text": {"zhcn": "减少训练数据样本的数量。降低对现有训练数据的遍历次数。AC（100%）", "enus": "Decrease the amount of training data examples. Reduce the number of passes on the existing training data.  AC (100%)"}, "option_flag": false}], "analysis": {"enus": "The correct answers to the question “A machine learning (ML) specialist is building a credit score model for a financial institution...” are “Increase the number of passes on the existing training data. Perform more hyperparameter tuning.” and “Add new domain - specific features. Use more complex models.”\n\nIncreasing the number of passes on the existing training data allows the model to learn more from the data, and hyperparameter tuning helps in finding the optimal settings for the model, which can enhance its accuracy. Adding new domain - specific features provides more relevant information for the model to learn patterns, and using more complex models can capture more intricate relationships in the data, thus improving accuracy.\n\nThe fake answer “Increase the amount of regularization. Use fewer feature combinations.” is incorrect because increasing regularization can restrict the model too much and using fewer feature combinations means less information for the model, both of which are likely to reduce accuracy. The option “Use fewer feature combinations. Decrease the number of numeric attribute bins.” also suffers from the problem of reducing the model's information, which won't improve accuracy. And “Decrease the amount of training data examples. Reduce the number of passes on the existing training data.” is wrong as reducing training data and passes means the model has less opportunity to learn, which will further degrade accuracy.\n\nCommon misconceptions might lead one to choose the fake options. For example, one might think that regularization always helps in preventing overfitting and assume that less data or fewer features will simplify the model and improve performance. However, in this case where the model has low accuracy on both training and test data, the focus should be on providing more information and better tuning to the model.", "zhcn": "针对“某金融机构的机器学习专家正在构建信用评分模型...”这一问题的正确答案是：“增加对现有训练数据的遍历次数”与“进行更深入的超参数调优”，以及“添加新的领域特定特征”和“采用更复杂的模型”。增加数据遍历次数能让模型更充分学习数据规律，超参数调优则有助于找到模型的最佳配置，从而提升预测精度。引入领域专属特征可为模型提供更具相关性的信息以供学习，而复杂模型能捕捉数据中更细微的关联，进而优化准确度。\n\n错误答案“增强正则化强度”和“减少特征组合”不可取，因为过度正则化会过度约束模型，削减特征组合则意味着降低信息量，二者均可能导致精度下降。同样，“减少特征组合”与“降低数值属性分箱数”也存在信息量缩减的问题，无法提升准确性。而“削减训练数据量”和“减少现有训练数据遍历次数”更是南辕北辙——减少学习机会只会进一步削弱模型性能。\n\n常见误解可能使人误选干扰项。例如认为正则化必然能防止过拟合，或误以为简化数据特征就能优化模型表现。但当前情境下模型在训练集和测试集上均表现不佳，核心策略应是增强信息供给并优化模型配置。"}, "answer": "AC"}, {"id": "362", "question": {"enus": "A data scientist uses Amazon SageMaker to perform hyperparameter tuning for a prototype machine leaming (ML) model. The data scientist's domain knowledge suggests that the hyperparameter is highly sensitive to changes. The optimal value, x, is in the 0.5 < x < 1.0 range. The data scientist's domain knowledge suggests that the optimal value is close to 1.0. The data scientist needs to find the optimal hyperparameter value with a minimum number of runs and with a high degree of consistent tuning conditions. Which hyperparameter scaling type should the data scientist use to meet these requirements? ", "zhcn": "一位数据科学家正借助Amazon SageMaker平台，为机器学习原型模型进行超参数调优。根据其专业领域的经验判断，该超参数对数值变化极为敏感，其最优解x应落在0.5至1.0的区间内，且极有可能趋近于1.0。在确保调优条件高度一致的前提下，该数据科学家需以最少的实验次数精准定位最优超参数值。请问，为达成此目标，应采用何种超参数缩放方式？"}, "option": [{"option_text": {"zhcn": "自动扩缩容", "enus": "Auto scaling"}, "option_flag": false}, {"option_text": {"zhcn": "线性扩展", "enus": "Linear scaling"}, "option_flag": false}, {"option_text": {"zhcn": "对数尺度", "enus": "Logarithmic scaling"}, "option_flag": false}, {"option_text": {"zhcn": "逆对数缩放", "enus": "Reverse logarithmic scaling"}, "option_flag": true}], "analysis": {"enus": "The correct answer to the question is ‘Reverse logarithmic scaling’. When a hyperparameter is highly sensitive to changes and the optimal value is known to be in a specific range (0.5 < x < 1.0) and close to 1.0, reverse logarithmic scaling is ideal. It allows for a higher density of search points near the upper end of the range (close to 1.0 in this case), which is where the optimal value is suspected to be. This way, the data scientist can find the optimal value with a minimum number of runs.\n\n‘Auto scaling’ is not suitable as it doesn't specifically target the area where the optimal value is likely to be. It doesn't provide the fine - grained search near 1.0. ‘Linear scaling’ distributes the search points evenly across the range. Since the hyperparameter is highly sensitive and we know the optimal is close to 1.0, an even distribution will waste runs on areas less likely to contain the optimal value. ‘Logarithmic scaling’ would concentrate more search points towards the lower end of the range, which is the opposite of what we need as our optimal value is close to 1.0. These reasons make reverse logarithmic scaling the real answer, distinguishing it from the fake options.", "zhcn": "对于该问题的正确答案是“Reverse logarithmic scaling”。当某个超参数对变化极为敏感，且已知其最优值位于特定区间（0.5 < x < 1.0）并趋近于1.0时，反向对数缩放法最为理想。这种方法能在区间高端（即接近1.0的区域）实现更高密度的搜索点分布，而最优值恰好可能存在于该区域。通过这种策略，数据科学家能够以最少的实验次数精准定位最优解。\n\n\"Auto scaling\"并不适用，因为它并未针对最优值可能存在的区域进行针对性搜索，无法在接近1.0的区间实现精细化探索。\"Linear scaling\"会将搜索点均匀分布在整个区间。鉴于该超参数的高敏感性且已知最优值接近1.0，均匀分布会浪费大量计算资源在低概率区域。\"Logarithmic scaling\"则会将搜索点向区间低端集中，这与我们需要的搜索方向完全相悖——因为最优值恰恰位于高端。这些原因使得反向对数缩放法成为唯一正解，从而与其他干扰项区分开来。"}, "answer": "D"}, {"id": "363", "question": {"enus": "A data scientist uses Amazon SageMaker Data Wrangler to analyze and visualize data. The data scientist wants to refine a training dataset by selecting predictor variables that are strongly predictive of the target variable. The target variable correlates with other predictor variables. The data scientist wants to understand the variance in the data along various directions in the feature space. Which solution will meet these requirements? ", "zhcn": "一位数据科学家运用亚马逊SageMaker数据整理工具进行数据分析和可视化。为优化训练数据集，该科学家需筛选出对目标变量具有强预测力的特征变量。由于目标变量与其他特征变量存在相关性，科学家需要理解数据在特征空间不同方向上的变异程度。何种方案可满足上述需求？"}, "option": [{"option_text": {"zhcn": "运用SageMaker Data Wrangler的多重共线性检测功能，通过方差膨胀系数（VIF）指标来量化变量间的关联程度。VIF分值越高，表明自变量之间的线性相关性越强。", "enus": "Use the SageMaker Data Wrangler multicollinearity measurement features with a variance inflation factor (VIF) score. Use the VIF score as a measurement of how closely the variables are related to each other."}, "option_flag": false}, {"option_text": {"zhcn": "利用SageMaker Data Wrangler的数据质量与洞察报告快速模型可视化功能，可预估基于当前数据训练的模型预期质量。", "enus": "Use the SageMaker Data Wrangler Data Quality and Insights Report quick model visualization to estimate the expected quality of a model that is trained on the data."}, "option_flag": false}, {"option_text": {"zhcn": "运用SageMaker Data Wrangler的多重共线性检测功能，结合主成分分析（PCA）算法，可构建包含全部预测变量的特征空间。", "enus": "Use the SageMaker Data Wrangler multicollinearity measurement features with the principal component analysis (PCA) algorithm to provide a feature space that includes all of the predictor variables."}, "option_flag": true}, {"option_text": {"zhcn": "使用SageMaker Data Wrangler的数据质量与洞察报告功能，可依据特征变量的预测能力对其进行评估分析。", "enus": "Use the SageMaker Data Wrangler Data Quality and Insights Report feature to review features by their predictive power."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Use the SageMaker Data Wrangler multicollinearity measurement features with the principal component analysis (PCA) algorithm to provide a feature space that includes all of the predictor variables.” The key requirement is to understand the variance in the data along various directions in the feature space while dealing with correlated predictor variables. PCA is designed to transform the original variables into a new set of uncorrelated variables called principal components, which helps in understanding the variance structure of the data.\n\nThe option of using the multicollinearity measurement features with a variance inflation factor (VIF) score only measures how closely the variables are related to each other. It does not directly help in understanding the variance in the data along different directions in the feature space.\n\nThe SageMaker Data Wrangler Data Quality and Insights Report quick model visualization is focused on estimating the expected quality of a trained model, not on understanding the variance in the feature space.\n\nThe option of using the Data Quality and Insights Report feature to review features by their predictive power is about evaluating the predictive ability of features, not about analyzing the variance in the data along different directions in the feature space.\n\nCommon misconceptions might lead one to choose the fake options. For example, thinking that just measuring variable relationships (VIF) is the same as understanding data variance, or confusing model quality estimation and feature predictive power review with the task of understanding data variance in the feature space.", "zhcn": "对于该问题的正确答案是：\"利用SageMaker Data Wrangler的多重共线性测量功能，结合主成分分析（PCA）算法，构建包含所有预测变量的特征空间。\"其核心要求在于理解特征空间中数据沿不同方向的方差变化，同时处理存在相关性的预测变量。PCA算法的本质是将原始变量转化为一组互不相关的新变量（即主成分），这有助于揭示数据的内在方差结构。\n\n若仅采用方差膨胀系数（VIF）进行多重共线性测量，只能评估变量间的关联紧密程度，无法直接展现特征空间中数据沿不同方向的方差分布特性。\n\nSageMaker Data Wrangler的数据质量与洞察报告中的快速模型可视化功能，主要用于评估训练模型的预期质量，而非分析特征空间的方差结构。\n\n而通过数据质量与洞察报告的功能按预测能力筛选特征，侧重的是评估特征的预测效能，与分析特征空间中的数据方差任务有本质区别。\n\n常见的误解可能导致选择错误选项。例如误认为仅测量变量关系（VIF）等同于理解数据方差，或是将模型质量评估、特征预测能力审查与理解特征空间数据方差的任务相混淆。"}, "answer": "C"}, {"id": "364", "question": {"enus": "A business to business (B2B) ecommerce company wants to develop a fair and equitable risk mitigation strategy to reject potentially fraudulent transactions. The company wants to reject fraudulent transactions despite the possibility of losing some profitable transactions or customers. Which solution will meet these requirements with the LEAST operational effort? ", "zhcn": "一家企业间电子商务公司希望制定一套公平合理的风险管控策略，用以拦截潜在欺诈交易。即便可能损失部分盈利性交易或客户，该公司仍坚持拒绝欺诈交易。在满足上述要求的前提下，何种方案能以最小的运营成本实现这一目标？"}, "option": [{"option_text": {"zhcn": "利用Amazon SageMaker，仅对公司过往销售过的产品交易予以批准。", "enus": "Use Amazon SageMaker to approve transactions only for products the company has sold in the past."}, "option_flag": false}, {"option_text": {"zhcn": "利用Amazon SageMaker，基于客户数据训练定制化的欺诈检测模型。", "enus": "Use Amazon SageMaker to train a custom fraud detection model based on customer data."}, "option_flag": false}, {"option_text": {"zhcn": "利用亚马逊欺诈检测预测接口，可对系统识别出的可疑活动进行自动化审核，及时拦截欺诈行为。", "enus": "Use the Amazon Fraud Detector prediction API to approve or deny any activities that Fraud Detector identifies as fraudulent."}, "option_flag": true}, {"option_text": {"zhcn": "利用Amazon Fraud Detector预测API识别潜在欺诈行为，以便企业能够及时核查并拦截欺诈交易。", "enus": "Use the Amazon Fraud Detector prediction API to identify potentially fraudulent activities so the company can review the activities and reject fraudulent transactions."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Use the Amazon Fraud Detector prediction API to approve or deny any activities that Fraud Detector identifies as fraudulent.” This option requires the least operational effort as it automates the process of approving or denying transactions based on Fraud Detector's identification of fraud, eliminating the need for further manual review.\n\nThe option “Use Amazon SageMaker to approve transactions only for products the company has sold in the past” is too restrictive. It limits transactions to only previously - sold products, which may lead to rejecting legitimate new business opportunities and requires significant effort to manage the product - based approval list.\n\n“Using Amazon SageMaker to train a custom fraud detection model based on customer data” demands high operational effort. It involves data collection, pre - processing, model training, and continuous model tuning, which is time - consuming and resource - intensive.\n\n“Using the Amazon Fraud Detector prediction API to identify potentially fraudulent activities so the company can review the activities and reject fraudulent transactions” still requires manual review. This means additional operational effort in terms of human resources and time to go through each potentially fraudulent activity. \n\nThe key factor that makes the real answer the best choice is its automation and minimal need for manual intervention, thus meeting the requirement of least operational effort.", "zhcn": "对于该问题，正确答案是\"利用亚马逊欺诈检测器的预测接口，自动审批或拒绝被该系统判定为欺诈的行为\"。此方案通过自动化处理欺诈检测器识别的可疑交易，省去了人工复核环节，实现了运营投入的最小化。  \n\n若选择\"仅通过亚马逊SageMaker审批公司曾售商品相关的交易\"，则限制过于严苛。该方案将交易范围局限在历史商品名录中，既可能错失合规的新商业机会，又需投入大量精力维护商品许可清单。  \n\n而\"采用亚马逊SageMaker基于客户数据训练定制化反欺诈模型\"的方案运营成本最高。该方案涉及数据采集、预处理、模型训练及持续优化等复杂环节，需要耗费大量时间与资源。  \n\n至于\"运用亚马逊欺诈检测器接口识别潜在欺诈行为供人工审核\"的方案，仍依赖人工复核机制。这意味着需要组建专业团队逐条审查可疑活动，在人力资源和时间层面产生额外运营成本。  \n\n真正使标准答案脱颖而出的关键，在于其实现了全流程自动化处理，最大程度减少人工干预，完美契合\"最低运营投入\"的核心要求。"}, "answer": "C"}, {"id": "365", "question": {"enus": "A data scientist needs to develop a model to detect fraud. The data scientist has less data for fraudulent transactions than for legitimate transactions. The data scientist needs to check for bias in the model before finalizing the model. The data scientist needs to develop the model quickly. Which solution will meet these requirements with the LEAST operational overhead? ", "zhcn": "数据科学家需要开发一个欺诈检测模型。目前掌握的欺诈交易数据量远少于正常交易数据。在模型定型前，数据科学家必须进行偏差检验，同时还需快速完成模型开发。哪种解决方案能以最小的运维成本满足这些要求？"}, "option": [{"option_text": {"zhcn": "在亚马逊EMR中运用合成少数类过采样技术（SMOTE）处理并消减数据偏差，通过亚马逊SageMaker Studio Classic进行模型开发，并借助亚马逊增强型人工智能服务（Amazon A2I）在模型定稿前完成偏差检测。", "enus": "Process and reduce bias by using the synthetic minority oversampling technique (SMOTE) in Amazon EMR. Use Amazon SageMaker Studio Classic to develop the model. Use Amazon Augmented Al (Amazon A2I) to check the model for bias before finalizing the model."}, "option_flag": false}, {"option_text": {"zhcn": "在Amazon EMR中运用合成少数类过采样技术（SMOTE）处理并消减数据偏差，继而通过Amazon SageMaker Clarify进行模型开发。在模型定稿前，可借助Amazon Augmented AI（Amazon A2I）对模型进行偏差校验，以确保其公正性。", "enus": "Process and reduce bias by using the synthetic minority oversampling technique (SMOTE) in Amazon EMR. Use Amazon SageMaker Clarify to develop the model. Use Amazon Augmented AI (Amazon A2I) to check the model for bias before finalizing the model."}, "option_flag": false}, {"option_text": {"zhcn": "在Amazon SageMaker Studio中运用合成少数类过采样技术（SMOTE）处理并消减数据偏差。通过Amazon SageMaker JumpStart构建模型框架，并借助Amazon SageMaker Clarify在模型定型前进行偏差检测，确保模型输出的公正性。", "enus": "Process and reduce bias by using the synthetic minority oversampling technique (SMOTE) in Amazon SageMaker Studio. Use Amazon SageMaker JumpStart to develop the model. Use Amazon SageMaker Clarify to check the model for bias before finalizing the model."}, "option_flag": true}, {"option_text": {"zhcn": "通过亚马逊SageMaker Studio笔记本实现偏见处理与消减，借助亚马逊SageMaker JumpStart进行模型开发，并在模型定稿前使用亚马逊SageMaker Model Monitor进行偏见检测。", "enus": "Process and reduce bias by using an Amazon SageMaker Studio notebook. Use Amazon SageMaker JumpStart to develop the model. Use Amazon SageMaker Model Monitor to check the model for bias before finalizing the model."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to “Process and reduce bias by using the synthetic minority oversampling technique (SMOTE) in Amazon SageMaker Studio. Use Amazon SageMaker JumpStart to develop the model. Use Amazon SageMaker Clarify to check the model for bias before finalizing the model.” \n\nThe key advantage is that it offers an integrated solution within Amazon SageMaker, minimizing operational overhead. Amazon SageMaker Studio provides a unified environment for data processing, model development, and bias checking. SageMaker JumpStart allows for quick model development with pre - built models and algorithms, which is essential as the data scientist needs to develop the model quickly. SageMaker Clarify is specifically designed to check for bias in models, providing a straightforward way to meet the bias - checking requirement.\n\nThe first and second fake options use Amazon EMR for SMOTE. Amazon EMR is more suitable for big - data processing on a cluster, adding unnecessary complexity and overhead compared to using SageMaker Studio. Also, using Amazon Augmented AI (Amazon A2I) for bias checking is not its primary function; A2I is more about human - in - the - loop workflows for tasks like content moderation and prediction verification, not specifically for bias detection.\n\nThe third fake option suggests using Amazon SageMaker Model Monitor. Model Monitor is mainly used for monitoring the performance and data quality of deployed models over time, not for checking bias during the model development phase. This misapplication of the tool would not effectively meet the requirements. These factors distinguish the real answer from the fake options and make it the most suitable choice.", "zhcn": "针对该问题的正确答案是：\"在Amazon SageMaker Studio中运用合成少数类过采样技术（SMOTE）处理并降低数据偏差，通过Amazon SageMaker JumpStart快速构建模型，并最终利用Amazon SageMaker Clarify完成模型偏差检测后定型。\"该方案的核心优势在于完全集成于Amazon SageMaker生态，能显著降低运维复杂度。Amazon SageMaker Studio为数据处理、模型开发和偏差检测提供统一工作环境；SageMaker JumpStart凭借预置模型与算法库加速模型构建，完美契合数据科学家快速开发的需求；而专精于偏差检测的SageMaker Clarify则能直接满足模型公平性验证要求。\n\n首两个干扰项主张采用Amazon EMR执行SMOTE处理。但EMR更适用于集群环境的大规模数据处理，相较SageMaker Studio的集成方案会引入不必要的复杂性。此外，选用Amazon Augmented AI（A2I）进行偏差检测实属功能误用——A2I的核心在于人机交互工作流（如内容审核与预测验证），并非专用于偏差探测。\n\n第三个干扰项推荐使用Amazon SageMaker Model Monitor。该工具主要针对已部署模型的性能与数据质量进行长期监控，而非在开发阶段进行偏差校验。错误使用该工具将无法有效达成检测要求。正是这些关键差异使得原答案从干扰项中脱颖而出，成为最契合需求的选择。"}, "answer": "C"}, {"id": "366", "question": {"enus": "A company has 2,000 retail stores. The company needs to develop a new model to predict demand based on holidays and weather conditions. The model must predict demand in each geographic area where the retail stores are located. Before deploying the newly developed model, the company wants to test the model for 2 to 3 days. The model needs to be robust enough to adapt to supply chain and retail store requirements. Which combination of steps should the company take to meet these requirements with the LEAST operational overhead? (Choose two.) ", "zhcn": "一家企业拥有2000家零售门店，现需开发新型预测模型，将节假日与天气状况纳入需求预测考量。该模型须针对每家门店所在区域进行精准需求预测。在正式部署前，企业计划对模型进行2至3天的测试，且模型需具备足够灵活性以适应供应链与门店运营需求。请问以下哪两项措施组合能以最低运营成本满足上述需求？（请选择两项）"}, "option": [{"option_text": {"zhcn": "采用亚马逊 Forecast Prophet 模型进行建模。", "enus": "Develop the model by using the Amazon Forecast Prophet model."}, "option_flag": false}, {"option_text": {"zhcn": "运用亚马逊预测的节假日特征化处理与天气指数来构建该模型。", "enus": "Develop the model by using the Amazon Forecast holidays featurization and weather index."}, "option_flag": true}, {"option_text": {"zhcn": "采用金丝雀部署策略，通过亚马逊SageMaker与AWS Step Functions服务实现模型部署。", "enus": "Deploy the model by using a canary strategy that uses Amazon SageMaker and AWS Step Functions."}, "option_flag": true}, {"option_text": {"zhcn": "采用亚马逊SageMaker流水线进行A/B测试，实现模型部署。", "enus": "Deploy the model by using an A/B testing strategy that uses Amazon SageMaker Pipelines."}, "option_flag": false}, {"option_text": {"zhcn": "采用亚马逊SageMaker与AWS Step Functions服务，通过A/B测试策略部署模型。流量分配比例为BC版本67%，BE版本33%。", "enus": "Deploy the model by using an A/B testing strategy that uses Amazon SageMaker and AWS Step Functions.  BC (67%)  BE (33%)"}, "option_flag": false}], "analysis": {"enus": "The correct answers to the question are “Develop the model by using the Amazon Forecast holidays featurization and weather index.” and “Deploy the model by using a canary strategy that uses Amazon SageMaker and AWS Step Functions.” \n\nFor model development, using Amazon Forecast's holidays featurization and weather index directly addresses the requirement of predicting demand based on holidays and weather conditions, allowing for a more tailored and accurate model. In contrast, just using the Amazon Forecast Prophet model does not specifically incorporate these crucial factors as effectively.\n\nWhen it comes to deployment, a canary strategy with Amazon SageMaker and AWS Step Functions has less operational overhead compared to A/B testing strategies. Canary deployment gradually rolls out the new model to a small subset of users or stores, minimizing potential disruptions and allowing quick detection of issues. A/B testing strategies, on the other hand, require more complex setup and monitoring as they compare two different versions of the model on a larger scale. This is why these two real answer options are chosen over the fake ones which either lack key features for model development or have higher operational overhead for deployment.", "zhcn": "该问题的正确答案是\"使用Amazon Forecast节假日特征工程与天气指数功能进行模型开发\"和\"采用基于Amazon SageMaker与AWS Step Functions的灰度发布策略进行模型部署\"。在模型开发层面，直接运用Amazon Forecast的节假日特征化和天气指数功能，能够精准契合基于节假日与天气条件预测需求的业务要求，从而构建出更贴合实际、预测精度更高的模型。相比之下，仅采用Amazon Forecast Prophet模型则无法同等程度地有效整合这些关键影响因素。\n\n就部署策略而言，基于Amazon SageMaker和AWS Step Functions的灰度发布方案比A/B测试策略具有更低的管理复杂度。灰度发布通过将新模型逐步推广至小范围用户或门店群体，既能有效控制潜在风险，又可实现问题快速发现。而A/B测试策略由于需要在大规模范围内并行运行两个不同版本的模型，其配置和监控体系都更为复杂。正因如此，我们选择这两个真实答案而非备选方案——后者要么在模型开发环节缺失核心功能，要么在部署阶段带来更高的运维负担。"}, "answer": "BC"}, {"id": "367", "question": {"enus": "A finance company has collected stock return data for 5,000 publicly traded companies. A financial analyst has a dataset that contains 2,000 attributes for each company. The financial analyst wants to use Amazon SageMaker to identify the top 15 attributes that are most valuable to predict future stock returns. Which solution will meet these requirements with the LEAST operational overhead? ", "zhcn": "一家金融公司已收集了5000家上市企业的股票回报数据。某金融分析师掌握的数据集包含每家企业的2000项特征属性。该分析师希望借助Amazon SageMaker甄选出对未来股票回报预测最具价值的15项核心属性。在满足需求的前提下，哪种解决方案能最大限度降低运维复杂度？"}, "option": [{"option_text": {"zhcn": "在 SageMaker 中运用线性学习器算法训练线性回归模型，以预测股票收益率。通过按系数绝对值大小进行排序，识别出最具预测力的特征。", "enus": "Use the linear leaner algorithm in SageMaker to train a linear regression model to predict the stock returns. Identify the most predictive features by ranking absolute coefficient values."}, "option_flag": false}, {"option_text": {"zhcn": "在SageMaker中运用随机森林回归算法训练模型，用以预测股票收益率。根据基尼重要性评分，筛选出最具预测力的特征变量。", "enus": "Use random forest regression in SageMaker to train a model to predict the stock returns. Identify the most predictive features based on Gini importance scores."}, "option_flag": false}, {"option_text": {"zhcn": "利用亚马逊SageMaker数据整理器的快速模型可视化功能预测股票收益，并根据快速模式的特征重要性评分识别最具预测力的特征。", "enus": "Use an Amazon SageMaker Data Wrangler quick model visualization to predict the stock returns. Identify the most predictive features based on the quick mode's feature importance scores."}, "option_flag": false}, {"option_text": {"zhcn": "利用Amazon SageMaker Autopilot构建回归模型以预测股票收益，并通过Amazon SageMaker Clarify报告识别最具预测性的特征。", "enus": "Use Amazon SageMaker Autopilot to build a regression model to predict the stock returns. Identify the most predictive features based on an Amazon SageMaker Clarify report."}, "option_flag": true}], "analysis": {"enus": "The correct answer to the question is to “Use Amazon SageMaker Autopilot to build a regression model to predict the stock returns. Identify the most predictive features based on an Amazon SageMaker Clarify report.” This solution offers the least operational overhead as SageMaker Autopilot automates the end - to - end process of building machine learning models, including data preprocessing, algorithm selection, and hyperparameter tuning. SageMaker Clarify then simplifies the identification of the most predictive features.\n\nThe option of using the linear leaner algorithm in SageMaker requires manual training of a linear regression model and ranking of absolute coefficient values. This involves more hands - on work such as choosing appropriate hyperparameters for the linear regression model.\n\nUsing random forest regression in SageMaker also demands manual model training and relying on Gini importance scores. Tuning the random forest model can be complex and time - consuming, increasing the operational overhead.\n\nAn Amazon SageMaker Data Wrangler quick model visualization is mainly for quick exploratory analysis and may not offer the same level of sophistication and accuracy as SageMaker Autopilot for comprehensive predictive modeling and feature selection. \n\nThe key advantage of the real answer is the high level of automation provided by SageMaker Autopilot, which reduces the need for extensive manual intervention and thus minimizes operational overhead.", "zhcn": "针对该问题的正确答案是：\"使用Amazon SageMaker Autopilot构建回归模型预测股票收益，并基于Amazon SageMaker Clarify报告确定最具预测性的特征\"。该方案能最大程度降低运维复杂度——SageMaker Autopilot可自动化完成机器学习模型构建的端到端流程，包括数据预处理、算法选择和超参数调优；而SageMaker Clarify则能轻松识别关键预测特征。\n\n若采用SageMaker线性学习器算法，则需要手动训练线性回归模型并对绝对值系数排序，这涉及更多人工操作（例如为线性回归模型选择合适的超参数）。使用SageMaker随机森林回归同样需手动训练模型并依赖基尼重要性评分，且调优过程复杂耗时，将显著增加运维负担。\n\n至于Amazon SageMaker Data Wrangler的快速模型可视化功能，主要适用于快速探索性分析，在综合预测建模与特征选择的精确度方面无法与SageMaker Autopilot相媲美。本题答案的核心优势在于SageMaker Autopilot提供的高度自动化能力，大幅减少了人工干预需求，从而有效控制运维成本。"}, "answer": "D"}, {"id": "368", "question": {"enus": "An ecommerce company is hosting a web application on Amazon EC2 instances to handle continuously changing customer demand. The EC2 instances are part of an Auto Scaling group. The company wants to implement a solution to distribute traffic from customers to the EC2 instances. The company must encrypt all traffic at all stages between the customers and the application servers. No decryption at intermediate points is allowed. Which solution will meet these requirements? ", "zhcn": "一家电商公司将其网络应用程序部署于亚马逊EC2实例之上，以应对持续波动的客户需求。这些EC2实例隶属于自动扩展组。该公司需要实施一套解决方案，将客户流量分发至各个EC2实例，且必须确保客户与应用服务器之间所有传输阶段的数据全程加密，禁止在中间节点进行解密。下列哪种方案符合这些要求？"}, "option": [{"option_text": {"zhcn": "创建应用型负载均衡器（ALB），并为该负载均衡器配置HTTPS监听器。", "enus": "Create an Application Load Balancer (ALB). Add an HTTPS listener to the AL"}, "option_flag": false}, {"option_text": {"zhcn": "将自动扩缩组配置为向ALB目标组注册实例。  \nB. 创建Amazon CloudFront分发服务。使用自定义SSL/TLS证书配置该分发，并将自动扩缩组设置为分发的源站。", "enus": "Configure the Auto Scaling group to register instances with the ALB's target group.  B. Create an Amazon CloudFront distribution. Configure the distribution with a custom SSL/TLS certificate. Set the Auto Scaling group as the distribution's origin."}, "option_flag": false}, {"option_text": {"zhcn": "创建网络负载均衡器（NLB）。为该负载均衡器添加TCP监听器。将自动扩展组配置为向NLB目标组注册实例。高票采纳方案。", "enus": "Create a Network Load Balancer (NLB). Add a TCP listener to the NLB. Configure the Auto Scaling group to register instances with the NLB's target group. Most Voted"}, "option_flag": true}, {"option_text": {"zhcn": "创建网关负载均衡器（GLB）。配置自动扩展组，将实例注册至GLB的目标组。", "enus": "Create a Gateway Load Balancer (GLB). Configure the Auto Scaling group to register instances with the GLB's target group."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is to create a Network Load Balancer (NLB), add a TCP listener, and configure the Auto Scaling group to register instances with the NLB's target group. This solution meets the requirement of encrypting all traffic between customers and application - servers without decryption at intermediate points because a TCP listener on an NLB operates at the transport layer. It can forward encrypted traffic directly to the EC2 instances without decrypting it in the middle.\n\nThe option of creating an Application Load Balancer (ALB) with an HTTPS listener decrypts the traffic at the ALB, which violates the no - decryption at intermediate points requirement. Amazon CloudFront is a content delivery network. While it can use SSL/TLS, it still involves decryption and re - encryption processes at different points, not meeting the strict no - decryption rule. A Gateway Load Balancer (GLB) is mainly used for deploying, scaling, and managing virtual appliances like firewalls, not for the direct distribution of customer traffic to application servers while maintaining end - to - end encryption. This is why the NLB solution is the real answer, distinguishing it from the fake options.", "zhcn": "针对此问题的正确答案是：创建一个网络负载均衡器（NLB），添加TCP监听器，并配置自动扩展组将实例注册到NLB的目标组。此方案能够满足客户与应用服务器之间全程流量加密、且无需在中间节点解密的要求——因为NLB的TCP监听器工作在传输层，可直接将加密流量转发至EC2实例，无需中途解密。  \n\n若采用创建应用负载均衡器（ALB）并配置HTTPS监听器的方案，流量会在ALB处解密，违背了\"中间节点不解密\"的要求。Amazon CloudFront作为内容分发网络，虽支持SSL/TLS协议，但仍涉及不同节点的解密与再加密过程，不符合严格的全程加密规范。而网关负载均衡器（GLB）主要用于部署、扩展和管理防火墙等虚拟设备，并非为在保持端到端加密的同时将客户流量直接分发至应用服务器而设计。  \n\n因此，NLB方案才是真正符合题意的正解，与其他干扰选项有着本质区别。"}, "answer": "C"}, {"id": "369", "question": {"enus": "A company has two on-premises data center locations. There is a company-managed router at each data center. Each data center has a dedicated AWS Direct Connect connection to a Direct Connect gateway through a private virtual interface. The router for the first location is advertising 110 routes to the Direct Connect gateway by using BGP, and the router for the second location is advertising 60 routes to the Direct Connect gateway by using BGP. The Direct Connect gateway is attached to a company VPC through a virtual private gateway. A network engineer receives reports that resources in the VPC are not reachable from various locations in either data center. The network engineer checks the VPC route table and sees that the routes from the first data center location are not being populated into the route table. The network engineer must resolve this issue in the most operationally efficient manner. What should the network engineer do to meet these requirements? ", "zhcn": "某公司拥有两处本地数据中心站点，每个站点均部署了由公司自主管理的路由器。各数据中心通过专用虚拟接口，经由独立的AWS Direct Connect链路连接至Direct Connect网关。第一处站点的路由器通过BGP协议向Direct Connect网关通告了110条路由，第二处站点则通告了60条路由。该Direct Connect网关通过虚拟私有网关与公司VPC相连。网络工程师接报称，两处数据中心内多个位置均无法访问VPC中的资源。经检查VPC路由表，工程师发现第一处数据中心的路由条目未正常注入路由表。当前需以最高操作效率解决此问题，网络工程师应采取何种措施？"}, "option": [{"option_text": {"zhcn": "移除直连网关，并在每台企业路由器与VPC的虚拟私有网关之间创建新的私有虚拟接口。", "enus": "Remove the Direct Connect gateway, and create a new private virtual interface from each company router to the virtual private gateway of the VPC."}, "option_flag": false}, {"option_text": {"zhcn": "调整路由器配置以汇总通告路由。最高票选方案。", "enus": "Change the router configurations to summarize the advertised routes. Most Voted"}, "option_flag": true}, {"option_text": {"zhcn": "请提交支持工单以提升通告至VPC路由表的路由配额上限。", "enus": "Open a support ticket to increase the quota on advertised routes to the VPC route table."}, "option_flag": false}, {"option_text": {"zhcn": "创建AWS Transit Gateway，将其连接至VPC，并将Direct Connect网关接入该中转网关。", "enus": "Create an AWS Transit Gateway. Attach the transit gateway to the VPC, and connect the Direct Connect gateway to the transit gateway."}, "option_flag": false}], "analysis": {"enus": "The correct answer to the question is ‘Change the router configurations to summarize the advertised routes’. This is because the VPC route table has a limit on the number of routes it can accept. The first data - center is advertising 110 routes, which might exceed the limit, causing the routes not to be populated. Route summarization reduces the number of individual routes being advertised, thus fitting within the VPC route - table limit.\n\nThe option ‘Remove the Direct Connect gateway, and create a new private virtual interface from each company router to the virtual private gateway of the VPC’ is a complex and time - consuming solution that involves re - establishing connections and potentially disrupting existing services. \n\nOpening a support ticket to increase the quota on advertised routes to the VPC route table is not operationally efficient as it depends on AWS support and might take time to process. \n\nCreating an AWS Transit Gateway, attaching it to the VPC, and connecting the Direct Connect gateway to the transit gateway is an over - engineered solution that adds unnecessary complexity and cost. \n\nThe simplicity and efficiency of route summarization make it the best choice to resolve the issue, distinguishing it from the more complex and less efficient fake options.", "zhcn": "针对该问题的正确答案是\"更改路由器配置以实现路由汇总通告\"。原因在于VPC路由表对可容纳的路由条目数量存在限制。首个数据中心当前通告110条路由，可能超出限额导致路由信息无法完整注入。通过路由汇总可减少通告的独立路由数量，从而满足VPC路由表的容量限制。\n\n至于\"移除直连网关，并为每台公司路由器创建通往VPC虚拟私有网关的新私有虚拟接口\"这一方案，不仅需要重建连接导致服务中断风险，实施过程也较为繁琐耗时。而通过提交支持工单申请提高VPC路由表通告配额的做法，由于依赖AWS技术支持且处理周期存在不确定性，并非高效运维之选。若采用创建AWS中转网关并连接直连网关的方案，则会引入不必要的架构复杂性和额外成本。\n\n相较这些复杂低效的替代方案，路由汇总以其简洁高效的特性，成为解决此问题的最佳选择。"}, "answer": "B"}]